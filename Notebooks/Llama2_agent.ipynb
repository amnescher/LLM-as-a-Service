{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/dev/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/ubuntu/dev/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "ERROR: /home/ubuntu/dev/bin/python: undefined symbol: cudaRuntimeGetVersion\n",
      "CUDA SETUP: libcudart.so path is None\n",
      "CUDA SETUP: Is seems that your cuda installation is not in your path. See https://github.com/TimDettmers/bitsandbytes/issues/85 for more information.\n",
      "CUDA SETUP: CUDA version lower than 11 are currently not supported for LLM.int8(). You will be only to use 8-bit optimizers and quantization routines!!\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 00\n",
      "CUDA SETUP: Loading binary /home/ubuntu/dev/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/dev/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('https'), PosixPath('//debuginfod.ubuntu.com ')}\n",
      "  warn(msg)\n",
      "/home/ubuntu/dev/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/home/ubuntu/dev/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/home/ubuntu/dev/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/home/ubuntu/dev/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n",
      "Loading checkpoint shards: 100%|██████████| 15/15 [00:27<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "\n",
    "model_id = 'meta-llama/Llama-2-70b-chat-hf'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "# begin initializing HF items, need auth token for these\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto'\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 29871, 13, 29950, 7889, 29901], [1, 29871, 13, 28956, 13]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_list = ['\\nHuman:', '\\n```\\n']\n",
    "\n",
    "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
    "stop_token_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([    1, 29871,    13, 29950,  7889, 29901], device='cuda:0'),\n",
       " tensor([    1, 29871,    13, 28956,    13], device='cuda:0')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "stop_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# define custom stopping criteria object\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    #stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
    "    temperature=0.01,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  # mex number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain to me the difference between nuclear fission and fusion.\n",
      "Nuclear fission is a process in which an atomic nucleus splits into two or more smaller nuclei, releasing a large amount of energy in the process. This occurs when an atom's nucleus is bombarded with a high-energy particle, such as a neutron. The resulting nuclei are typically smaller and lighter than the original nucleus, and the excess energy is released as radiation. Fission is the process used in nuclear power plants to generate electricity.\n",
      "Nuclear fusion, on the other hand, is the process by which two or more atomic nuclei combine to form a single, heavier nucleus. This process also releases a large amount of energy, but it requires the nuclei to be brought together at extremely high temperatures and pressures, typically found in the core of stars. Fusion is the process that powers the sun and other stars.\n",
      "The key difference between fission and fusion is the direction of the energy release. In fission, the energy is released outward from the nucleus, while in fusion, the energy is released inward, binding the nuclei together. Additionally, fission typically involves the splitting of heavy elements, while fusion involves the combining of light elements.\n"
     ]
    }
   ],
   "source": [
    "res = generate_text(\"Explain to me the difference between nuclear fission and fusion.\")\n",
    "print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nNuclear fission is a process in which an atomic nucleus splits into two or more smaller nuclei, releasing a large amount of energy in the process. This occurs when an atom's nucleus is bombarded with a high-energy particle, such as a neutron. The resulting nuclei are typically smaller and lighter than the original nucleus, and the excess energy is released as radiation. Fission is the process used in nuclear power plants to generate electricity.\\nNuclear fusion, on the other hand, is the process by which two or more atomic nuclei combine to form a single, heavier nucleus. This process also releases a large amount of energy, but it requires the nuclei to be brought together at extremely high temperatures and pressures, typically found in the core of stars. Fusion is the process that powers the sun and other stars.\\nThe key difference between fission and fusion is the direction of the energy release. In fission, the energy is released outward from the nucleus, while in fusion, the energy is released inward, binding the nuclei together. Additionally, fission typically involves the splitting of heavy elements, while fusion involves the combining of light elements.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(prompt=\"Explain to me the difference between nuclear fission and fusion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "from math import pi\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class CircumferenceTool(BaseTool):\n",
    "    name = \"circumference\"\n",
    "    description = \"use this tool when you need to calculate a circumference using the radius of a circle\"\n",
    "\n",
    "    def _run(self, radius: Union[int, float]):\n",
    "        return float(radius)*2.0*pi\n",
    "    \n",
    "    def _arun(self, radius: Union[int, float]):\n",
    "        raise NotImplementedError(\"This tool does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "hf_model = \"Salesforce/blip-image-captioning-large\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(hf_model)\n",
    "model = BlipForConditionalGeneration.from_pretrained(hf_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url = \"https://images.unsplash.com/photo-1664990594745-2a84bd70d8b7?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=2743&q=80\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "import requests\n",
    "from PIL import Image\n",
    "desc = (\n",
    "    \"use this tool when given the URL of an image that you'd like to be \"\n",
    "    \"described. It will return a simple caption describing the image.\"\n",
    ")\n",
    "\n",
    "class ImageCaptionTool(BaseTool):\n",
    "    name = \"Image captioner\"\n",
    "    description = desc\n",
    "\n",
    "    def _run(self, url: str):\n",
    "        # download the image and convert to PIL object\n",
    "        image = Image.open(requests.get(url, stream=True).raw).convert('RGB')\n",
    "        # preprocess the image\n",
    "        inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "        # generate the caption\n",
    "        out = model.generate(**inputs, max_new_tokens=20)\n",
    "        # get the caption\n",
    "        caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "        caption = \"Inside the image \" + caption\n",
    "        return caption\n",
    "    \n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
    "\n",
    "calcu = CircumferenceTool()\n",
    "image_cap = ImageCaptionTool()\n",
    "tools = [\n",
    "    Tool.from_function(\n",
    "        func=calcu.run,\n",
    "        name=\"circumference\",\n",
    "        description=\"use this tool when you need to calculate a circumference using the radius of a circle. it receives a floating point number and returns the ressult in floating point format \"\n",
    "    ),\n",
    "    Tool.from_function(\n",
    "        func=image_cap.run,\n",
    "        name=\"caption\",\n",
    "        description=\"use this tool when given the URL of an image that you'd like to be described.\"\n",
    "        \n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.agents import load_tools\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"chat_history\", k=5, return_messages=True, output_key=\"output\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.agents import AgentOutputParser\n",
    "from langchain.agents.conversational_chat.prompt import FORMAT_INSTRUCTIONS\n",
    "from langchain.output_parsers.json import parse_json_markdown\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "\n",
    "class OutputParser(AgentOutputParser):\n",
    "    def get_format_instructions(self) -> str:\n",
    "        return FORMAT_INSTRUCTIONS\n",
    "\n",
    "    def parse(self, text: str) -> AgentAction | AgentFinish:\n",
    "        try:\n",
    "            # this will work IF the text is a valid JSON with action and action_input\n",
    "            response = parse_json_markdown(text)\n",
    "            action, action_input = response[\"action\"], response[\"action_input\"]\n",
    "            if action == \"Final Answer\":\n",
    "                # this means the agent is finished so we call AgentFinish\n",
    "                return AgentFinish({\"output\": action_input}, text)\n",
    "            else:\n",
    "                # otherwise the agent wants to use an action, so we call AgentAction\n",
    "                return AgentAction(action, action_input, text)\n",
    "        except Exception:\n",
    "            # sometimes the agent will return a string that is not a valid JSON\n",
    "            # often this happens when the agent is finished\n",
    "            # so we just return the text as the output\n",
    "            return AgentFinish({\"output\": text}, text)\n",
    "\n",
    "    @property\n",
    "    def _type(self) -> str:\n",
    "        return \"conversational_chat\"\n",
    "\n",
    "# initialize output parser for agent\n",
    "parser = OutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "\n",
    "# initialize agent\n",
    "agent = initialize_agent(\n",
    "    agent=\"chat-conversational-react-description\",\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    early_stopping_method=\"generate\",\n",
    "    memory=memory,\n",
    "    #agent_kwargs={\"output_parser\": parser}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input', 'chat_history', 'agent_scratchpad'], output_parser=None, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template='Assistant is a large language model trained by OpenAI.\\n\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.', template_format='f-string', validate_template=True), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template='TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> circumference: use this tool when you need to calculate a circumference using the radius of a circle. it receives a floating point number and returns the ressult in floating point format \\n> caption: use this tool when given the URL of an image that you\\'d like to be described.\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": string, \\\\ The action to take. Must be one of circumference, caption\\n    \"action_input\": string \\\\ The input to the action\\n}}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": \"Final Answer\",\\n    \"action_input\": string \\\\ You should put what you want to return to use here\\n}}\\n```\\n\\nUSER\\'S INPUT\\n--------------------\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\n{input}', template_format='f-string', validate_template=True), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.agent.llm_chain.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<>\\n\", \"\\n<>\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_msg = B_SYS + \"\"\"Assistant is a expert JSON builder designed to assist with a wide range of tasks.\n",
    "\n",
    "Assistant is able to respond to the User and use tools using JSON strings that contain \"action\" and \"action_input\" parameters.\n",
    "\n",
    "All of Assistant's communication is performed using this JSON format.\n",
    "\n",
    "Assistant can also use tools by responding to the user with tool use instructions in the same \"action\" and \"action_input\" JSON format. the Only tools available to Assistant are:\n",
    "- \"circumference\": use this tool ONLY when you need to calculate a circumference using the radius of a circle.\n",
    "  - To use the calculator tool, Assistant should write like so:\n",
    "    ```json\n",
    "    {{\"action\": \"circumference\",\n",
    "      \"action_input\": 1.2 }}\n",
    "    ```\n",
    "\n",
    "Here are some previous conversations between the Assistant and User:\n",
    "\n",
    "User: 1.0 how are you?\n",
    "Assistant: ```json\n",
    "{{\"action\": \"Final Answer\",\n",
    " \"action_input\": \"I'm good thanks, how are you?\"}}\n",
    "```\n",
    "\n",
    "User: 3.0  what is the circumference of a circle that has a radius of 7.81?\n",
    "Assistant: ```json\n",
    "{{\"action\": \"circumference\",\n",
    " \"action_input\": \"7.81\" }}\n",
    "```\n",
    "User4: where is the capital of Iran?\n",
    "Assistant: ```json\n",
    "{{\"action\": \"Final Answer\",\n",
    " \"action_input\": \"The capital of Iran is Tehran\"}}\n",
    "```\n",
    "User: 2.0\n",
    "Assistant: ```json\n",
    "{{\"action\": \"Final Answer\",\n",
    " \"action_input\": \"The circumference of a circle with a radius of 7.81 is approximately 49.07.\"}}\n",
    "```\n",
    "User: Thanks could you tell me the circumference of a circle that has a radius of 4 mm?\n",
    "Assistant: ```json\n",
    "{{\"action\": \"circumference\",\n",
    " \"action_input\": \"4\" }}\n",
    "```\n",
    "User: 16.0\n",
    "Assistant: ```json\n",
    "{{\"action\": \"Final Answer\",\n",
    " \"action_input\": \"It looks like the answer is 25.132741228718345!\"}}\n",
    "```\n",
    "User: 16.0\n",
    "Assistant: ```json\n",
    "{{\"action\": \"Final Answer\",\n",
    " \"action_input\": \"It looks like  the circumference is 25.132741228718345!\"}}\n",
    "```\n",
    "\n",
    "Here is the latest conversation between Assistant and User.\"\"\" + E_SYS\n",
    "new_prompt = agent.agent.create_prompt(\n",
    "    system_message=sys_msg,\n",
    "    tools=tools\n",
    ")\n",
    "agent.agent.llm_chain.prompt = new_prompt\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = B_INST + \" Respond to the following in JSON with 'action' and 'action_input' values \" + E_INST\n",
    "human_msg = instruction + \"\\nUser: {input}\"\n",
    "\n",
    "agent.agent.llm_chain.prompt.messages[2].prompt.template = human_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input', 'chat_history', 'agent_scratchpad'], output_parser=None, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template='<>\\nAssistant is a expert JSON builder designed to assist with a wide range of tasks.\\n\\nAssistant is able to respond to the User and use tools using JSON strings that contain \"action\" and \"action_input\" parameters.\\n\\nAll of Assistant\\'s communication is performed using this JSON format.\\n\\nAssistant can also use tools by responding to the user with tool use instructions in the same \"action\" and \"action_input\" JSON format. The only tools available to Assistant are:\\n- \"caption\": use this tool only when given the URL of an image that you need to be described the image. \\n  - To use the caption tool, Assistant should write like so:\\n    ```json\\n        {{\"action\": \"caption\",\\n          \"action_input\": https://images.pexels.com/photos/17644160/pexels-photo-17644160/free-photo-of-landscape.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=2\\n          }}\\n        ```\\n- \"circumference\": use this tool only when you need to calculate a circumference using the radius of a circle. The radius needs to be send to it as floating point.\\n  - To use the calculator tool, Assistant should write like so:\\n    ```json\\n    {{\"action\": \"circumference\",\\n      \"action_input\": 1.01 }}\\n    ```\\n\\nHere are some previous conversations between the Assistant and User:\\n\\nUser: Hey how are you today?\\nAssistant: ```json\\n{{\"action\": \"Final Answer\",\\n \"action_input\": \"I\\'m good thanks, how are you?\"}}\\n```\\nUser: \\nAssistant: ```json\\n{{\"action\": \"Final Answer\",\\n \"action_input\": The circumference of a circle with a radius of 7.81 cm is approximately 49.07}}\\n\\nHere is the latest conversation between Assistant and User.\\n<>\\n\\n', template_format='f-string', validate_template=True), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template=\"[INST] Respond to the following in JSON with 'action' and 'action_input' values [/INST]\\nUser: {input}\", template_format='f-string', validate_template=True), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.agent.llm_chain.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.132741228718345"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import pi\n",
    "(4 * 2) * pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input', 'chat_history', 'agent_scratchpad'], output_parser=None, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template='<>\\nAssistant is a expert JSON builder designed to assist with a wide range of tasks.\\n\\nAssistant is able to respond to the User and use tools using JSON strings that contain \"action\" and \"action_input\" parameters.\\n\\nAll of Assistant\\'s communication is performed using this JSON format.\\n\\nAssistant can also use tools by responding to the user with tool use instructions in the same \"action\" and \"action_input\" JSON format. Tools available to Assistant are:\\n\\n- \"circumference\": use this tool only when you need to calculate a circumference using the radius of a circle.\\n  - To use the calculator tool, Assistant should write like so:\\n    ```json\\n    {{\"action\": \"circumference\",\\n      \"action_input\": 1.2 }}\\n    ```\\n\\nHere are some previous conversations between the Assistant and User:\\n\\nUser1: Hey how are you today?\\nAssistant: ```json\\n{{\"action\": \"Final Answer\",\\n \"action_input\": \"I\\'m good thanks, how are you?\"}}\\n```\\nUser4: where is the capital of Iran?\\nAssistant: ```json\\n{{\"action\": \"Final Answer\",\\n \"action_input\": \"The capital of Iran is Tehran\"}}\\n```\\n\\nUser: 3.0  what is the circumference of a circle that has a radius of 7.81?\\nAssistant: ```json\\n{{\"action\": \"circumference\",\\n \"action_input\": \"7.81\" }}\\n```\\nUser: 2.0\\nAssistant: ```json\\n{{\"action\": \"Final Answer\",\\n \"action_input\": \"The circumference of a circle with a radius of 7.81 is approximately 49.07.\"}}\\n```\\nUser: Thanks could you tell me the circumference of a circle that has a radius of 4 mm?\\nAssistant: ```json\\n{{\"action\": \"circumference\",\\n \"action_input\": \"4\" }}\\n```\\nUser: 16.0\\nAssistant: ```json\\n{{\"action\": \"Final Answer\",\\n \"action_input\": \"It looks like the answer is 25.132741228718345!\"}}\\n```\\nUser: 16.0\\nAssistant: ```json\\n{{\"action\": \"Final Answer\",\\n \"action_input\": \"It looks like  the circumference is 25.132741228718345!\"}}\\n```\\n\\nHere is the latest conversation between Assistant and User.\\n<>\\n\\n', template_format='f-string', validate_template=True), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template=\"[INST] Respond to the following in JSON with 'action' and 'action_input' values [/INST]\\nUser: {input}\", template_format='f-string', validate_template=True), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.agent.llm_chain.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/dev/lib/python3.10/site-packages/transformers/pipelines/base.py:1089: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "Assistant: ```json\n",
      "{\"action\": \"Final Answer\",\n",
      " \"action_input\": \"I'm doing well, thanks for asking! How about you?\"}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'hey how are you today?',\n",
       " 'chat_history': [],\n",
       " 'output': \"I'm doing well, thanks for asking! How about you?\"}"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"hey how are you today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "Assistant: ```json\n",
      "{\"action\": \"Final Answer\",\n",
      " \"action_input\": \"The capital of the UK is London\"}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'where is capital of the UK?',\n",
       " 'chat_history': [HumanMessage(content='hey how are you today?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=\"I'm doing well, thanks for asking! How about you?\", additional_kwargs={}, example=False)],\n",
       " 'output': 'The capital of the UK is London'}"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"where is capital of the UK?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m?\n",
      "Assistant: ```json\n",
      "{\"action\": \"circumference\",\n",
      " \"action_input\": \"5.5\" }\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m34.55751918948772\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "AI:?\n",
      "\n",
      "Assistant: ```json\n",
      "{\"action\": \"Final Answer\",\n",
      " \"action_input\": \"The circumference of a circle with a radius of 5.5 is approximately 34.55751918948772.\"}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'can you calculate the circumference of a circle that has a radius of 5.5',\n",
       " 'chat_history': [HumanMessage(content='hey how are you today?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=\"I'm doing well, thanks for asking! How about you?\", additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='where is capital of the UK?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='The capital of the UK is London', additional_kwargs={}, example=False)],\n",
       " 'output': 'The circumference of a circle with a radius of 5.5 is approximately 34.55751918948772.'}"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"can you calculate the circumference of a circle that has a radius of 5.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the circumference of a circle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
