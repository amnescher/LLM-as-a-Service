{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/dev/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/ubuntu/dev/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "ERROR: /home/ubuntu/dev/bin/python: undefined symbol: cudaRuntimeGetVersion\n",
      "CUDA SETUP: libcudart.so path is None\n",
      "CUDA SETUP: Is seems that your cuda installation is not in your path. See https://github.com/TimDettmers/bitsandbytes/issues/85 for more information.\n",
      "CUDA SETUP: CUDA version lower than 11 are currently not supported for LLM.int8(). You will be only to use 8-bit optimizers and quantization routines!!\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 00\n",
      "CUDA SETUP: Loading binary /home/ubuntu/dev/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/dev/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('https'), PosixPath('//debuginfod.ubuntu.com ')}\n",
      "  warn(msg)\n",
      "/home/ubuntu/dev/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/home/ubuntu/dev/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/home/ubuntu/dev/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/home/ubuntu/dev/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n",
      "Loading checkpoint shards: 100%|██████████| 9/9 [00:39<00:00,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "\n",
    "model_name = '/home/ubuntu/model/falcon-40b-instruct'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto'\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "# we create a list of stopping criteria\n",
    "stop_token_ids = [\n",
    "    tokenizer.convert_tokens_to_ids(x) for x in [\n",
    "        ['Human', ':'], ['AI', ':']\n",
    "    ]\n",
    "]\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# define custom stopping criteria object\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'RWForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
    "    temperature=0.0,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  # mex number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os to set API key\n",
    "import os\n",
    "# Import OpenAI as main LLM service\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "# Bring in streamlit for UI/app interface\n",
    "# Import PDF document loaders...there's other ones as well!\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "# Import chroma as the vector store \n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Import vector store stuff\n",
    "from langchain.agents.agent_toolkits import (\n",
    "    create_vectorstore_agent,\n",
    "    VectorStoreToolkit,\n",
    "    VectorStoreInfo\n",
    ")\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "env_variables = dotenv_values('env.env')\n",
    "# Set APIkey for OpenAI Service\n",
    "# Can sub this out for other LLM providers\n",
    "os.environ['OPENAI_API_KEY'] = env_variables['Open_API']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n"
     ]
    }
   ],
   "source": [
    "text = \"hello word\"\n",
    "embed = embeddings.embed_query(text)\n",
    "print(len(embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and load PDF Loader\n",
    "loader = PyPDFLoader('paper.pdf')\n",
    "# Split pages from pdf \n",
    "pages = loader.load_and_split()[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make a directory called chroma_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Chroma' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Load documents into vector database aka ChromaDB\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m db5 \u001b[39m=\u001b[39m Chroma\u001b[39m.\u001b[39mfrom_documents(pages, embeddings, persist_directory\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./chroma_db\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m db5\u001b[39m.\u001b[39mpersist()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Chroma' is not defined"
     ]
    }
   ],
   "source": [
    "# load documents into vector database aka ChromaDB and store in on the disk\n",
    "db5 = Chroma.from_documents(pages, embeddings, persist_directory=\"./chroma_db\")\n",
    "db5.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Solar Irradiance Anticipative Transformer\\nThomas M. Mercier\\nBournemouth University\\ntmercier2@gmail.comTasmiat Rahman\\nUniversity of Southampton\\nt.rahman@soton.ac.ukAmin Sabet\\nEscherCloud AI\\na.sabet@eschercloud.ai\\nAbstract\\nThis paper proposes an anticipative transformer-based\\nmodel for short-term solar irradiance forecasting. Given a\\nsequence of sky images, our proposed vision transformer\\nencodes features of consecutive images, feeding into a\\ntransformer decoder to predict irradiance values associated\\nwith future unseen sky images. We show that our model ef-\\nfectively learns to attend only to relevant features in im-\\nages in order to forecast irradiance. Moreover, the pro-\\nposed anticipative transformer captures long-range depen-\\ndencies between sky images to achieve a forecasting skill of\\n21.45 % on a 15 minute ahead prediction for a newly intro-\\nduced dataset of all-sky images when compared to a smart\\npersistence model.\\n1. Introduction\\nSolar energy has emerged as one of the most promis-\\ning alternatives to non-renewable energy sources. As the\\nphotovoltaic (PV) industry grows at pace from gigawatt to\\nterawatt scale, the need for more accurate and efﬁcient fore-\\ncasting of PV output becomes ever more critical. Grid scale\\nsolar based power generation poses challenges for grid op-\\nerators due to the intermittent nature of the supply [2, 23].\\nSince solar irradiance is a key predictor of PV output, irra-\\ndiance forecasting on a sub-hour level can greatly support\\nstable and economical power generation. Even forecasting\\n5 minutes into the future is critical in PV systems to bal-\\nance storage and load for intermittency as well as having\\nbeneﬁts in energy minute by minute trading. The level of\\nsolar irradiance seen in a particular location varies based on\\nthe cyclical changes of the season, the sun position through-\\nout the day and the weather conditions. While the ﬁrst two\\nfactors are consistently predictable, weather conditions, es-\\npecially the level of cloud cover make purely time based\\npredictions inaccurate [30].\\nTwo common approaches for short term irradiance fore-\\ncasting are the use of statistical methods derived from past\\nirradiance measurements and image based forecasts using\\neither ground based sky images or satellite imagery [6].\\nFigure 1. High level overview of model operation. The backbone\\nencodes features from each sky image and the head predicts future\\nirradiance.\\nCommon deep learning (DL) based approaches make use\\nof convolutional neural networks (CNNs) to extract features\\nfrom images that can then be used to give an associated irra-\\ndiance value [23]. Ordinarily to predict irradiance, a series\\nof consecutive images are used in either a 3 dimensional\\nCNN or a combination of a CNN and a long short term\\nmemory (LSTM) based architecture [26]. This is ultimately\\nbased on the temporal information contained in the series\\nof images. In contrast to LSTM based models, transformers\\noffer both the ability to process sequences in parallel as well\\nas excellent modeling of long-term dependencies [21]. The\\nrecent application of the self-attention based transformer ar-\\nchitecture to computer vision tasks combined with the high\\nperformance of transformer-based networks for tasks where\\nlong term dependencies are crucial, makes this type of net-\\nwork attractive for solar irradiance forecasting [23, 36]. We\\npropose utilizing a self-attention based backbone network\\nthat creates feature representations for each frame in a se-\\nquence of all-sky images and then using a Generative Pre-\\nThis CVPR workshop paper is the Open Access version, provided by the Computer Vision Foundation.\\nExcept for this watermark, it is identical to the accepted version;\\nthe final published version of the proceedings is available on IEEE Xplore.\\n2064', metadata={'source': 'paper.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# send a query to DB based on the similarity between embedding of query and data in DB\n",
    "query = \"What is the title of paper\"\n",
    "db5.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solar Irradiance Anticipative Transformer\n",
      "Thomas M. Mercier\n",
      "Bournemouth University\n",
      "tmercier2@gmail.comTasmiat Rahman\n",
      "University of Southampton\n",
      "t.rahman@soton.ac.ukAmin Sabet\n",
      "EscherCloud AI\n",
      "a.sabet@eschercloud.ai\n",
      "Abstract\n",
      "This paper proposes an anticipative transformer-based\n",
      "model for short-term solar irradiance forecasting. Given a\n",
      "sequence of sky images, our proposed vision transformer\n",
      "encodes features of consecutive images, feeding into a\n",
      "transformer decoder to predict irradiance values associated\n",
      "with future unseen sky images. We show that our model ef-\n",
      "fectively learns to attend only to relevant features in im-\n",
      "ages in order to forecast irradiance. Moreover, the pro-\n",
      "posed anticipative transformer captures long-range depen-\n",
      "dencies between sky images to achieve a forecasting skill of\n",
      "21.45 % on a 15 minute ahead prediction for a newly intro-\n",
      "duced dataset of all-sky images when compared to a smart\n",
      "persistence model.\n",
      "1. Introduction\n",
      "Solar energy has emerged as one of the most promis-\n",
      "ing alternatives to non-renewable energy sources. As the\n",
      "photovoltaic (PV) industry grows at pace from gigawatt to\n",
      "terawatt scale, the need for more accurate and efﬁcient fore-\n",
      "casting of PV output becomes ever more critical. Grid scale\n",
      "solar based power generation poses challenges for grid op-\n",
      "erators due to the intermittent nature of the supply [2, 23].\n",
      "Since solar irradiance is a key predictor of PV output, irra-\n",
      "diance forecasting on a sub-hour level can greatly support\n",
      "stable and economical power generation. Even forecasting\n",
      "5 minutes into the future is critical in PV systems to bal-\n",
      "ance storage and load for intermittency as well as having\n",
      "beneﬁts in energy minute by minute trading. The level of\n",
      "solar irradiance seen in a particular location varies based on\n",
      "the cyclical changes of the season, the sun position through-\n",
      "out the day and the weather conditions. While the ﬁrst two\n",
      "factors are consistently predictable, weather conditions, es-\n",
      "pecially the level of cloud cover make purely time based\n",
      "predictions inaccurate [30].\n",
      "Two common approaches for short term irradiance fore-\n",
      "casting are the use of statistical methods derived from past\n",
      "irradiance measurements and image based forecasts using\n",
      "either ground based sky images or satellite imagery [6].\n",
      "Figure 1. High level overview of model operation. The backbone\n",
      "encodes features from each sky image and the head predicts future\n",
      "irradiance.\n",
      "Common deep learning (DL) based approaches make use\n",
      "of convolutional neural networks (CNNs) to extract features\n",
      "from images that can then be used to give an associated irra-\n",
      "diance value [23]. Ordinarily to predict irradiance, a series\n",
      "of consecutive images are used in either a 3 dimensional\n",
      "CNN or a combination of a CNN and a long short term\n",
      "memory (LSTM) based architecture [26]. This is ultimately\n",
      "based on the temporal information contained in the series\n",
      "of images. In contrast to LSTM based models, transformers\n",
      "offer both the ability to process sequences in parallel as well\n",
      "as excellent modeling of long-term dependencies [21]. The\n",
      "recent application of the self-attention based transformer ar-\n",
      "chitecture to computer vision tasks combined with the high\n",
      "performance of transformer-based networks for tasks where\n",
      "long term dependencies are crucial, makes this type of net-\n",
      "work attractive for solar irradiance forecasting [23, 36]. We\n",
      "propose utilizing a self-attention based backbone network\n",
      "that creates feature representations for each frame in a se-\n",
      "quence of all-sky images and then using a Generative Pre-\n",
      "This CVPR workshop paper is the Open Access version, provided by the Computer Vision Foundation.\n",
      "Except for this watermark, it is identical to the accepted version;\n",
      "the final published version of the proceedings is available on IEEE Xplore.\n",
      "2064\n"
     ]
    }
   ],
   "source": [
    "# Load DB stored on the disk and send a query to it\n",
    "db3 = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)\n",
    "docs = db3.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_info = VectorStoreInfo(\n",
    "    name=\"paper\",\n",
    "    description=\"a scietfic paper as a pdf\",\n",
    "    vectorstore=db3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the document store into a langchain toolkit\n",
    "toolkit = VectorStoreToolkit(vectorstore_info=vectorstore_info,llm = llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = create_vectorstore_agent(\n",
    "    llm=llm,\n",
    "    toolkit=toolkit,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find the title of the paper\n",
      "Action: paper\n",
      "Action Input: the name of the paper\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3m Solar Irradiance Anticipative Transformer\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the title of the paper\n",
      "Final Answer: The title of the paper is \"Solar Irradiance Anticipative Transformer\".\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The title of the paper is \"Solar Irradiance Anticipative Transformer\".'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# send a query to llm to find the answer using info in DB\n",
    "agent_executor.run(\"what is the title of this paper?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
