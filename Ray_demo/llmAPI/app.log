2023-11-16 10:28:43,078 - INFO - Created a temporary directory at /tmp/tmp40rexoeq
2023-11-16 10:28:43,079 - INFO - Writing /tmp/tmp40rexoeq/_remote_module_non_scriptable.py
2023-11-16 10:28:47,464 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-16 10:29:05,462 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-16 10:29:14,176 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-16 10:43:39,420 - INFO - Created a temporary directory at /tmp/tmpgf3b0dpl
2023-11-16 10:43:39,420 - INFO - Writing /tmp/tmpgf3b0dpl/_remote_module_non_scriptable.py
2023-11-16 10:43:41,697 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-16 10:44:18,671 - INFO - Created a temporary directory at /tmp/tmp_pph2hxc
2023-11-16 10:44:18,672 - INFO - Writing /tmp/tmp_pph2hxc/_remote_module_non_scriptable.py
2023-11-16 10:44:20,979 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-16 10:44:38,533 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-16 10:44:47,288 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-16 11:08:25,502 - INFO - Created a temporary directory at /tmp/tmpewtfe5jt
2023-11-16 11:08:25,502 - INFO - Writing /tmp/tmpewtfe5jt/_remote_module_non_scriptable.py
2023-11-16 11:08:27,842 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-16 11:08:45,263 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-16 11:08:54,039 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-16 11:12:37,695 - INFO - Created a temporary directory at /tmp/tmpadnpm7or
2023-11-16 11:12:37,695 - INFO - Writing /tmp/tmpadnpm7or/_remote_module_non_scriptable.py
2023-11-16 11:12:40,036 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-16 11:17:30,520 - INFO - Created a temporary directory at /tmp/tmpozgmf3cd
2023-11-16 11:17:30,520 - INFO - Writing /tmp/tmpozgmf3cd/_remote_module_non_scriptable.py
2023-11-16 11:17:32,842 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-16 11:17:50,594 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-16 11:17:59,362 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-16 13:56:50,799 - INFO - Created a temporary directory at /tmp/tmp6xltg6vk
2023-11-16 13:56:50,799 - INFO - Writing /tmp/tmp6xltg6vk/_remote_module_non_scriptable.py
2023-11-16 13:56:53,130 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-16 13:57:10,786 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-16 13:57:19,527 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-16 16:38:08,187 - INFO - Created a temporary directory at /tmp/tmp2x1h8gqn
2023-11-16 16:38:08,187 - INFO - Writing /tmp/tmp2x1h8gqn/_remote_module_non_scriptable.py
2023-11-16 16:38:10,460 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-16 16:38:28,054 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-16 16:38:36,787 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 13:17:26,890 - INFO - Created a temporary directory at /tmp/tmpic_bcnim
2023-11-17 13:17:26,890 - INFO - Writing /tmp/tmpic_bcnim/_remote_module_non_scriptable.py
2023-11-17 13:17:29,177 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-17 13:17:46,706 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-17 13:17:55,422 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 13:20:00,432 - INFO - Created a temporary directory at /tmp/tmpjh79yy92
2023-11-17 13:20:00,432 - INFO - Writing /tmp/tmpjh79yy92/_remote_module_non_scriptable.py
2023-11-17 13:20:02,751 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-17 13:20:20,300 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-17 13:20:29,076 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 13:32:14,753 - INFO - Created a temporary directory at /tmp/tmpwzm8z2rt
2023-11-17 13:32:14,754 - INFO - Writing /tmp/tmpwzm8z2rt/_remote_module_non_scriptable.py
2023-11-17 13:32:17,060 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-17 13:32:34,612 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-17 13:32:43,423 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 13:43:52,096 - INFO - Created a temporary directory at /tmp/tmpnpblkxu1
2023-11-17 13:43:52,096 - INFO - Writing /tmp/tmpnpblkxu1/_remote_module_non_scriptable.py
2023-11-17 13:43:54,422 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-17 13:44:11,946 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-17 13:44:20,682 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 13:48:19,994 - INFO - Created a temporary directory at /tmp/tmpfbq3u_0v
2023-11-17 13:48:19,995 - INFO - Writing /tmp/tmpfbq3u_0v/_remote_module_non_scriptable.py
2023-11-17 13:48:22,333 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-17 13:48:39,786 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-17 13:48:48,510 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 13:52:55,427 - INFO - Created a temporary directory at /tmp/tmpex8gn7x6
2023-11-17 13:52:55,427 - INFO - Writing /tmp/tmpex8gn7x6/_remote_module_non_scriptable.py
2023-11-17 13:52:57,737 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-17 13:53:15,238 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-17 13:53:23,986 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 14:53:17,942 - INFO - Created a temporary directory at /tmp/tmpswd7owy1
2023-11-17 14:53:17,942 - INFO - Writing /tmp/tmpswd7owy1/_remote_module_non_scriptable.py
2023-11-17 14:53:20,222 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-17 14:53:37,626 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-17 14:53:46,323 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 15:14:50,176 - INFO - Created a temporary directory at /tmp/tmpivtdxefc
2023-11-17 15:14:50,176 - INFO - Writing /tmp/tmpivtdxefc/_remote_module_non_scriptable.py
2023-11-17 15:14:52,494 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-17 15:15:10,137 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-17 15:15:18,880 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 15:17:43,855 - INFO - Created a temporary directory at /tmp/tmp5xd6gl_f
2023-11-17 15:17:43,855 - INFO - Writing /tmp/tmp5xd6gl_f/_remote_module_non_scriptable.py
2023-11-17 15:17:46,210 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-17 15:18:03,816 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-17 15:18:12,547 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 15:20:13,075 - INFO - Created a temporary directory at /tmp/tmpsckwbjlh
2023-11-17 15:20:13,075 - INFO - Writing /tmp/tmpsckwbjlh/_remote_module_non_scriptable.py
2023-11-17 15:20:15,416 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-17 15:20:32,864 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-17 15:20:41,613 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 15:29:03,756 - INFO - Received requests to /inference endpoint
2023-11-17 15:29:03,857 - INFO - Received a batch of request with batch size of: 1 
2023-11-17 15:29:03,858 - INFO - Received request: {'username': 'amin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-17 15:29:03,860 - ERROR - Error processing the request: name 'time' is not defined
2023-11-17 15:29:15,146 - INFO - Received requests to /inference endpoint
2023-11-17 15:29:15,247 - INFO - Received a batch of request with batch size of: 1 
2023-11-17 15:29:15,247 - INFO - Received request: {'username': 'amin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-17 15:29:15,248 - ERROR - Error processing the request: name 'time' is not defined
2023-11-20 11:14:02,694 - INFO - Created a temporary directory at /tmp/tmpn0s4u1ju
2023-11-20 11:14:02,695 - INFO - Writing /tmp/tmpn0s4u1ju/_remote_module_non_scriptable.py
2023-11-20 11:14:09,191 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-20 11:24:47,681 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-20 11:25:10,405 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-20 11:28:22,467 - INFO - Created a temporary directory at /tmp/tmp47as7c_x
2023-11-20 11:28:22,467 - INFO - Writing /tmp/tmp47as7c_x/_remote_module_non_scriptable.py
2023-11-20 11:28:24,761 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-20 11:28:45,695 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-20 11:28:54,618 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-20 12:04:30,088 - INFO - Received requests to /inference endpoint
2023-11-20 12:04:30,189 - INFO - Received a batch of request with batch size of: 1 
2023-11-20 12:04:30,190 - INFO - Received request: {'username': 'amin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-20 12:04:30,192 - ERROR - Error processing the request: name 'time' is not defined
2023-11-20 12:11:51,021 - INFO - Created a temporary directory at /tmp/tmppj91djaj
2023-11-20 12:11:51,021 - INFO - Writing /tmp/tmppj91djaj/_remote_module_non_scriptable.py
2023-11-20 12:11:53,325 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-20 12:12:10,995 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-20 12:12:19,779 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-20 12:14:28,694 - INFO - Received requests to /inference endpoint
2023-11-20 12:14:28,795 - INFO - Received a batch of request with batch size of: 1 
2023-11-20 12:14:28,795 - INFO - Received request: {'username': 'amin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-20 12:14:28,798 - ERROR - Error processing the request: name 'time' is not defined
2023-11-20 12:15:30,904 - INFO - Received requests to /inference endpoint
2023-11-20 12:15:31,005 - INFO - Received a batch of request with batch size of: 1 
2023-11-20 12:15:31,005 - INFO - Received request: {'username': 'amin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-20 12:15:31,005 - ERROR - Error processing the request: name 'time' is not defined
2023-11-20 12:17:30,660 - INFO - Created a temporary directory at /tmp/tmpjb1zh064
2023-11-20 12:17:30,660 - INFO - Writing /tmp/tmpjb1zh064/_remote_module_non_scriptable.py
2023-11-20 12:17:32,965 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-20 12:17:50,498 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-20 12:17:59,256 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-20 12:20:08,787 - INFO - Received requests to /inference endpoint
2023-11-20 12:20:08,888 - INFO - Received a batch of request with batch size of: 1 
2023-11-20 12:20:08,888 - INFO - Received request: {'username': 'amin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-20 12:20:17,930 - INFO - Processed the request successfully
2023-11-20 12:25:38,633 - INFO - Created a temporary directory at /tmp/tmp6j752gcz
2023-11-20 12:25:38,633 - INFO - Writing /tmp/tmp6j752gcz/_remote_module_non_scriptable.py
2023-11-20 12:25:40,899 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-20 12:25:58,258 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-20 12:26:06,966 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-20 12:26:36,396 - INFO - Received requests to /inference endpoint
2023-11-20 12:26:36,497 - INFO - Received a batch of request with batch size of: 1 
2023-11-20 12:26:36,497 - INFO - Received request: {'username': 'amin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-20 12:26:43,258 - INFO - Processed the request successfully
2023-11-20 12:30:25,377 - INFO - Received requests to /inference endpoint
2023-11-20 12:30:25,478 - INFO - Received a batch of request with batch size of: 1 
2023-11-20 12:30:25,478 - INFO - Received request: {'username': 'amin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-20 12:30:31,658 - INFO - Processed the request successfully
2023-11-20 12:32:34,636 - INFO - Received requests to /inference endpoint
2023-11-20 12:32:34,738 - INFO - Received a batch of request with batch size of: 1 
2023-11-20 12:32:34,738 - INFO - Received request: {'username': 'admin', 'prompt': 'HI', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-20 12:32:40,799 - INFO - Processed the request successfully
2023-11-20 12:53:57,962 - INFO - Created a temporary directory at /tmp/tmpmccgt757
2023-11-20 12:53:57,962 - INFO - Writing /tmp/tmpmccgt757/_remote_module_non_scriptable.py
2023-11-20 12:54:00,281 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-20 12:54:17,694 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-20 12:54:26,424 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-20 12:54:36,397 - ERROR - An error occurred: 'VDBaseInput' object has no attribute 'mode'
2023-11-20 12:56:12,810 - INFO - Created a temporary directory at /tmp/tmp_3e5c5ut
2023-11-20 12:56:12,810 - INFO - Writing /tmp/tmp_3e5c5ut/_remote_module_non_scriptable.py
2023-11-20 12:56:15,107 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-20 12:56:32,403 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-20 12:56:41,107 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-20 12:59:54,117 - INFO - Created a temporary directory at /tmp/tmptro6pmkp
2023-11-20 12:59:54,117 - INFO - Writing /tmp/tmptro6pmkp/_remote_module_non_scriptable.py
2023-11-20 12:59:56,401 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-20 13:00:13,970 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-20 13:00:22,694 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-20 13:02:02,511 - INFO - Collection created for user admin: {'collection_name': 'admin_xc'}
2023-11-20 13:02:11,864 - INFO - Collection created for user amin: {'collection_name': 'amin_xc'}
2023-11-20 13:05:18,411 - INFO - Received requests to /inference endpoint
2023-11-20 13:05:18,512 - INFO - Received a batch of request with batch size of: 1 
2023-11-20 13:05:18,513 - INFO - Received request: {'username': 'admin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-20 13:05:25,325 - INFO - Processed the request successfully
2023-11-20 13:11:41,928 - INFO - Collection created for user amin: {'collection_name': 'amin_xc'}
2023-11-20 13:12:33,821 - INFO - Collection created for user admin: {'collection_name': 'admin_vc'}
2023-11-20 14:16:56,429 - INFO - Created a temporary directory at /tmp/tmp4wtg9gbw
2023-11-20 14:16:56,429 - INFO - Writing /tmp/tmp4wtg9gbw/_remote_module_non_scriptable.py
2023-11-20 14:16:58,723 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-20 14:17:16,193 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-20 14:17:24,931 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-20 14:17:28,736 - INFO - Collection created for user admin: {'collection_name': 'admin_ff'}
2023-11-20 14:36:18,105 - INFO - Collection created for user admin: {'collection_name': 'admin_None'}
2023-11-20 14:36:28,260 - INFO - Collection created for user admin: {'collection_name': 'admin_zz'}
2023-11-21 10:08:48,545 - INFO - Collection created for user admin: {'collection_name': 'admin_test1'}
2023-11-21 10:09:28,307 - INFO - Collection created for user admin: {'collection_name': 'admin_test2'}
2023-11-21 10:26:38,196 - INFO -  file Car rental doc.pdf received for user admin: 
2023-11-21 10:26:38,218 - INFO - Collection created for user admin: {'collection_name': 'admin_test3'}
2023-11-21 11:48:17,956 - INFO - request processed successfully username='admin' collection_name=None mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/admin_Car rental doc.pdf': %s
2023-11-21 12:14:49,986 - INFO - Unable to poll TPU GCE metadata: HTTPConnectionPool(host='metadata.google.internal', port=80): Max retries exceeded with url: /computeMetadata/v1/instance/attributes/accelerator-type (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7efd24c6cf40>: Failed to establish a new connection: [Errno -2] Name or service not known'))
2023-11-21 12:27:14,382 - INFO - Unable to poll TPU GCE metadata: HTTPConnectionPool(host='metadata.google.internal', port=80): Max retries exceeded with url: /computeMetadata/v1/instance/attributes/accelerator-type (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fbe397a4f70>: Failed to establish a new connection: [Errno -2] Name or service not known'))
2023-11-21 12:31:03,268 - INFO - Unable to poll TPU GCE metadata: HTTPConnectionPool(host='metadata.google.internal', port=80): Max retries exceeded with url: /computeMetadata/v1/instance/attributes/accelerator-type (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7efdc1655090>: Failed to establish a new connection: [Errno -2] Name or service not known'))
2023-11-21 12:42:19,617 - INFO - request processed successfully username='admin' collection_name='Admin_pdf' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files': %s
2023-11-23 11:12:51,049 - INFO - Created a temporary directory at /tmp/tmp1uh5qtym
2023-11-23 11:12:51,049 - INFO - Writing /tmp/tmp1uh5qtym/_remote_module_non_scriptable.py
2023-11-23 11:12:57,415 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 11:23:44,404 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 11:24:08,688 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 12:09:17,103 - INFO - Created a temporary directory at /tmp/tmpn7qlos_4
2023-11-23 12:09:17,104 - INFO - Writing /tmp/tmpn7qlos_4/_remote_module_non_scriptable.py
2023-11-23 12:09:19,414 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:09:40,259 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 12:09:49,126 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 12:23:24,289 - INFO - Created a temporary directory at /tmp/tmp52ctgij0
2023-11-23 12:23:24,290 - INFO - Writing /tmp/tmp52ctgij0/_remote_module_non_scriptable.py
2023-11-23 12:23:26,577 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:23:44,091 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 12:23:52,776 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 12:23:59,341 - INFO - Created a temporary directory at /tmp/tmp50ngzdjy
2023-11-23 12:23:59,341 - INFO - Writing /tmp/tmp50ngzdjy/_remote_module_non_scriptable.py
2023-11-23 12:24:01,677 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:24:24,679 - INFO - Created a temporary directory at /tmp/tmpmqxeqq11
2023-11-23 12:24:24,680 - INFO - Writing /tmp/tmpmqxeqq11/_remote_module_non_scriptable.py
2023-11-23 12:24:27,032 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:24:49,788 - INFO - Created a temporary directory at /tmp/tmp2qwruxb3
2023-11-23 12:24:49,788 - INFO - Writing /tmp/tmp2qwruxb3/_remote_module_non_scriptable.py
2023-11-23 12:24:52,138 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:25:14,908 - INFO - Created a temporary directory at /tmp/tmpnfixem8q
2023-11-23 12:25:14,908 - INFO - Writing /tmp/tmpnfixem8q/_remote_module_non_scriptable.py
2023-11-23 12:25:17,245 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:25:40,456 - INFO - Created a temporary directory at /tmp/tmprmnm6fnz
2023-11-23 12:25:40,456 - INFO - Writing /tmp/tmprmnm6fnz/_remote_module_non_scriptable.py
2023-11-23 12:25:42,829 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:26:05,709 - INFO - Created a temporary directory at /tmp/tmpte34exz3
2023-11-23 12:26:05,709 - INFO - Writing /tmp/tmpte34exz3/_remote_module_non_scriptable.py
2023-11-23 12:26:08,037 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:26:31,138 - INFO - Created a temporary directory at /tmp/tmp4rmjy08y
2023-11-23 12:26:31,138 - INFO - Writing /tmp/tmp4rmjy08y/_remote_module_non_scriptable.py
2023-11-23 12:26:33,432 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:26:56,541 - INFO - Created a temporary directory at /tmp/tmp4p9ddlmp
2023-11-23 12:26:56,541 - INFO - Writing /tmp/tmp4p9ddlmp/_remote_module_non_scriptable.py
2023-11-23 12:26:58,910 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:27:29,210 - INFO - Created a temporary directory at /tmp/tmp1li9fg41
2023-11-23 12:27:29,211 - INFO - Writing /tmp/tmp1li9fg41/_remote_module_non_scriptable.py
2023-11-23 12:27:31,546 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:28:33,686 - INFO - Created a temporary directory at /tmp/tmpfh7j7eez
2023-11-23 12:28:33,686 - INFO - Writing /tmp/tmpfh7j7eez/_remote_module_non_scriptable.py
2023-11-23 12:28:36,034 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:29:38,654 - INFO - Created a temporary directory at /tmp/tmp74ismo3e
2023-11-23 12:29:38,654 - INFO - Writing /tmp/tmp74ismo3e/_remote_module_non_scriptable.py
2023-11-23 12:29:40,982 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:30:42,780 - INFO - Created a temporary directory at /tmp/tmp4dbh3c2j
2023-11-23 12:30:42,780 - INFO - Writing /tmp/tmp4dbh3c2j/_remote_module_non_scriptable.py
2023-11-23 12:30:45,146 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:31:47,314 - INFO - Created a temporary directory at /tmp/tmpjass4rlb
2023-11-23 12:31:47,314 - INFO - Writing /tmp/tmpjass4rlb/_remote_module_non_scriptable.py
2023-11-23 12:31:49,690 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:32:52,257 - INFO - Created a temporary directory at /tmp/tmpo4l9kwnc
2023-11-23 12:32:52,257 - INFO - Writing /tmp/tmpo4l9kwnc/_remote_module_non_scriptable.py
2023-11-23 12:32:54,574 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:33:56,892 - INFO - Created a temporary directory at /tmp/tmpozv5f5k2
2023-11-23 12:33:56,892 - INFO - Writing /tmp/tmpozv5f5k2/_remote_module_non_scriptable.py
2023-11-23 12:33:59,197 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:35:01,320 - INFO - Created a temporary directory at /tmp/tmpyyk7iue2
2023-11-23 12:35:01,321 - INFO - Writing /tmp/tmpyyk7iue2/_remote_module_non_scriptable.py
2023-11-23 12:35:03,681 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:37:10,276 - INFO - Created a temporary directory at /tmp/tmpyfr5vwwk
2023-11-23 12:37:10,276 - INFO - Writing /tmp/tmpyfr5vwwk/_remote_module_non_scriptable.py
2023-11-23 12:37:12,655 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:38:15,069 - INFO - Created a temporary directory at /tmp/tmp3lf146a0
2023-11-23 12:38:15,069 - INFO - Writing /tmp/tmp3lf146a0/_remote_module_non_scriptable.py
2023-11-23 12:38:17,415 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:39:20,351 - INFO - Created a temporary directory at /tmp/tmppq5_4gwz
2023-11-23 12:39:20,351 - INFO - Writing /tmp/tmppq5_4gwz/_remote_module_non_scriptable.py
2023-11-23 12:39:22,661 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:40:25,264 - INFO - Created a temporary directory at /tmp/tmp63psze5p
2023-11-23 12:40:25,265 - INFO - Writing /tmp/tmp63psze5p/_remote_module_non_scriptable.py
2023-11-23 12:40:27,610 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:41:30,310 - INFO - Created a temporary directory at /tmp/tmpys1qln4v
2023-11-23 12:41:30,310 - INFO - Writing /tmp/tmpys1qln4v/_remote_module_non_scriptable.py
2023-11-23 12:41:32,657 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:42:34,582 - INFO - Created a temporary directory at /tmp/tmpx97u88py
2023-11-23 12:42:34,582 - INFO - Writing /tmp/tmpx97u88py/_remote_module_non_scriptable.py
2023-11-23 12:42:36,917 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:43:39,615 - INFO - Created a temporary directory at /tmp/tmpb7v8wpbr
2023-11-23 12:43:39,615 - INFO - Writing /tmp/tmpb7v8wpbr/_remote_module_non_scriptable.py
2023-11-23 12:43:41,966 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:44:44,212 - INFO - Created a temporary directory at /tmp/tmp20e82bof
2023-11-23 12:44:44,212 - INFO - Writing /tmp/tmp20e82bof/_remote_module_non_scriptable.py
2023-11-23 12:44:46,573 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:45:48,322 - INFO - Created a temporary directory at /tmp/tmpfm0muokk
2023-11-23 12:45:48,323 - INFO - Writing /tmp/tmpfm0muokk/_remote_module_non_scriptable.py
2023-11-23 12:45:50,682 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:46:52,508 - INFO - Created a temporary directory at /tmp/tmpnr_tekat
2023-11-23 12:46:52,509 - INFO - Writing /tmp/tmpnr_tekat/_remote_module_non_scriptable.py
2023-11-23 12:46:54,881 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:47:57,043 - INFO - Created a temporary directory at /tmp/tmpb138zrkz
2023-11-23 12:47:57,044 - INFO - Writing /tmp/tmpb138zrkz/_remote_module_non_scriptable.py
2023-11-23 12:47:59,379 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:49:01,198 - INFO - Created a temporary directory at /tmp/tmp26lisu9h
2023-11-23 12:49:01,198 - INFO - Writing /tmp/tmp26lisu9h/_remote_module_non_scriptable.py
2023-11-23 12:49:03,521 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:50:05,792 - INFO - Created a temporary directory at /tmp/tmp61mdubsu
2023-11-23 12:50:05,792 - INFO - Writing /tmp/tmp61mdubsu/_remote_module_non_scriptable.py
2023-11-23 12:50:08,122 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:51:11,560 - INFO - Created a temporary directory at /tmp/tmp_yh7aoo4
2023-11-23 12:51:11,560 - INFO - Writing /tmp/tmp_yh7aoo4/_remote_module_non_scriptable.py
2023-11-23 12:51:13,964 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:52:14,714 - INFO - Created a temporary directory at /tmp/tmp8pyhkrew
2023-11-23 12:52:14,715 - INFO - Writing /tmp/tmp8pyhkrew/_remote_module_non_scriptable.py
2023-11-23 12:52:17,070 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:53:19,247 - INFO - Created a temporary directory at /tmp/tmpdcagsmgx
2023-11-23 12:53:19,247 - INFO - Writing /tmp/tmpdcagsmgx/_remote_module_non_scriptable.py
2023-11-23 12:53:21,603 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:54:24,034 - INFO - Created a temporary directory at /tmp/tmp2uf87urb
2023-11-23 12:54:24,034 - INFO - Writing /tmp/tmp2uf87urb/_remote_module_non_scriptable.py
2023-11-23 12:54:26,392 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:55:28,963 - INFO - Created a temporary directory at /tmp/tmp3awgtdft
2023-11-23 12:55:28,964 - INFO - Writing /tmp/tmp3awgtdft/_remote_module_non_scriptable.py
2023-11-23 12:55:31,325 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:56:32,894 - INFO - Created a temporary directory at /tmp/tmpe_9rfdka
2023-11-23 12:56:32,894 - INFO - Writing /tmp/tmpe_9rfdka/_remote_module_non_scriptable.py
2023-11-23 12:56:35,237 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:57:37,015 - INFO - Created a temporary directory at /tmp/tmphcwlzw3g
2023-11-23 12:57:37,015 - INFO - Writing /tmp/tmphcwlzw3g/_remote_module_non_scriptable.py
2023-11-23 12:57:39,388 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:58:02,698 - INFO - Created a temporary directory at /tmp/tmp6pms8at1
2023-11-23 12:58:02,698 - INFO - Writing /tmp/tmp6pms8at1/_remote_module_non_scriptable.py
2023-11-23 12:58:05,044 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:58:22,618 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 12:58:31,300 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 13:13:29,302 - INFO - Created a temporary directory at /tmp/tmpuutsaogr
2023-11-23 13:13:29,302 - INFO - Writing /tmp/tmpuutsaogr/_remote_module_non_scriptable.py
2023-11-23 13:13:31,609 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 13:13:49,189 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 13:13:57,927 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 13:18:05,478 - INFO - Created a temporary directory at /tmp/tmp4cobtbia
2023-11-23 13:18:05,478 - INFO - Writing /tmp/tmp4cobtbia/_remote_module_non_scriptable.py
2023-11-23 13:18:07,744 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 13:18:25,232 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 13:18:33,889 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 13:21:14,851 - INFO - Received requests to /inference endpoint
2023-11-23 13:21:14,953 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 13:21:14,953 - INFO - Received request: {'username': 'admin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-23 13:21:24,101 - INFO - Processed the request successfully
2023-11-23 13:23:15,418 - INFO - Created a temporary directory at /tmp/tmpk2suv46u
2023-11-23 13:23:15,419 - INFO - Writing /tmp/tmpk2suv46u/_remote_module_non_scriptable.py
2023-11-23 13:23:17,554 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 13:23:35,938 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 13:23:44,634 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 13:23:50,599 - INFO - Created a temporary directory at /tmp/tmp5c09l6yy
2023-11-23 13:23:50,599 - INFO - Writing /tmp/tmp5c09l6yy/_remote_module_non_scriptable.py
2023-11-23 13:23:52,784 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 13:24:11,888 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 13:24:20,615 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 13:25:55,503 - INFO - Received requests to /inference endpoint
2023-11-23 13:25:55,604 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 13:25:55,604 - INFO - Received request: {'username': 'admin', 'prompt': 'hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-23 13:26:02,572 - INFO - Processed the request successfully
2023-11-23 14:33:21,216 - INFO - Created a temporary directory at /tmp/tmpjb8ykvvk
2023-11-23 14:33:21,216 - INFO - Writing /tmp/tmpjb8ykvvk/_remote_module_non_scriptable.py
2023-11-23 14:33:23,368 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 14:33:40,879 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 14:33:49,607 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 14:33:56,713 - INFO - Created a temporary directory at /tmp/tmpbckzbvbo
2023-11-23 14:33:56,713 - INFO - Writing /tmp/tmpbckzbvbo/_remote_module_non_scriptable.py
2023-11-23 14:33:58,978 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 14:34:16,672 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 14:34:25,394 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 14:42:58,320 - INFO - Received requests to /inference endpoint
2023-11-23 14:42:58,421 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 14:42:58,422 - INFO - Received request: {'username': 'admin', 'prompt': 'hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'llama_70b'}
2023-11-23 14:43:05,361 - INFO - Processed the request successfully
2023-11-23 14:43:12,311 - INFO - Received requests to /inference endpoint
2023-11-23 14:43:12,412 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 14:43:12,413 - INFO - Received request: {'username': 'admin', 'prompt': 'hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'llama_13b'}
2023-11-23 14:43:19,189 - INFO - Processed the request successfully
2023-11-23 15:30:56,689 - INFO - Received requests to /inference endpoint
2023-11-23 15:30:56,790 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 15:30:56,790 - INFO - Received request: {'username': 'alex', 'prompt': 'hi', 'memory': False, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'llama_70b'}
2023-11-23 15:31:03,235 - INFO - Processed the request successfully
2023-11-23 15:39:21,434 - INFO - Received requests to /inference endpoint
2023-11-23 15:39:21,535 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 15:39:21,535 - INFO - Received request: {'username': 'alex', 'prompt': 'Hi', 'memory': False, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-11-23 15:39:27,700 - INFO - Processed the request successfully
2023-11-23 15:39:37,523 - INFO - Received requests to /inference endpoint
2023-11-23 15:39:37,625 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 15:39:37,625 - INFO - Received request: {'username': 'alex', 'prompt': 'Hi', 'memory': False, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-23 15:39:43,538 - INFO - Processed the request successfully
2023-11-23 15:44:58,145 - INFO - Received requests to /inference endpoint
2023-11-23 15:44:58,246 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 15:44:58,246 - INFO - Received request: {'username': 'alex', 'prompt': 'my name is Amin', 'memory': False, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-23 15:45:02,916 - INFO - Processed the request successfully
2023-11-23 15:45:14,247 - INFO - Received requests to /inference endpoint
2023-11-23 15:45:14,348 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 15:45:14,348 - INFO - Received request: {'username': 'alex', 'prompt': 'what was my previous question ?', 'memory': False, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-23 15:45:23,367 - INFO - Processed the request successfully
2023-11-23 15:45:40,795 - INFO - Received requests to /inference endpoint
2023-11-23 15:45:40,896 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 15:45:40,896 - INFO - Received request: {'username': 'alex', 'prompt': 'I told you name, what was my name?', 'memory': False, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-23 15:45:46,943 - INFO - Processed the request successfully
2023-11-23 15:46:04,201 - INFO - Received requests to /inference endpoint
2023-11-23 15:46:04,302 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 15:46:04,302 - INFO - Received request: {'username': 'alex', 'prompt': 'my name is Amin', 'memory': False, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-23 15:46:08,827 - INFO - Processed the request successfully
2023-11-23 15:46:15,746 - INFO - Received requests to /inference endpoint
2023-11-23 15:46:15,847 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 15:46:15,847 - INFO - Received request: {'username': 'alex', 'prompt': 'what was my name?', 'memory': False, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-23 15:46:25,574 - INFO - Processed the request successfully
2023-11-23 16:00:44,051 - INFO - Received requests to /inference endpoint
2023-11-23 16:00:44,152 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 16:00:44,152 - INFO - Received request: {'username': 'alex', 'prompt': 'where is London?', 'memory': False, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-23 16:00:58,162 - INFO - Processed the request successfully
2023-11-23 16:07:32,433 - INFO - Received requests to /inference endpoint
2023-11-23 16:07:32,534 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 16:07:32,534 - INFO - Received request: {'username': 'alex', 'prompt': 'what was my previous question about?', 'memory': False, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-23 16:07:41,463 - INFO - Processed the request successfully
2023-11-23 16:12:21,583 - INFO - Received requests to /inference endpoint
2023-11-23 16:12:21,684 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 16:12:21,684 - INFO - Received request: {'username': 'alex', 'prompt': 'hi', 'memory': False, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-23 16:12:28,046 - INFO - Processed the request successfully
2023-11-23 16:14:54,577 - INFO - Created a temporary directory at /tmp/tmpe76emsue
2023-11-23 16:14:54,577 - INFO - Writing /tmp/tmpe76emsue/_remote_module_non_scriptable.py
2023-11-23 16:14:56,681 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 16:15:14,381 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 16:15:23,087 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 16:15:29,688 - INFO - Created a temporary directory at /tmp/tmpj2a3qw3q
2023-11-23 16:15:29,689 - INFO - Writing /tmp/tmpj2a3qw3q/_remote_module_non_scriptable.py
2023-11-23 16:15:31,851 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 16:15:49,766 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 16:15:58,513 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 16:19:39,870 - INFO - Received requests to /inference endpoint
2023-11-23 16:19:39,971 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 16:19:39,971 - INFO - Received request: {'username': 'admin', 'prompt': 'hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'string'}
2023-11-23 16:19:46,762 - INFO - Processed the request successfully
2023-11-23 16:20:04,412 - INFO - Received requests to /inference endpoint
2023-11-23 16:20:04,514 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 16:20:04,514 - INFO - Received request: {'username': 'admin', 'prompt': 'my name is admin', 'memory': True, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'string'}
2023-11-23 16:20:09,768 - INFO - Processed the request successfully
2023-11-23 16:20:23,309 - INFO - Received requests to /inference endpoint
2023-11-23 16:20:23,410 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 16:20:23,411 - INFO - Received request: {'username': 'admin', 'prompt': 'what was my name?', 'memory': True, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'string'}
2023-11-23 16:20:30,829 - INFO - Processed the request successfully
2023-11-23 16:20:57,682 - INFO - Received requests to /inference endpoint
2023-11-23 16:20:57,783 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 16:20:57,783 - INFO - Received request: {'username': 'admin', 'prompt': 'where is the capital of the UK?', 'memory': True, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'string'}
2023-11-23 16:21:04,300 - INFO - Processed the request successfully
2023-11-23 16:21:23,741 - INFO - Received requests to /inference endpoint
2023-11-23 16:21:23,842 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 16:21:23,842 - INFO - Received request: {'username': 'admin', 'prompt': 'what was my previous question about?', 'memory': True, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'string'}
2023-11-23 16:21:32,897 - INFO - Processed the request successfully
2023-11-24 09:43:45,747 - INFO - Received requests to /inference endpoint
2023-11-24 09:43:45,848 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 09:43:45,848 - INFO - Received request: {'username': 'admin', 'prompt': 'Hi, my name is Admin', 'memory': True, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'Llama_70b'}
2023-11-24 09:43:55,246 - INFO - Processed the request successfully
2023-11-24 09:44:18,755 - INFO - Received requests to /inference endpoint
2023-11-24 09:44:18,856 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 09:44:18,856 - INFO - Received request: {'username': 'admin', 'prompt': 'what was my previous prompt?', 'memory': True, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'Llama_70b'}
2023-11-24 09:44:27,432 - INFO - Processed the request successfully
2023-11-24 09:44:47,855 - INFO - Received requests to /inference endpoint
2023-11-24 09:44:47,957 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 09:44:47,957 - INFO - Received request: {'username': 'admin', 'prompt': 'what was my previous prompt?', 'memory': True, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'Llama_13b'}
2023-11-24 09:44:56,326 - INFO - Processed the request successfully
2023-11-24 11:57:07,619 - INFO - Received requests to /inference endpoint
2023-11-24 11:57:07,720 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 11:57:07,720 - INFO - Received request: {'username': 'alex', 'prompt': 'where is the capital of France?', 'memory': False, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-11-24 11:57:18,765 - INFO - Processed the request successfully
2023-11-24 11:57:32,035 - INFO - Received requests to /inference endpoint
2023-11-24 11:57:32,136 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 11:57:32,137 - INFO - Received request: {'username': 'alex', 'prompt': 'what was my previous question ?', 'memory': False, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-11-24 11:57:41,574 - INFO - Processed the request successfully
2023-11-24 11:58:00,398 - INFO - Received requests to /inference endpoint
2023-11-24 11:58:00,499 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 11:58:00,499 - INFO - Received request: {'username': 'alex', 'prompt': 'what was my previous prompt?', 'memory': False, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-11-24 11:58:07,056 - INFO - Processed the request successfully
2023-11-24 11:58:22,765 - INFO - Received requests to /inference endpoint
2023-11-24 11:58:22,866 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 11:58:22,866 - INFO - Received request: {'username': 'alex', 'prompt': 'where is the capital of France?', 'memory': False, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-11-24 11:58:33,997 - INFO - Processed the request successfully
2023-11-24 11:58:51,482 - INFO - Received requests to /inference endpoint
2023-11-24 11:58:51,583 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 11:58:51,584 - INFO - Received request: {'username': 'alex', 'prompt': 'what was my previous prompt about?', 'memory': False, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-11-24 11:59:00,208 - INFO - Processed the request successfully
2023-11-24 12:01:15,545 - INFO - Received requests to /inference endpoint
2023-11-24 12:01:15,647 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 12:01:15,647 - INFO - Received request: {'username': 'alex', 'prompt': 'where is the capital of France?', 'memory': True, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-11-24 12:01:20,173 - INFO - Processed the request successfully
2023-11-24 12:01:32,396 - INFO - Received requests to /inference endpoint
2023-11-24 12:01:32,497 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 12:01:32,497 - INFO - Received request: {'username': 'alex', 'prompt': 'What was my previous prompt?', 'memory': True, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-11-24 12:01:41,192 - INFO - Processed the request successfully
2023-11-24 12:06:49,837 - INFO - Received requests to /inference endpoint
2023-11-24 12:06:49,938 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 12:06:49,938 - INFO - Received request: {'username': 'alex', 'prompt': 'did I ask question about paris?', 'memory': True, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-11-24 12:06:56,663 - INFO - Processed the request successfully
2023-11-24 12:07:30,597 - INFO - Received requests to /inference endpoint
2023-11-24 12:07:30,698 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 12:07:30,698 - INFO - Received request: {'username': 'alex', 'prompt': 'where is London located?', 'memory': True, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-11-24 12:07:41,822 - INFO - Processed the request successfully
2023-11-24 12:08:00,768 - INFO - Received requests to /inference endpoint
2023-11-24 12:08:00,869 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 12:08:00,869 - INFO - Received request: {'username': 'alex', 'prompt': 'did I ask question about London?', 'memory': True, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-24 12:08:06,731 - INFO - Processed the request successfully
2023-11-24 12:08:23,289 - INFO - Received requests to /inference endpoint
2023-11-24 12:08:23,390 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 12:08:23,390 - INFO - Received request: {'username': 'alex', 'prompt': 'did I asked where London is Located?', 'memory': True, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-24 12:08:31,514 - INFO - Processed the request successfully
2023-11-24 12:09:00,315 - INFO - Received requests to /inference endpoint
2023-11-24 12:09:00,416 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 12:09:00,417 - INFO - Received request: {'username': 'alex', 'prompt': 'give me brief of our chats before this prompt', 'memory': True, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-24 12:09:23,082 - INFO - Processed the request successfully
2023-11-24 12:58:21,180 - INFO - Created a temporary directory at /tmp/tmpoi9rm3t6
2023-11-24 12:58:21,180 - INFO - Writing /tmp/tmpoi9rm3t6/_remote_module_non_scriptable.py
2023-11-24 12:58:23,306 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 12:58:40,894 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 12:58:49,645 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-24 12:58:56,194 - INFO - Created a temporary directory at /tmp/tmp906n0fra
2023-11-24 12:58:56,194 - INFO - Writing /tmp/tmp906n0fra/_remote_module_non_scriptable.py
2023-11-24 12:58:58,396 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 12:59:16,220 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 12:59:24,937 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-24 13:42:14,482 - INFO - Created a temporary directory at /tmp/tmpxi8mh_ak
2023-11-24 13:42:14,482 - INFO - Writing /tmp/tmpxi8mh_ak/_remote_module_non_scriptable.py
2023-11-24 13:42:16,638 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 13:42:34,329 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 13:42:43,072 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-24 13:42:49,588 - INFO - Created a temporary directory at /tmp/tmpyi32iyuj
2023-11-24 13:42:49,588 - INFO - Writing /tmp/tmpyi32iyuj/_remote_module_non_scriptable.py
2023-11-24 13:42:51,804 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 13:43:09,365 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 13:43:18,074 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-24 13:45:31,246 - INFO - Created a temporary directory at /tmp/tmpzsfn_vvw
2023-11-24 13:45:31,246 - INFO - Writing /tmp/tmpzsfn_vvw/_remote_module_non_scriptable.py
2023-11-24 13:45:33,448 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 13:45:51,250 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 13:45:59,988 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-24 13:46:06,278 - INFO - Created a temporary directory at /tmp/tmpjxsa7vpl
2023-11-24 13:46:06,278 - INFO - Writing /tmp/tmpjxsa7vpl/_remote_module_non_scriptable.py
2023-11-24 13:46:08,508 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 16:14:29,257 - INFO - Created a temporary directory at /tmp/tmpwi0t5sf9
2023-11-24 16:14:29,258 - INFO - Writing /tmp/tmpwi0t5sf9/_remote_module_non_scriptable.py
2023-11-24 16:14:31,556 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 16:14:49,192 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 16:15:04,555 - INFO - Created a temporary directory at /tmp/tmpfgxakqhk
2023-11-24 16:15:04,556 - INFO - Writing /tmp/tmpfgxakqhk/_remote_module_non_scriptable.py
2023-11-24 16:16:08,239 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 16:16:12,441 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 16:19:20,847 - INFO - Created a temporary directory at /tmp/tmpbyn56i3p
2023-11-24 16:19:20,847 - INFO - Writing /tmp/tmpbyn56i3p/_remote_module_non_scriptable.py
2023-11-24 16:19:23,271 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 16:19:40,999 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 16:19:56,558 - INFO - Created a temporary directory at /tmp/tmp4mh0e5us
2023-11-24 16:19:56,558 - INFO - Writing /tmp/tmp4mh0e5us/_remote_module_non_scriptable.py
2023-11-24 16:19:58,422 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 16:20:02,922 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 16:24:52,413 - INFO - Created a temporary directory at /tmp/tmpjkss21or
2023-11-24 16:24:52,413 - INFO - Writing /tmp/tmpjkss21or/_remote_module_non_scriptable.py
2023-11-24 16:24:54,738 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 16:25:12,084 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 16:25:26,647 - INFO - Created a temporary directory at /tmp/tmpgiw8zzz6
2023-11-24 16:25:26,647 - INFO - Writing /tmp/tmpgiw8zzz6/_remote_module_non_scriptable.py
2023-11-24 16:25:28,448 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 16:25:32,666 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 16:26:27,492 - INFO - Created a temporary directory at /tmp/tmpgd0se94d
2023-11-24 16:26:27,493 - INFO - Writing /tmp/tmpgd0se94d/_remote_module_non_scriptable.py
2023-11-24 16:26:29,828 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 16:26:47,350 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 16:29:55,708 - INFO - Created a temporary directory at /tmp/tmp6kcast2l
2023-11-24 16:29:55,708 - INFO - Writing /tmp/tmp6kcast2l/_remote_module_non_scriptable.py
2023-11-24 16:29:58,038 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 16:30:15,722 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 16:31:35,276 - INFO - Created a temporary directory at /tmp/tmpbs261uga
2023-11-24 16:31:35,277 - INFO - Writing /tmp/tmpbs261uga/_remote_module_non_scriptable.py
2023-11-24 16:31:37,588 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 16:31:55,077 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-27 09:52:02,915 - INFO - Created a temporary directory at /tmp/tmpme30l9h9
2023-11-27 09:52:02,915 - INFO - Writing /tmp/tmpme30l9h9/_remote_module_non_scriptable.py
2023-11-27 09:52:05,183 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-27 09:52:22,711 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-27 09:52:37,971 - INFO - Created a temporary directory at /tmp/tmpkazi7fy2
2023-11-27 09:52:37,971 - INFO - Writing /tmp/tmpkazi7fy2/_remote_module_non_scriptable.py
2023-11-27 09:52:39,805 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-27 09:52:43,763 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-27 09:54:02,579 - INFO - Created a temporary directory at /tmp/tmpkj0bmpr9
2023-11-27 09:54:02,580 - INFO - Writing /tmp/tmpkj0bmpr9/_remote_module_non_scriptable.py
2023-11-27 09:54:04,862 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-27 09:54:22,230 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-27 09:54:36,775 - INFO - Created a temporary directory at /tmp/tmpk_d8gkff
2023-11-27 09:54:36,776 - INFO - Writing /tmp/tmpk_d8gkff/_remote_module_non_scriptable.py
2023-11-27 09:54:38,606 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-27 09:54:42,469 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-27 09:57:00,310 - INFO - request processed successfully username='admin' collection_name='videos' mode='create_collection' vectorDB_type='Weaviate' file_path='string': %s
2023-11-27 10:01:33,265 - ERROR - An error occurred: [Errno 20] Not a directory: '/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/docs/paper.pdf'
2023-11-27 10:09:22,302 - ERROR - An error occurred: [Errno 20] Not a directory: '/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/admin_Car rental doc.pdf'
2023-11-27 10:55:03,854 - ERROR - An error occurred: [Errno 20] Not a directory: '/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/docs/paper.pdf'
2023-11-27 11:00:59,336 - INFO - request processed successfully username='admin' collection_name='Admin_videos' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/f84b2db7d4a8f72d': %s
2023-11-27 11:05:11,372 - INFO - request processed successfully username='admin' collection_name='Admin_videos' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/536b0eacb9e16257': %s
2023-11-27 11:24:52,458 - ERROR - An error occurred: (sqlite3.OperationalError) attempt to write a readonly database
[SQL: UPDATE users SET collection_names=? WHERE users.id = ?]
[parameters: ('admin_Video,admin_Video2,admin_Test,admin_my_collection,admin_xc,admin_vc,admin_ff,admin_None,admin_zz,admin_test1,admin_test2,admin_test3,admin_pdf,A_video,A_web,Admin_ssd,Admin_videos,Admin_amin', 1)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2023-11-27 11:26:12,608 - ERROR - An error occurred: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.OperationalError) attempt to write a readonly database
[SQL: UPDATE users SET collection_names=? WHERE users.id = ?]
[parameters: ('admin_Video,admin_Video2,admin_Test,admin_my_collection,admin_xc,admin_vc,admin_ff,admin_None,admin_zz,admin_test1,admin_test2,admin_test3,admin_pdf,A_video,A_web,Admin_ssd,Admin_videos,Admin_amin', 1)]
(Background on this error at: https://sqlalche.me/e/20/e3q8) (Background on this error at: https://sqlalche.me/e/20/7s2a)
2023-11-27 11:28:24,866 - INFO - request processed successfully username='admin' collection_name='Admin_General_collection' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/59906acd283673a0': %s
2023-11-27 12:27:48,177 - ERROR - An error occurred: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.OperationalError) attempt to write a readonly database
[SQL: UPDATE users SET collection_names=? WHERE users.id = ?]
[parameters: ('admin_Video,admin_Video2,admin_Test,admin_my_collection,admin_xc,admin_vc,admin_ff,admin_None,admin_zz,admin_test1,admin_test2,admin_test3,admin_pdf,A_video,A_web,Admin_ssd,Admin_videos,Admin_amin', 1)]
(Background on this error at: https://sqlalche.me/e/20/e3q8) (Background on this error at: https://sqlalche.me/e/20/7s2a)
2023-11-27 12:33:17,440 - INFO - request processed successfully username='amin' collection_name=None mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/38a6b2c006a2e2d9': %s
2023-12-05 12:49:46,413 - INFO - Created a temporary directory at /tmp/tmpfd17agu7
2023-12-05 12:49:46,413 - INFO - Writing /tmp/tmpfd17agu7/_remote_module_non_scriptable.py
2023-12-05 12:49:50,023 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-05 12:59:22,312 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-05 12:59:38,525 - INFO - Created a temporary directory at /tmp/tmp704ler0i
2023-12-05 12:59:38,525 - INFO - Writing /tmp/tmp704ler0i/_remote_module_non_scriptable.py
2023-12-06 10:14:45,285 - INFO - Created a temporary directory at /tmp/tmpemk428xz
2023-12-06 10:14:45,285 - INFO - Writing /tmp/tmpemk428xz/_remote_module_non_scriptable.py
2023-12-06 10:14:48,704 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-06 10:20:26,244 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-06 10:20:42,262 - INFO - Created a temporary directory at /tmp/tmpilr_7vuk
2023-12-06 10:20:42,263 - INFO - Writing /tmp/tmpilr_7vuk/_remote_module_non_scriptable.py
2023-12-06 10:21:56,387 - INFO - Created a temporary directory at /tmp/tmpuh6vs2_q
2023-12-06 10:21:56,387 - INFO - Writing /tmp/tmpuh6vs2_q/_remote_module_non_scriptable.py
2023-12-06 10:23:24,436 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-06 10:23:41,517 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-06 10:25:34,841 - INFO - Created a temporary directory at /tmp/tmpx6t7m6ts
2023-12-06 10:25:34,841 - INFO - Writing /tmp/tmpx6t7m6ts/_remote_module_non_scriptable.py
2023-12-06 10:25:37,172 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-06 10:27:34,676 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-06 10:27:49,354 - INFO - Created a temporary directory at /tmp/tmp7l33bz1s
2023-12-06 10:27:49,355 - INFO - Writing /tmp/tmp7l33bz1s/_remote_module_non_scriptable.py
2023-12-06 10:27:51,218 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-06 10:27:56,180 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-06 11:04:19,573 - INFO - Received requests to /inference endpoint
2023-12-06 11:04:19,674 - INFO - Received a batch of request with batch size of: 1 
2023-12-06 11:04:19,675 - INFO - Received request: {'username': 'admin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': False, 'collection_name': 'Amin', 'llm_model': 'Llama_70b'}
2023-12-06 11:04:54,237 - INFO - Received requests to /inference endpoint
2023-12-06 11:04:54,338 - INFO - Received a batch of request with batch size of: 1 
2023-12-06 11:04:54,338 - INFO - Received request: {'username': 'admin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': False, 'collection_name': 'General_collection', 'llm_model': 'Llama_70b'}
2023-12-06 11:05:03,064 - INFO - Received requests to /inference endpoint
2023-12-06 11:05:03,165 - INFO - Received a batch of request with batch size of: 1 
2023-12-06 11:05:03,165 - INFO - Received request: {'username': 'admin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': False, 'collection_name': 'General_collection', 'llm_model': 'Llama_70b'}
2023-12-06 11:05:46,516 - INFO - Received requests to /inference endpoint
2023-12-06 11:05:46,617 - INFO - Received a batch of request with batch size of: 1 
2023-12-06 11:05:46,617 - INFO - Received request: {'username': 'admin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': False, 'collection_name': 'General_collection', 'llm_model': 'Llama_70b'}
2023-12-06 11:06:33,448 - INFO - Received requests to /inference endpoint
2023-12-06 11:06:33,549 - INFO - Received a batch of request with batch size of: 1 
2023-12-06 11:06:33,549 - INFO - Received request: {'username': 'amin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': False, 'collection_name': 'General_collection', 'llm_model': 'Llama_70b'}
2023-12-06 11:06:33,572 - ERROR - Error processing the request: Error during query: [{'locations': [{'column': 6, 'line': 1}], 'message': 'Cannot query field "General_collection" on type "GetObjectsObj". Did you mean "Admin_General_collection"?', 'path': None}]
2023-12-06 11:17:34,034 - INFO - checking the request/ username='amin' class_name='video' mode='create_class' vectorDB_type='Weaviate' file_path='string': %s
2023-12-06 11:17:34,123 - INFO - checkpoint 1
2023-12-06 11:17:34,123 - INFO - checkpoint 2 amin: %s
2023-12-06 11:17:34,123 - INFO - checkpoint 2 amin_video: %s
2023-12-06 11:24:44,014 - INFO - checking the request/ username='amin' class_name='document' mode='create_class' vectorDB_type='Weaviate' file_path='string': %s
2023-12-06 11:24:44,101 - INFO - checkpoint 1
2023-12-06 11:24:44,101 - INFO - checkpoint 2 amin: %s
2023-12-06 11:24:44,101 - INFO - checkpoint 2 amin_document: %s
2023-12-06 11:24:44,120 - INFO - success: class document created for user amin
2023-12-06 11:31:00,074 - INFO - checking the request/ username='amin' class_name='web' mode='create_class' vectorDB_type='Weaviate' file_path='string': %s
2023-12-06 11:31:00,135 - INFO - checkpoint 1
2023-12-06 11:31:00,135 - INFO - checkpoint 2 amin: %s
2023-12-06 11:31:00,135 - INFO - checkpoint 2 amin_web: %s
2023-12-06 11:31:00,166 - INFO - success: class web created for user amin
2023-12-06 11:46:23,071 - INFO - checking the request/ username='amin' class_name='web2' mode='create_class' vectorDB_type='Weaviate' file_path='string': %s
2023-12-06 11:46:23,132 - INFO - checkpoint 1
2023-12-06 11:46:23,133 - INFO - checkpoint 2 amin: %s
2023-12-06 11:46:23,133 - INFO - checkpoint 2 amin_web2: %s
2023-12-06 11:46:23,177 - INFO - class name added successfully to database
2023-12-06 11:46:23,177 - INFO - success: class web2 created for user amin
2023-12-06 11:52:25,070 - INFO - request processed successfully username='amin' class_name='web2' mode='delete_class' vectorDB_type='Weaviate' file_path='string': %s
2023-12-06 11:52:25,070 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 12:02:08,040 - INFO - Created a temporary directory at /tmp/tmpvmh12ryr
2023-12-06 12:02:08,040 - INFO - Writing /tmp/tmpvmh12ryr/_remote_module_non_scriptable.py
2023-12-06 12:02:10,623 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-06 12:02:45,785 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-06 12:03:01,497 - INFO - Created a temporary directory at /tmp/tmpnuw3eece
2023-12-06 12:03:01,497 - INFO - Writing /tmp/tmpnuw3eece/_remote_module_non_scriptable.py
2023-12-06 12:03:03,336 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-06 12:03:07,744 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-06 12:03:51,534 - INFO - Received requests to /inference endpoint
2023-12-06 12:03:51,635 - INFO - Received a batch of request with batch size of: 1 
2023-12-06 12:03:51,636 - INFO - Received request: {'username': 'amin', 'prompt': 'hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': False, 'collection_name': 'web', 'llm_model': 'Llama_70b'}
2023-12-06 12:04:09,090 - INFO - Received requests to /inference endpoint
2023-12-06 12:04:09,191 - INFO - Received a batch of request with batch size of: 1 
2023-12-06 12:04:09,191 - INFO - Received request: {'username': 'amin', 'prompt': 'hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': False, 'collection_name': 'video', 'llm_model': 'Llama_70b'}
2023-12-06 13:31:34,071 - INFO - checking the request/ username='string' class_name='ee' mode='create_collection' vectorDB_type='Weaviate' file_path='string': %s
2023-12-06 13:31:34,158 - INFO - checkpoint 1
2023-12-06 13:31:34,158 - INFO - checkpoint 2 string: %s
2023-12-06 13:31:34,158 - INFO - checkpoint 2 string_ee: %s
2023-12-06 13:31:34,243 - INFO - class name added successfully to database
2023-12-06 13:31:34,243 - INFO - success: class ee created for user string
2023-12-06 13:33:22,281 - INFO - checking the request/ username='amin' class_name='cd' mode='create_collection' vectorDB_type='Weaviate' file_path=None: %s
2023-12-06 13:33:22,369 - INFO - checkpoint 1
2023-12-06 13:33:22,369 - INFO - checkpoint 2 amin: %s
2023-12-06 13:33:22,369 - INFO - checkpoint 2 amin_cd: %s
2023-12-06 13:33:22,405 - INFO - class name added successfully to database
2023-12-06 13:33:22,405 - INFO - success: class cd created for user amin
2023-12-06 13:33:44,717 - INFO - request processed successfully username='amin' class_name=None mode=None vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/bf47e2ef6b86afed': %s
2023-12-06 13:33:44,717 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 14:00:52,783 - INFO - Created a temporary directory at /tmp/tmphrl8ikoy
2023-12-06 14:00:52,784 - INFO - Writing /tmp/tmphrl8ikoy/_remote_module_non_scriptable.py
2023-12-06 14:00:55,129 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-06 14:01:18,877 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-06 14:01:34,022 - INFO - Created a temporary directory at /tmp/tmpi4v65qg6
2023-12-06 14:01:34,022 - INFO - Writing /tmp/tmpi4v65qg6/_remote_module_non_scriptable.py
2023-12-06 14:01:35,856 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-06 14:01:40,113 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-06 14:03:32,776 - INFO - actors creation successful [Actor(WeaviateEmbedder, 5ae9dee07e49ab95d952437401000000), Actor(WeaviateEmbedder, c394f0a0dbcccd8776b8dd9101000000), Actor(WeaviateEmbedder, 3743bbe20c6b8637fe9468ca01000000)]: %s
2023-12-06 14:03:32,777 - INFO - check 1st step of ray was successful
2023-12-06 14:03:32,777 - INFO - check if ray was successful:
2023-12-06 14:03:32,777 - INFO - check weaviate add data, 
2023-12-06 14:03:32,777 - INFO - request processed successfully username='amin' class_name='web' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/docs': %s
2023-12-06 14:03:34,491 - INFO - Check the data that is being passed [{'page_content': '3 Temporal Early Exits Video Object Detection Pipeline\nWe propose a novel video object detection pipeline to tackle the computational complexity of video\nobject detection. An overview of the proposed pipeline is shown in ﬁgure 2. Given a sequence of\nvideo frames, multiple Temporal Early Exit Modules (TEEM) are used to build the early exit branches.\nEach early exit branch uses features extracted in the early stages of the feature network to identify\nany semantic variation between the features of consecutive video frames. If any semantic variations\nis distinguished by a TEEM, a full computation effort (main branch) will be needed to process the\nvideo frame. On the other hand, the computation process of a video frame without any semantic\nvariations will be terminated at an early exit branch and the detection results from the previous frame\nwill be reused. We describe the implementation details of the proposed pipeline in this section.\n3.1 Scenery Change\nThe key challenge in identifying feature variations is qualifying semantic dissimilarities between\nimages [ 1]. To address this challenge, we introduce a scenery change (SC)metric to quantify\nsemantic variations between video frames. We deﬁne a semantic variation to be only a variation in\nobjects of interest where noisy variations are considered to be nuisance. Given any pairs of video\nframes, a TEEM aims to identify semantic differences between frames. A scenery changes between a\npair of video frames (fri,fri+j), if the maximum motion in the Moving Fields of Interest (MFI) is\ngreater than the variation threshold value τvaras follows,\nSC(fri,fri+j) ={\n1 max( MFI(fri,fri+j))>τvar\n0otherwise. (1)\nWe compute MFI set by measuring intersection over union (IoU) of each object of interest, Ok, across\nframe pairs as follows,\nMFI(ok) ={\n1−IoU(Ofri\nk,Ofri+j\nk )ifOk∈fri∧fri+j\n1 Otherwise. (2)\nEach element in MFI quantiﬁes the variation in an object of interest between a pair of video frames.\nThe scenery change metric distinguishes noisy variations from semantic variations in video frame\npairs. Figure 3 visualizes some examples of scenery change metric for video frame pair.\n3.2 Temporal Early Exit Module\nTo detect a scenery change between frame pairs (fri,fri+j), we propose TEEM which consists of\nAttention andClassiﬁer components. This section describes these components in details. Figure 2\nshows an overview of a TEEM.\nTheAttention component is represented as a function parameterized by θAttenlin equation (3).FAttn\ntakesZl\niandZl\ni+jas inputs, and outputs an attention map AttMaplwhich encodes the semantic\nvariations happened between friandfri+j. In equation (3),Zl\niandZl\ni+jare image features of\nthe previous friand currentfri+jvideo frames, respectively, encoded by convolutional layer lin\nthe feature network. The attention function captures the semantic variations between frame pairs\nin the representation space by subtracting Zl\nifromZl\ni+jin equation (4). Then, by concatenating\nthe subtraction results with the feature representation of the current video frame, Zl\ni+j, a 2D spatial\nattention map associated with the video frame pair is generated in equation (6).\nAttMapl=FAttn(Zl\ni,Zl\ni+j;θAttenl). (3)\nZsub=Zi+j−Zj, (4)\n¯Zc=Conct [Zi:Zsub], (5)\nAttMapl=Sigmoid (Conv(¯Zc)), (6)\n4', 'document_title': 'paper.pdf'}, {'page_content': 'In equations (6)and(5),Conct [; ],Conv ,Sigmoid indicate concatenation, convolutional layer and\nsigmoid function, respectively. Following [ 13], to avoid introducing any form of global normalization,\nwe use sigmoid, instead of softmax, for computing the attention map. The attention map makes\nTEEM learn and focus on the motions of objects of interest between frame pairs. Notably, a TEEM\nrequires storing the latest feature maps Zl\nito be used for successive frames.\nThe generated attention map alongside the feature map of the current video frame Zl\ni+jare fed to\naclassifier to detect any scenery change between frame pairs. The classifier component is a\nclassiﬁer with two outputs which represented as a function FClass, parameterized by θClassin equation\n(7). It takes the attention map, AttMapl, and the feature map of the current video frame Zl\ni+jand\nproduces a class label. The possible labels are changed and unchanged frame. To classify the video\nframe, the AttMaplis multiplied element-wise with the feature map of current video frame to get the\nreﬁned feature maps ZAttin equation (8). The reﬁned feature maps are passed through a convolutional\nlayer and a fully connected layer, equation (9), to produce a class label.\nClsl\ni=FClass(Zl\ni+j,AttMapl;θClass). (7)\nZAtt=AttMapl⊙Zl\ni+j. (8)\nClsl\ni=FC(ReLU (Norm (Conv(Zatt)))). (9)\nIn equation (9)FC, ReLU, Norm and Conv indicate fully connected, ReLU, batch normalization and\nconvolutional layers, respectively. Clsl\niis the output of the TEEM, located at the convolutional layer\nlof feature network. Each video frame can be classiﬁed by a TEEM located in an early exit branch\ninto a changed or unchanged pair.\n3.3 Training Policy\nAn early exit branch, including a TEEM, can be added after any convolutional layer in the feature\nnetwork. However, the location of an early exit branch in the feature network determines its\nperformance in identifying semantic variations. Therefore, a temporal early attention video object\ndetection pipeline have multiple early exits to identify semantic variations. However, each TEEM in\nan early exit branch is regarded as a stand-alone module, hence, trained separately from the backbone\nnetwork. Let (fri,fri+j)be a sampled video frame pair and θbe all parameters in TEEM in early\nexitl. For a target label, P, the objective is to minimize the cross-entropy loss as follows:\nLTEEMk(P,Pθ;θ) =−∑\nkPk·log(Pθk) (10)\nwherekis classiﬁer outputs and,\nPθ=Softmax (S), (11)\nand\nS=FTEEM l(fri,fri+j;θ). (12)\nHere,FTEEM lis the the output of the TEEM in early exit l. In the forward pass of training, a video\nframe pair is passed through the network, including both the main branch and early exits. The\noutput of each early exit is recorded to calculate the error of TEEM in the early exit. In backward\npropagation, the error of each exit is passed back only through the TEEMs and the weights of TEEM\nare updated using gradient descent. Notably, the weights of the main branch are not updated during\ntraining of the TEEMs.\n3.4 Fast inference in a Temporal Early Exit Video Object Detection Pipeline\nAlgorithm 1 summarizes the temporal early exit video object detection pipeline inference algorithm.\nGiven a pre-trained object detection network as the main branch, Fmain, withjearly exit branches,\nthe ﬁrst video frame is processed by the main branch. The inference for successive frames proceeds\nby feeding frames through the feature network of the main branch until reaching the ﬁrst early exit\nbranch. Then, the softmax and entropy of FTEEM in the early exit are calculated to evaluate the\nconﬁdence of the classiﬁer in prediction. If the entropy is less than the threshold γand the class\nlabel with the maximum score (probability) is unchanged, the inference procedure terminates and\n5', 'document_title': 'paper.pdf'}, {'page_content': 'object detection results from the previous frame are returned. If the class label with maximum score\nis changed, then the frame is processed by the main branch. For the entropy greater than γ, the frame\ncontinues to be processed by the feature network till the next early exit branch. If the softmax and\nentropy of all early exits are greater than thresholds, the frame continues to be processed by the main\nbranch.\nAlgorithm 1 Inference algorithm for video object detection\nRequire: Video frames{I}\nEnsure: detection =None\nfori←1to←length (I)do\nifi== 1 then\ndetection←Fmain(Ii)\nreturn detection\nelse\nforj←1toj= #( early exits )do\ny←FTEEM j(Ii,Ii−1)\nS←softmax (y)\ne←entropy (S)\nife<τ and arg max (S) == Unchanged then\nreturn detection\nelse ife<γ and arg max (S) == Changed then\ndetection←Fmain(Ii)\nreturn detection\nelse ife>γ then\npass\nend if\nend for\nend if\nend for\n4 Dataset\nThe CDnet dataset [ 5] provides a realistic, camera-captured, diverse set of videos for change detection.\nThe videos have been selected to cover a wide range of detection challenges and are representative\nof typical indoor and outdoor visual data captured in surveillance, smart environment. This dataset\nconsists of 33 camera-captured videos ( ∼70,000 frames) with boats, trucks, and pedestrians as objects\nof interest. It contains a range of challenging factors, including turbulence, dynamic background,\ncamera jitter challenging weather and pan-tilt-zooming (PTZ). All videos come with ground-truth\nsegmentation of motion areas for each video frame. However, CDnet dataset doesn’t provide labels\nfor the individual instances of moving objects. This makes measuring SC metric challenging and\ninaccurate in scenarios with multiple moving objects. To evaluate those scenarios, we also build\nthe scenery change dataset. Our dataset is based on VIRAT dataset [ 15] which is inspired and used\nby many change detection applications [ 17]. We used Vatic engine [ 27] to label individual moving\nobjects with bounding boxes. The dataset consists of 20 camera-captured videos with pedestrian, car\nand cyclist as objects of interest. We believe our dataset can complement existing change detection\ndatasets like CDnet[5].\n4.1 Dataset Balancing\nTo train TEEMs, video frame pairs need to be randomly sampled from videos. However, random\nsampling of frame pairs causes the unbalanced class dataset (changed scenery and unchanged scenery\nclasses). The balance of classes depends on the interval between sampled video frame pairs. For\nlong sampling interval, the dataset skews to changed scenery frame pairs. On the other hand, a short\nsampling interval leads to skewing to unchanged scenery. As a result of the unbalanced dataset,\nthe classiﬁer of TEEMs tends to ignore small classes, while concentrating on learning to classify\nlarge ones accurately. We tackle the unbalanced dataset problem by proposing the dynamic sampling\ninterval approach. To this end, we divide the semantic variation into ten classes. Each class represents\n6', 'document_title': 'paper.pdf'}]: %s
2023-12-06 14:03:34,492 - INFO - Check the results [{'page_content': '3 Temporal Early Exits Video Object Detection Pipeline\nWe propose a novel video object detection pipeline to tackle the computational complexity of video\nobject detection. An overview of the proposed pipeline is shown in ﬁgure 2. Given a sequence of\nvideo frames, multiple Temporal Early Exit Modules (TEEM) are used to build the early exit branches.\nEach early exit branch uses features extracted in the early stages of the feature network to identify\nany semantic variation between the features of consecutive video frames. If any semantic variations\nis distinguished by a TEEM, a full computation effort (main branch) will be needed to process the\nvideo frame. On the other hand, the computation process of a video frame without any semantic\nvariations will be terminated at an early exit branch and the detection results from the previous frame\nwill be reused. We describe the implementation details of the proposed pipeline in this section.\n3.1 Scenery Change\nThe key challenge in identifying feature variations is qualifying semantic dissimilarities between\nimages [ 1]. To address this challenge, we introduce a scenery change (SC)metric to quantify\nsemantic variations between video frames. We deﬁne a semantic variation to be only a variation in\nobjects of interest where noisy variations are considered to be nuisance. Given any pairs of video\nframes, a TEEM aims to identify semantic differences between frames. A scenery changes between a\npair of video frames (fri,fri+j), if the maximum motion in the Moving Fields of Interest (MFI) is\ngreater than the variation threshold value τvaras follows,\nSC(fri,fri+j) ={\n1 max( MFI(fri,fri+j))>τvar\n0otherwise. (1)\nWe compute MFI set by measuring intersection over union (IoU) of each object of interest, Ok, across\nframe pairs as follows,\nMFI(ok) ={\n1−IoU(Ofri\nk,Ofri+j\nk )ifOk∈fri∧fri+j\n1 Otherwise. (2)\nEach element in MFI quantiﬁes the variation in an object of interest between a pair of video frames.\nThe scenery change metric distinguishes noisy variations from semantic variations in video frame\npairs. Figure 3 visualizes some examples of scenery change metric for video frame pair.\n3.2 Temporal Early Exit Module\nTo detect a scenery change between frame pairs (fri,fri+j), we propose TEEM which consists of\nAttention andClassiﬁer components. This section describes these components in details. Figure 2\nshows an overview of a TEEM.\nTheAttention component is represented as a function parameterized by θAttenlin equation (3).FAttn\ntakesZl\niandZl\ni+jas inputs, and outputs an attention map AttMaplwhich encodes the semantic\nvariations happened between friandfri+j. In equation (3),Zl\niandZl\ni+jare image features of\nthe previous friand currentfri+jvideo frames, respectively, encoded by convolutional layer lin\nthe feature network. The attention function captures the semantic variations between frame pairs\nin the representation space by subtracting Zl\nifromZl\ni+jin equation (4). Then, by concatenating\nthe subtraction results with the feature representation of the current video frame, Zl\ni+j, a 2D spatial\nattention map associated with the video frame pair is generated in equation (6).\nAttMapl=FAttn(Zl\ni,Zl\ni+j;θAttenl). (3)\nZsub=Zi+j−Zj, (4)\n¯Zc=Conct [Zi:Zsub], (5)\nAttMapl=Sigmoid (Conv(¯Zc)), (6)\n4', 'document_title': 'paper.pdf'}, {'page_content': 'In equations (6)and(5),Conct [; ],Conv ,Sigmoid indicate concatenation, convolutional layer and\nsigmoid function, respectively. Following [ 13], to avoid introducing any form of global normalization,\nwe use sigmoid, instead of softmax, for computing the attention map. The attention map makes\nTEEM learn and focus on the motions of objects of interest between frame pairs. Notably, a TEEM\nrequires storing the latest feature maps Zl\nito be used for successive frames.\nThe generated attention map alongside the feature map of the current video frame Zl\ni+jare fed to\naclassifier to detect any scenery change between frame pairs. The classifier component is a\nclassiﬁer with two outputs which represented as a function FClass, parameterized by θClassin equation\n(7). It takes the attention map, AttMapl, and the feature map of the current video frame Zl\ni+jand\nproduces a class label. The possible labels are changed and unchanged frame. To classify the video\nframe, the AttMaplis multiplied element-wise with the feature map of current video frame to get the\nreﬁned feature maps ZAttin equation (8). The reﬁned feature maps are passed through a convolutional\nlayer and a fully connected layer, equation (9), to produce a class label.\nClsl\ni=FClass(Zl\ni+j,AttMapl;θClass). (7)\nZAtt=AttMapl⊙Zl\ni+j. (8)\nClsl\ni=FC(ReLU (Norm (Conv(Zatt)))). (9)\nIn equation (9)FC, ReLU, Norm and Conv indicate fully connected, ReLU, batch normalization and\nconvolutional layers, respectively. Clsl\niis the output of the TEEM, located at the convolutional layer\nlof feature network. Each video frame can be classiﬁed by a TEEM located in an early exit branch\ninto a changed or unchanged pair.\n3.3 Training Policy\nAn early exit branch, including a TEEM, can be added after any convolutional layer in the feature\nnetwork. However, the location of an early exit branch in the feature network determines its\nperformance in identifying semantic variations. Therefore, a temporal early attention video object\ndetection pipeline have multiple early exits to identify semantic variations. However, each TEEM in\nan early exit branch is regarded as a stand-alone module, hence, trained separately from the backbone\nnetwork. Let (fri,fri+j)be a sampled video frame pair and θbe all parameters in TEEM in early\nexitl. For a target label, P, the objective is to minimize the cross-entropy loss as follows:\nLTEEMk(P,Pθ;θ) =−∑\nkPk·log(Pθk) (10)\nwherekis classiﬁer outputs and,\nPθ=Softmax (S), (11)\nand\nS=FTEEM l(fri,fri+j;θ). (12)\nHere,FTEEM lis the the output of the TEEM in early exit l. In the forward pass of training, a video\nframe pair is passed through the network, including both the main branch and early exits. The\noutput of each early exit is recorded to calculate the error of TEEM in the early exit. In backward\npropagation, the error of each exit is passed back only through the TEEMs and the weights of TEEM\nare updated using gradient descent. Notably, the weights of the main branch are not updated during\ntraining of the TEEMs.\n3.4 Fast inference in a Temporal Early Exit Video Object Detection Pipeline\nAlgorithm 1 summarizes the temporal early exit video object detection pipeline inference algorithm.\nGiven a pre-trained object detection network as the main branch, Fmain, withjearly exit branches,\nthe ﬁrst video frame is processed by the main branch. The inference for successive frames proceeds\nby feeding frames through the feature network of the main branch until reaching the ﬁrst early exit\nbranch. Then, the softmax and entropy of FTEEM in the early exit are calculated to evaluate the\nconﬁdence of the classiﬁer in prediction. If the entropy is less than the threshold γand the class\nlabel with the maximum score (probability) is unchanged, the inference procedure terminates and\n5', 'document_title': 'paper.pdf'}, {'page_content': 'object detection results from the previous frame are returned. If the class label with maximum score\nis changed, then the frame is processed by the main branch. For the entropy greater than γ, the frame\ncontinues to be processed by the feature network till the next early exit branch. If the softmax and\nentropy of all early exits are greater than thresholds, the frame continues to be processed by the main\nbranch.\nAlgorithm 1 Inference algorithm for video object detection\nRequire: Video frames{I}\nEnsure: detection =None\nfori←1to←length (I)do\nifi== 1 then\ndetection←Fmain(Ii)\nreturn detection\nelse\nforj←1toj= #( early exits )do\ny←FTEEM j(Ii,Ii−1)\nS←softmax (y)\ne←entropy (S)\nife<τ and arg max (S) == Unchanged then\nreturn detection\nelse ife<γ and arg max (S) == Changed then\ndetection←Fmain(Ii)\nreturn detection\nelse ife>γ then\npass\nend if\nend for\nend if\nend for\n4 Dataset\nThe CDnet dataset [ 5] provides a realistic, camera-captured, diverse set of videos for change detection.\nThe videos have been selected to cover a wide range of detection challenges and are representative\nof typical indoor and outdoor visual data captured in surveillance, smart environment. This dataset\nconsists of 33 camera-captured videos ( ∼70,000 frames) with boats, trucks, and pedestrians as objects\nof interest. It contains a range of challenging factors, including turbulence, dynamic background,\ncamera jitter challenging weather and pan-tilt-zooming (PTZ). All videos come with ground-truth\nsegmentation of motion areas for each video frame. However, CDnet dataset doesn’t provide labels\nfor the individual instances of moving objects. This makes measuring SC metric challenging and\ninaccurate in scenarios with multiple moving objects. To evaluate those scenarios, we also build\nthe scenery change dataset. Our dataset is based on VIRAT dataset [ 15] which is inspired and used\nby many change detection applications [ 17]. We used Vatic engine [ 27] to label individual moving\nobjects with bounding boxes. The dataset consists of 20 camera-captured videos with pedestrian, car\nand cyclist as objects of interest. We believe our dataset can complement existing change detection\ndatasets like CDnet[5].\n4.1 Dataset Balancing\nTo train TEEMs, video frame pairs need to be randomly sampled from videos. However, random\nsampling of frame pairs causes the unbalanced class dataset (changed scenery and unchanged scenery\nclasses). The balance of classes depends on the interval between sampled video frame pairs. For\nlong sampling interval, the dataset skews to changed scenery frame pairs. On the other hand, a short\nsampling interval leads to skewing to unchanged scenery. As a result of the unbalanced dataset,\nthe classiﬁer of TEEMs tends to ignore small classes, while concentrating on learning to classify\nlarge ones accurately. We tackle the unbalanced dataset problem by proposing the dynamic sampling\ninterval approach. To this end, we divide the semantic variation into ten classes. Each class represents\n6', 'document_title': 'paper.pdf'}]: %s
2023-12-06 14:03:35,935 - INFO - Check the data that is being passed [{'page_content': 'Temporal Early Exits for Efﬁcient Video Object\nDetection\nAmin Sabet\nSchool of Electronics and Computer Science\nUniversity of Southampton, UK\nms4r18@soton.ac.ukJonathon Hare\nSchool of Electronics and Computer Science\nUniversity of Southampton, UK\njsh2@ecs.soton.ac.uk,\nBashir Al-Hashimi\nFaculty of Natural and Mathematical Sciences\nKing’s College London, UK\nbashir.al-hashimi@kcl.ac.ukGeoff V . Merrett\nSchool of Electronics and Computer Science\nUniversity of Southampton, UK\ngvm@ecs.soton.ac.uk\nAbstract\nTransferring image-based object detectors to the domain of video remains chal-\nlenging under resource constraints. Previous efforts utilised optical ﬂow to allow\nunchanged features to be propagated, however, the overhead is considerable when\nworking with very slowly changing scenes from applications such as surveillance.\nIn this paper, we propose temporal early exits to reduce the computational complex-\nity of per-frame video object detection. Multiple temporal early exit modules with\nlow computational overhead are inserted at early layers of the backbone network to\nidentify the semantic differences between consecutive frames. Full computation\nis only required if the frame is identiﬁed as having a semantic change to previous\nframes; otherwise, detection results from previous frames are reused. Experiments\non CDnet show that our method signiﬁcantly reduces the computational complexity\nand execution of per-frame video object detection up to 34×compared to existing\nmethods with an acceptable reduction of 2.2% in mAP.\n1 Introduction\nObject detection is one of the fundamental tasks in computer vision and serves as a core approach\nin many practical applications, such as robotics and video surveillance [ 26,2]. Object detection in\nstatic images has achieved remarkable successes in recent years using CNNs [ 7]. However, video\nobject detection has now emerged as a new challenge beyond image data. This is due to the high\ncomputational cost introduced by applying existing image object detection networks on numerous\nindividual video frames. Figure 1-(a) shows the overview of the per-frame video object detection\napproach, where all video frames are processed by a similar CNN. Deploying per-frame video object\ndetection becomes even more challenging for resource and energy-constrained applications.\nDeep optical ﬂow approaches [ 29] tackle the computational complexity challenge of video object\ndetection by taking advantage of temporal information in videos. They exploit feature similarity\nbetween consecutive frames to reduce the expensive feature computation on most frames and improve\nthe speed. Instead of extracting features of all frames by a deep CNN, deep optical ﬂow uses a lighter\nnetwork to extract, propagate and aggregate features of video frames with similar features to previous\nframes. Figure 1-(b) shows the overview of deep optical ﬂow approaches [24].\nFeature similarity between successive video frames occurs often in applications such as facility\nmonitoring and surveillance systems, where the camera is static and there are less frequently moving\nobjects in the videos [ 11]. Whilst reducing computational complexity, optical ﬂow approaches require\nPreprint. Under review.arXiv:2106.11208v1  [cs.CV]  21 Jun 2021', 'document_title': 'paper.pdf'}, {'page_content': 'Figure 1: Comparison video object detection approaches. (a) Conventional approach: applying deep\nCNNs on individual frames. (b) Deep ﬂow estimation: employing lighter ﬂow estimation network\nto propagate features across frames. (c) Proposed pipeline: identifying semantic variation in early\nstages of network and avoiding deep CNN computation for unchanged video frames\nsubstantial computational effort to generate and aggregating feature maps even though the features of\nsuccessive frames remain unchanged.\nTo address the challenge of identifying and processing frames with unchanged features, we propose\na computationally lightweight, Temporal Early Exit Module (TEEM), which identiﬁes semantic\nvariations between consecutive frames. We show that the full computational effort of a network [ 17] is\nnot required to distinguish and detect semantic changes between frames. We then use TEEM to build\na per-frame video object detection pipeline, shown in ﬁgure 1-(C). In this pipeline, a TEEM identiﬁes\nsemantic variation between features of consecutive frames in the very early stages of the feature\nnetwork. Then, the TEEM conditionally activates the deeper layers of the network if a semantic\ndifference is detected between frames. If a frame is identiﬁed to be semantically unchanged, then\nthe object detection results from the previous frame are reused. By identifying unchanged frames at\nearlier stages and thus, avoiding processing video frames, the proposed pipeline signiﬁcantly reduces\nthe computational complexity and speeds up the video object detection.\nThe contributions of this paper are as follows.\n•We propose Temporal Early Exit Modules which exploit the features of the early con-\nvolutional layers to infer a 2D spatial attention map between video frames. The atten-\ntion map encodes the semantic variations between consecutive frames. The attention map is\nthen used to generate reﬁned feature maps, feeding into a classiﬁer to classify frames into\nchanged and unchanged categories. The experiments on CDnet dataset [ 5] show that the\nTEEM classiﬁes frames into changed and unchanged classes with 94% accuracy.\n•We demonstrate a temporal early exit video object detection pipeline that uses TEEMs\nto conditionally process video frames. Full computation effort is required only for pro-\ncessing video frames with a semantic variation to previous frames. The evaluation of the\nproposed pipline on CDnet dataset [ 5] shows up to 34×speed up of per-frame video object\ndetection with less than 2.2% reduction in mAP.\n2 Related Work\nOptical Flow Estimation. Recent optical ﬂow estimation approaches divide all video frames into\nkey frame and non-key frame sets [ 30] [24] [29]. While deep networks are only applied on the\nkey-frames, a lighter ﬂow estimation network is exploited to obtain the features of non-key frames\n(shown in Figure 1), which results in speeding up the algorithm. However, the ﬂow estimation\napproaches rely on selecting the best key-frame, where the features of non-key frames are estimated\nfrom key-frame features. Deep feature ﬂow network (DFF) [ 30] adopts a simple ﬁxed key-frame\nselection scheme and fails to take account of the quality of key-frame and the optical-ﬂow. The\ndifferential network [ 24] proposes a binary differential classiﬁer network to detect the key-frames\nwith classiﬁcation accuracy of up to 76%. In order to address the challenge of ﬁnding the optimal key\nvideo frames, instead of replacing the temporal features with the features of key-frames, PSLA [ 6]\nproposes to progressively update the temporal features through a recursive feature updating network.\nTo summarise, while ﬂow estimation networks speed up video object detection, they require large\nand redundant computations for feature propagation for the majority of unchanged video frames.\n2', 'document_title': 'paper.pdf'}, {'page_content': 'Figure 2: Overview of the proposed temporal early exits video object detection pipeline.TEEMs are\nadded to feature network of a object detection network (main branch) to detect unchanged video\nframes and avoid redundant computations.\n(A)\n(B)\n(C)\nfri objects of interest i fri+j objects of interest i+j MFI (fri,fri+j)\nFigure 3: Scenery change examples, (A) shows a changed scenery with the IoU=0. (B) shows an\nunchanged scenery with the IoU=1 (C) shows a changed scenery with the IoU=0.19\nEarly Exit CNNs. In a similar line of work to ours, early exit classiﬁers promote faster inference\nby allowing classiﬁcation for easy instances to exit the network early. This class of algorithms\nare based on the observation that often the features learned at earlier stages of a deep network can\ncorrectly infer a large subset of the data samples. For example, BranchyNet [ 25] is a neural network\narchitecture which adds side branches to the main branch, the original backbone network, to allow\ncertain test samples to exit early. BranchyNet is trained by the joint optimization of loss functions for\nall exit points. In the follow-up work, conditional deep learning (CDL) [ 16] measures the efﬁciency\nimprovement due to the addition of the linear classiﬁer at each convolutional layer to ﬁnd the optimal\nlayer for early exit branches in the backbone network. Early exit approaches reduce the runtime and\nenergy consumption of image classiﬁcation. However, they have been developed only for image\nclassiﬁcation applications.\nScene Change Detection. Scene change detection is a fundamental problem in the ﬁeld of computer\nvision. The most classical approach for scene change detection is image differencing [ 21,22], which\ngenerates a change map by determining the set of pixels that are ‘signiﬁcantly different’ across two\nimages. Then, a binary mask is generated by thresholding the change map. Although, this method\nhas low computational cost, the raw pixel features are incapable of effectively differentiating between\nsemantic changes and noise. To obtain more discriminative features, image rationing [ 12], change\nvector analysis [ 4,3], Markov random ﬁelds [ 14] and dictionary learning [ 12,10] have been proposed.\nHowever, they are still sensitive to noise and illumination changes [19].\n3', 'document_title': 'paper.pdf'}]: %s
2023-12-06 14:03:35,935 - INFO - Check the results [{'page_content': 'Temporal Early Exits for Efﬁcient Video Object\nDetection\nAmin Sabet\nSchool of Electronics and Computer Science\nUniversity of Southampton, UK\nms4r18@soton.ac.ukJonathon Hare\nSchool of Electronics and Computer Science\nUniversity of Southampton, UK\njsh2@ecs.soton.ac.uk,\nBashir Al-Hashimi\nFaculty of Natural and Mathematical Sciences\nKing’s College London, UK\nbashir.al-hashimi@kcl.ac.ukGeoff V . Merrett\nSchool of Electronics and Computer Science\nUniversity of Southampton, UK\ngvm@ecs.soton.ac.uk\nAbstract\nTransferring image-based object detectors to the domain of video remains chal-\nlenging under resource constraints. Previous efforts utilised optical ﬂow to allow\nunchanged features to be propagated, however, the overhead is considerable when\nworking with very slowly changing scenes from applications such as surveillance.\nIn this paper, we propose temporal early exits to reduce the computational complex-\nity of per-frame video object detection. Multiple temporal early exit modules with\nlow computational overhead are inserted at early layers of the backbone network to\nidentify the semantic differences between consecutive frames. Full computation\nis only required if the frame is identiﬁed as having a semantic change to previous\nframes; otherwise, detection results from previous frames are reused. Experiments\non CDnet show that our method signiﬁcantly reduces the computational complexity\nand execution of per-frame video object detection up to 34×compared to existing\nmethods with an acceptable reduction of 2.2% in mAP.\n1 Introduction\nObject detection is one of the fundamental tasks in computer vision and serves as a core approach\nin many practical applications, such as robotics and video surveillance [ 26,2]. Object detection in\nstatic images has achieved remarkable successes in recent years using CNNs [ 7]. However, video\nobject detection has now emerged as a new challenge beyond image data. This is due to the high\ncomputational cost introduced by applying existing image object detection networks on numerous\nindividual video frames. Figure 1-(a) shows the overview of the per-frame video object detection\napproach, where all video frames are processed by a similar CNN. Deploying per-frame video object\ndetection becomes even more challenging for resource and energy-constrained applications.\nDeep optical ﬂow approaches [ 29] tackle the computational complexity challenge of video object\ndetection by taking advantage of temporal information in videos. They exploit feature similarity\nbetween consecutive frames to reduce the expensive feature computation on most frames and improve\nthe speed. Instead of extracting features of all frames by a deep CNN, deep optical ﬂow uses a lighter\nnetwork to extract, propagate and aggregate features of video frames with similar features to previous\nframes. Figure 1-(b) shows the overview of deep optical ﬂow approaches [24].\nFeature similarity between successive video frames occurs often in applications such as facility\nmonitoring and surveillance systems, where the camera is static and there are less frequently moving\nobjects in the videos [ 11]. Whilst reducing computational complexity, optical ﬂow approaches require\nPreprint. Under review.arXiv:2106.11208v1  [cs.CV]  21 Jun 2021', 'document_title': 'paper.pdf'}, {'page_content': 'Figure 1: Comparison video object detection approaches. (a) Conventional approach: applying deep\nCNNs on individual frames. (b) Deep ﬂow estimation: employing lighter ﬂow estimation network\nto propagate features across frames. (c) Proposed pipeline: identifying semantic variation in early\nstages of network and avoiding deep CNN computation for unchanged video frames\nsubstantial computational effort to generate and aggregating feature maps even though the features of\nsuccessive frames remain unchanged.\nTo address the challenge of identifying and processing frames with unchanged features, we propose\na computationally lightweight, Temporal Early Exit Module (TEEM), which identiﬁes semantic\nvariations between consecutive frames. We show that the full computational effort of a network [ 17] is\nnot required to distinguish and detect semantic changes between frames. We then use TEEM to build\na per-frame video object detection pipeline, shown in ﬁgure 1-(C). In this pipeline, a TEEM identiﬁes\nsemantic variation between features of consecutive frames in the very early stages of the feature\nnetwork. Then, the TEEM conditionally activates the deeper layers of the network if a semantic\ndifference is detected between frames. If a frame is identiﬁed to be semantically unchanged, then\nthe object detection results from the previous frame are reused. By identifying unchanged frames at\nearlier stages and thus, avoiding processing video frames, the proposed pipeline signiﬁcantly reduces\nthe computational complexity and speeds up the video object detection.\nThe contributions of this paper are as follows.\n•We propose Temporal Early Exit Modules which exploit the features of the early con-\nvolutional layers to infer a 2D spatial attention map between video frames. The atten-\ntion map encodes the semantic variations between consecutive frames. The attention map is\nthen used to generate reﬁned feature maps, feeding into a classiﬁer to classify frames into\nchanged and unchanged categories. The experiments on CDnet dataset [ 5] show that the\nTEEM classiﬁes frames into changed and unchanged classes with 94% accuracy.\n•We demonstrate a temporal early exit video object detection pipeline that uses TEEMs\nto conditionally process video frames. Full computation effort is required only for pro-\ncessing video frames with a semantic variation to previous frames. The evaluation of the\nproposed pipline on CDnet dataset [ 5] shows up to 34×speed up of per-frame video object\ndetection with less than 2.2% reduction in mAP.\n2 Related Work\nOptical Flow Estimation. Recent optical ﬂow estimation approaches divide all video frames into\nkey frame and non-key frame sets [ 30] [24] [29]. While deep networks are only applied on the\nkey-frames, a lighter ﬂow estimation network is exploited to obtain the features of non-key frames\n(shown in Figure 1), which results in speeding up the algorithm. However, the ﬂow estimation\napproaches rely on selecting the best key-frame, where the features of non-key frames are estimated\nfrom key-frame features. Deep feature ﬂow network (DFF) [ 30] adopts a simple ﬁxed key-frame\nselection scheme and fails to take account of the quality of key-frame and the optical-ﬂow. The\ndifferential network [ 24] proposes a binary differential classiﬁer network to detect the key-frames\nwith classiﬁcation accuracy of up to 76%. In order to address the challenge of ﬁnding the optimal key\nvideo frames, instead of replacing the temporal features with the features of key-frames, PSLA [ 6]\nproposes to progressively update the temporal features through a recursive feature updating network.\nTo summarise, while ﬂow estimation networks speed up video object detection, they require large\nand redundant computations for feature propagation for the majority of unchanged video frames.\n2', 'document_title': 'paper.pdf'}, {'page_content': 'Figure 2: Overview of the proposed temporal early exits video object detection pipeline.TEEMs are\nadded to feature network of a object detection network (main branch) to detect unchanged video\nframes and avoid redundant computations.\n(A)\n(B)\n(C)\nfri objects of interest i fri+j objects of interest i+j MFI (fri,fri+j)\nFigure 3: Scenery change examples, (A) shows a changed scenery with the IoU=0. (B) shows an\nunchanged scenery with the IoU=1 (C) shows a changed scenery with the IoU=0.19\nEarly Exit CNNs. In a similar line of work to ours, early exit classiﬁers promote faster inference\nby allowing classiﬁcation for easy instances to exit the network early. This class of algorithms\nare based on the observation that often the features learned at earlier stages of a deep network can\ncorrectly infer a large subset of the data samples. For example, BranchyNet [ 25] is a neural network\narchitecture which adds side branches to the main branch, the original backbone network, to allow\ncertain test samples to exit early. BranchyNet is trained by the joint optimization of loss functions for\nall exit points. In the follow-up work, conditional deep learning (CDL) [ 16] measures the efﬁciency\nimprovement due to the addition of the linear classiﬁer at each convolutional layer to ﬁnd the optimal\nlayer for early exit branches in the backbone network. Early exit approaches reduce the runtime and\nenergy consumption of image classiﬁcation. However, they have been developed only for image\nclassiﬁcation applications.\nScene Change Detection. Scene change detection is a fundamental problem in the ﬁeld of computer\nvision. The most classical approach for scene change detection is image differencing [ 21,22], which\ngenerates a change map by determining the set of pixels that are ‘signiﬁcantly different’ across two\nimages. Then, a binary mask is generated by thresholding the change map. Although, this method\nhas low computational cost, the raw pixel features are incapable of effectively differentiating between\nsemantic changes and noise. To obtain more discriminative features, image rationing [ 12], change\nvector analysis [ 4,3], Markov random ﬁelds [ 14] and dictionary learning [ 12,10] have been proposed.\nHowever, they are still sensitive to noise and illumination changes [19].\n3', 'document_title': 'paper.pdf'}]: %s
2023-12-06 14:03:37,448 - INFO - Check the data that is being passed [{'page_content': 'a variation threshold between frame pairs i.e., class one and two represent frame pairs with variation\nthreshold of [0, 0.1) and [0.1, 0.2), respectively. Consequently, starting from a random sampling\ninterval, we sample frame pairs, measure the minimum IoU of objects across frame pairs and classify\nthem into one of the ten classes. As the number of pairs in one class exceeds a threshold, we reject\nthe sampling that falls into the class and adjusts the sampling interval toward the direction which\nincreases classes with a smaller number of samples until all classes are balanced. We also face another\nclass unbalance issue due to videos with different lengths — i.e., some videos have thousands of\nframes while some have a few dozens of frames. We address this problem by upsampling frame pairs\nfrom shorter videos.\n5 Experiments and Results\nIn this section, we describe the experimental setup used to evaluate the performance of the temporal\nearly exit video object detection pipeline. The experimental results are also presented in this section.\n5.1 Implementation and Experimental Setup\nFor the main branch of the video object detection pipeline , Fmain, we use Faster-RCNN [ 20] with\nResnet50 and Resnet101 feature networks trained on MS COCO detection dataset [ 9]. We use\nTEE-Faster-RCNN to refer to this pipeline. Following Residual attention network [ 28], we add early\nexit branches after each Bottleneck and Basic blocks of the feature network. Therefore, TEE-Faster-\nRCNN consists of four early exits. We train TEEMs for 40 epochs using the Adam Optimizer [ 8]\nwith a learning rate of 0.001 and a batch size of 64. In both training and inference, the images have\nshorter sides of 224 pixels. We use 25K and 5k video frame pairs for training and testing, respectively.\nTraining was performed on 4 GPUs and testing run time is measured on a single GTX 1080 GPU. We\nset the entropy to 0.97 at test time. Our model is implemented using PyTorch [ 18], and our code and\ndataset will be made publicly available. We evaluate the accuracy, computational complexity, and run\ntime of TEE-Faster-RCNN.\n5.2 Classiﬁcation Accuracy of Early Exits:\nWe evaluate the accuracy of early exits using classiﬁcation accuracy, precision, recall, and F1 metrics.\nAccuracy metric enumerates the number of correct predicted changed and unchanged frames by\nTEEMs in early exit branches. Precision quantiﬁes the number of frames which predicted as the\nchanged frames and belong to the changed frame class. Recall quantiﬁes the number of changed\nframe predictions made out of all changed frame examples.\nTable 1 shows the measured accuracy, precision, recall and F1 for early exits of TEE-Faster-RCNN\nwith Resnet50 and Resnet101 feature networks. TEE-Faster-RCNN is trained for the variation\nthreshold of 0.4, equation (1). Exit1, Exit2, Exit3, and Exit4 are located after the ﬁrst, second, third,\nand fourth Bottleneck modules in the Faster-RCNN feature network, respectively. The Exit1 (with\nResnet50 feature network) classiﬁes frames into change and unchanged categories with 89% accuracy.\nThe classiﬁcation accuracy increases up to 91% and 93% in Exit2 and Exit3. The observations show\nlower accuracy for Exit4 82% compares to Exit3. We believe that lower accuracy of Exit4 is due to\nthe low-resolution feature maps used by the TEEM in early exit 4 to identify variations.\nExit4 uses 7×7low-resolution but semantically strong feature, encoding very high-level concepts,\nto identify semantic variations across frames. Since the high-level features remain often unchanged\nacross consecutive video frames, Exit4 achieves lower accuracy in classifying video frames with\nsmall semantic variation i.e., when the location of a small object changes slightly between two frames.\nHowever, Exit4 accurately detects unchanged frame pairs as well as frame pairs with signiﬁcant\nvariations.\nCompared to TEEMs, DFF [ 24] detects the key-frames (refers to changed frames) with 76% accuracy,\nyet with high computational cost. DFF concatenates the input frame pairs and sends them as a\nsix-channel depth input through a differential network to get the difference feature map. However,\ninstead of processing input frame pairs together to identify variations, we proposed to process a video\nframe once and store and reuse the intermediate feature maps to build an attention map that encodes\nthe semantic differences between frame pairs with small computational overhead.\n7', 'document_title': 'paper.pdf'}, {'page_content': 'Table 1: Accuracy, precision, recall, and F1 scores measures for TEEMs in Faster-RCNN with\nResnet50 and Resnet101 feature networks.\nResnet50 Resnet101\nAcc Pr Re F1 Acc Pr Re F1\nExit1 0.88 0.89 0.88 0.89 0.89 0.89 0.89 0.89\nExit2 0.93 0.93 0.93 0.93 0.91 0.92 0.92 0.92\nExit3 0.91 0.91 0.92 0.91 0.94 0.92 0.92 0.93\nExit4 0.86 0.82 0.86 0.84 0.85 0.83 0.87 0.85\nfri fri+jTEEM 1TEEM 2TEEM 3TEEM 4\nFigure 4: Class activation maps of video frame pairs show that TEEMs effectively learn to focus only\non the motions of the objects of interest between video frames to identify scenery change.\n5.3 Class Activation Map of TEEMs\nTo visualize the performance of TEEMs, ﬁgure 4 shows class activation map of TEEMs. Each class\nactivation map shows which parts of a video frame have contributed more to the ﬁnal output of\nthe TEEMs. Figure 4 shows that TEEMs effectively learn to focus on the moving ﬁelds of interest\nbetween frame pairs to identify variations. Futhermore, Figure 4 illustrates that TEEM1 and TEEM2\nidentify moving objects in higher resolution because the input feature maps into TEEM1 and TEEM2\nhave high resolution. The resolution of input feature maps into TEEM1 and TEEM2 are 56×56\nand28×28resolution, respectively. However, the class activation map of TEEM3 and TEEM4 are\ncoarse-grain because they use low-resolution feature maps of 14×14and7×7, respectively.\n5.4 Computational Complexity\nTable 2 compares computational cost (number of MAC operations and parameters) and run time\n(frames per second) of processing video frames by the original Faster-RCNN and TEE-Faster-RCNN\nbranches. The ﬁrst row of table 2 indicates the original Faster-RCNN network, including feature\nnetwork, region proposal network, and region-based convolutional neural network.\nExit 1, Exit2, Exit3, and Exit4 refer to computational paths of TEE-Faster-RCNN from the input of\nFaster-RCNN to TEEM1, TEEM2, TEEM3, and TEEM4, respectively. The second column of table 2\nshows the required MAC operations and parameters for each computation path of TEE-Faster-RCNN\nwith Resnet50 feature network. The third columns shows the speed of each early exit. The fourth\ncolumn shows the computational complexity the TEEM added in early exits. The second part of table\n2 shows the same results for TEE-Faster-RCNN with resnet101 feature network.\nThe original Faster-RCNN with Resnet51 feature network requires up to 134G MAC operations\nand 41M parameters to process a video frame. The high computation requirement limits the video\nobject detection speed to 18 fps, 14 fps for the Faster-RCNN with Resnet101 feature network. Using\nthe same network architecture for processing all video frames regardless of the semantic variations\nbetween neighbouring frames leads to the inferior use of limited energy and computational resources.\nHowever, the TEE-Faster-RCNN uses computationally lightweight early exit branches to process\nunchanged video frames. Table 2 shows that early exit branches have substantially less computations\nand memory requirements which speeds up processing video frames. Exit1 branch requires only\n8', 'document_title': 'paper.pdf'}, {'page_content': 'Table 2: Computational complexity and speed of TEE-Faster-RCNN video object detection.\nResnet51 Resnet101\nbranch(Op, Par) Speed TEEM(Opr,Par) branch(Op, Par) Speed TEEM(Opr,Par)\nFR (134G , 41M) 18fps - (181G , 60) 14fps -\nExit1 (1.7G , 0.525M) 628fps (0.94G , 0.3M) (1.7G, 0.525M) 597fps (0.94G , 0.3M)\nExit2 (2.7G , 2.615M) 390fps (0.93G , 1.17M) (3.7G, 2.910M) 400fps (0.93G , 1.17M)\nExit3 (3.5G , 9.733M) 263fps (0.23G , 1.19M) (9.1G, 30.195M) 140fps (0.23G , 1.19M)\nExit4 (5G , 23.808) 218fps (0.25G , 5.129M) (9.9G, 50.289M) 126fps (0.2G , 5.129M)\nTable 3: Comparing the detection accuracy of TEE-Faster-RCNN with per-frame Faster-RCNN\nUpdating ratio mAP mIoU\nFaster-RCNN 1 0.231 0.8\nFixed-Step 7 0.211 0.76\nFixed-Step 10 0.183 0.75\nFixed-Step 20 0.16 0.70\nTEE-Faster-RCNN 20 0.209 0.75\n1.7G MAC operations and uses 0.5 M parameters. Signiﬁcant reduction in computation complexity\nis because of avoided parts of the feature network, region proposal network, and region-based\nconvolutional neural network. This reduction in computations speeds up processing unchanged video\nframes up to 628 fps. The required MAC operation for Exit2, Exit3 and Exit4 branches are 2.7G,\n3.5G, and 5G, respectively. Exit2, Exit3 and Exit4 speed up processing unchanged frames to 390 fps,\n263 fps, and 218 fps, respectively. The fourth column of table 2 shows the required MAC operation\nand number of parameters for TEEMs.\n5.5 Detection Accuracy\nHaving tested the classiﬁcation performance of TEEMs, we then evaluate the mean average precision\n(mAP @0.35:0.05:0.75) and mean intersection over union (mIOU) of TEE-Faster-RCNN video object\ndetection. Table 3 compares the accuracy of TEE-Faster-RCNN detection results with the per-frame\noriginal Faster-RCNN object detection. TEE-Faster-RCNN updates the detection results at the\naverage step of 20. Therefore, for a fair comparison we performed Faster-RCNN with different ﬁxed\nupdating steps. The results reﬂect that the accuracy of TEE-Faster-RCNN cannot compete with the\nper-frame video object detection approach. Whilst, TEE-Faster-RCNN achieves the same accuracy\nof per-frame Faster-RCNN with the ﬁxed updating steps of 7, its updating step of TEE-Faster-RCNN\nis 20 on average. Notably, TEE-Faster-RCNN does not aim to improve the accuracy of detection but\nintroduces a simple yet effective approach to signiﬁcantly reduce the computational complexity of\nvideo object detection for the video applications with less frequent moving objects e.g., the CDnet\ndataset [ 5]. For applications with frequent moving objects such as the ImageNet-VID dataset [ 23],\nmore complex methods like optical ﬂow approaches are needed to achieve better accuracy.\n6 Conclusion\nWe proposes a temporal early exit object detection pipeline to reduce the computational complexity\nof per-frame video object detection. The proposed approach takes advantage of infrequent variation\nbetween features of consecutive video frames to avoid redundant computation. Video frames with\ninvariant features are identiﬁed in the early stages of the network with very low computation effort. For\nthe unchanged video frames, detection results from previous frames are reused. A full computation\neffort is only required if a video frame is identiﬁed with semantic variations compared to previous\nframes. The proposed approach accelerates per-frame video object detection up to 34×with less than\n2.2 % reduction in mAP.\n9', 'document_title': 'paper.pdf'}, {'page_content': 'References\n[1]Pablo F Alcantarilla, Simon Stent, German Ros, Roberto Arroyo, and Riccardo Gherardi. Street-\nview change detection with deconvolutional networks. Autonomous Robots , 42(7):1301–1322,\n2018.\n[2]Ali Borji, Ming-Ming Cheng, Huaizu Jiang, and Jia Li. Salient object detection: A benchmark.\nIEEE transactions on image processing , 24(12):5706–5722, 2015.\n[3]Francesca Bovolo and Lorenzo Bruzzone. A theoretical framework for unsupervised change\ndetection based on change vector analysis in the polar domain. IEEE Transactions on Geoscience\nand Remote Sensing , 45(1):218–236, 2006.\n[4]Lorenzo Bruzzone and D Fernandez Prieto. An adaptive semiparametric and context-based\napproach to unsupervised change detection in multitemporal remote-sensing images. IEEE\nTransactions on image processing , 11(4):452–466, 2002.\n[5]Nil Goyette, Pierre-Marc Jodoin, Fatih Porikli, Janusz Konrad, and Prakash Ishwar. Changede-\ntection. net: A new change detection benchmark dataset. In 2012 IEEE computer society\nconference on computer vision and pattern recognition workshops , pages 1–8. IEEE, 2012.\n[6]Chaoxu Guo, Bin Fan, Jie Gu, Qian Zhang, Shiming Xiang, Veronique Prinet, and Chunhong\nPan. Progressive sparse local attention for video object detection. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision , pages 3909–3918, 2019.\n[7]Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi,\nIan Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, et al. Speed/accuracy trade-offs\nfor modern convolutional object detectors. In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 7310–7311, 2017.\n[8]Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014.\n[9]Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European\nconference on computer vision , pages 740–755. Springer, 2014.\n[10] Xiaoqiang Lu, Yuan Yuan, and Xiangtao Zheng. Joint dictionary learning for multispectral\nchange detection. IEEE transactions on cybernetics , 47(4):884–897, 2016.\n[11] Yuan Luo, Hanxing Zhou, Qin Tan, Xuefeng Chen, and Mingjing Yun. Key frame extraction of\nsurveillance video based on moving object detection and image similarity. Pattern Recognition\nand Image Analysis , 28(2):225–231, 2018.\n[12] H MAHMOUDZADEH. Digital change detection using remotely sensed data for monitoring\ngreen space destruction in tabriz. 2007.\n[13] David Mascharka, Philip Tran, Ryan Soklaski, and Arjun Majumdar. Transparency by design:\nClosing the gap between performance and interpretability in visual reasoning. In Proceedings\nof the IEEE conference on computer vision and pattern recognition , pages 4942–4950, 2018.\n[14] Gabriele Moser, Elena Angiati, and Sebastiano B Serpico. Multiscale unsupervised change\ndetection on optical images by markov random ﬁelds and wavelets. IEEE Geoscience and\nRemote Sensing Letters , 8(4):725–729, 2011.\n[15] Sangmin Oh, Anthony Hoogs, Amitha Perera, Naresh Cuntoor, Chia-Chih Chen, Jong Taek Lee,\nSaurajit Mukherjee, JK Aggarwal, Hyungtae Lee, Larry Davis, et al. A large-scale benchmark\ndataset for event recognition in surveillance video. In CVPR 2011 , pages 3153–3160. IEEE,\n2011.\n[16] Priyadarshini Panda, Abhronil Sengupta, and Kaushik Roy. Conditional deep learning for\nenergy-efﬁcient and enhanced pattern recognition. In 2016 Design, Automation & Test in\nEurope Conference & Exhibition (DATE) , pages 475–480. IEEE, 2016.\n10', 'document_title': 'paper.pdf'}, {'page_content': '[17] Dong Huk Park, Trevor Darrell, and Anna Rohrbach. Robust change captioning. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision , pages 4624–4633, 2019.\n[18] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative\nstyle, high-performance deep learning library. arXiv preprint arXiv:1912.01703 , 2019.\n[19] Richard J Radke, Srinivas Andra, Omar Al-Kofahi, and Badrinath Roysam. Image change\ndetection algorithms: a systematic survey. IEEE transactions on image processing , 14(3):\n294–307, 2005.\n[20] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time\nobject detection with region proposal networks. arXiv preprint arXiv:1506.01497 , 2015.\n[21] Paul Rosin. Thresholding for change detection. In Sixth International Conference on Computer\nVision (IEEE Cat. No. 98CH36271) , pages 274–279. IEEE, 1998.\n[22] Paul L Rosin and Efstathios Ioannidis. Evaluation of global image thresholding for change\ndetection. Pattern recognition letters , 24(14):2345–2356, 2003.\n[23] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual\nrecognition challenge. International journal of computer vision , 115(3):211–252, 2015.\n[24] Jing Shi and Chenliang Xu. Differential network for video object detection.\n[25] Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Branchynet: Fast inference\nvia early exiting from deep neural networks. In 2016 23rd International Conference on Pattern\nRecognition (ICPR) , pages 2464–2469. IEEE, 2016.\n[26] Yonglong Tian, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning strong parts for\npedestrian detection. In Proceedings of the IEEE international conference on computer vision ,\npages 1904–1912, 2015.\n[27] Carl V ondrick, Donald Patterson, and Deva Ramanan. Efﬁciently scaling up crowdsourced\nvideo annotation. International Journal of Computer Vision , pages 1–21. ISSN 0920-5691.\nURL http://dx.doi.org/10.1007/s11263-012-0564-1 . 10.1007/s11263-012-0564-1.\n[28] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang\nWang, and Xiaoou Tang. Residual attention network for image classiﬁcation. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition , pages 3156–3164, 2017.\n[29] Philippe Weinzaepfel, Jerome Revaud, Zaid Harchaoui, and Cordelia Schmid. Deepﬂow: Large\ndisplacement optical ﬂow with deep matching. In Proceedings of the IEEE international\nconference on computer vision , pages 1385–1392, 2013.\n[30] Xizhou Zhu, Yuwen Xiong, Jifeng Dai, Lu Yuan, and Yichen Wei. Deep feature ﬂow for video\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 2349–2358, 2017.\n11', 'document_title': 'paper.pdf'}]: %s
2023-12-06 14:03:37,448 - INFO - Check the results [{'page_content': 'a variation threshold between frame pairs i.e., class one and two represent frame pairs with variation\nthreshold of [0, 0.1) and [0.1, 0.2), respectively. Consequently, starting from a random sampling\ninterval, we sample frame pairs, measure the minimum IoU of objects across frame pairs and classify\nthem into one of the ten classes. As the number of pairs in one class exceeds a threshold, we reject\nthe sampling that falls into the class and adjusts the sampling interval toward the direction which\nincreases classes with a smaller number of samples until all classes are balanced. We also face another\nclass unbalance issue due to videos with different lengths — i.e., some videos have thousands of\nframes while some have a few dozens of frames. We address this problem by upsampling frame pairs\nfrom shorter videos.\n5 Experiments and Results\nIn this section, we describe the experimental setup used to evaluate the performance of the temporal\nearly exit video object detection pipeline. The experimental results are also presented in this section.\n5.1 Implementation and Experimental Setup\nFor the main branch of the video object detection pipeline , Fmain, we use Faster-RCNN [ 20] with\nResnet50 and Resnet101 feature networks trained on MS COCO detection dataset [ 9]. We use\nTEE-Faster-RCNN to refer to this pipeline. Following Residual attention network [ 28], we add early\nexit branches after each Bottleneck and Basic blocks of the feature network. Therefore, TEE-Faster-\nRCNN consists of four early exits. We train TEEMs for 40 epochs using the Adam Optimizer [ 8]\nwith a learning rate of 0.001 and a batch size of 64. In both training and inference, the images have\nshorter sides of 224 pixels. We use 25K and 5k video frame pairs for training and testing, respectively.\nTraining was performed on 4 GPUs and testing run time is measured on a single GTX 1080 GPU. We\nset the entropy to 0.97 at test time. Our model is implemented using PyTorch [ 18], and our code and\ndataset will be made publicly available. We evaluate the accuracy, computational complexity, and run\ntime of TEE-Faster-RCNN.\n5.2 Classiﬁcation Accuracy of Early Exits:\nWe evaluate the accuracy of early exits using classiﬁcation accuracy, precision, recall, and F1 metrics.\nAccuracy metric enumerates the number of correct predicted changed and unchanged frames by\nTEEMs in early exit branches. Precision quantiﬁes the number of frames which predicted as the\nchanged frames and belong to the changed frame class. Recall quantiﬁes the number of changed\nframe predictions made out of all changed frame examples.\nTable 1 shows the measured accuracy, precision, recall and F1 for early exits of TEE-Faster-RCNN\nwith Resnet50 and Resnet101 feature networks. TEE-Faster-RCNN is trained for the variation\nthreshold of 0.4, equation (1). Exit1, Exit2, Exit3, and Exit4 are located after the ﬁrst, second, third,\nand fourth Bottleneck modules in the Faster-RCNN feature network, respectively. The Exit1 (with\nResnet50 feature network) classiﬁes frames into change and unchanged categories with 89% accuracy.\nThe classiﬁcation accuracy increases up to 91% and 93% in Exit2 and Exit3. The observations show\nlower accuracy for Exit4 82% compares to Exit3. We believe that lower accuracy of Exit4 is due to\nthe low-resolution feature maps used by the TEEM in early exit 4 to identify variations.\nExit4 uses 7×7low-resolution but semantically strong feature, encoding very high-level concepts,\nto identify semantic variations across frames. Since the high-level features remain often unchanged\nacross consecutive video frames, Exit4 achieves lower accuracy in classifying video frames with\nsmall semantic variation i.e., when the location of a small object changes slightly between two frames.\nHowever, Exit4 accurately detects unchanged frame pairs as well as frame pairs with signiﬁcant\nvariations.\nCompared to TEEMs, DFF [ 24] detects the key-frames (refers to changed frames) with 76% accuracy,\nyet with high computational cost. DFF concatenates the input frame pairs and sends them as a\nsix-channel depth input through a differential network to get the difference feature map. However,\ninstead of processing input frame pairs together to identify variations, we proposed to process a video\nframe once and store and reuse the intermediate feature maps to build an attention map that encodes\nthe semantic differences between frame pairs with small computational overhead.\n7', 'document_title': 'paper.pdf'}, {'page_content': 'Table 1: Accuracy, precision, recall, and F1 scores measures for TEEMs in Faster-RCNN with\nResnet50 and Resnet101 feature networks.\nResnet50 Resnet101\nAcc Pr Re F1 Acc Pr Re F1\nExit1 0.88 0.89 0.88 0.89 0.89 0.89 0.89 0.89\nExit2 0.93 0.93 0.93 0.93 0.91 0.92 0.92 0.92\nExit3 0.91 0.91 0.92 0.91 0.94 0.92 0.92 0.93\nExit4 0.86 0.82 0.86 0.84 0.85 0.83 0.87 0.85\nfri fri+jTEEM 1TEEM 2TEEM 3TEEM 4\nFigure 4: Class activation maps of video frame pairs show that TEEMs effectively learn to focus only\non the motions of the objects of interest between video frames to identify scenery change.\n5.3 Class Activation Map of TEEMs\nTo visualize the performance of TEEMs, ﬁgure 4 shows class activation map of TEEMs. Each class\nactivation map shows which parts of a video frame have contributed more to the ﬁnal output of\nthe TEEMs. Figure 4 shows that TEEMs effectively learn to focus on the moving ﬁelds of interest\nbetween frame pairs to identify variations. Futhermore, Figure 4 illustrates that TEEM1 and TEEM2\nidentify moving objects in higher resolution because the input feature maps into TEEM1 and TEEM2\nhave high resolution. The resolution of input feature maps into TEEM1 and TEEM2 are 56×56\nand28×28resolution, respectively. However, the class activation map of TEEM3 and TEEM4 are\ncoarse-grain because they use low-resolution feature maps of 14×14and7×7, respectively.\n5.4 Computational Complexity\nTable 2 compares computational cost (number of MAC operations and parameters) and run time\n(frames per second) of processing video frames by the original Faster-RCNN and TEE-Faster-RCNN\nbranches. The ﬁrst row of table 2 indicates the original Faster-RCNN network, including feature\nnetwork, region proposal network, and region-based convolutional neural network.\nExit 1, Exit2, Exit3, and Exit4 refer to computational paths of TEE-Faster-RCNN from the input of\nFaster-RCNN to TEEM1, TEEM2, TEEM3, and TEEM4, respectively. The second column of table 2\nshows the required MAC operations and parameters for each computation path of TEE-Faster-RCNN\nwith Resnet50 feature network. The third columns shows the speed of each early exit. The fourth\ncolumn shows the computational complexity the TEEM added in early exits. The second part of table\n2 shows the same results for TEE-Faster-RCNN with resnet101 feature network.\nThe original Faster-RCNN with Resnet51 feature network requires up to 134G MAC operations\nand 41M parameters to process a video frame. The high computation requirement limits the video\nobject detection speed to 18 fps, 14 fps for the Faster-RCNN with Resnet101 feature network. Using\nthe same network architecture for processing all video frames regardless of the semantic variations\nbetween neighbouring frames leads to the inferior use of limited energy and computational resources.\nHowever, the TEE-Faster-RCNN uses computationally lightweight early exit branches to process\nunchanged video frames. Table 2 shows that early exit branches have substantially less computations\nand memory requirements which speeds up processing video frames. Exit1 branch requires only\n8', 'document_title': 'paper.pdf'}, {'page_content': 'Table 2: Computational complexity and speed of TEE-Faster-RCNN video object detection.\nResnet51 Resnet101\nbranch(Op, Par) Speed TEEM(Opr,Par) branch(Op, Par) Speed TEEM(Opr,Par)\nFR (134G , 41M) 18fps - (181G , 60) 14fps -\nExit1 (1.7G , 0.525M) 628fps (0.94G , 0.3M) (1.7G, 0.525M) 597fps (0.94G , 0.3M)\nExit2 (2.7G , 2.615M) 390fps (0.93G , 1.17M) (3.7G, 2.910M) 400fps (0.93G , 1.17M)\nExit3 (3.5G , 9.733M) 263fps (0.23G , 1.19M) (9.1G, 30.195M) 140fps (0.23G , 1.19M)\nExit4 (5G , 23.808) 218fps (0.25G , 5.129M) (9.9G, 50.289M) 126fps (0.2G , 5.129M)\nTable 3: Comparing the detection accuracy of TEE-Faster-RCNN with per-frame Faster-RCNN\nUpdating ratio mAP mIoU\nFaster-RCNN 1 0.231 0.8\nFixed-Step 7 0.211 0.76\nFixed-Step 10 0.183 0.75\nFixed-Step 20 0.16 0.70\nTEE-Faster-RCNN 20 0.209 0.75\n1.7G MAC operations and uses 0.5 M parameters. Signiﬁcant reduction in computation complexity\nis because of avoided parts of the feature network, region proposal network, and region-based\nconvolutional neural network. This reduction in computations speeds up processing unchanged video\nframes up to 628 fps. The required MAC operation for Exit2, Exit3 and Exit4 branches are 2.7G,\n3.5G, and 5G, respectively. Exit2, Exit3 and Exit4 speed up processing unchanged frames to 390 fps,\n263 fps, and 218 fps, respectively. The fourth column of table 2 shows the required MAC operation\nand number of parameters for TEEMs.\n5.5 Detection Accuracy\nHaving tested the classiﬁcation performance of TEEMs, we then evaluate the mean average precision\n(mAP @0.35:0.05:0.75) and mean intersection over union (mIOU) of TEE-Faster-RCNN video object\ndetection. Table 3 compares the accuracy of TEE-Faster-RCNN detection results with the per-frame\noriginal Faster-RCNN object detection. TEE-Faster-RCNN updates the detection results at the\naverage step of 20. Therefore, for a fair comparison we performed Faster-RCNN with different ﬁxed\nupdating steps. The results reﬂect that the accuracy of TEE-Faster-RCNN cannot compete with the\nper-frame video object detection approach. Whilst, TEE-Faster-RCNN achieves the same accuracy\nof per-frame Faster-RCNN with the ﬁxed updating steps of 7, its updating step of TEE-Faster-RCNN\nis 20 on average. Notably, TEE-Faster-RCNN does not aim to improve the accuracy of detection but\nintroduces a simple yet effective approach to signiﬁcantly reduce the computational complexity of\nvideo object detection for the video applications with less frequent moving objects e.g., the CDnet\ndataset [ 5]. For applications with frequent moving objects such as the ImageNet-VID dataset [ 23],\nmore complex methods like optical ﬂow approaches are needed to achieve better accuracy.\n6 Conclusion\nWe proposes a temporal early exit object detection pipeline to reduce the computational complexity\nof per-frame video object detection. The proposed approach takes advantage of infrequent variation\nbetween features of consecutive video frames to avoid redundant computation. Video frames with\ninvariant features are identiﬁed in the early stages of the network with very low computation effort. For\nthe unchanged video frames, detection results from previous frames are reused. A full computation\neffort is only required if a video frame is identiﬁed with semantic variations compared to previous\nframes. The proposed approach accelerates per-frame video object detection up to 34×with less than\n2.2 % reduction in mAP.\n9', 'document_title': 'paper.pdf'}, {'page_content': 'References\n[1]Pablo F Alcantarilla, Simon Stent, German Ros, Roberto Arroyo, and Riccardo Gherardi. Street-\nview change detection with deconvolutional networks. Autonomous Robots , 42(7):1301–1322,\n2018.\n[2]Ali Borji, Ming-Ming Cheng, Huaizu Jiang, and Jia Li. Salient object detection: A benchmark.\nIEEE transactions on image processing , 24(12):5706–5722, 2015.\n[3]Francesca Bovolo and Lorenzo Bruzzone. A theoretical framework for unsupervised change\ndetection based on change vector analysis in the polar domain. IEEE Transactions on Geoscience\nand Remote Sensing , 45(1):218–236, 2006.\n[4]Lorenzo Bruzzone and D Fernandez Prieto. An adaptive semiparametric and context-based\napproach to unsupervised change detection in multitemporal remote-sensing images. IEEE\nTransactions on image processing , 11(4):452–466, 2002.\n[5]Nil Goyette, Pierre-Marc Jodoin, Fatih Porikli, Janusz Konrad, and Prakash Ishwar. Changede-\ntection. net: A new change detection benchmark dataset. In 2012 IEEE computer society\nconference on computer vision and pattern recognition workshops , pages 1–8. IEEE, 2012.\n[6]Chaoxu Guo, Bin Fan, Jie Gu, Qian Zhang, Shiming Xiang, Veronique Prinet, and Chunhong\nPan. Progressive sparse local attention for video object detection. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision , pages 3909–3918, 2019.\n[7]Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi,\nIan Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, et al. Speed/accuracy trade-offs\nfor modern convolutional object detectors. In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 7310–7311, 2017.\n[8]Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014.\n[9]Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European\nconference on computer vision , pages 740–755. Springer, 2014.\n[10] Xiaoqiang Lu, Yuan Yuan, and Xiangtao Zheng. Joint dictionary learning for multispectral\nchange detection. IEEE transactions on cybernetics , 47(4):884–897, 2016.\n[11] Yuan Luo, Hanxing Zhou, Qin Tan, Xuefeng Chen, and Mingjing Yun. Key frame extraction of\nsurveillance video based on moving object detection and image similarity. Pattern Recognition\nand Image Analysis , 28(2):225–231, 2018.\n[12] H MAHMOUDZADEH. Digital change detection using remotely sensed data for monitoring\ngreen space destruction in tabriz. 2007.\n[13] David Mascharka, Philip Tran, Ryan Soklaski, and Arjun Majumdar. Transparency by design:\nClosing the gap between performance and interpretability in visual reasoning. In Proceedings\nof the IEEE conference on computer vision and pattern recognition , pages 4942–4950, 2018.\n[14] Gabriele Moser, Elena Angiati, and Sebastiano B Serpico. Multiscale unsupervised change\ndetection on optical images by markov random ﬁelds and wavelets. IEEE Geoscience and\nRemote Sensing Letters , 8(4):725–729, 2011.\n[15] Sangmin Oh, Anthony Hoogs, Amitha Perera, Naresh Cuntoor, Chia-Chih Chen, Jong Taek Lee,\nSaurajit Mukherjee, JK Aggarwal, Hyungtae Lee, Larry Davis, et al. A large-scale benchmark\ndataset for event recognition in surveillance video. In CVPR 2011 , pages 3153–3160. IEEE,\n2011.\n[16] Priyadarshini Panda, Abhronil Sengupta, and Kaushik Roy. Conditional deep learning for\nenergy-efﬁcient and enhanced pattern recognition. In 2016 Design, Automation & Test in\nEurope Conference & Exhibition (DATE) , pages 475–480. IEEE, 2016.\n10', 'document_title': 'paper.pdf'}, {'page_content': '[17] Dong Huk Park, Trevor Darrell, and Anna Rohrbach. Robust change captioning. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision , pages 4624–4633, 2019.\n[18] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative\nstyle, high-performance deep learning library. arXiv preprint arXiv:1912.01703 , 2019.\n[19] Richard J Radke, Srinivas Andra, Omar Al-Kofahi, and Badrinath Roysam. Image change\ndetection algorithms: a systematic survey. IEEE transactions on image processing , 14(3):\n294–307, 2005.\n[20] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time\nobject detection with region proposal networks. arXiv preprint arXiv:1506.01497 , 2015.\n[21] Paul Rosin. Thresholding for change detection. In Sixth International Conference on Computer\nVision (IEEE Cat. No. 98CH36271) , pages 274–279. IEEE, 1998.\n[22] Paul L Rosin and Efstathios Ioannidis. Evaluation of global image thresholding for change\ndetection. Pattern recognition letters , 24(14):2345–2356, 2003.\n[23] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual\nrecognition challenge. International journal of computer vision , 115(3):211–252, 2015.\n[24] Jing Shi and Chenliang Xu. Differential network for video object detection.\n[25] Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Branchynet: Fast inference\nvia early exiting from deep neural networks. In 2016 23rd International Conference on Pattern\nRecognition (ICPR) , pages 2464–2469. IEEE, 2016.\n[26] Yonglong Tian, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning strong parts for\npedestrian detection. In Proceedings of the IEEE international conference on computer vision ,\npages 1904–1912, 2015.\n[27] Carl V ondrick, Donald Patterson, and Deva Ramanan. Efﬁciently scaling up crowdsourced\nvideo annotation. International Journal of Computer Vision , pages 1–21. ISSN 0920-5691.\nURL http://dx.doi.org/10.1007/s11263-012-0564-1 . 10.1007/s11263-012-0564-1.\n[28] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang\nWang, and Xiaoou Tang. Residual attention network for image classiﬁcation. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition , pages 3156–3164, 2017.\n[29] Philippe Weinzaepfel, Jerome Revaud, Zaid Harchaoui, and Cordelia Schmid. Deepﬂow: Large\ndisplacement optical ﬂow with deep matching. In Proceedings of the IEEE international\nconference on computer vision , pages 1385–1392, 2013.\n[30] Xizhou Zhu, Yuwen Xiong, Jifeng Dai, Lu Yuan, and Yichen Wei. Deep feature ﬂow for video\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 2349–2358, 2017.\n11', 'document_title': 'paper.pdf'}]: %s
2023-12-06 14:05:36,539 - INFO - Received requests to /inference endpoint
2023-12-06 14:05:36,640 - INFO - Received a batch of request with batch size of: 1 
2023-12-06 14:05:36,640 - INFO - Received request: {'username': 'amin', 'prompt': 'give me a summary of  paper', 'memory': False, 'conversation_number': 0, 'AI_assistance': False, 'collection_name': 'web', 'llm_model': 'Llama_13b'}
2023-12-06 14:06:16,541 - INFO - Processed the request successfully
2023-12-06 15:55:42,728 - INFO - request processed successfully username='amin' class_name=None mode=None vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/39b003ed377ad2b1': %s
2023-12-06 15:55:42,728 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 16:02:33,162 - INFO - request processed successfully username='amin' class_name=None mode=None vectorDB_type='Weaviate' file_path=None: %s
2023-12-06 16:02:33,162 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 16:06:03,313 - INFO - request processed successfully username='amin' class_name=None mode=None vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/739c23f5a50ad4f9': %s
2023-12-06 16:06:03,313 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 16:06:09,981 - INFO - request processed successfully username='amin' class_name=None mode=None vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/b19ae0db2bb367cf': %s
2023-12-06 16:06:09,981 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 16:08:38,388 - INFO - request processed successfully username='amin' class_name=None mode=None vectorDB_type='Weaviate' file_path=None: %s
2023-12-06 16:08:38,388 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 16:09:44,982 - INFO - request processed successfully username='amin' class_name=None mode=None vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/9d5f6182ff1d72bf': %s
2023-12-06 16:09:44,982 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 16:10:10,877 - INFO - request processed successfully username='amin' class_name=None mode=None vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/b547716aa2174f0e': %s
2023-12-06 16:10:10,877 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 16:10:31,046 - INFO - request processed successfully username='amin' class_name=None mode=None vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/a7297a2e0806883d': %s
2023-12-06 16:10:31,046 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 16:10:38,626 - INFO - request processed successfully username='amin' class_name=None mode=None vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/311bea31f43d0826': %s
2023-12-06 16:10:38,626 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 16:20:45,570 - INFO - checking the request/ username='amin' class_name='sdee' mode='create_collection' vectorDB_type='Weaviate' file_path=None: %s
2023-12-06 16:20:45,663 - INFO - checkpoint 1
2023-12-06 16:20:45,663 - INFO - checkpoint 2 amin: %s
2023-12-06 16:20:45,663 - INFO - checkpoint 2 amin_sdee: %s
2023-12-06 16:20:45,711 - INFO - class name added successfully to database
2023-12-06 16:20:45,711 - INFO - success: class sdee created for user amin
