2023-11-16 10:28:43,078 - INFO - Created a temporary directory at /tmp/tmp40rexoeq
2023-11-16 10:28:43,079 - INFO - Writing /tmp/tmp40rexoeq/_remote_module_non_scriptable.py
2023-11-16 10:28:47,464 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-16 10:29:05,462 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-16 10:29:14,176 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-16 10:43:39,420 - INFO - Created a temporary directory at /tmp/tmpgf3b0dpl
2023-11-16 10:43:39,420 - INFO - Writing /tmp/tmpgf3b0dpl/_remote_module_non_scriptable.py
2023-11-16 10:43:41,697 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-16 10:44:18,671 - INFO - Created a temporary directory at /tmp/tmp_pph2hxc
2023-11-16 10:44:18,672 - INFO - Writing /tmp/tmp_pph2hxc/_remote_module_non_scriptable.py
2023-11-16 10:44:20,979 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-16 10:44:38,533 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-16 10:44:47,288 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-16 11:08:25,502 - INFO - Created a temporary directory at /tmp/tmpewtfe5jt
2023-11-16 11:08:25,502 - INFO - Writing /tmp/tmpewtfe5jt/_remote_module_non_scriptable.py
2023-11-16 11:08:27,842 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-16 11:08:45,263 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-16 11:08:54,039 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-16 11:12:37,695 - INFO - Created a temporary directory at /tmp/tmpadnpm7or
2023-11-16 11:12:37,695 - INFO - Writing /tmp/tmpadnpm7or/_remote_module_non_scriptable.py
2023-11-16 11:12:40,036 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-16 11:17:30,520 - INFO - Created a temporary directory at /tmp/tmpozgmf3cd
2023-11-16 11:17:30,520 - INFO - Writing /tmp/tmpozgmf3cd/_remote_module_non_scriptable.py
2023-11-16 11:17:32,842 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-16 11:17:50,594 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-16 11:17:59,362 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-16 13:56:50,799 - INFO - Created a temporary directory at /tmp/tmp6xltg6vk
2023-11-16 13:56:50,799 - INFO - Writing /tmp/tmp6xltg6vk/_remote_module_non_scriptable.py
2023-11-16 13:56:53,130 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-16 13:57:10,786 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-16 13:57:19,527 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-16 16:38:08,187 - INFO - Created a temporary directory at /tmp/tmp2x1h8gqn
2023-11-16 16:38:08,187 - INFO - Writing /tmp/tmp2x1h8gqn/_remote_module_non_scriptable.py
2023-11-16 16:38:10,460 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-16 16:38:28,054 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-16 16:38:36,787 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 13:17:26,890 - INFO - Created a temporary directory at /tmp/tmpic_bcnim
2023-11-17 13:17:26,890 - INFO - Writing /tmp/tmpic_bcnim/_remote_module_non_scriptable.py
2023-11-17 13:17:29,177 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-17 13:17:46,706 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-17 13:17:55,422 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 13:20:00,432 - INFO - Created a temporary directory at /tmp/tmpjh79yy92
2023-11-17 13:20:00,432 - INFO - Writing /tmp/tmpjh79yy92/_remote_module_non_scriptable.py
2023-11-17 13:20:02,751 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-17 13:20:20,300 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-17 13:20:29,076 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 13:32:14,753 - INFO - Created a temporary directory at /tmp/tmpwzm8z2rt
2023-11-17 13:32:14,754 - INFO - Writing /tmp/tmpwzm8z2rt/_remote_module_non_scriptable.py
2023-11-17 13:32:17,060 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-17 13:32:34,612 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-17 13:32:43,423 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 13:43:52,096 - INFO - Created a temporary directory at /tmp/tmpnpblkxu1
2023-11-17 13:43:52,096 - INFO - Writing /tmp/tmpnpblkxu1/_remote_module_non_scriptable.py
2023-11-17 13:43:54,422 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-17 13:44:11,946 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-17 13:44:20,682 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 13:48:19,994 - INFO - Created a temporary directory at /tmp/tmpfbq3u_0v
2023-11-17 13:48:19,995 - INFO - Writing /tmp/tmpfbq3u_0v/_remote_module_non_scriptable.py
2023-11-17 13:48:22,333 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-17 13:48:39,786 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-17 13:48:48,510 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 13:52:55,427 - INFO - Created a temporary directory at /tmp/tmpex8gn7x6
2023-11-17 13:52:55,427 - INFO - Writing /tmp/tmpex8gn7x6/_remote_module_non_scriptable.py
2023-11-17 13:52:57,737 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-17 13:53:15,238 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-17 13:53:23,986 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 14:53:17,942 - INFO - Created a temporary directory at /tmp/tmpswd7owy1
2023-11-17 14:53:17,942 - INFO - Writing /tmp/tmpswd7owy1/_remote_module_non_scriptable.py
2023-11-17 14:53:20,222 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-17 14:53:37,626 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-17 14:53:46,323 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 15:14:50,176 - INFO - Created a temporary directory at /tmp/tmpivtdxefc
2023-11-17 15:14:50,176 - INFO - Writing /tmp/tmpivtdxefc/_remote_module_non_scriptable.py
2023-11-17 15:14:52,494 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-17 15:15:10,137 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-17 15:15:18,880 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 15:17:43,855 - INFO - Created a temporary directory at /tmp/tmp5xd6gl_f
2023-11-17 15:17:43,855 - INFO - Writing /tmp/tmp5xd6gl_f/_remote_module_non_scriptable.py
2023-11-17 15:17:46,210 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-17 15:18:03,816 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-17 15:18:12,547 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 15:20:13,075 - INFO - Created a temporary directory at /tmp/tmpsckwbjlh
2023-11-17 15:20:13,075 - INFO - Writing /tmp/tmpsckwbjlh/_remote_module_non_scriptable.py
2023-11-17 15:20:15,416 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-17 15:20:32,864 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-17 15:20:41,613 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 15:29:03,756 - INFO - Received requests to /inference endpoint
2023-11-17 15:29:03,857 - INFO - Received a batch of request with batch size of: 1 
2023-11-17 15:29:03,858 - INFO - Received request: {'username': 'amin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-17 15:29:03,860 - ERROR - Error processing the request: name 'time' is not defined
2023-11-17 15:29:15,146 - INFO - Received requests to /inference endpoint
2023-11-17 15:29:15,247 - INFO - Received a batch of request with batch size of: 1 
2023-11-17 15:29:15,247 - INFO - Received request: {'username': 'amin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-17 15:29:15,248 - ERROR - Error processing the request: name 'time' is not defined
2023-11-20 11:14:02,694 - INFO - Created a temporary directory at /tmp/tmpn0s4u1ju
2023-11-20 11:14:02,695 - INFO - Writing /tmp/tmpn0s4u1ju/_remote_module_non_scriptable.py
2023-11-20 11:14:09,191 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-20 11:24:47,681 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-20 11:25:10,405 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-20 11:28:22,467 - INFO - Created a temporary directory at /tmp/tmp47as7c_x
2023-11-20 11:28:22,467 - INFO - Writing /tmp/tmp47as7c_x/_remote_module_non_scriptable.py
2023-11-20 11:28:24,761 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-20 11:28:45,695 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-20 11:28:54,618 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-20 12:04:30,088 - INFO - Received requests to /inference endpoint
2023-11-20 12:04:30,189 - INFO - Received a batch of request with batch size of: 1 
2023-11-20 12:04:30,190 - INFO - Received request: {'username': 'amin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-20 12:04:30,192 - ERROR - Error processing the request: name 'time' is not defined
2023-11-20 12:11:51,021 - INFO - Created a temporary directory at /tmp/tmppj91djaj
2023-11-20 12:11:51,021 - INFO - Writing /tmp/tmppj91djaj/_remote_module_non_scriptable.py
2023-11-20 12:11:53,325 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-20 12:12:10,995 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-20 12:12:19,779 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-20 12:14:28,694 - INFO - Received requests to /inference endpoint
2023-11-20 12:14:28,795 - INFO - Received a batch of request with batch size of: 1 
2023-11-20 12:14:28,795 - INFO - Received request: {'username': 'amin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-20 12:14:28,798 - ERROR - Error processing the request: name 'time' is not defined
2023-11-20 12:15:30,904 - INFO - Received requests to /inference endpoint
2023-11-20 12:15:31,005 - INFO - Received a batch of request with batch size of: 1 
2023-11-20 12:15:31,005 - INFO - Received request: {'username': 'amin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-20 12:15:31,005 - ERROR - Error processing the request: name 'time' is not defined
2023-11-20 12:17:30,660 - INFO - Created a temporary directory at /tmp/tmpjb1zh064
2023-11-20 12:17:30,660 - INFO - Writing /tmp/tmpjb1zh064/_remote_module_non_scriptable.py
2023-11-20 12:17:32,965 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-20 12:17:50,498 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-20 12:17:59,256 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-20 12:20:08,787 - INFO - Received requests to /inference endpoint
2023-11-20 12:20:08,888 - INFO - Received a batch of request with batch size of: 1 
2023-11-20 12:20:08,888 - INFO - Received request: {'username': 'amin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-20 12:20:17,930 - INFO - Processed the request successfully
2023-11-20 12:25:38,633 - INFO - Created a temporary directory at /tmp/tmp6j752gcz
2023-11-20 12:25:38,633 - INFO - Writing /tmp/tmp6j752gcz/_remote_module_non_scriptable.py
2023-11-20 12:25:40,899 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-20 12:25:58,258 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-20 12:26:06,966 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-20 12:26:36,396 - INFO - Received requests to /inference endpoint
2023-11-20 12:26:36,497 - INFO - Received a batch of request with batch size of: 1 
2023-11-20 12:26:36,497 - INFO - Received request: {'username': 'amin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-20 12:26:43,258 - INFO - Processed the request successfully
2023-11-20 12:30:25,377 - INFO - Received requests to /inference endpoint
2023-11-20 12:30:25,478 - INFO - Received a batch of request with batch size of: 1 
2023-11-20 12:30:25,478 - INFO - Received request: {'username': 'amin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-20 12:30:31,658 - INFO - Processed the request successfully
2023-11-20 12:32:34,636 - INFO - Received requests to /inference endpoint
2023-11-20 12:32:34,738 - INFO - Received a batch of request with batch size of: 1 
2023-11-20 12:32:34,738 - INFO - Received request: {'username': 'admin', 'prompt': 'HI', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-20 12:32:40,799 - INFO - Processed the request successfully
2023-11-20 12:53:57,962 - INFO - Created a temporary directory at /tmp/tmpmccgt757
2023-11-20 12:53:57,962 - INFO - Writing /tmp/tmpmccgt757/_remote_module_non_scriptable.py
2023-11-20 12:54:00,281 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-20 12:54:17,694 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-20 12:54:26,424 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-20 12:54:36,397 - ERROR - An error occurred: 'VDBaseInput' object has no attribute 'mode'
2023-11-20 12:56:12,810 - INFO - Created a temporary directory at /tmp/tmp_3e5c5ut
2023-11-20 12:56:12,810 - INFO - Writing /tmp/tmp_3e5c5ut/_remote_module_non_scriptable.py
2023-11-20 12:56:15,107 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-20 12:56:32,403 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-20 12:56:41,107 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-20 12:59:54,117 - INFO - Created a temporary directory at /tmp/tmptro6pmkp
2023-11-20 12:59:54,117 - INFO - Writing /tmp/tmptro6pmkp/_remote_module_non_scriptable.py
2023-11-20 12:59:56,401 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-20 13:00:13,970 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-20 13:00:22,694 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-20 13:02:02,511 - INFO - Collection created for user admin: {'collection_name': 'admin_xc'}
2023-11-20 13:02:11,864 - INFO - Collection created for user amin: {'collection_name': 'amin_xc'}
2023-11-20 13:05:18,411 - INFO - Received requests to /inference endpoint
2023-11-20 13:05:18,512 - INFO - Received a batch of request with batch size of: 1 
2023-11-20 13:05:18,513 - INFO - Received request: {'username': 'admin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-20 13:05:25,325 - INFO - Processed the request successfully
2023-11-20 13:11:41,928 - INFO - Collection created for user amin: {'collection_name': 'amin_xc'}
2023-11-20 13:12:33,821 - INFO - Collection created for user admin: {'collection_name': 'admin_vc'}
2023-11-20 14:16:56,429 - INFO - Created a temporary directory at /tmp/tmp4wtg9gbw
2023-11-20 14:16:56,429 - INFO - Writing /tmp/tmp4wtg9gbw/_remote_module_non_scriptable.py
2023-11-20 14:16:58,723 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-20 14:17:16,193 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-20 14:17:24,931 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-20 14:17:28,736 - INFO - Collection created for user admin: {'collection_name': 'admin_ff'}
2023-11-20 14:36:18,105 - INFO - Collection created for user admin: {'collection_name': 'admin_None'}
2023-11-20 14:36:28,260 - INFO - Collection created for user admin: {'collection_name': 'admin_zz'}
2023-11-21 10:08:48,545 - INFO - Collection created for user admin: {'collection_name': 'admin_test1'}
2023-11-21 10:09:28,307 - INFO - Collection created for user admin: {'collection_name': 'admin_test2'}
2023-11-21 10:26:38,196 - INFO -  file Car rental doc.pdf received for user admin: 
2023-11-21 10:26:38,218 - INFO - Collection created for user admin: {'collection_name': 'admin_test3'}
2023-11-21 11:48:17,956 - INFO - request processed successfully username='admin' collection_name=None mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/admin_Car rental doc.pdf': %s
2023-11-21 12:14:49,986 - INFO - Unable to poll TPU GCE metadata: HTTPConnectionPool(host='metadata.google.internal', port=80): Max retries exceeded with url: /computeMetadata/v1/instance/attributes/accelerator-type (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7efd24c6cf40>: Failed to establish a new connection: [Errno -2] Name or service not known'))
2023-11-21 12:27:14,382 - INFO - Unable to poll TPU GCE metadata: HTTPConnectionPool(host='metadata.google.internal', port=80): Max retries exceeded with url: /computeMetadata/v1/instance/attributes/accelerator-type (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fbe397a4f70>: Failed to establish a new connection: [Errno -2] Name or service not known'))
2023-11-21 12:31:03,268 - INFO - Unable to poll TPU GCE metadata: HTTPConnectionPool(host='metadata.google.internal', port=80): Max retries exceeded with url: /computeMetadata/v1/instance/attributes/accelerator-type (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7efdc1655090>: Failed to establish a new connection: [Errno -2] Name or service not known'))
2023-11-21 12:42:19,617 - INFO - request processed successfully username='admin' collection_name='Admin_pdf' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files': %s
2023-11-23 11:12:51,049 - INFO - Created a temporary directory at /tmp/tmp1uh5qtym
2023-11-23 11:12:51,049 - INFO - Writing /tmp/tmp1uh5qtym/_remote_module_non_scriptable.py
2023-11-23 11:12:57,415 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 11:23:44,404 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 11:24:08,688 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 12:09:17,103 - INFO - Created a temporary directory at /tmp/tmpn7qlos_4
2023-11-23 12:09:17,104 - INFO - Writing /tmp/tmpn7qlos_4/_remote_module_non_scriptable.py
2023-11-23 12:09:19,414 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:09:40,259 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 12:09:49,126 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 12:23:24,289 - INFO - Created a temporary directory at /tmp/tmp52ctgij0
2023-11-23 12:23:24,290 - INFO - Writing /tmp/tmp52ctgij0/_remote_module_non_scriptable.py
2023-11-23 12:23:26,577 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:23:44,091 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 12:23:52,776 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 12:23:59,341 - INFO - Created a temporary directory at /tmp/tmp50ngzdjy
2023-11-23 12:23:59,341 - INFO - Writing /tmp/tmp50ngzdjy/_remote_module_non_scriptable.py
2023-11-23 12:24:01,677 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:24:24,679 - INFO - Created a temporary directory at /tmp/tmpmqxeqq11
2023-11-23 12:24:24,680 - INFO - Writing /tmp/tmpmqxeqq11/_remote_module_non_scriptable.py
2023-11-23 12:24:27,032 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:24:49,788 - INFO - Created a temporary directory at /tmp/tmp2qwruxb3
2023-11-23 12:24:49,788 - INFO - Writing /tmp/tmp2qwruxb3/_remote_module_non_scriptable.py
2023-11-23 12:24:52,138 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:25:14,908 - INFO - Created a temporary directory at /tmp/tmpnfixem8q
2023-11-23 12:25:14,908 - INFO - Writing /tmp/tmpnfixem8q/_remote_module_non_scriptable.py
2023-11-23 12:25:17,245 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:25:40,456 - INFO - Created a temporary directory at /tmp/tmprmnm6fnz
2023-11-23 12:25:40,456 - INFO - Writing /tmp/tmprmnm6fnz/_remote_module_non_scriptable.py
2023-11-23 12:25:42,829 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:26:05,709 - INFO - Created a temporary directory at /tmp/tmpte34exz3
2023-11-23 12:26:05,709 - INFO - Writing /tmp/tmpte34exz3/_remote_module_non_scriptable.py
2023-11-23 12:26:08,037 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:26:31,138 - INFO - Created a temporary directory at /tmp/tmp4rmjy08y
2023-11-23 12:26:31,138 - INFO - Writing /tmp/tmp4rmjy08y/_remote_module_non_scriptable.py
2023-11-23 12:26:33,432 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:26:56,541 - INFO - Created a temporary directory at /tmp/tmp4p9ddlmp
2023-11-23 12:26:56,541 - INFO - Writing /tmp/tmp4p9ddlmp/_remote_module_non_scriptable.py
2023-11-23 12:26:58,910 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:27:29,210 - INFO - Created a temporary directory at /tmp/tmp1li9fg41
2023-11-23 12:27:29,211 - INFO - Writing /tmp/tmp1li9fg41/_remote_module_non_scriptable.py
2023-11-23 12:27:31,546 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:28:33,686 - INFO - Created a temporary directory at /tmp/tmpfh7j7eez
2023-11-23 12:28:33,686 - INFO - Writing /tmp/tmpfh7j7eez/_remote_module_non_scriptable.py
2023-11-23 12:28:36,034 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:29:38,654 - INFO - Created a temporary directory at /tmp/tmp74ismo3e
2023-11-23 12:29:38,654 - INFO - Writing /tmp/tmp74ismo3e/_remote_module_non_scriptable.py
2023-11-23 12:29:40,982 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:30:42,780 - INFO - Created a temporary directory at /tmp/tmp4dbh3c2j
2023-11-23 12:30:42,780 - INFO - Writing /tmp/tmp4dbh3c2j/_remote_module_non_scriptable.py
2023-11-23 12:30:45,146 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:31:47,314 - INFO - Created a temporary directory at /tmp/tmpjass4rlb
2023-11-23 12:31:47,314 - INFO - Writing /tmp/tmpjass4rlb/_remote_module_non_scriptable.py
2023-11-23 12:31:49,690 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:32:52,257 - INFO - Created a temporary directory at /tmp/tmpo4l9kwnc
2023-11-23 12:32:52,257 - INFO - Writing /tmp/tmpo4l9kwnc/_remote_module_non_scriptable.py
2023-11-23 12:32:54,574 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:33:56,892 - INFO - Created a temporary directory at /tmp/tmpozv5f5k2
2023-11-23 12:33:56,892 - INFO - Writing /tmp/tmpozv5f5k2/_remote_module_non_scriptable.py
2023-11-23 12:33:59,197 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:35:01,320 - INFO - Created a temporary directory at /tmp/tmpyyk7iue2
2023-11-23 12:35:01,321 - INFO - Writing /tmp/tmpyyk7iue2/_remote_module_non_scriptable.py
2023-11-23 12:35:03,681 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:37:10,276 - INFO - Created a temporary directory at /tmp/tmpyfr5vwwk
2023-11-23 12:37:10,276 - INFO - Writing /tmp/tmpyfr5vwwk/_remote_module_non_scriptable.py
2023-11-23 12:37:12,655 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:38:15,069 - INFO - Created a temporary directory at /tmp/tmp3lf146a0
2023-11-23 12:38:15,069 - INFO - Writing /tmp/tmp3lf146a0/_remote_module_non_scriptable.py
2023-11-23 12:38:17,415 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:39:20,351 - INFO - Created a temporary directory at /tmp/tmppq5_4gwz
2023-11-23 12:39:20,351 - INFO - Writing /tmp/tmppq5_4gwz/_remote_module_non_scriptable.py
2023-11-23 12:39:22,661 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:40:25,264 - INFO - Created a temporary directory at /tmp/tmp63psze5p
2023-11-23 12:40:25,265 - INFO - Writing /tmp/tmp63psze5p/_remote_module_non_scriptable.py
2023-11-23 12:40:27,610 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:41:30,310 - INFO - Created a temporary directory at /tmp/tmpys1qln4v
2023-11-23 12:41:30,310 - INFO - Writing /tmp/tmpys1qln4v/_remote_module_non_scriptable.py
2023-11-23 12:41:32,657 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:42:34,582 - INFO - Created a temporary directory at /tmp/tmpx97u88py
2023-11-23 12:42:34,582 - INFO - Writing /tmp/tmpx97u88py/_remote_module_non_scriptable.py
2023-11-23 12:42:36,917 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:43:39,615 - INFO - Created a temporary directory at /tmp/tmpb7v8wpbr
2023-11-23 12:43:39,615 - INFO - Writing /tmp/tmpb7v8wpbr/_remote_module_non_scriptable.py
2023-11-23 12:43:41,966 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:44:44,212 - INFO - Created a temporary directory at /tmp/tmp20e82bof
2023-11-23 12:44:44,212 - INFO - Writing /tmp/tmp20e82bof/_remote_module_non_scriptable.py
2023-11-23 12:44:46,573 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:45:48,322 - INFO - Created a temporary directory at /tmp/tmpfm0muokk
2023-11-23 12:45:48,323 - INFO - Writing /tmp/tmpfm0muokk/_remote_module_non_scriptable.py
2023-11-23 12:45:50,682 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:46:52,508 - INFO - Created a temporary directory at /tmp/tmpnr_tekat
2023-11-23 12:46:52,509 - INFO - Writing /tmp/tmpnr_tekat/_remote_module_non_scriptable.py
2023-11-23 12:46:54,881 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:47:57,043 - INFO - Created a temporary directory at /tmp/tmpb138zrkz
2023-11-23 12:47:57,044 - INFO - Writing /tmp/tmpb138zrkz/_remote_module_non_scriptable.py
2023-11-23 12:47:59,379 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:49:01,198 - INFO - Created a temporary directory at /tmp/tmp26lisu9h
2023-11-23 12:49:01,198 - INFO - Writing /tmp/tmp26lisu9h/_remote_module_non_scriptable.py
2023-11-23 12:49:03,521 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:50:05,792 - INFO - Created a temporary directory at /tmp/tmp61mdubsu
2023-11-23 12:50:05,792 - INFO - Writing /tmp/tmp61mdubsu/_remote_module_non_scriptable.py
2023-11-23 12:50:08,122 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:51:11,560 - INFO - Created a temporary directory at /tmp/tmp_yh7aoo4
2023-11-23 12:51:11,560 - INFO - Writing /tmp/tmp_yh7aoo4/_remote_module_non_scriptable.py
2023-11-23 12:51:13,964 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:52:14,714 - INFO - Created a temporary directory at /tmp/tmp8pyhkrew
2023-11-23 12:52:14,715 - INFO - Writing /tmp/tmp8pyhkrew/_remote_module_non_scriptable.py
2023-11-23 12:52:17,070 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:53:19,247 - INFO - Created a temporary directory at /tmp/tmpdcagsmgx
2023-11-23 12:53:19,247 - INFO - Writing /tmp/tmpdcagsmgx/_remote_module_non_scriptable.py
2023-11-23 12:53:21,603 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:54:24,034 - INFO - Created a temporary directory at /tmp/tmp2uf87urb
2023-11-23 12:54:24,034 - INFO - Writing /tmp/tmp2uf87urb/_remote_module_non_scriptable.py
2023-11-23 12:54:26,392 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:55:28,963 - INFO - Created a temporary directory at /tmp/tmp3awgtdft
2023-11-23 12:55:28,964 - INFO - Writing /tmp/tmp3awgtdft/_remote_module_non_scriptable.py
2023-11-23 12:55:31,325 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:56:32,894 - INFO - Created a temporary directory at /tmp/tmpe_9rfdka
2023-11-23 12:56:32,894 - INFO - Writing /tmp/tmpe_9rfdka/_remote_module_non_scriptable.py
2023-11-23 12:56:35,237 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:57:37,015 - INFO - Created a temporary directory at /tmp/tmphcwlzw3g
2023-11-23 12:57:37,015 - INFO - Writing /tmp/tmphcwlzw3g/_remote_module_non_scriptable.py
2023-11-23 12:57:39,388 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:58:02,698 - INFO - Created a temporary directory at /tmp/tmp6pms8at1
2023-11-23 12:58:02,698 - INFO - Writing /tmp/tmp6pms8at1/_remote_module_non_scriptable.py
2023-11-23 12:58:05,044 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:58:22,618 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 12:58:31,300 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 13:13:29,302 - INFO - Created a temporary directory at /tmp/tmpuutsaogr
2023-11-23 13:13:29,302 - INFO - Writing /tmp/tmpuutsaogr/_remote_module_non_scriptable.py
2023-11-23 13:13:31,609 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 13:13:49,189 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 13:13:57,927 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 13:18:05,478 - INFO - Created a temporary directory at /tmp/tmp4cobtbia
2023-11-23 13:18:05,478 - INFO - Writing /tmp/tmp4cobtbia/_remote_module_non_scriptable.py
2023-11-23 13:18:07,744 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 13:18:25,232 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 13:18:33,889 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 13:21:14,851 - INFO - Received requests to /inference endpoint
2023-11-23 13:21:14,953 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 13:21:14,953 - INFO - Received request: {'username': 'admin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-23 13:21:24,101 - INFO - Processed the request successfully
2023-11-23 13:23:15,418 - INFO - Created a temporary directory at /tmp/tmpk2suv46u
2023-11-23 13:23:15,419 - INFO - Writing /tmp/tmpk2suv46u/_remote_module_non_scriptable.py
2023-11-23 13:23:17,554 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 13:23:35,938 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 13:23:44,634 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 13:23:50,599 - INFO - Created a temporary directory at /tmp/tmp5c09l6yy
2023-11-23 13:23:50,599 - INFO - Writing /tmp/tmp5c09l6yy/_remote_module_non_scriptable.py
2023-11-23 13:23:52,784 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 13:24:11,888 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 13:24:20,615 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 13:25:55,503 - INFO - Received requests to /inference endpoint
2023-11-23 13:25:55,604 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 13:25:55,604 - INFO - Received request: {'username': 'admin', 'prompt': 'hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-23 13:26:02,572 - INFO - Processed the request successfully
2023-11-23 14:33:21,216 - INFO - Created a temporary directory at /tmp/tmpjb8ykvvk
2023-11-23 14:33:21,216 - INFO - Writing /tmp/tmpjb8ykvvk/_remote_module_non_scriptable.py
2023-11-23 14:33:23,368 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 14:33:40,879 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 14:33:49,607 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 14:33:56,713 - INFO - Created a temporary directory at /tmp/tmpbckzbvbo
2023-11-23 14:33:56,713 - INFO - Writing /tmp/tmpbckzbvbo/_remote_module_non_scriptable.py
2023-11-23 14:33:58,978 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 14:34:16,672 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 14:34:25,394 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 14:42:58,320 - INFO - Received requests to /inference endpoint
2023-11-23 14:42:58,421 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 14:42:58,422 - INFO - Received request: {'username': 'admin', 'prompt': 'hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'llama_70b'}
2023-11-23 14:43:05,361 - INFO - Processed the request successfully
2023-11-23 14:43:12,311 - INFO - Received requests to /inference endpoint
2023-11-23 14:43:12,412 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 14:43:12,413 - INFO - Received request: {'username': 'admin', 'prompt': 'hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'llama_13b'}
2023-11-23 14:43:19,189 - INFO - Processed the request successfully
2023-11-23 15:30:56,689 - INFO - Received requests to /inference endpoint
2023-11-23 15:30:56,790 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 15:30:56,790 - INFO - Received request: {'username': 'alex', 'prompt': 'hi', 'memory': False, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'llama_70b'}
2023-11-23 15:31:03,235 - INFO - Processed the request successfully
2023-11-23 15:39:21,434 - INFO - Received requests to /inference endpoint
2023-11-23 15:39:21,535 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 15:39:21,535 - INFO - Received request: {'username': 'alex', 'prompt': 'Hi', 'memory': False, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-11-23 15:39:27,700 - INFO - Processed the request successfully
2023-11-23 15:39:37,523 - INFO - Received requests to /inference endpoint
2023-11-23 15:39:37,625 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 15:39:37,625 - INFO - Received request: {'username': 'alex', 'prompt': 'Hi', 'memory': False, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-23 15:39:43,538 - INFO - Processed the request successfully
2023-11-23 15:44:58,145 - INFO - Received requests to /inference endpoint
2023-11-23 15:44:58,246 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 15:44:58,246 - INFO - Received request: {'username': 'alex', 'prompt': 'my name is Amin', 'memory': False, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-23 15:45:02,916 - INFO - Processed the request successfully
2023-11-23 15:45:14,247 - INFO - Received requests to /inference endpoint
2023-11-23 15:45:14,348 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 15:45:14,348 - INFO - Received request: {'username': 'alex', 'prompt': 'what was my previous question ?', 'memory': False, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-23 15:45:23,367 - INFO - Processed the request successfully
2023-11-23 15:45:40,795 - INFO - Received requests to /inference endpoint
2023-11-23 15:45:40,896 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 15:45:40,896 - INFO - Received request: {'username': 'alex', 'prompt': 'I told you name, what was my name?', 'memory': False, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-23 15:45:46,943 - INFO - Processed the request successfully
2023-11-23 15:46:04,201 - INFO - Received requests to /inference endpoint
2023-11-23 15:46:04,302 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 15:46:04,302 - INFO - Received request: {'username': 'alex', 'prompt': 'my name is Amin', 'memory': False, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-23 15:46:08,827 - INFO - Processed the request successfully
2023-11-23 15:46:15,746 - INFO - Received requests to /inference endpoint
2023-11-23 15:46:15,847 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 15:46:15,847 - INFO - Received request: {'username': 'alex', 'prompt': 'what was my name?', 'memory': False, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-23 15:46:25,574 - INFO - Processed the request successfully
2023-11-23 16:00:44,051 - INFO - Received requests to /inference endpoint
2023-11-23 16:00:44,152 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 16:00:44,152 - INFO - Received request: {'username': 'alex', 'prompt': 'where is London?', 'memory': False, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-23 16:00:58,162 - INFO - Processed the request successfully
2023-11-23 16:07:32,433 - INFO - Received requests to /inference endpoint
2023-11-23 16:07:32,534 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 16:07:32,534 - INFO - Received request: {'username': 'alex', 'prompt': 'what was my previous question about?', 'memory': False, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-23 16:07:41,463 - INFO - Processed the request successfully
2023-11-23 16:12:21,583 - INFO - Received requests to /inference endpoint
2023-11-23 16:12:21,684 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 16:12:21,684 - INFO - Received request: {'username': 'alex', 'prompt': 'hi', 'memory': False, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-23 16:12:28,046 - INFO - Processed the request successfully
2023-11-23 16:14:54,577 - INFO - Created a temporary directory at /tmp/tmpe76emsue
2023-11-23 16:14:54,577 - INFO - Writing /tmp/tmpe76emsue/_remote_module_non_scriptable.py
2023-11-23 16:14:56,681 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 16:15:14,381 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 16:15:23,087 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 16:15:29,688 - INFO - Created a temporary directory at /tmp/tmpj2a3qw3q
2023-11-23 16:15:29,689 - INFO - Writing /tmp/tmpj2a3qw3q/_remote_module_non_scriptable.py
2023-11-23 16:15:31,851 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 16:15:49,766 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 16:15:58,513 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 16:19:39,870 - INFO - Received requests to /inference endpoint
2023-11-23 16:19:39,971 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 16:19:39,971 - INFO - Received request: {'username': 'admin', 'prompt': 'hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'string'}
2023-11-23 16:19:46,762 - INFO - Processed the request successfully
2023-11-23 16:20:04,412 - INFO - Received requests to /inference endpoint
2023-11-23 16:20:04,514 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 16:20:04,514 - INFO - Received request: {'username': 'admin', 'prompt': 'my name is admin', 'memory': True, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'string'}
2023-11-23 16:20:09,768 - INFO - Processed the request successfully
2023-11-23 16:20:23,309 - INFO - Received requests to /inference endpoint
2023-11-23 16:20:23,410 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 16:20:23,411 - INFO - Received request: {'username': 'admin', 'prompt': 'what was my name?', 'memory': True, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'string'}
2023-11-23 16:20:30,829 - INFO - Processed the request successfully
2023-11-23 16:20:57,682 - INFO - Received requests to /inference endpoint
2023-11-23 16:20:57,783 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 16:20:57,783 - INFO - Received request: {'username': 'admin', 'prompt': 'where is the capital of the UK?', 'memory': True, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'string'}
2023-11-23 16:21:04,300 - INFO - Processed the request successfully
2023-11-23 16:21:23,741 - INFO - Received requests to /inference endpoint
2023-11-23 16:21:23,842 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 16:21:23,842 - INFO - Received request: {'username': 'admin', 'prompt': 'what was my previous question about?', 'memory': True, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'string'}
2023-11-23 16:21:32,897 - INFO - Processed the request successfully
2023-11-24 09:43:45,747 - INFO - Received requests to /inference endpoint
2023-11-24 09:43:45,848 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 09:43:45,848 - INFO - Received request: {'username': 'admin', 'prompt': 'Hi, my name is Admin', 'memory': True, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'Llama_70b'}
2023-11-24 09:43:55,246 - INFO - Processed the request successfully
2023-11-24 09:44:18,755 - INFO - Received requests to /inference endpoint
2023-11-24 09:44:18,856 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 09:44:18,856 - INFO - Received request: {'username': 'admin', 'prompt': 'what was my previous prompt?', 'memory': True, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'Llama_70b'}
2023-11-24 09:44:27,432 - INFO - Processed the request successfully
2023-11-24 09:44:47,855 - INFO - Received requests to /inference endpoint
2023-11-24 09:44:47,957 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 09:44:47,957 - INFO - Received request: {'username': 'admin', 'prompt': 'what was my previous prompt?', 'memory': True, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'Llama_13b'}
2023-11-24 09:44:56,326 - INFO - Processed the request successfully
2023-11-24 11:57:07,619 - INFO - Received requests to /inference endpoint
2023-11-24 11:57:07,720 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 11:57:07,720 - INFO - Received request: {'username': 'alex', 'prompt': 'where is the capital of France?', 'memory': False, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-11-24 11:57:18,765 - INFO - Processed the request successfully
2023-11-24 11:57:32,035 - INFO - Received requests to /inference endpoint
2023-11-24 11:57:32,136 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 11:57:32,137 - INFO - Received request: {'username': 'alex', 'prompt': 'what was my previous question ?', 'memory': False, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-11-24 11:57:41,574 - INFO - Processed the request successfully
2023-11-24 11:58:00,398 - INFO - Received requests to /inference endpoint
2023-11-24 11:58:00,499 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 11:58:00,499 - INFO - Received request: {'username': 'alex', 'prompt': 'what was my previous prompt?', 'memory': False, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-11-24 11:58:07,056 - INFO - Processed the request successfully
2023-11-24 11:58:22,765 - INFO - Received requests to /inference endpoint
2023-11-24 11:58:22,866 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 11:58:22,866 - INFO - Received request: {'username': 'alex', 'prompt': 'where is the capital of France?', 'memory': False, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-11-24 11:58:33,997 - INFO - Processed the request successfully
2023-11-24 11:58:51,482 - INFO - Received requests to /inference endpoint
2023-11-24 11:58:51,583 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 11:58:51,584 - INFO - Received request: {'username': 'alex', 'prompt': 'what was my previous prompt about?', 'memory': False, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-11-24 11:59:00,208 - INFO - Processed the request successfully
2023-11-24 12:01:15,545 - INFO - Received requests to /inference endpoint
2023-11-24 12:01:15,647 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 12:01:15,647 - INFO - Received request: {'username': 'alex', 'prompt': 'where is the capital of France?', 'memory': True, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-11-24 12:01:20,173 - INFO - Processed the request successfully
2023-11-24 12:01:32,396 - INFO - Received requests to /inference endpoint
2023-11-24 12:01:32,497 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 12:01:32,497 - INFO - Received request: {'username': 'alex', 'prompt': 'What was my previous prompt?', 'memory': True, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-11-24 12:01:41,192 - INFO - Processed the request successfully
2023-11-24 12:06:49,837 - INFO - Received requests to /inference endpoint
2023-11-24 12:06:49,938 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 12:06:49,938 - INFO - Received request: {'username': 'alex', 'prompt': 'did I ask question about paris?', 'memory': True, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-11-24 12:06:56,663 - INFO - Processed the request successfully
2023-11-24 12:07:30,597 - INFO - Received requests to /inference endpoint
2023-11-24 12:07:30,698 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 12:07:30,698 - INFO - Received request: {'username': 'alex', 'prompt': 'where is London located?', 'memory': True, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-11-24 12:07:41,822 - INFO - Processed the request successfully
2023-11-24 12:08:00,768 - INFO - Received requests to /inference endpoint
2023-11-24 12:08:00,869 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 12:08:00,869 - INFO - Received request: {'username': 'alex', 'prompt': 'did I ask question about London?', 'memory': True, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-24 12:08:06,731 - INFO - Processed the request successfully
2023-11-24 12:08:23,289 - INFO - Received requests to /inference endpoint
2023-11-24 12:08:23,390 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 12:08:23,390 - INFO - Received request: {'username': 'alex', 'prompt': 'did I asked where London is Located?', 'memory': True, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-24 12:08:31,514 - INFO - Processed the request successfully
2023-11-24 12:09:00,315 - INFO - Received requests to /inference endpoint
2023-11-24 12:09:00,416 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 12:09:00,417 - INFO - Received request: {'username': 'alex', 'prompt': 'give me brief of our chats before this prompt', 'memory': True, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-24 12:09:23,082 - INFO - Processed the request successfully
2023-11-24 12:58:21,180 - INFO - Created a temporary directory at /tmp/tmpoi9rm3t6
2023-11-24 12:58:21,180 - INFO - Writing /tmp/tmpoi9rm3t6/_remote_module_non_scriptable.py
2023-11-24 12:58:23,306 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 12:58:40,894 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 12:58:49,645 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-24 12:58:56,194 - INFO - Created a temporary directory at /tmp/tmp906n0fra
2023-11-24 12:58:56,194 - INFO - Writing /tmp/tmp906n0fra/_remote_module_non_scriptable.py
2023-11-24 12:58:58,396 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 12:59:16,220 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 12:59:24,937 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-24 13:42:14,482 - INFO - Created a temporary directory at /tmp/tmpxi8mh_ak
2023-11-24 13:42:14,482 - INFO - Writing /tmp/tmpxi8mh_ak/_remote_module_non_scriptable.py
2023-11-24 13:42:16,638 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 13:42:34,329 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 13:42:43,072 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-24 13:42:49,588 - INFO - Created a temporary directory at /tmp/tmpyi32iyuj
2023-11-24 13:42:49,588 - INFO - Writing /tmp/tmpyi32iyuj/_remote_module_non_scriptable.py
2023-11-24 13:42:51,804 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 13:43:09,365 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 13:43:18,074 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-24 13:45:31,246 - INFO - Created a temporary directory at /tmp/tmpzsfn_vvw
2023-11-24 13:45:31,246 - INFO - Writing /tmp/tmpzsfn_vvw/_remote_module_non_scriptable.py
2023-11-24 13:45:33,448 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 13:45:51,250 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 13:45:59,988 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-24 13:46:06,278 - INFO - Created a temporary directory at /tmp/tmpjxsa7vpl
2023-11-24 13:46:06,278 - INFO - Writing /tmp/tmpjxsa7vpl/_remote_module_non_scriptable.py
2023-11-24 13:46:08,508 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 16:14:29,257 - INFO - Created a temporary directory at /tmp/tmpwi0t5sf9
2023-11-24 16:14:29,258 - INFO - Writing /tmp/tmpwi0t5sf9/_remote_module_non_scriptable.py
2023-11-24 16:14:31,556 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 16:14:49,192 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 16:15:04,555 - INFO - Created a temporary directory at /tmp/tmpfgxakqhk
2023-11-24 16:15:04,556 - INFO - Writing /tmp/tmpfgxakqhk/_remote_module_non_scriptable.py
2023-11-24 16:16:08,239 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 16:16:12,441 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 16:19:20,847 - INFO - Created a temporary directory at /tmp/tmpbyn56i3p
2023-11-24 16:19:20,847 - INFO - Writing /tmp/tmpbyn56i3p/_remote_module_non_scriptable.py
2023-11-24 16:19:23,271 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 16:19:40,999 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 16:19:56,558 - INFO - Created a temporary directory at /tmp/tmp4mh0e5us
2023-11-24 16:19:56,558 - INFO - Writing /tmp/tmp4mh0e5us/_remote_module_non_scriptable.py
2023-11-24 16:19:58,422 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 16:20:02,922 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 16:24:52,413 - INFO - Created a temporary directory at /tmp/tmpjkss21or
2023-11-24 16:24:52,413 - INFO - Writing /tmp/tmpjkss21or/_remote_module_non_scriptable.py
2023-11-24 16:24:54,738 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 16:25:12,084 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 16:25:26,647 - INFO - Created a temporary directory at /tmp/tmpgiw8zzz6
2023-11-24 16:25:26,647 - INFO - Writing /tmp/tmpgiw8zzz6/_remote_module_non_scriptable.py
2023-11-24 16:25:28,448 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 16:25:32,666 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 16:26:27,492 - INFO - Created a temporary directory at /tmp/tmpgd0se94d
2023-11-24 16:26:27,493 - INFO - Writing /tmp/tmpgd0se94d/_remote_module_non_scriptable.py
2023-11-24 16:26:29,828 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 16:26:47,350 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 16:29:55,708 - INFO - Created a temporary directory at /tmp/tmp6kcast2l
2023-11-24 16:29:55,708 - INFO - Writing /tmp/tmp6kcast2l/_remote_module_non_scriptable.py
2023-11-24 16:29:58,038 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 16:30:15,722 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 16:31:35,276 - INFO - Created a temporary directory at /tmp/tmpbs261uga
2023-11-24 16:31:35,277 - INFO - Writing /tmp/tmpbs261uga/_remote_module_non_scriptable.py
2023-11-24 16:31:37,588 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 16:31:55,077 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-27 09:52:02,915 - INFO - Created a temporary directory at /tmp/tmpme30l9h9
2023-11-27 09:52:02,915 - INFO - Writing /tmp/tmpme30l9h9/_remote_module_non_scriptable.py
2023-11-27 09:52:05,183 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-27 09:52:22,711 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-27 09:52:37,971 - INFO - Created a temporary directory at /tmp/tmpkazi7fy2
2023-11-27 09:52:37,971 - INFO - Writing /tmp/tmpkazi7fy2/_remote_module_non_scriptable.py
2023-11-27 09:52:39,805 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-27 09:52:43,763 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-27 09:54:02,579 - INFO - Created a temporary directory at /tmp/tmpkj0bmpr9
2023-11-27 09:54:02,580 - INFO - Writing /tmp/tmpkj0bmpr9/_remote_module_non_scriptable.py
2023-11-27 09:54:04,862 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-27 09:54:22,230 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-27 09:54:36,775 - INFO - Created a temporary directory at /tmp/tmpk_d8gkff
2023-11-27 09:54:36,776 - INFO - Writing /tmp/tmpk_d8gkff/_remote_module_non_scriptable.py
2023-11-27 09:54:38,606 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-27 09:54:42,469 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-27 09:57:00,310 - INFO - request processed successfully username='admin' collection_name='videos' mode='create_collection' vectorDB_type='Weaviate' file_path='string': %s
2023-11-27 10:01:33,265 - ERROR - An error occurred: [Errno 20] Not a directory: '/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/docs/paper.pdf'
2023-11-27 10:09:22,302 - ERROR - An error occurred: [Errno 20] Not a directory: '/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/admin_Car rental doc.pdf'
2023-11-27 10:55:03,854 - ERROR - An error occurred: [Errno 20] Not a directory: '/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/docs/paper.pdf'
2023-11-27 11:00:59,336 - INFO - request processed successfully username='admin' collection_name='Admin_videos' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/f84b2db7d4a8f72d': %s
2023-11-27 11:05:11,372 - INFO - request processed successfully username='admin' collection_name='Admin_videos' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/536b0eacb9e16257': %s
2023-11-27 11:24:52,458 - ERROR - An error occurred: (sqlite3.OperationalError) attempt to write a readonly database
[SQL: UPDATE users SET collection_names=? WHERE users.id = ?]
[parameters: ('admin_Video,admin_Video2,admin_Test,admin_my_collection,admin_xc,admin_vc,admin_ff,admin_None,admin_zz,admin_test1,admin_test2,admin_test3,admin_pdf,A_video,A_web,Admin_ssd,Admin_videos,Admin_amin', 1)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2023-11-27 11:26:12,608 - ERROR - An error occurred: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.OperationalError) attempt to write a readonly database
[SQL: UPDATE users SET collection_names=? WHERE users.id = ?]
[parameters: ('admin_Video,admin_Video2,admin_Test,admin_my_collection,admin_xc,admin_vc,admin_ff,admin_None,admin_zz,admin_test1,admin_test2,admin_test3,admin_pdf,A_video,A_web,Admin_ssd,Admin_videos,Admin_amin', 1)]
(Background on this error at: https://sqlalche.me/e/20/e3q8) (Background on this error at: https://sqlalche.me/e/20/7s2a)
2023-11-27 11:28:24,866 - INFO - request processed successfully username='admin' collection_name='Admin_General_collection' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/59906acd283673a0': %s
2023-11-27 12:27:48,177 - ERROR - An error occurred: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.OperationalError) attempt to write a readonly database
[SQL: UPDATE users SET collection_names=? WHERE users.id = ?]
[parameters: ('admin_Video,admin_Video2,admin_Test,admin_my_collection,admin_xc,admin_vc,admin_ff,admin_None,admin_zz,admin_test1,admin_test2,admin_test3,admin_pdf,A_video,A_web,Admin_ssd,Admin_videos,Admin_amin', 1)]
(Background on this error at: https://sqlalche.me/e/20/e3q8) (Background on this error at: https://sqlalche.me/e/20/7s2a)
2023-11-27 12:33:17,440 - INFO - request processed successfully username='amin' collection_name=None mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/38a6b2c006a2e2d9': %s
2023-12-05 12:49:46,413 - INFO - Created a temporary directory at /tmp/tmpfd17agu7
2023-12-05 12:49:46,413 - INFO - Writing /tmp/tmpfd17agu7/_remote_module_non_scriptable.py
2023-12-05 12:49:50,023 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-05 12:59:22,312 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-05 12:59:38,525 - INFO - Created a temporary directory at /tmp/tmp704ler0i
2023-12-05 12:59:38,525 - INFO - Writing /tmp/tmp704ler0i/_remote_module_non_scriptable.py
2023-12-06 10:14:45,285 - INFO - Created a temporary directory at /tmp/tmpemk428xz
2023-12-06 10:14:45,285 - INFO - Writing /tmp/tmpemk428xz/_remote_module_non_scriptable.py
2023-12-06 10:14:48,704 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-06 10:20:26,244 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-06 10:20:42,262 - INFO - Created a temporary directory at /tmp/tmpilr_7vuk
2023-12-06 10:20:42,263 - INFO - Writing /tmp/tmpilr_7vuk/_remote_module_non_scriptable.py
2023-12-06 10:21:56,387 - INFO - Created a temporary directory at /tmp/tmpuh6vs2_q
2023-12-06 10:21:56,387 - INFO - Writing /tmp/tmpuh6vs2_q/_remote_module_non_scriptable.py
2023-12-06 10:23:24,436 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-06 10:23:41,517 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-06 10:25:34,841 - INFO - Created a temporary directory at /tmp/tmpx6t7m6ts
2023-12-06 10:25:34,841 - INFO - Writing /tmp/tmpx6t7m6ts/_remote_module_non_scriptable.py
2023-12-06 10:25:37,172 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-06 10:27:34,676 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-06 10:27:49,354 - INFO - Created a temporary directory at /tmp/tmp7l33bz1s
2023-12-06 10:27:49,355 - INFO - Writing /tmp/tmp7l33bz1s/_remote_module_non_scriptable.py
2023-12-06 10:27:51,218 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-06 10:27:56,180 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-06 11:04:19,573 - INFO - Received requests to /inference endpoint
2023-12-06 11:04:19,674 - INFO - Received a batch of request with batch size of: 1 
2023-12-06 11:04:19,675 - INFO - Received request: {'username': 'admin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': False, 'collection_name': 'Amin', 'llm_model': 'Llama_70b'}
2023-12-06 11:04:54,237 - INFO - Received requests to /inference endpoint
2023-12-06 11:04:54,338 - INFO - Received a batch of request with batch size of: 1 
2023-12-06 11:04:54,338 - INFO - Received request: {'username': 'admin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': False, 'collection_name': 'General_collection', 'llm_model': 'Llama_70b'}
2023-12-06 11:05:03,064 - INFO - Received requests to /inference endpoint
2023-12-06 11:05:03,165 - INFO - Received a batch of request with batch size of: 1 
2023-12-06 11:05:03,165 - INFO - Received request: {'username': 'admin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': False, 'collection_name': 'General_collection', 'llm_model': 'Llama_70b'}
2023-12-06 11:05:46,516 - INFO - Received requests to /inference endpoint
2023-12-06 11:05:46,617 - INFO - Received a batch of request with batch size of: 1 
2023-12-06 11:05:46,617 - INFO - Received request: {'username': 'admin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': False, 'collection_name': 'General_collection', 'llm_model': 'Llama_70b'}
2023-12-06 11:06:33,448 - INFO - Received requests to /inference endpoint
2023-12-06 11:06:33,549 - INFO - Received a batch of request with batch size of: 1 
2023-12-06 11:06:33,549 - INFO - Received request: {'username': 'amin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': False, 'collection_name': 'General_collection', 'llm_model': 'Llama_70b'}
2023-12-06 11:06:33,572 - ERROR - Error processing the request: Error during query: [{'locations': [{'column': 6, 'line': 1}], 'message': 'Cannot query field "General_collection" on type "GetObjectsObj". Did you mean "Admin_General_collection"?', 'path': None}]
2023-12-06 11:17:34,034 - INFO - checking the request/ username='amin' class_name='video' mode='create_class' vectorDB_type='Weaviate' file_path='string': %s
2023-12-06 11:17:34,123 - INFO - checkpoint 1
2023-12-06 11:17:34,123 - INFO - checkpoint 2 amin: %s
2023-12-06 11:17:34,123 - INFO - checkpoint 2 amin_video: %s
2023-12-06 11:24:44,014 - INFO - checking the request/ username='amin' class_name='document' mode='create_class' vectorDB_type='Weaviate' file_path='string': %s
2023-12-06 11:24:44,101 - INFO - checkpoint 1
2023-12-06 11:24:44,101 - INFO - checkpoint 2 amin: %s
2023-12-06 11:24:44,101 - INFO - checkpoint 2 amin_document: %s
2023-12-06 11:24:44,120 - INFO - success: class document created for user amin
2023-12-06 11:31:00,074 - INFO - checking the request/ username='amin' class_name='web' mode='create_class' vectorDB_type='Weaviate' file_path='string': %s
2023-12-06 11:31:00,135 - INFO - checkpoint 1
2023-12-06 11:31:00,135 - INFO - checkpoint 2 amin: %s
2023-12-06 11:31:00,135 - INFO - checkpoint 2 amin_web: %s
2023-12-06 11:31:00,166 - INFO - success: class web created for user amin
2023-12-06 11:46:23,071 - INFO - checking the request/ username='amin' class_name='web2' mode='create_class' vectorDB_type='Weaviate' file_path='string': %s
2023-12-06 11:46:23,132 - INFO - checkpoint 1
2023-12-06 11:46:23,133 - INFO - checkpoint 2 amin: %s
2023-12-06 11:46:23,133 - INFO - checkpoint 2 amin_web2: %s
2023-12-06 11:46:23,177 - INFO - class name added successfully to database
2023-12-06 11:46:23,177 - INFO - success: class web2 created for user amin
2023-12-06 11:52:25,070 - INFO - request processed successfully username='amin' class_name='web2' mode='delete_class' vectorDB_type='Weaviate' file_path='string': %s
2023-12-06 11:52:25,070 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 12:02:08,040 - INFO - Created a temporary directory at /tmp/tmpvmh12ryr
2023-12-06 12:02:08,040 - INFO - Writing /tmp/tmpvmh12ryr/_remote_module_non_scriptable.py
2023-12-06 12:02:10,623 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-06 12:02:45,785 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-06 12:03:01,497 - INFO - Created a temporary directory at /tmp/tmpnuw3eece
2023-12-06 12:03:01,497 - INFO - Writing /tmp/tmpnuw3eece/_remote_module_non_scriptable.py
2023-12-06 12:03:03,336 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-06 12:03:07,744 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-06 12:03:51,534 - INFO - Received requests to /inference endpoint
2023-12-06 12:03:51,635 - INFO - Received a batch of request with batch size of: 1 
2023-12-06 12:03:51,636 - INFO - Received request: {'username': 'amin', 'prompt': 'hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': False, 'collection_name': 'web', 'llm_model': 'Llama_70b'}
2023-12-06 12:04:09,090 - INFO - Received requests to /inference endpoint
2023-12-06 12:04:09,191 - INFO - Received a batch of request with batch size of: 1 
2023-12-06 12:04:09,191 - INFO - Received request: {'username': 'amin', 'prompt': 'hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': False, 'collection_name': 'video', 'llm_model': 'Llama_70b'}
2023-12-06 13:31:34,071 - INFO - checking the request/ username='string' class_name='ee' mode='create_collection' vectorDB_type='Weaviate' file_path='string': %s
2023-12-06 13:31:34,158 - INFO - checkpoint 1
2023-12-06 13:31:34,158 - INFO - checkpoint 2 string: %s
2023-12-06 13:31:34,158 - INFO - checkpoint 2 string_ee: %s
2023-12-06 13:31:34,243 - INFO - class name added successfully to database
2023-12-06 13:31:34,243 - INFO - success: class ee created for user string
2023-12-06 13:33:22,281 - INFO - checking the request/ username='amin' class_name='cd' mode='create_collection' vectorDB_type='Weaviate' file_path=None: %s
2023-12-06 13:33:22,369 - INFO - checkpoint 1
2023-12-06 13:33:22,369 - INFO - checkpoint 2 amin: %s
2023-12-06 13:33:22,369 - INFO - checkpoint 2 amin_cd: %s
2023-12-06 13:33:22,405 - INFO - class name added successfully to database
2023-12-06 13:33:22,405 - INFO - success: class cd created for user amin
2023-12-06 13:33:44,717 - INFO - request processed successfully username='amin' class_name=None mode=None vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/bf47e2ef6b86afed': %s
2023-12-06 13:33:44,717 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 14:00:52,783 - INFO - Created a temporary directory at /tmp/tmphrl8ikoy
2023-12-06 14:00:52,784 - INFO - Writing /tmp/tmphrl8ikoy/_remote_module_non_scriptable.py
2023-12-06 14:00:55,129 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-06 14:01:18,877 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-06 14:01:34,022 - INFO - Created a temporary directory at /tmp/tmpi4v65qg6
2023-12-06 14:01:34,022 - INFO - Writing /tmp/tmpi4v65qg6/_remote_module_non_scriptable.py
2023-12-06 14:01:35,856 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-06 14:01:40,113 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-06 14:03:32,776 - INFO - actors creation successful [Actor(WeaviateEmbedder, 5ae9dee07e49ab95d952437401000000), Actor(WeaviateEmbedder, c394f0a0dbcccd8776b8dd9101000000), Actor(WeaviateEmbedder, 3743bbe20c6b8637fe9468ca01000000)]: %s
2023-12-06 14:03:32,777 - INFO - check 1st step of ray was successful
2023-12-06 14:03:32,777 - INFO - check if ray was successful:
2023-12-06 14:03:32,777 - INFO - check weaviate add data, 
2023-12-06 14:03:32,777 - INFO - request processed successfully username='amin' class_name='web' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/docs': %s
2023-12-06 14:03:34,491 - INFO - Check the data that is being passed [{'page_content': '3 Temporal Early Exits Video Object Detection Pipeline\nWe propose a novel video object detection pipeline to tackle the computational complexity of video\nobject detection. An overview of the proposed pipeline is shown in gure 2. Given a sequence of\nvideo frames, multiple Temporal Early Exit Modules (TEEM) are used to build the early exit branches.\nEach early exit branch uses features extracted in the early stages of the feature network to identify\nany semantic variation between the features of consecutive video frames. If any semantic variations\nis distinguished by a TEEM, a full computation effort (main branch) will be needed to process the\nvideo frame. On the other hand, the computation process of a video frame without any semantic\nvariations will be terminated at an early exit branch and the detection results from the previous frame\nwill be reused. We describe the implementation details of the proposed pipeline in this section.\n3.1 Scenery Change\nThe key challenge in identifying feature variations is qualifying semantic dissimilarities between\nimages [ 1]. To address this challenge, we introduce a scenery change (SC)metric to quantify\nsemantic variations between video frames. We dene a semantic variation to be only a variation in\nobjects of interest where noisy variations are considered to be nuisance. Given any pairs of video\nframes, a TEEM aims to identify semantic differences between frames. A scenery changes between a\npair of video frames (fri,fri+j), if the maximum motion in the Moving Fields of Interest (MFI) is\ngreater than the variation threshold value varas follows,\nSC(fri,fri+j) ={\n1 max( MFI(fri,fri+j))>var\n0otherwise. (1)\nWe compute MFI set by measuring intersection over union (IoU) of each object of interest, Ok, across\nframe pairs as follows,\nMFI(ok) ={\n1IoU(Ofri\nk,Ofri+j\nk )ifOkfrifri+j\n1 Otherwise. (2)\nEach element in MFI quanties the variation in an object of interest between a pair of video frames.\nThe scenery change metric distinguishes noisy variations from semantic variations in video frame\npairs. Figure 3 visualizes some examples of scenery change metric for video frame pair.\n3.2 Temporal Early Exit Module\nTo detect a scenery change between frame pairs (fri,fri+j), we propose TEEM which consists of\nAttention andClassier components. This section describes these components in details. Figure 2\nshows an overview of a TEEM.\nTheAttention component is represented as a function parameterized by Attenlin equation (3).FAttn\ntakesZl\niandZl\ni+jas inputs, and outputs an attention map AttMaplwhich encodes the semantic\nvariations happened between friandfri+j. In equation (3),Zl\niandZl\ni+jare image features of\nthe previous friand currentfri+jvideo frames, respectively, encoded by convolutional layer lin\nthe feature network. The attention function captures the semantic variations between frame pairs\nin the representation space by subtracting Zl\nifromZl\ni+jin equation (4). Then, by concatenating\nthe subtraction results with the feature representation of the current video frame, Zl\ni+j, a 2D spatial\nattention map associated with the video frame pair is generated in equation (6).\nAttMapl=FAttn(Zl\ni,Zl\ni+j;Attenl). (3)\nZsub=Zi+jZj, (4)\nZc=Conct [Zi:Zsub], (5)\nAttMapl=Sigmoid (Conv(Zc)), (6)\n4', 'document_title': 'paper.pdf'}, {'page_content': 'In equations (6)and(5),Conct [; ],Conv ,Sigmoid indicate concatenation, convolutional layer and\nsigmoid function, respectively. Following [ 13], to avoid introducing any form of global normalization,\nwe use sigmoid, instead of softmax, for computing the attention map. The attention map makes\nTEEM learn and focus on the motions of objects of interest between frame pairs. Notably, a TEEM\nrequires storing the latest feature maps Zl\nito be used for successive frames.\nThe generated attention map alongside the feature map of the current video frame Zl\ni+jare fed to\naclassifier to detect any scenery change between frame pairs. The classifier component is a\nclassier with two outputs which represented as a function FClass, parameterized by Classin equation\n(7). It takes the attention map, AttMapl, and the feature map of the current video frame Zl\ni+jand\nproduces a class label. The possible labels are changed and unchanged frame. To classify the video\nframe, the AttMaplis multiplied element-wise with the feature map of current video frame to get the\nrened feature maps ZAttin equation (8). The rened feature maps are passed through a convolutional\nlayer and a fully connected layer, equation (9), to produce a class label.\nClsl\ni=FClass(Zl\ni+j,AttMapl;Class). (7)\nZAtt=AttMaplZl\ni+j. (8)\nClsl\ni=FC(ReLU (Norm (Conv(Zatt)))). (9)\nIn equation (9)FC, ReLU, Norm and Conv indicate fully connected, ReLU, batch normalization and\nconvolutional layers, respectively. Clsl\niis the output of the TEEM, located at the convolutional layer\nlof feature network. Each video frame can be classied by a TEEM located in an early exit branch\ninto a changed or unchanged pair.\n3.3 Training Policy\nAn early exit branch, including a TEEM, can be added after any convolutional layer in the feature\nnetwork. However, the location of an early exit branch in the feature network determines its\nperformance in identifying semantic variations. Therefore, a temporal early attention video object\ndetection pipeline have multiple early exits to identify semantic variations. However, each TEEM in\nan early exit branch is regarded as a stand-alone module, hence, trained separately from the backbone\nnetwork. Let (fri,fri+j)be a sampled video frame pair and be all parameters in TEEM in early\nexitl. For a target label, P, the objective is to minimize the cross-entropy loss as follows:\nLTEEMk(P,P;) =\nkPklog(Pk) (10)\nwherekis classier outputs and,\nP=Softmax (S), (11)\nand\nS=FTEEM l(fri,fri+j;). (12)\nHere,FTEEM lis the the output of the TEEM in early exit l. In the forward pass of training, a video\nframe pair is passed through the network, including both the main branch and early exits. The\noutput of each early exit is recorded to calculate the error of TEEM in the early exit. In backward\npropagation, the error of each exit is passed back only through the TEEMs and the weights of TEEM\nare updated using gradient descent. Notably, the weights of the main branch are not updated during\ntraining of the TEEMs.\n3.4 Fast inference in a Temporal Early Exit Video Object Detection Pipeline\nAlgorithm 1 summarizes the temporal early exit video object detection pipeline inference algorithm.\nGiven a pre-trained object detection network as the main branch, Fmain, withjearly exit branches,\nthe rst video frame is processed by the main branch. The inference for successive frames proceeds\nby feeding frames through the feature network of the main branch until reaching the rst early exit\nbranch. Then, the softmax and entropy of FTEEM in the early exit are calculated to evaluate the\ncondence of the classier in prediction. If the entropy is less than the threshold and the class\nlabel with the maximum score (probability) is unchanged, the inference procedure terminates and\n5', 'document_title': 'paper.pdf'}, {'page_content': 'object detection results from the previous frame are returned. If the class label with maximum score\nis changed, then the frame is processed by the main branch. For the entropy greater than , the frame\ncontinues to be processed by the feature network till the next early exit branch. If the softmax and\nentropy of all early exits are greater than thresholds, the frame continues to be processed by the main\nbranch.\nAlgorithm 1 Inference algorithm for video object detection\nRequire: Video frames{I}\nEnsure: detection =None\nfori1tolength (I)do\nifi== 1 then\ndetectionFmain(Ii)\nreturn detection\nelse\nforj1toj= #( early exits )do\nyFTEEM j(Ii,Ii1)\nSsoftmax (y)\neentropy (S)\nife< and arg max (S) == Unchanged then\nreturn detection\nelse ife< and arg max (S) == Changed then\ndetectionFmain(Ii)\nreturn detection\nelse ife> then\npass\nend if\nend for\nend if\nend for\n4 Dataset\nThe CDnet dataset [ 5] provides a realistic, camera-captured, diverse set of videos for change detection.\nThe videos have been selected to cover a wide range of detection challenges and are representative\nof typical indoor and outdoor visual data captured in surveillance, smart environment. This dataset\nconsists of 33 camera-captured videos ( 70,000 frames) with boats, trucks, and pedestrians as objects\nof interest. It contains a range of challenging factors, including turbulence, dynamic background,\ncamera jitter challenging weather and pan-tilt-zooming (PTZ). All videos come with ground-truth\nsegmentation of motion areas for each video frame. However, CDnet dataset doesnt provide labels\nfor the individual instances of moving objects. This makes measuring SC metric challenging and\ninaccurate in scenarios with multiple moving objects. To evaluate those scenarios, we also build\nthe scenery change dataset. Our dataset is based on VIRAT dataset [ 15] which is inspired and used\nby many change detection applications [ 17]. We used Vatic engine [ 27] to label individual moving\nobjects with bounding boxes. The dataset consists of 20 camera-captured videos with pedestrian, car\nand cyclist as objects of interest. We believe our dataset can complement existing change detection\ndatasets like CDnet[5].\n4.1 Dataset Balancing\nTo train TEEMs, video frame pairs need to be randomly sampled from videos. However, random\nsampling of frame pairs causes the unbalanced class dataset (changed scenery and unchanged scenery\nclasses). The balance of classes depends on the interval between sampled video frame pairs. For\nlong sampling interval, the dataset skews to changed scenery frame pairs. On the other hand, a short\nsampling interval leads to skewing to unchanged scenery. As a result of the unbalanced dataset,\nthe classier of TEEMs tends to ignore small classes, while concentrating on learning to classify\nlarge ones accurately. We tackle the unbalanced dataset problem by proposing the dynamic sampling\ninterval approach. To this end, we divide the semantic variation into ten classes. Each class represents\n6', 'document_title': 'paper.pdf'}]: %s
2023-12-06 14:03:34,492 - INFO - Check the results [{'page_content': '3 Temporal Early Exits Video Object Detection Pipeline\nWe propose a novel video object detection pipeline to tackle the computational complexity of video\nobject detection. An overview of the proposed pipeline is shown in gure 2. Given a sequence of\nvideo frames, multiple Temporal Early Exit Modules (TEEM) are used to build the early exit branches.\nEach early exit branch uses features extracted in the early stages of the feature network to identify\nany semantic variation between the features of consecutive video frames. If any semantic variations\nis distinguished by a TEEM, a full computation effort (main branch) will be needed to process the\nvideo frame. On the other hand, the computation process of a video frame without any semantic\nvariations will be terminated at an early exit branch and the detection results from the previous frame\nwill be reused. We describe the implementation details of the proposed pipeline in this section.\n3.1 Scenery Change\nThe key challenge in identifying feature variations is qualifying semantic dissimilarities between\nimages [ 1]. To address this challenge, we introduce a scenery change (SC)metric to quantify\nsemantic variations between video frames. We dene a semantic variation to be only a variation in\nobjects of interest where noisy variations are considered to be nuisance. Given any pairs of video\nframes, a TEEM aims to identify semantic differences between frames. A scenery changes between a\npair of video frames (fri,fri+j), if the maximum motion in the Moving Fields of Interest (MFI) is\ngreater than the variation threshold value varas follows,\nSC(fri,fri+j) ={\n1 max( MFI(fri,fri+j))>var\n0otherwise. (1)\nWe compute MFI set by measuring intersection over union (IoU) of each object of interest, Ok, across\nframe pairs as follows,\nMFI(ok) ={\n1IoU(Ofri\nk,Ofri+j\nk )ifOkfrifri+j\n1 Otherwise. (2)\nEach element in MFI quanties the variation in an object of interest between a pair of video frames.\nThe scenery change metric distinguishes noisy variations from semantic variations in video frame\npairs. Figure 3 visualizes some examples of scenery change metric for video frame pair.\n3.2 Temporal Early Exit Module\nTo detect a scenery change between frame pairs (fri,fri+j), we propose TEEM which consists of\nAttention andClassier components. This section describes these components in details. Figure 2\nshows an overview of a TEEM.\nTheAttention component is represented as a function parameterized by Attenlin equation (3).FAttn\ntakesZl\niandZl\ni+jas inputs, and outputs an attention map AttMaplwhich encodes the semantic\nvariations happened between friandfri+j. In equation (3),Zl\niandZl\ni+jare image features of\nthe previous friand currentfri+jvideo frames, respectively, encoded by convolutional layer lin\nthe feature network. The attention function captures the semantic variations between frame pairs\nin the representation space by subtracting Zl\nifromZl\ni+jin equation (4). Then, by concatenating\nthe subtraction results with the feature representation of the current video frame, Zl\ni+j, a 2D spatial\nattention map associated with the video frame pair is generated in equation (6).\nAttMapl=FAttn(Zl\ni,Zl\ni+j;Attenl). (3)\nZsub=Zi+jZj, (4)\nZc=Conct [Zi:Zsub], (5)\nAttMapl=Sigmoid (Conv(Zc)), (6)\n4', 'document_title': 'paper.pdf'}, {'page_content': 'In equations (6)and(5),Conct [; ],Conv ,Sigmoid indicate concatenation, convolutional layer and\nsigmoid function, respectively. Following [ 13], to avoid introducing any form of global normalization,\nwe use sigmoid, instead of softmax, for computing the attention map. The attention map makes\nTEEM learn and focus on the motions of objects of interest between frame pairs. Notably, a TEEM\nrequires storing the latest feature maps Zl\nito be used for successive frames.\nThe generated attention map alongside the feature map of the current video frame Zl\ni+jare fed to\naclassifier to detect any scenery change between frame pairs. The classifier component is a\nclassier with two outputs which represented as a function FClass, parameterized by Classin equation\n(7). It takes the attention map, AttMapl, and the feature map of the current video frame Zl\ni+jand\nproduces a class label. The possible labels are changed and unchanged frame. To classify the video\nframe, the AttMaplis multiplied element-wise with the feature map of current video frame to get the\nrened feature maps ZAttin equation (8). The rened feature maps are passed through a convolutional\nlayer and a fully connected layer, equation (9), to produce a class label.\nClsl\ni=FClass(Zl\ni+j,AttMapl;Class). (7)\nZAtt=AttMaplZl\ni+j. (8)\nClsl\ni=FC(ReLU (Norm (Conv(Zatt)))). (9)\nIn equation (9)FC, ReLU, Norm and Conv indicate fully connected, ReLU, batch normalization and\nconvolutional layers, respectively. Clsl\niis the output of the TEEM, located at the convolutional layer\nlof feature network. Each video frame can be classied by a TEEM located in an early exit branch\ninto a changed or unchanged pair.\n3.3 Training Policy\nAn early exit branch, including a TEEM, can be added after any convolutional layer in the feature\nnetwork. However, the location of an early exit branch in the feature network determines its\nperformance in identifying semantic variations. Therefore, a temporal early attention video object\ndetection pipeline have multiple early exits to identify semantic variations. However, each TEEM in\nan early exit branch is regarded as a stand-alone module, hence, trained separately from the backbone\nnetwork. Let (fri,fri+j)be a sampled video frame pair and be all parameters in TEEM in early\nexitl. For a target label, P, the objective is to minimize the cross-entropy loss as follows:\nLTEEMk(P,P;) =\nkPklog(Pk) (10)\nwherekis classier outputs and,\nP=Softmax (S), (11)\nand\nS=FTEEM l(fri,fri+j;). (12)\nHere,FTEEM lis the the output of the TEEM in early exit l. In the forward pass of training, a video\nframe pair is passed through the network, including both the main branch and early exits. The\noutput of each early exit is recorded to calculate the error of TEEM in the early exit. In backward\npropagation, the error of each exit is passed back only through the TEEMs and the weights of TEEM\nare updated using gradient descent. Notably, the weights of the main branch are not updated during\ntraining of the TEEMs.\n3.4 Fast inference in a Temporal Early Exit Video Object Detection Pipeline\nAlgorithm 1 summarizes the temporal early exit video object detection pipeline inference algorithm.\nGiven a pre-trained object detection network as the main branch, Fmain, withjearly exit branches,\nthe rst video frame is processed by the main branch. The inference for successive frames proceeds\nby feeding frames through the feature network of the main branch until reaching the rst early exit\nbranch. Then, the softmax and entropy of FTEEM in the early exit are calculated to evaluate the\ncondence of the classier in prediction. If the entropy is less than the threshold and the class\nlabel with the maximum score (probability) is unchanged, the inference procedure terminates and\n5', 'document_title': 'paper.pdf'}, {'page_content': 'object detection results from the previous frame are returned. If the class label with maximum score\nis changed, then the frame is processed by the main branch. For the entropy greater than , the frame\ncontinues to be processed by the feature network till the next early exit branch. If the softmax and\nentropy of all early exits are greater than thresholds, the frame continues to be processed by the main\nbranch.\nAlgorithm 1 Inference algorithm for video object detection\nRequire: Video frames{I}\nEnsure: detection =None\nfori1tolength (I)do\nifi== 1 then\ndetectionFmain(Ii)\nreturn detection\nelse\nforj1toj= #( early exits )do\nyFTEEM j(Ii,Ii1)\nSsoftmax (y)\neentropy (S)\nife< and arg max (S) == Unchanged then\nreturn detection\nelse ife< and arg max (S) == Changed then\ndetectionFmain(Ii)\nreturn detection\nelse ife> then\npass\nend if\nend for\nend if\nend for\n4 Dataset\nThe CDnet dataset [ 5] provides a realistic, camera-captured, diverse set of videos for change detection.\nThe videos have been selected to cover a wide range of detection challenges and are representative\nof typical indoor and outdoor visual data captured in surveillance, smart environment. This dataset\nconsists of 33 camera-captured videos ( 70,000 frames) with boats, trucks, and pedestrians as objects\nof interest. It contains a range of challenging factors, including turbulence, dynamic background,\ncamera jitter challenging weather and pan-tilt-zooming (PTZ). All videos come with ground-truth\nsegmentation of motion areas for each video frame. However, CDnet dataset doesnt provide labels\nfor the individual instances of moving objects. This makes measuring SC metric challenging and\ninaccurate in scenarios with multiple moving objects. To evaluate those scenarios, we also build\nthe scenery change dataset. Our dataset is based on VIRAT dataset [ 15] which is inspired and used\nby many change detection applications [ 17]. We used Vatic engine [ 27] to label individual moving\nobjects with bounding boxes. The dataset consists of 20 camera-captured videos with pedestrian, car\nand cyclist as objects of interest. We believe our dataset can complement existing change detection\ndatasets like CDnet[5].\n4.1 Dataset Balancing\nTo train TEEMs, video frame pairs need to be randomly sampled from videos. However, random\nsampling of frame pairs causes the unbalanced class dataset (changed scenery and unchanged scenery\nclasses). The balance of classes depends on the interval between sampled video frame pairs. For\nlong sampling interval, the dataset skews to changed scenery frame pairs. On the other hand, a short\nsampling interval leads to skewing to unchanged scenery. As a result of the unbalanced dataset,\nthe classier of TEEMs tends to ignore small classes, while concentrating on learning to classify\nlarge ones accurately. We tackle the unbalanced dataset problem by proposing the dynamic sampling\ninterval approach. To this end, we divide the semantic variation into ten classes. Each class represents\n6', 'document_title': 'paper.pdf'}]: %s
2023-12-06 14:03:35,935 - INFO - Check the data that is being passed [{'page_content': 'Temporal Early Exits for Efcient Video Object\nDetection\nAmin Sabet\nSchool of Electronics and Computer Science\nUniversity of Southampton, UK\nms4r18@soton.ac.ukJonathon Hare\nSchool of Electronics and Computer Science\nUniversity of Southampton, UK\njsh2@ecs.soton.ac.uk,\nBashir Al-Hashimi\nFaculty of Natural and Mathematical Sciences\nKings College London, UK\nbashir.al-hashimi@kcl.ac.ukGeoff V . Merrett\nSchool of Electronics and Computer Science\nUniversity of Southampton, UK\ngvm@ecs.soton.ac.uk\nAbstract\nTransferring image-based object detectors to the domain of video remains chal-\nlenging under resource constraints. Previous efforts utilised optical ow to allow\nunchanged features to be propagated, however, the overhead is considerable when\nworking with very slowly changing scenes from applications such as surveillance.\nIn this paper, we propose temporal early exits to reduce the computational complex-\nity of per-frame video object detection. Multiple temporal early exit modules with\nlow computational overhead are inserted at early layers of the backbone network to\nidentify the semantic differences between consecutive frames. Full computation\nis only required if the frame is identied as having a semantic change to previous\nframes; otherwise, detection results from previous frames are reused. Experiments\non CDnet show that our method signicantly reduces the computational complexity\nand execution of per-frame video object detection up to 34compared to existing\nmethods with an acceptable reduction of 2.2% in mAP.\n1 Introduction\nObject detection is one of the fundamental tasks in computer vision and serves as a core approach\nin many practical applications, such as robotics and video surveillance [ 26,2]. Object detection in\nstatic images has achieved remarkable successes in recent years using CNNs [ 7]. However, video\nobject detection has now emerged as a new challenge beyond image data. This is due to the high\ncomputational cost introduced by applying existing image object detection networks on numerous\nindividual video frames. Figure 1-(a) shows the overview of the per-frame video object detection\napproach, where all video frames are processed by a similar CNN. Deploying per-frame video object\ndetection becomes even more challenging for resource and energy-constrained applications.\nDeep optical ow approaches [ 29] tackle the computational complexity challenge of video object\ndetection by taking advantage of temporal information in videos. They exploit feature similarity\nbetween consecutive frames to reduce the expensive feature computation on most frames and improve\nthe speed. Instead of extracting features of all frames by a deep CNN, deep optical ow uses a lighter\nnetwork to extract, propagate and aggregate features of video frames with similar features to previous\nframes. Figure 1-(b) shows the overview of deep optical ow approaches [24].\nFeature similarity between successive video frames occurs often in applications such as facility\nmonitoring and surveillance systems, where the camera is static and there are less frequently moving\nobjects in the videos [ 11]. Whilst reducing computational complexity, optical ow approaches require\nPreprint. Under review.arXiv:2106.11208v1  [cs.CV]  21 Jun 2021', 'document_title': 'paper.pdf'}, {'page_content': 'Figure 1: Comparison video object detection approaches. (a) Conventional approach: applying deep\nCNNs on individual frames. (b) Deep ow estimation: employing lighter ow estimation network\nto propagate features across frames. (c) Proposed pipeline: identifying semantic variation in early\nstages of network and avoiding deep CNN computation for unchanged video frames\nsubstantial computational effort to generate and aggregating feature maps even though the features of\nsuccessive frames remain unchanged.\nTo address the challenge of identifying and processing frames with unchanged features, we propose\na computationally lightweight, Temporal Early Exit Module (TEEM), which identies semantic\nvariations between consecutive frames. We show that the full computational effort of a network [ 17] is\nnot required to distinguish and detect semantic changes between frames. We then use TEEM to build\na per-frame video object detection pipeline, shown in gure 1-(C). In this pipeline, a TEEM identies\nsemantic variation between features of consecutive frames in the very early stages of the feature\nnetwork. Then, the TEEM conditionally activates the deeper layers of the network if a semantic\ndifference is detected between frames. If a frame is identied to be semantically unchanged, then\nthe object detection results from the previous frame are reused. By identifying unchanged frames at\nearlier stages and thus, avoiding processing video frames, the proposed pipeline signicantly reduces\nthe computational complexity and speeds up the video object detection.\nThe contributions of this paper are as follows.\nWe propose Temporal Early Exit Modules which exploit the features of the early con-\nvolutional layers to infer a 2D spatial attention map between video frames. The atten-\ntion map encodes the semantic variations between consecutive frames. The attention map is\nthen used to generate rened feature maps, feeding into a classier to classify frames into\nchanged and unchanged categories. The experiments on CDnet dataset [ 5] show that the\nTEEM classies frames into changed and unchanged classes with 94% accuracy.\nWe demonstrate a temporal early exit video object detection pipeline that uses TEEMs\nto conditionally process video frames. Full computation effort is required only for pro-\ncessing video frames with a semantic variation to previous frames. The evaluation of the\nproposed pipline on CDnet dataset [ 5] shows up to 34speed up of per-frame video object\ndetection with less than 2.2% reduction in mAP.\n2 Related Work\nOptical Flow Estimation. Recent optical ow estimation approaches divide all video frames into\nkey frame and non-key frame sets [ 30] [24] [29]. While deep networks are only applied on the\nkey-frames, a lighter ow estimation network is exploited to obtain the features of non-key frames\n(shown in Figure 1), which results in speeding up the algorithm. However, the ow estimation\napproaches rely on selecting the best key-frame, where the features of non-key frames are estimated\nfrom key-frame features. Deep feature ow network (DFF) [ 30] adopts a simple xed key-frame\nselection scheme and fails to take account of the quality of key-frame and the optical-ow. The\ndifferential network [ 24] proposes a binary differential classier network to detect the key-frames\nwith classication accuracy of up to 76%. In order to address the challenge of nding the optimal key\nvideo frames, instead of replacing the temporal features with the features of key-frames, PSLA [ 6]\nproposes to progressively update the temporal features through a recursive feature updating network.\nTo summarise, while ow estimation networks speed up video object detection, they require large\nand redundant computations for feature propagation for the majority of unchanged video frames.\n2', 'document_title': 'paper.pdf'}, {'page_content': 'Figure 2: Overview of the proposed temporal early exits video object detection pipeline.TEEMs are\nadded to feature network of a object detection network (main branch) to detect unchanged video\nframes and avoid redundant computations.\n(A)\n(B)\n(C)\nfri objects of interest i fri+j objects of interest i+j MFI (fri,fri+j)\nFigure 3: Scenery change examples, (A) shows a changed scenery with the IoU=0. (B) shows an\nunchanged scenery with the IoU=1 (C) shows a changed scenery with the IoU=0.19\nEarly Exit CNNs. In a similar line of work to ours, early exit classiers promote faster inference\nby allowing classication for easy instances to exit the network early. This class of algorithms\nare based on the observation that often the features learned at earlier stages of a deep network can\ncorrectly infer a large subset of the data samples. For example, BranchyNet [ 25] is a neural network\narchitecture which adds side branches to the main branch, the original backbone network, to allow\ncertain test samples to exit early. BranchyNet is trained by the joint optimization of loss functions for\nall exit points. In the follow-up work, conditional deep learning (CDL) [ 16] measures the efciency\nimprovement due to the addition of the linear classier at each convolutional layer to nd the optimal\nlayer for early exit branches in the backbone network. Early exit approaches reduce the runtime and\nenergy consumption of image classication. However, they have been developed only for image\nclassication applications.\nScene Change Detection. Scene change detection is a fundamental problem in the eld of computer\nvision. The most classical approach for scene change detection is image differencing [ 21,22], which\ngenerates a change map by determining the set of pixels that are signicantly different across two\nimages. Then, a binary mask is generated by thresholding the change map. Although, this method\nhas low computational cost, the raw pixel features are incapable of effectively differentiating between\nsemantic changes and noise. To obtain more discriminative features, image rationing [ 12], change\nvector analysis [ 4,3], Markov random elds [ 14] and dictionary learning [ 12,10] have been proposed.\nHowever, they are still sensitive to noise and illumination changes [19].\n3', 'document_title': 'paper.pdf'}]: %s
2023-12-06 14:03:35,935 - INFO - Check the results [{'page_content': 'Temporal Early Exits for Efcient Video Object\nDetection\nAmin Sabet\nSchool of Electronics and Computer Science\nUniversity of Southampton, UK\nms4r18@soton.ac.ukJonathon Hare\nSchool of Electronics and Computer Science\nUniversity of Southampton, UK\njsh2@ecs.soton.ac.uk,\nBashir Al-Hashimi\nFaculty of Natural and Mathematical Sciences\nKings College London, UK\nbashir.al-hashimi@kcl.ac.ukGeoff V . Merrett\nSchool of Electronics and Computer Science\nUniversity of Southampton, UK\ngvm@ecs.soton.ac.uk\nAbstract\nTransferring image-based object detectors to the domain of video remains chal-\nlenging under resource constraints. Previous efforts utilised optical ow to allow\nunchanged features to be propagated, however, the overhead is considerable when\nworking with very slowly changing scenes from applications such as surveillance.\nIn this paper, we propose temporal early exits to reduce the computational complex-\nity of per-frame video object detection. Multiple temporal early exit modules with\nlow computational overhead are inserted at early layers of the backbone network to\nidentify the semantic differences between consecutive frames. Full computation\nis only required if the frame is identied as having a semantic change to previous\nframes; otherwise, detection results from previous frames are reused. Experiments\non CDnet show that our method signicantly reduces the computational complexity\nand execution of per-frame video object detection up to 34compared to existing\nmethods with an acceptable reduction of 2.2% in mAP.\n1 Introduction\nObject detection is one of the fundamental tasks in computer vision and serves as a core approach\nin many practical applications, such as robotics and video surveillance [ 26,2]. Object detection in\nstatic images has achieved remarkable successes in recent years using CNNs [ 7]. However, video\nobject detection has now emerged as a new challenge beyond image data. This is due to the high\ncomputational cost introduced by applying existing image object detection networks on numerous\nindividual video frames. Figure 1-(a) shows the overview of the per-frame video object detection\napproach, where all video frames are processed by a similar CNN. Deploying per-frame video object\ndetection becomes even more challenging for resource and energy-constrained applications.\nDeep optical ow approaches [ 29] tackle the computational complexity challenge of video object\ndetection by taking advantage of temporal information in videos. They exploit feature similarity\nbetween consecutive frames to reduce the expensive feature computation on most frames and improve\nthe speed. Instead of extracting features of all frames by a deep CNN, deep optical ow uses a lighter\nnetwork to extract, propagate and aggregate features of video frames with similar features to previous\nframes. Figure 1-(b) shows the overview of deep optical ow approaches [24].\nFeature similarity between successive video frames occurs often in applications such as facility\nmonitoring and surveillance systems, where the camera is static and there are less frequently moving\nobjects in the videos [ 11]. Whilst reducing computational complexity, optical ow approaches require\nPreprint. Under review.arXiv:2106.11208v1  [cs.CV]  21 Jun 2021', 'document_title': 'paper.pdf'}, {'page_content': 'Figure 1: Comparison video object detection approaches. (a) Conventional approach: applying deep\nCNNs on individual frames. (b) Deep ow estimation: employing lighter ow estimation network\nto propagate features across frames. (c) Proposed pipeline: identifying semantic variation in early\nstages of network and avoiding deep CNN computation for unchanged video frames\nsubstantial computational effort to generate and aggregating feature maps even though the features of\nsuccessive frames remain unchanged.\nTo address the challenge of identifying and processing frames with unchanged features, we propose\na computationally lightweight, Temporal Early Exit Module (TEEM), which identies semantic\nvariations between consecutive frames. We show that the full computational effort of a network [ 17] is\nnot required to distinguish and detect semantic changes between frames. We then use TEEM to build\na per-frame video object detection pipeline, shown in gure 1-(C). In this pipeline, a TEEM identies\nsemantic variation between features of consecutive frames in the very early stages of the feature\nnetwork. Then, the TEEM conditionally activates the deeper layers of the network if a semantic\ndifference is detected between frames. If a frame is identied to be semantically unchanged, then\nthe object detection results from the previous frame are reused. By identifying unchanged frames at\nearlier stages and thus, avoiding processing video frames, the proposed pipeline signicantly reduces\nthe computational complexity and speeds up the video object detection.\nThe contributions of this paper are as follows.\nWe propose Temporal Early Exit Modules which exploit the features of the early con-\nvolutional layers to infer a 2D spatial attention map between video frames. The atten-\ntion map encodes the semantic variations between consecutive frames. The attention map is\nthen used to generate rened feature maps, feeding into a classier to classify frames into\nchanged and unchanged categories. The experiments on CDnet dataset [ 5] show that the\nTEEM classies frames into changed and unchanged classes with 94% accuracy.\nWe demonstrate a temporal early exit video object detection pipeline that uses TEEMs\nto conditionally process video frames. Full computation effort is required only for pro-\ncessing video frames with a semantic variation to previous frames. The evaluation of the\nproposed pipline on CDnet dataset [ 5] shows up to 34speed up of per-frame video object\ndetection with less than 2.2% reduction in mAP.\n2 Related Work\nOptical Flow Estimation. Recent optical ow estimation approaches divide all video frames into\nkey frame and non-key frame sets [ 30] [24] [29]. While deep networks are only applied on the\nkey-frames, a lighter ow estimation network is exploited to obtain the features of non-key frames\n(shown in Figure 1), which results in speeding up the algorithm. However, the ow estimation\napproaches rely on selecting the best key-frame, where the features of non-key frames are estimated\nfrom key-frame features. Deep feature ow network (DFF) [ 30] adopts a simple xed key-frame\nselection scheme and fails to take account of the quality of key-frame and the optical-ow. The\ndifferential network [ 24] proposes a binary differential classier network to detect the key-frames\nwith classication accuracy of up to 76%. In order to address the challenge of nding the optimal key\nvideo frames, instead of replacing the temporal features with the features of key-frames, PSLA [ 6]\nproposes to progressively update the temporal features through a recursive feature updating network.\nTo summarise, while ow estimation networks speed up video object detection, they require large\nand redundant computations for feature propagation for the majority of unchanged video frames.\n2', 'document_title': 'paper.pdf'}, {'page_content': 'Figure 2: Overview of the proposed temporal early exits video object detection pipeline.TEEMs are\nadded to feature network of a object detection network (main branch) to detect unchanged video\nframes and avoid redundant computations.\n(A)\n(B)\n(C)\nfri objects of interest i fri+j objects of interest i+j MFI (fri,fri+j)\nFigure 3: Scenery change examples, (A) shows a changed scenery with the IoU=0. (B) shows an\nunchanged scenery with the IoU=1 (C) shows a changed scenery with the IoU=0.19\nEarly Exit CNNs. In a similar line of work to ours, early exit classiers promote faster inference\nby allowing classication for easy instances to exit the network early. This class of algorithms\nare based on the observation that often the features learned at earlier stages of a deep network can\ncorrectly infer a large subset of the data samples. For example, BranchyNet [ 25] is a neural network\narchitecture which adds side branches to the main branch, the original backbone network, to allow\ncertain test samples to exit early. BranchyNet is trained by the joint optimization of loss functions for\nall exit points. In the follow-up work, conditional deep learning (CDL) [ 16] measures the efciency\nimprovement due to the addition of the linear classier at each convolutional layer to nd the optimal\nlayer for early exit branches in the backbone network. Early exit approaches reduce the runtime and\nenergy consumption of image classication. However, they have been developed only for image\nclassication applications.\nScene Change Detection. Scene change detection is a fundamental problem in the eld of computer\nvision. The most classical approach for scene change detection is image differencing [ 21,22], which\ngenerates a change map by determining the set of pixels that are signicantly different across two\nimages. Then, a binary mask is generated by thresholding the change map. Although, this method\nhas low computational cost, the raw pixel features are incapable of effectively differentiating between\nsemantic changes and noise. To obtain more discriminative features, image rationing [ 12], change\nvector analysis [ 4,3], Markov random elds [ 14] and dictionary learning [ 12,10] have been proposed.\nHowever, they are still sensitive to noise and illumination changes [19].\n3', 'document_title': 'paper.pdf'}]: %s
2023-12-06 14:03:37,448 - INFO - Check the data that is being passed [{'page_content': 'a variation threshold between frame pairs i.e., class one and two represent frame pairs with variation\nthreshold of [0, 0.1) and [0.1, 0.2), respectively. Consequently, starting from a random sampling\ninterval, we sample frame pairs, measure the minimum IoU of objects across frame pairs and classify\nthem into one of the ten classes. As the number of pairs in one class exceeds a threshold, we reject\nthe sampling that falls into the class and adjusts the sampling interval toward the direction which\nincreases classes with a smaller number of samples until all classes are balanced. We also face another\nclass unbalance issue due to videos with different lengths  i.e., some videos have thousands of\nframes while some have a few dozens of frames. We address this problem by upsampling frame pairs\nfrom shorter videos.\n5 Experiments and Results\nIn this section, we describe the experimental setup used to evaluate the performance of the temporal\nearly exit video object detection pipeline. The experimental results are also presented in this section.\n5.1 Implementation and Experimental Setup\nFor the main branch of the video object detection pipeline , Fmain, we use Faster-RCNN [ 20] with\nResnet50 and Resnet101 feature networks trained on MS COCO detection dataset [ 9]. We use\nTEE-Faster-RCNN to refer to this pipeline. Following Residual attention network [ 28], we add early\nexit branches after each Bottleneck and Basic blocks of the feature network. Therefore, TEE-Faster-\nRCNN consists of four early exits. We train TEEMs for 40 epochs using the Adam Optimizer [ 8]\nwith a learning rate of 0.001 and a batch size of 64. In both training and inference, the images have\nshorter sides of 224 pixels. We use 25K and 5k video frame pairs for training and testing, respectively.\nTraining was performed on 4 GPUs and testing run time is measured on a single GTX 1080 GPU. We\nset the entropy to 0.97 at test time. Our model is implemented using PyTorch [ 18], and our code and\ndataset will be made publicly available. We evaluate the accuracy, computational complexity, and run\ntime of TEE-Faster-RCNN.\n5.2 Classication Accuracy of Early Exits:\nWe evaluate the accuracy of early exits using classication accuracy, precision, recall, and F1 metrics.\nAccuracy metric enumerates the number of correct predicted changed and unchanged frames by\nTEEMs in early exit branches. Precision quanties the number of frames which predicted as the\nchanged frames and belong to the changed frame class. Recall quanties the number of changed\nframe predictions made out of all changed frame examples.\nTable 1 shows the measured accuracy, precision, recall and F1 for early exits of TEE-Faster-RCNN\nwith Resnet50 and Resnet101 feature networks. TEE-Faster-RCNN is trained for the variation\nthreshold of 0.4, equation (1). Exit1, Exit2, Exit3, and Exit4 are located after the rst, second, third,\nand fourth Bottleneck modules in the Faster-RCNN feature network, respectively. The Exit1 (with\nResnet50 feature network) classies frames into change and unchanged categories with 89% accuracy.\nThe classication accuracy increases up to 91% and 93% in Exit2 and Exit3. The observations show\nlower accuracy for Exit4 82% compares to Exit3. We believe that lower accuracy of Exit4 is due to\nthe low-resolution feature maps used by the TEEM in early exit 4 to identify variations.\nExit4 uses 77low-resolution but semantically strong feature, encoding very high-level concepts,\nto identify semantic variations across frames. Since the high-level features remain often unchanged\nacross consecutive video frames, Exit4 achieves lower accuracy in classifying video frames with\nsmall semantic variation i.e., when the location of a small object changes slightly between two frames.\nHowever, Exit4 accurately detects unchanged frame pairs as well as frame pairs with signicant\nvariations.\nCompared to TEEMs, DFF [ 24] detects the key-frames (refers to changed frames) with 76% accuracy,\nyet with high computational cost. DFF concatenates the input frame pairs and sends them as a\nsix-channel depth input through a differential network to get the difference feature map. However,\ninstead of processing input frame pairs together to identify variations, we proposed to process a video\nframe once and store and reuse the intermediate feature maps to build an attention map that encodes\nthe semantic differences between frame pairs with small computational overhead.\n7', 'document_title': 'paper.pdf'}, {'page_content': 'Table 1: Accuracy, precision, recall, and F1 scores measures for TEEMs in Faster-RCNN with\nResnet50 and Resnet101 feature networks.\nResnet50 Resnet101\nAcc Pr Re F1 Acc Pr Re F1\nExit1 0.88 0.89 0.88 0.89 0.89 0.89 0.89 0.89\nExit2 0.93 0.93 0.93 0.93 0.91 0.92 0.92 0.92\nExit3 0.91 0.91 0.92 0.91 0.94 0.92 0.92 0.93\nExit4 0.86 0.82 0.86 0.84 0.85 0.83 0.87 0.85\nfri fri+jTEEM 1TEEM 2TEEM 3TEEM 4\nFigure 4: Class activation maps of video frame pairs show that TEEMs effectively learn to focus only\non the motions of the objects of interest between video frames to identify scenery change.\n5.3 Class Activation Map of TEEMs\nTo visualize the performance of TEEMs, gure 4 shows class activation map of TEEMs. Each class\nactivation map shows which parts of a video frame have contributed more to the nal output of\nthe TEEMs. Figure 4 shows that TEEMs effectively learn to focus on the moving elds of interest\nbetween frame pairs to identify variations. Futhermore, Figure 4 illustrates that TEEM1 and TEEM2\nidentify moving objects in higher resolution because the input feature maps into TEEM1 and TEEM2\nhave high resolution. The resolution of input feature maps into TEEM1 and TEEM2 are 5656\nand2828resolution, respectively. However, the class activation map of TEEM3 and TEEM4 are\ncoarse-grain because they use low-resolution feature maps of 1414and77, respectively.\n5.4 Computational Complexity\nTable 2 compares computational cost (number of MAC operations and parameters) and run time\n(frames per second) of processing video frames by the original Faster-RCNN and TEE-Faster-RCNN\nbranches. The rst row of table 2 indicates the original Faster-RCNN network, including feature\nnetwork, region proposal network, and region-based convolutional neural network.\nExit 1, Exit2, Exit3, and Exit4 refer to computational paths of TEE-Faster-RCNN from the input of\nFaster-RCNN to TEEM1, TEEM2, TEEM3, and TEEM4, respectively. The second column of table 2\nshows the required MAC operations and parameters for each computation path of TEE-Faster-RCNN\nwith Resnet50 feature network. The third columns shows the speed of each early exit. The fourth\ncolumn shows the computational complexity the TEEM added in early exits. The second part of table\n2 shows the same results for TEE-Faster-RCNN with resnet101 feature network.\nThe original Faster-RCNN with Resnet51 feature network requires up to 134G MAC operations\nand 41M parameters to process a video frame. The high computation requirement limits the video\nobject detection speed to 18 fps, 14 fps for the Faster-RCNN with Resnet101 feature network. Using\nthe same network architecture for processing all video frames regardless of the semantic variations\nbetween neighbouring frames leads to the inferior use of limited energy and computational resources.\nHowever, the TEE-Faster-RCNN uses computationally lightweight early exit branches to process\nunchanged video frames. Table 2 shows that early exit branches have substantially less computations\nand memory requirements which speeds up processing video frames. Exit1 branch requires only\n8', 'document_title': 'paper.pdf'}, {'page_content': 'Table 2: Computational complexity and speed of TEE-Faster-RCNN video object detection.\nResnet51 Resnet101\nbranch(Op, Par) Speed TEEM(Opr,Par) branch(Op, Par) Speed TEEM(Opr,Par)\nFR (134G , 41M) 18fps - (181G , 60) 14fps -\nExit1 (1.7G , 0.525M) 628fps (0.94G , 0.3M) (1.7G, 0.525M) 597fps (0.94G , 0.3M)\nExit2 (2.7G , 2.615M) 390fps (0.93G , 1.17M) (3.7G, 2.910M) 400fps (0.93G , 1.17M)\nExit3 (3.5G , 9.733M) 263fps (0.23G , 1.19M) (9.1G, 30.195M) 140fps (0.23G , 1.19M)\nExit4 (5G , 23.808) 218fps (0.25G , 5.129M) (9.9G, 50.289M) 126fps (0.2G , 5.129M)\nTable 3: Comparing the detection accuracy of TEE-Faster-RCNN with per-frame Faster-RCNN\nUpdating ratio mAP mIoU\nFaster-RCNN 1 0.231 0.8\nFixed-Step 7 0.211 0.76\nFixed-Step 10 0.183 0.75\nFixed-Step 20 0.16 0.70\nTEE-Faster-RCNN 20 0.209 0.75\n1.7G MAC operations and uses 0.5 M parameters. Signicant reduction in computation complexity\nis because of avoided parts of the feature network, region proposal network, and region-based\nconvolutional neural network. This reduction in computations speeds up processing unchanged video\nframes up to 628 fps. The required MAC operation for Exit2, Exit3 and Exit4 branches are 2.7G,\n3.5G, and 5G, respectively. Exit2, Exit3 and Exit4 speed up processing unchanged frames to 390 fps,\n263 fps, and 218 fps, respectively. The fourth column of table 2 shows the required MAC operation\nand number of parameters for TEEMs.\n5.5 Detection Accuracy\nHaving tested the classication performance of TEEMs, we then evaluate the mean average precision\n(mAP @0.35:0.05:0.75) and mean intersection over union (mIOU) of TEE-Faster-RCNN video object\ndetection. Table 3 compares the accuracy of TEE-Faster-RCNN detection results with the per-frame\noriginal Faster-RCNN object detection. TEE-Faster-RCNN updates the detection results at the\naverage step of 20. Therefore, for a fair comparison we performed Faster-RCNN with different xed\nupdating steps. The results reect that the accuracy of TEE-Faster-RCNN cannot compete with the\nper-frame video object detection approach. Whilst, TEE-Faster-RCNN achieves the same accuracy\nof per-frame Faster-RCNN with the xed updating steps of 7, its updating step of TEE-Faster-RCNN\nis 20 on average. Notably, TEE-Faster-RCNN does not aim to improve the accuracy of detection but\nintroduces a simple yet effective approach to signicantly reduce the computational complexity of\nvideo object detection for the video applications with less frequent moving objects e.g., the CDnet\ndataset [ 5]. For applications with frequent moving objects such as the ImageNet-VID dataset [ 23],\nmore complex methods like optical ow approaches are needed to achieve better accuracy.\n6 Conclusion\nWe proposes a temporal early exit object detection pipeline to reduce the computational complexity\nof per-frame video object detection. The proposed approach takes advantage of infrequent variation\nbetween features of consecutive video frames to avoid redundant computation. Video frames with\ninvariant features are identied in the early stages of the network with very low computation effort. For\nthe unchanged video frames, detection results from previous frames are reused. A full computation\neffort is only required if a video frame is identied with semantic variations compared to previous\nframes. The proposed approach accelerates per-frame video object detection up to 34with less than\n2.2 % reduction in mAP.\n9', 'document_title': 'paper.pdf'}, {'page_content': 'References\n[1]Pablo F Alcantarilla, Simon Stent, German Ros, Roberto Arroyo, and Riccardo Gherardi. Street-\nview change detection with deconvolutional networks. Autonomous Robots , 42(7):13011322,\n2018.\n[2]Ali Borji, Ming-Ming Cheng, Huaizu Jiang, and Jia Li. Salient object detection: A benchmark.\nIEEE transactions on image processing , 24(12):57065722, 2015.\n[3]Francesca Bovolo and Lorenzo Bruzzone. A theoretical framework for unsupervised change\ndetection based on change vector analysis in the polar domain. IEEE Transactions on Geoscience\nand Remote Sensing , 45(1):218236, 2006.\n[4]Lorenzo Bruzzone and D Fernandez Prieto. An adaptive semiparametric and context-based\napproach to unsupervised change detection in multitemporal remote-sensing images. IEEE\nTransactions on image processing , 11(4):452466, 2002.\n[5]Nil Goyette, Pierre-Marc Jodoin, Fatih Porikli, Janusz Konrad, and Prakash Ishwar. Changede-\ntection. net: A new change detection benchmark dataset. In 2012 IEEE computer society\nconference on computer vision and pattern recognition workshops , pages 18. IEEE, 2012.\n[6]Chaoxu Guo, Bin Fan, Jie Gu, Qian Zhang, Shiming Xiang, Veronique Prinet, and Chunhong\nPan. Progressive sparse local attention for video object detection. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision , pages 39093918, 2019.\n[7]Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi,\nIan Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, et al. Speed/accuracy trade-offs\nfor modern convolutional object detectors. In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 73107311, 2017.\n[8]Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014.\n[9]Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDollr, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European\nconference on computer vision , pages 740755. Springer, 2014.\n[10] Xiaoqiang Lu, Yuan Yuan, and Xiangtao Zheng. Joint dictionary learning for multispectral\nchange detection. IEEE transactions on cybernetics , 47(4):884897, 2016.\n[11] Yuan Luo, Hanxing Zhou, Qin Tan, Xuefeng Chen, and Mingjing Yun. Key frame extraction of\nsurveillance video based on moving object detection and image similarity. Pattern Recognition\nand Image Analysis , 28(2):225231, 2018.\n[12] H MAHMOUDZADEH. Digital change detection using remotely sensed data for monitoring\ngreen space destruction in tabriz. 2007.\n[13] David Mascharka, Philip Tran, Ryan Soklaski, and Arjun Majumdar. Transparency by design:\nClosing the gap between performance and interpretability in visual reasoning. In Proceedings\nof the IEEE conference on computer vision and pattern recognition , pages 49424950, 2018.\n[14] Gabriele Moser, Elena Angiati, and Sebastiano B Serpico. Multiscale unsupervised change\ndetection on optical images by markov random elds and wavelets. IEEE Geoscience and\nRemote Sensing Letters , 8(4):725729, 2011.\n[15] Sangmin Oh, Anthony Hoogs, Amitha Perera, Naresh Cuntoor, Chia-Chih Chen, Jong Taek Lee,\nSaurajit Mukherjee, JK Aggarwal, Hyungtae Lee, Larry Davis, et al. A large-scale benchmark\ndataset for event recognition in surveillance video. In CVPR 2011 , pages 31533160. IEEE,\n2011.\n[16] Priyadarshini Panda, Abhronil Sengupta, and Kaushik Roy. Conditional deep learning for\nenergy-efcient and enhanced pattern recognition. In 2016 Design, Automation & Test in\nEurope Conference & Exhibition (DATE) , pages 475480. IEEE, 2016.\n10', 'document_title': 'paper.pdf'}, {'page_content': '[17] Dong Huk Park, Trevor Darrell, and Anna Rohrbach. Robust change captioning. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision , pages 46244633, 2019.\n[18] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative\nstyle, high-performance deep learning library. arXiv preprint arXiv:1912.01703 , 2019.\n[19] Richard J Radke, Srinivas Andra, Omar Al-Kofahi, and Badrinath Roysam. Image change\ndetection algorithms: a systematic survey. IEEE transactions on image processing , 14(3):\n294307, 2005.\n[20] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time\nobject detection with region proposal networks. arXiv preprint arXiv:1506.01497 , 2015.\n[21] Paul Rosin. Thresholding for change detection. In Sixth International Conference on Computer\nVision (IEEE Cat. No. 98CH36271) , pages 274279. IEEE, 1998.\n[22] Paul L Rosin and Efstathios Ioannidis. Evaluation of global image thresholding for change\ndetection. Pattern recognition letters , 24(14):23452356, 2003.\n[23] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual\nrecognition challenge. International journal of computer vision , 115(3):211252, 2015.\n[24] Jing Shi and Chenliang Xu. Differential network for video object detection.\n[25] Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Branchynet: Fast inference\nvia early exiting from deep neural networks. In 2016 23rd International Conference on Pattern\nRecognition (ICPR) , pages 24642469. IEEE, 2016.\n[26] Yonglong Tian, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning strong parts for\npedestrian detection. In Proceedings of the IEEE international conference on computer vision ,\npages 19041912, 2015.\n[27] Carl V ondrick, Donald Patterson, and Deva Ramanan. Efciently scaling up crowdsourced\nvideo annotation. International Journal of Computer Vision , pages 121. ISSN 0920-5691.\nURL http://dx.doi.org/10.1007/s11263-012-0564-1 . 10.1007/s11263-012-0564-1.\n[28] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang\nWang, and Xiaoou Tang. Residual attention network for image classication. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition , pages 31563164, 2017.\n[29] Philippe Weinzaepfel, Jerome Revaud, Zaid Harchaoui, and Cordelia Schmid. Deepow: Large\ndisplacement optical ow with deep matching. In Proceedings of the IEEE international\nconference on computer vision , pages 13851392, 2013.\n[30] Xizhou Zhu, Yuwen Xiong, Jifeng Dai, Lu Yuan, and Yichen Wei. Deep feature ow for video\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 23492358, 2017.\n11', 'document_title': 'paper.pdf'}]: %s
2023-12-06 14:03:37,448 - INFO - Check the results [{'page_content': 'a variation threshold between frame pairs i.e., class one and two represent frame pairs with variation\nthreshold of [0, 0.1) and [0.1, 0.2), respectively. Consequently, starting from a random sampling\ninterval, we sample frame pairs, measure the minimum IoU of objects across frame pairs and classify\nthem into one of the ten classes. As the number of pairs in one class exceeds a threshold, we reject\nthe sampling that falls into the class and adjusts the sampling interval toward the direction which\nincreases classes with a smaller number of samples until all classes are balanced. We also face another\nclass unbalance issue due to videos with different lengths  i.e., some videos have thousands of\nframes while some have a few dozens of frames. We address this problem by upsampling frame pairs\nfrom shorter videos.\n5 Experiments and Results\nIn this section, we describe the experimental setup used to evaluate the performance of the temporal\nearly exit video object detection pipeline. The experimental results are also presented in this section.\n5.1 Implementation and Experimental Setup\nFor the main branch of the video object detection pipeline , Fmain, we use Faster-RCNN [ 20] with\nResnet50 and Resnet101 feature networks trained on MS COCO detection dataset [ 9]. We use\nTEE-Faster-RCNN to refer to this pipeline. Following Residual attention network [ 28], we add early\nexit branches after each Bottleneck and Basic blocks of the feature network. Therefore, TEE-Faster-\nRCNN consists of four early exits. We train TEEMs for 40 epochs using the Adam Optimizer [ 8]\nwith a learning rate of 0.001 and a batch size of 64. In both training and inference, the images have\nshorter sides of 224 pixels. We use 25K and 5k video frame pairs for training and testing, respectively.\nTraining was performed on 4 GPUs and testing run time is measured on a single GTX 1080 GPU. We\nset the entropy to 0.97 at test time. Our model is implemented using PyTorch [ 18], and our code and\ndataset will be made publicly available. We evaluate the accuracy, computational complexity, and run\ntime of TEE-Faster-RCNN.\n5.2 Classication Accuracy of Early Exits:\nWe evaluate the accuracy of early exits using classication accuracy, precision, recall, and F1 metrics.\nAccuracy metric enumerates the number of correct predicted changed and unchanged frames by\nTEEMs in early exit branches. Precision quanties the number of frames which predicted as the\nchanged frames and belong to the changed frame class. Recall quanties the number of changed\nframe predictions made out of all changed frame examples.\nTable 1 shows the measured accuracy, precision, recall and F1 for early exits of TEE-Faster-RCNN\nwith Resnet50 and Resnet101 feature networks. TEE-Faster-RCNN is trained for the variation\nthreshold of 0.4, equation (1). Exit1, Exit2, Exit3, and Exit4 are located after the rst, second, third,\nand fourth Bottleneck modules in the Faster-RCNN feature network, respectively. The Exit1 (with\nResnet50 feature network) classies frames into change and unchanged categories with 89% accuracy.\nThe classication accuracy increases up to 91% and 93% in Exit2 and Exit3. The observations show\nlower accuracy for Exit4 82% compares to Exit3. We believe that lower accuracy of Exit4 is due to\nthe low-resolution feature maps used by the TEEM in early exit 4 to identify variations.\nExit4 uses 77low-resolution but semantically strong feature, encoding very high-level concepts,\nto identify semantic variations across frames. Since the high-level features remain often unchanged\nacross consecutive video frames, Exit4 achieves lower accuracy in classifying video frames with\nsmall semantic variation i.e., when the location of a small object changes slightly between two frames.\nHowever, Exit4 accurately detects unchanged frame pairs as well as frame pairs with signicant\nvariations.\nCompared to TEEMs, DFF [ 24] detects the key-frames (refers to changed frames) with 76% accuracy,\nyet with high computational cost. DFF concatenates the input frame pairs and sends them as a\nsix-channel depth input through a differential network to get the difference feature map. However,\ninstead of processing input frame pairs together to identify variations, we proposed to process a video\nframe once and store and reuse the intermediate feature maps to build an attention map that encodes\nthe semantic differences between frame pairs with small computational overhead.\n7', 'document_title': 'paper.pdf'}, {'page_content': 'Table 1: Accuracy, precision, recall, and F1 scores measures for TEEMs in Faster-RCNN with\nResnet50 and Resnet101 feature networks.\nResnet50 Resnet101\nAcc Pr Re F1 Acc Pr Re F1\nExit1 0.88 0.89 0.88 0.89 0.89 0.89 0.89 0.89\nExit2 0.93 0.93 0.93 0.93 0.91 0.92 0.92 0.92\nExit3 0.91 0.91 0.92 0.91 0.94 0.92 0.92 0.93\nExit4 0.86 0.82 0.86 0.84 0.85 0.83 0.87 0.85\nfri fri+jTEEM 1TEEM 2TEEM 3TEEM 4\nFigure 4: Class activation maps of video frame pairs show that TEEMs effectively learn to focus only\non the motions of the objects of interest between video frames to identify scenery change.\n5.3 Class Activation Map of TEEMs\nTo visualize the performance of TEEMs, gure 4 shows class activation map of TEEMs. Each class\nactivation map shows which parts of a video frame have contributed more to the nal output of\nthe TEEMs. Figure 4 shows that TEEMs effectively learn to focus on the moving elds of interest\nbetween frame pairs to identify variations. Futhermore, Figure 4 illustrates that TEEM1 and TEEM2\nidentify moving objects in higher resolution because the input feature maps into TEEM1 and TEEM2\nhave high resolution. The resolution of input feature maps into TEEM1 and TEEM2 are 5656\nand2828resolution, respectively. However, the class activation map of TEEM3 and TEEM4 are\ncoarse-grain because they use low-resolution feature maps of 1414and77, respectively.\n5.4 Computational Complexity\nTable 2 compares computational cost (number of MAC operations and parameters) and run time\n(frames per second) of processing video frames by the original Faster-RCNN and TEE-Faster-RCNN\nbranches. The rst row of table 2 indicates the original Faster-RCNN network, including feature\nnetwork, region proposal network, and region-based convolutional neural network.\nExit 1, Exit2, Exit3, and Exit4 refer to computational paths of TEE-Faster-RCNN from the input of\nFaster-RCNN to TEEM1, TEEM2, TEEM3, and TEEM4, respectively. The second column of table 2\nshows the required MAC operations and parameters for each computation path of TEE-Faster-RCNN\nwith Resnet50 feature network. The third columns shows the speed of each early exit. The fourth\ncolumn shows the computational complexity the TEEM added in early exits. The second part of table\n2 shows the same results for TEE-Faster-RCNN with resnet101 feature network.\nThe original Faster-RCNN with Resnet51 feature network requires up to 134G MAC operations\nand 41M parameters to process a video frame. The high computation requirement limits the video\nobject detection speed to 18 fps, 14 fps for the Faster-RCNN with Resnet101 feature network. Using\nthe same network architecture for processing all video frames regardless of the semantic variations\nbetween neighbouring frames leads to the inferior use of limited energy and computational resources.\nHowever, the TEE-Faster-RCNN uses computationally lightweight early exit branches to process\nunchanged video frames. Table 2 shows that early exit branches have substantially less computations\nand memory requirements which speeds up processing video frames. Exit1 branch requires only\n8', 'document_title': 'paper.pdf'}, {'page_content': 'Table 2: Computational complexity and speed of TEE-Faster-RCNN video object detection.\nResnet51 Resnet101\nbranch(Op, Par) Speed TEEM(Opr,Par) branch(Op, Par) Speed TEEM(Opr,Par)\nFR (134G , 41M) 18fps - (181G , 60) 14fps -\nExit1 (1.7G , 0.525M) 628fps (0.94G , 0.3M) (1.7G, 0.525M) 597fps (0.94G , 0.3M)\nExit2 (2.7G , 2.615M) 390fps (0.93G , 1.17M) (3.7G, 2.910M) 400fps (0.93G , 1.17M)\nExit3 (3.5G , 9.733M) 263fps (0.23G , 1.19M) (9.1G, 30.195M) 140fps (0.23G , 1.19M)\nExit4 (5G , 23.808) 218fps (0.25G , 5.129M) (9.9G, 50.289M) 126fps (0.2G , 5.129M)\nTable 3: Comparing the detection accuracy of TEE-Faster-RCNN with per-frame Faster-RCNN\nUpdating ratio mAP mIoU\nFaster-RCNN 1 0.231 0.8\nFixed-Step 7 0.211 0.76\nFixed-Step 10 0.183 0.75\nFixed-Step 20 0.16 0.70\nTEE-Faster-RCNN 20 0.209 0.75\n1.7G MAC operations and uses 0.5 M parameters. Signicant reduction in computation complexity\nis because of avoided parts of the feature network, region proposal network, and region-based\nconvolutional neural network. This reduction in computations speeds up processing unchanged video\nframes up to 628 fps. The required MAC operation for Exit2, Exit3 and Exit4 branches are 2.7G,\n3.5G, and 5G, respectively. Exit2, Exit3 and Exit4 speed up processing unchanged frames to 390 fps,\n263 fps, and 218 fps, respectively. The fourth column of table 2 shows the required MAC operation\nand number of parameters for TEEMs.\n5.5 Detection Accuracy\nHaving tested the classication performance of TEEMs, we then evaluate the mean average precision\n(mAP @0.35:0.05:0.75) and mean intersection over union (mIOU) of TEE-Faster-RCNN video object\ndetection. Table 3 compares the accuracy of TEE-Faster-RCNN detection results with the per-frame\noriginal Faster-RCNN object detection. TEE-Faster-RCNN updates the detection results at the\naverage step of 20. Therefore, for a fair comparison we performed Faster-RCNN with different xed\nupdating steps. The results reect that the accuracy of TEE-Faster-RCNN cannot compete with the\nper-frame video object detection approach. Whilst, TEE-Faster-RCNN achieves the same accuracy\nof per-frame Faster-RCNN with the xed updating steps of 7, its updating step of TEE-Faster-RCNN\nis 20 on average. Notably, TEE-Faster-RCNN does not aim to improve the accuracy of detection but\nintroduces a simple yet effective approach to signicantly reduce the computational complexity of\nvideo object detection for the video applications with less frequent moving objects e.g., the CDnet\ndataset [ 5]. For applications with frequent moving objects such as the ImageNet-VID dataset [ 23],\nmore complex methods like optical ow approaches are needed to achieve better accuracy.\n6 Conclusion\nWe proposes a temporal early exit object detection pipeline to reduce the computational complexity\nof per-frame video object detection. The proposed approach takes advantage of infrequent variation\nbetween features of consecutive video frames to avoid redundant computation. Video frames with\ninvariant features are identied in the early stages of the network with very low computation effort. For\nthe unchanged video frames, detection results from previous frames are reused. A full computation\neffort is only required if a video frame is identied with semantic variations compared to previous\nframes. The proposed approach accelerates per-frame video object detection up to 34with less than\n2.2 % reduction in mAP.\n9', 'document_title': 'paper.pdf'}, {'page_content': 'References\n[1]Pablo F Alcantarilla, Simon Stent, German Ros, Roberto Arroyo, and Riccardo Gherardi. Street-\nview change detection with deconvolutional networks. Autonomous Robots , 42(7):13011322,\n2018.\n[2]Ali Borji, Ming-Ming Cheng, Huaizu Jiang, and Jia Li. Salient object detection: A benchmark.\nIEEE transactions on image processing , 24(12):57065722, 2015.\n[3]Francesca Bovolo and Lorenzo Bruzzone. A theoretical framework for unsupervised change\ndetection based on change vector analysis in the polar domain. IEEE Transactions on Geoscience\nand Remote Sensing , 45(1):218236, 2006.\n[4]Lorenzo Bruzzone and D Fernandez Prieto. An adaptive semiparametric and context-based\napproach to unsupervised change detection in multitemporal remote-sensing images. IEEE\nTransactions on image processing , 11(4):452466, 2002.\n[5]Nil Goyette, Pierre-Marc Jodoin, Fatih Porikli, Janusz Konrad, and Prakash Ishwar. Changede-\ntection. net: A new change detection benchmark dataset. In 2012 IEEE computer society\nconference on computer vision and pattern recognition workshops , pages 18. IEEE, 2012.\n[6]Chaoxu Guo, Bin Fan, Jie Gu, Qian Zhang, Shiming Xiang, Veronique Prinet, and Chunhong\nPan. Progressive sparse local attention for video object detection. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision , pages 39093918, 2019.\n[7]Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi,\nIan Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, et al. Speed/accuracy trade-offs\nfor modern convolutional object detectors. In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 73107311, 2017.\n[8]Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014.\n[9]Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDollr, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European\nconference on computer vision , pages 740755. Springer, 2014.\n[10] Xiaoqiang Lu, Yuan Yuan, and Xiangtao Zheng. Joint dictionary learning for multispectral\nchange detection. IEEE transactions on cybernetics , 47(4):884897, 2016.\n[11] Yuan Luo, Hanxing Zhou, Qin Tan, Xuefeng Chen, and Mingjing Yun. Key frame extraction of\nsurveillance video based on moving object detection and image similarity. Pattern Recognition\nand Image Analysis , 28(2):225231, 2018.\n[12] H MAHMOUDZADEH. Digital change detection using remotely sensed data for monitoring\ngreen space destruction in tabriz. 2007.\n[13] David Mascharka, Philip Tran, Ryan Soklaski, and Arjun Majumdar. Transparency by design:\nClosing the gap between performance and interpretability in visual reasoning. In Proceedings\nof the IEEE conference on computer vision and pattern recognition , pages 49424950, 2018.\n[14] Gabriele Moser, Elena Angiati, and Sebastiano B Serpico. Multiscale unsupervised change\ndetection on optical images by markov random elds and wavelets. IEEE Geoscience and\nRemote Sensing Letters , 8(4):725729, 2011.\n[15] Sangmin Oh, Anthony Hoogs, Amitha Perera, Naresh Cuntoor, Chia-Chih Chen, Jong Taek Lee,\nSaurajit Mukherjee, JK Aggarwal, Hyungtae Lee, Larry Davis, et al. A large-scale benchmark\ndataset for event recognition in surveillance video. In CVPR 2011 , pages 31533160. IEEE,\n2011.\n[16] Priyadarshini Panda, Abhronil Sengupta, and Kaushik Roy. Conditional deep learning for\nenergy-efcient and enhanced pattern recognition. In 2016 Design, Automation & Test in\nEurope Conference & Exhibition (DATE) , pages 475480. IEEE, 2016.\n10', 'document_title': 'paper.pdf'}, {'page_content': '[17] Dong Huk Park, Trevor Darrell, and Anna Rohrbach. Robust change captioning. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision , pages 46244633, 2019.\n[18] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative\nstyle, high-performance deep learning library. arXiv preprint arXiv:1912.01703 , 2019.\n[19] Richard J Radke, Srinivas Andra, Omar Al-Kofahi, and Badrinath Roysam. Image change\ndetection algorithms: a systematic survey. IEEE transactions on image processing , 14(3):\n294307, 2005.\n[20] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time\nobject detection with region proposal networks. arXiv preprint arXiv:1506.01497 , 2015.\n[21] Paul Rosin. Thresholding for change detection. In Sixth International Conference on Computer\nVision (IEEE Cat. No. 98CH36271) , pages 274279. IEEE, 1998.\n[22] Paul L Rosin and Efstathios Ioannidis. Evaluation of global image thresholding for change\ndetection. Pattern recognition letters , 24(14):23452356, 2003.\n[23] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual\nrecognition challenge. International journal of computer vision , 115(3):211252, 2015.\n[24] Jing Shi and Chenliang Xu. Differential network for video object detection.\n[25] Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Branchynet: Fast inference\nvia early exiting from deep neural networks. In 2016 23rd International Conference on Pattern\nRecognition (ICPR) , pages 24642469. IEEE, 2016.\n[26] Yonglong Tian, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning strong parts for\npedestrian detection. In Proceedings of the IEEE international conference on computer vision ,\npages 19041912, 2015.\n[27] Carl V ondrick, Donald Patterson, and Deva Ramanan. Efciently scaling up crowdsourced\nvideo annotation. International Journal of Computer Vision , pages 121. ISSN 0920-5691.\nURL http://dx.doi.org/10.1007/s11263-012-0564-1 . 10.1007/s11263-012-0564-1.\n[28] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang\nWang, and Xiaoou Tang. Residual attention network for image classication. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition , pages 31563164, 2017.\n[29] Philippe Weinzaepfel, Jerome Revaud, Zaid Harchaoui, and Cordelia Schmid. Deepow: Large\ndisplacement optical ow with deep matching. In Proceedings of the IEEE international\nconference on computer vision , pages 13851392, 2013.\n[30] Xizhou Zhu, Yuwen Xiong, Jifeng Dai, Lu Yuan, and Yichen Wei. Deep feature ow for video\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 23492358, 2017.\n11', 'document_title': 'paper.pdf'}]: %s
2023-12-06 14:05:36,539 - INFO - Received requests to /inference endpoint
2023-12-06 14:05:36,640 - INFO - Received a batch of request with batch size of: 1 
2023-12-06 14:05:36,640 - INFO - Received request: {'username': 'amin', 'prompt': 'give me a summary of  paper', 'memory': False, 'conversation_number': 0, 'AI_assistance': False, 'collection_name': 'web', 'llm_model': 'Llama_13b'}
2023-12-06 14:06:16,541 - INFO - Processed the request successfully
2023-12-06 15:55:42,728 - INFO - request processed successfully username='amin' class_name=None mode=None vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/39b003ed377ad2b1': %s
2023-12-06 15:55:42,728 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 16:02:33,162 - INFO - request processed successfully username='amin' class_name=None mode=None vectorDB_type='Weaviate' file_path=None: %s
2023-12-06 16:02:33,162 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 16:06:03,313 - INFO - request processed successfully username='amin' class_name=None mode=None vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/739c23f5a50ad4f9': %s
2023-12-06 16:06:03,313 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 16:06:09,981 - INFO - request processed successfully username='amin' class_name=None mode=None vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/b19ae0db2bb367cf': %s
2023-12-06 16:06:09,981 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 16:08:38,388 - INFO - request processed successfully username='amin' class_name=None mode=None vectorDB_type='Weaviate' file_path=None: %s
2023-12-06 16:08:38,388 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 16:09:44,982 - INFO - request processed successfully username='amin' class_name=None mode=None vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/9d5f6182ff1d72bf': %s
2023-12-06 16:09:44,982 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 16:10:10,877 - INFO - request processed successfully username='amin' class_name=None mode=None vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/b547716aa2174f0e': %s
2023-12-06 16:10:10,877 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 16:10:31,046 - INFO - request processed successfully username='amin' class_name=None mode=None vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/a7297a2e0806883d': %s
2023-12-06 16:10:31,046 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 16:10:38,626 - INFO - request processed successfully username='amin' class_name=None mode=None vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/311bea31f43d0826': %s
2023-12-06 16:10:38,626 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 16:20:45,570 - INFO - checking the request/ username='amin' class_name='sdee' mode='create_collection' vectorDB_type='Weaviate' file_path=None: %s
2023-12-06 16:20:45,663 - INFO - checkpoint 1
2023-12-06 16:20:45,663 - INFO - checkpoint 2 amin: %s
2023-12-06 16:20:45,663 - INFO - checkpoint 2 amin_sdee: %s
2023-12-06 16:20:45,711 - INFO - class name added successfully to database
2023-12-06 16:20:45,711 - INFO - success: class sdee created for user amin
2023-12-07 10:15:06,924 - INFO - Created a temporary directory at /tmp/tmpymd0ku3l
2023-12-07 10:15:06,924 - INFO - Writing /tmp/tmpymd0ku3l/_remote_module_non_scriptable.py
2023-12-07 10:15:09,289 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-07 10:15:34,232 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-07 10:15:49,009 - INFO - Created a temporary directory at /tmp/tmp8pin0ush
2023-12-07 10:15:49,009 - INFO - Writing /tmp/tmp8pin0ush/_remote_module_non_scriptable.py
2023-12-07 10:15:50,821 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-07 10:15:55,067 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-07 10:16:04,557 - INFO - checking the request/ username='amin' class_name='genAI' mode='create_collection' vectorDB_type='Weaviate' file_path=None: %s
2023-12-07 10:16:04,617 - INFO - checkpoint 1
2023-12-07 10:16:04,617 - INFO - checkpoint 2 amin: %s
2023-12-07 10:16:04,617 - INFO - checkpoint 2 amin_genAI: %s
2023-12-07 10:16:04,659 - INFO - class name added successfully to database
2023-12-07 10:16:04,659 - INFO - success: class genAI created for user amin
2023-12-07 10:17:35,864 - INFO - request received username='amin' class_name='genAI' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/0273b9b7518984f6': %s
2023-12-07 10:17:36,445 - INFO - actors creation successful [Actor(WeaviateEmbedder, 81e0adc786a4f367b581fb0501000000), Actor(WeaviateEmbedder, 550d19d444353d63f639428a01000000), Actor(WeaviateEmbedder, cf13543a67d30513b458ef2901000000)]: %s
2023-12-07 10:17:36,446 - INFO - check 1st step of ray was successful
2023-12-07 10:17:36,446 - INFO - check if ray was successful:
2023-12-07 10:17:36,446 - INFO - check weaviate add data, 
2023-12-07 10:17:36,446 - INFO - request processed successfully username='amin' class_name='genAI' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/0273b9b7518984f6': %s
2023-12-07 10:17:38,260 - INFO - Check the data that is being passed [{'page_content': 'DINOv2: Learning Robust Visual Features\nwithout Supervision\nMaxime Oquab, Timothe Darcet, Tho Moutakanni,\nHuy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza,\nFrancisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba,\nRussell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat,\nVasu Sharma, Gabriel Synnaeve, Hu Xu, Herv Jegou, Julien Mairal1,\nPatrick Labatut, Armand Joulin, Piotr Bojanowski\nMeta AI Research1Inria\ncore teamequal contribution\nAbstract\nThe recent breakthroughs in natural language processing for model pretraining on large\nquantities of data have opened the way for similar foundation models in computer vision.\nThese models could greatly simplify the use of images in any system by producing all-\npurpose visual features, i.e., features that work across image distributions and tasks without\nnetuning. This work shows that existing pretraining methods, especially self-supervised\nmethods, can produce such features if trained on enough curated data from diverse sources.\nWe revisit existing approaches and combine dierent techniques to scale our pretraining in\nterms of data and model size. Most of the technical contributions aim at accelerating and\nstabilizing the training at scale. In terms of data, we propose an automatic pipeline to build\na dedicated, diverse, and curated image dataset instead of uncurated data, as typically done\nin the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy\net al., 2020) with 1B parameters and distill it into a series of smaller models that surpass\nthe best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the\nbenchmarks at image and pixel levels.\n1 Introduction\nLearning task-agnostic pretrained representations have become the standard in Natural Language Process-\ning (NLP) (Radford et al.; Rael et al., 2020; Chowdhery et al., 2022; Homann et al., 2022; Touvron et al.,\n2023). One can use these features as they are, i.e., without ne-tuning, and achieve performances on down-\nstream tasks that are signicantly better than those produced by task-specic models (Brown et al., 2020).\nThis success has been fueled by pretraining on large quantities of raw text using pretext objectives, such as\nlanguage modeling (Radford et al., 2017) or word vectors (Devlin et al., 2018), that require no supervision.\nFollowing this paradigm shift in NLP, we expect similar foundation models to appear in computer vi-\nsion (Bommasani et al., 2021). These models should generate visual features that work out of the box on\nany task, both at the image level, e.g., image classication, and pixel level, e.g., segmentation. Most promis-\ning eorts towards these foundation models focus on text-guided pretraining, i.e., using a form of textual\nsupervision to guide the training of the features (Joulin et al., 2016; Mahajan et al., 2018; Radford et al.,\n2021). This form of text-guided pretraining limits the information that can be retained about the image\nsince captions only approximate the rich information in images, and complex pixel-level information may\nAll the authors are aliated to Meta, except Julien Mairal who is aliated to Inria. Timothe Darcet and Pierre Fernandez\nhave a co-aliation with Inria. Tho Moutakanni has a co-aliation with Universit Paris Saclay. Alaaeldin El-Nouby has a\nco-aliation with Inria and ENS-PSL. Correspondence: {qas, timdarcet, theomoutakanni, ajoulin, bojanowski}@meta.com\n1arXiv:2304.07193v1  [cs.CV]  14 Apr 2023', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 1: Visualization of the rst PCA components. We compute a PCA between the patches of the\nimages from the same column (a, b, c and d) and show their rst 3 components. Each component is matched\nto a dierent color channel. Same parts are matched between related images despite changes of pose, style\nor even objects. Background is removed by thresholding the rst PCA component.\nnot surface with this supervision. Furthermore, these image encoders require aligned text-image corpora and\nhence, do not oer the exibility of their text counterparts, that is, to learn from raw data alone.\nAn alternative to text-guided pretraining is self-supervised learning (Caron et al., 2018; Chen et al., 2020;\nHe et al., 2021) where features are learned from images alone. These approaches are conceptually closer to\npretext tasks such as language modeling and can capture information at the image and pixel level (Caron\net al., 2021). However, despite their potential to learn all-purposed features, most of the advances in\nself-supervised learning were made in the context of pretraining on a small curated dataset, ImageNet-\n1k (Russakovsky et al., 2015). Some eorts on scaling these approaches beyond ImageNet-1k have been\nattempted (Caron et al., 2019; Goyal et al., 2021; 2022a), but they focused on uncurated datasets, which\ntypically lead to a signicant drop in the quality of the features. This is explained by the lack of control\nover the data quality and diversity, which are essential to produce good features.\nIn this work, we explore if self-supervised learning has the potential to learn all-purposed visual features if\npretrained on a large quantity of curated data. We revisit existing discriminative self-supervised approaches\nthat learn features at both the image and patch level, such as iBOT (Zhou et al., 2021), and we reconsider\nsomeoftheirdesignchoicesunderthelensofalargerdataset. Mostofourtechnicalcontributionsaretailored\ntoward stabilizing and accelerating discriminative self-supervised learning when scaling in model and data\nsizes. These improvements make our approach around 2 faster and require 3 less memory than similar\ndiscriminative self-supervised methods, allowing us to leverage longer training with larger batch sizes.\nRegarding pretraining data, we have built an automatic pipeline to lter and rebalance datasets from an\nextensive collection of uncurated images. This pipeline is inspired by pipelines used in NLP (Wenzek et al.,\n2019), where data similarities are used instead of external metadata and do not require manual annotation.\nA major diculty when dealing with images in the wild is to rebalance concepts and avoid overtting on a\nfew dominant modes. In this work, a naive clustering approach works reasonably well to resolve this issue.\nWe gathered a small but diverse corpus of 142M images to validate our approach.\nFinally, we provide a variety of pretrained visual models, called DINOv2, trained with dierent Vision\nTransformers (ViT) (Dosovitskiy et al., 2016) architectures on our data. We release all the models and\nthe code to retrain DINOv2 on any data. We validate the quality of DINOv2 on various computer vision\nbenchmarks at both image and pixel levels as we scale them, as summarized in Fig. 2. We conclude that self-\n2', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': '101010111012\nflops7578818487Accuracy\nInet-1k\n101010111012\nflops40485664mIoU\nSegmentation\n101010111012\nflops0.91.21.51.8 R-MSE\nMonocular Depth \n101010111012\nflops80848892Accuracy\nClassification\n101010111012\nflops4856647280Accuracy\nFinegrained Classification\n101010111012\nflops30456075mAP\nInstance Retrieval\n101010111012\nflops4050607080Accuracy\nImageNet-{A,R,Sketch}\n101010111012\nflops5055606570Accuracy\nVideo Understanding\nSSL\nWSL\nDINOv2Figure 2: Evolution of performance when scaling in parameters. We show performance on eight\ntypes of vision tasks, as presented in Sec. 7, and average metrics with each type. Features are extracted\nfrom our self-supervised encoders, DINOv2 (dark blue), and we compare them with self-supervised methods\n(pale orange), as well as weakly-supervised methods (dark pink). We report the best-performing weakly-\nsupervised models performance as a dashed horizontal line. Our family of models drastically improves over\nthe previous state of the art in self-supervised learning and reaches performance comparable with weakly-\nsupervised features. See Sec. 7 for a detailed analysis.\nsupervised pretraining alone is a good candidate for learning transferable frozen features that are competitive\nwith the best openly available weakly-supervised models.\n2 Related Work\nIntra-image self-supervised training. A rst family of self-supervised methods focuses on pretext tasks\nbuilt from the image, i.e., extracting a signal from the image to be predicted from the rest of the image.\nThis idea has become prevalent with the work of Doersch et al. (2015), where they train by predicting the\ncontext of a given patch. Many other pretext tasks were introduced based on re-colorizing images (Zhang\net al., 2016), predicting transformations (Gidaris et al., 2018), inpainting (Pathak et al., 2016) or patch\nre-ordering (Noroozi & Favaro, 2016; Misra & Maaten, 2020). Recently, the emergence of patch-based\narchitectures, like ViTs, has led to a revisit of inpainting for pre-training (He et al., 2021; Bao et al., 2021;\nEl-Nouby et al., 2021), potentially in feature space (Assran et al., 2023; Baevski et al., 2022). Of particular\ninterest, He et al. (2021) show that a masked auto-encoder (MAE) learns features that provide substantial\nimprovements when netuned on downstream tasks. This property of MAEs has been further validated\non video (Tong et al., 2022), audio (Xu et al., 2022), and across other modalities (Girdhar et al., 2022).\nHowever, their features require supervised netuning, while our features perform well out of the box.\nDiscriminativeself-supervisedlearning. Thesecondlineofwork, closertoours, isusingdiscriminative\nsignals between images or groups of images to learn features. This family of methods has roots in early\ndeep learning work (Hadsell et al., 2006) but became popular with the emergence of instance classication\nmethods (Dosovitskiy et al., 2014; Bojanowski & Joulin, 2017; Wu et al., 2018). Several improvements\nwere made based either on instance-level objectives (Hna et al., 2019; He et al., 2020; Chen & He, 2020;\nChen et al., 2020; Grill et al., 2020; Caron et al., 2021) or clustering (Caron et al., 2018; Asano et al.,\n2020; Caron et al., 2020). These methods provide performant frozen features on standard benchmarks like\nImageNet (Russakovsky et al., 2015), but they are hard to scale to larger model sizes (Chen et al., 2021). In\n3', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Uncur ated Data \nAugment ed Cur ated Data\nCurated Data\n Embedding\n Deduplication\n RetrievalFigure 3: Overview of our data processing pipeline. Images from curated and uncurated data sources\nare rst mapped to embeddings. Uncurated images are then deduplicated before being matched to curated\nimages. The resulting combination augments the initial dataset through a self-supervised retrieval system.\nthis work, we revisit the training of these approaches in the context of large pretraining datasets and models.\nIn particular, we build on top of Zhou et al. (2021) that we nd particularly suited for scaling.\nScaling self-supervised pretraining. A growing body of work has focused on the scaling abilities of\nself-supervised learning in terms of data and model size (Caron et al., 2019; Goyal et al., 2019; Tian et al.,\n2021; Goyal et al., 2022a). Most of these works use large quantities of uncurated data to train models\nwithout supervision. They show evidence that discriminative methods scale with data, but because of the\npoor quality of the pretraining data, most of the results are obtained by netuning the features. Of particular\ninterest, Goyal et al. (2021) have also shown that these methods benet from scaling in model size given\nenough pretrained data. This line of work questions the ability of self-supervised methods to work on any\ndata while we focus on producing the best pretrained encoders.\nAutomatic data curation. Our dataset construction borrows from the image retrieval community (Wein-\nzaepfeletal.,2021;Radenovietal.,2018b;Bermanetal.,2019;Douzeetal.,2009;Toliasetal.,2015;Revaud\net al., 2019). In particular, the use of retrieval to augment the training set has been studied in the context\nof semi-supervised learning (Yalniz et al., 2019). Similarly, others have used hashtags or other metadata to\nlter uncurated datasets (Mahajan et al., 2018; Radford et al., 2021). Unlike this work, we use no metadata\nnor supervision to lter images and leverage visual similarity between images. Our approach is inspired by\ntext curation pipelines (Wenzek et al., 2019), where a language model is trained on Wikipedia to score texts\nextracted from an uncurated source.\n3 Data Processing\nWe assemble our curated LVD-142M dataset by retrieving, from a large pool of uncurated data, images that\nare close to those in several curated datasets. We describe below the main components in our data pipeline\nincluding the curated/uncurated data sources, the image deduplication step and the retrieval system. Our\npipeline does not require any metadata or text and directly works with images, as shown in Fig. 3. We refer\nthe reader to appendix A for more details on our approach.\nData sources. Our selection of curated datasets is detailed in the appendix (Table 15) and contains\nImageNet-22k, the train split of ImageNet-1k, Google Landmarks and several ne-grained datasets. For the\nuncurated data source, we collect a raw unltered dataset of images from a publicly available repository of\ncrawled web data. From each web page in the repository, we extract URL links of images from <img>tags.\nWe discards URLs that are unsafe or restricted by domains, and post-process the downloaded images (PCA\nhash deduplication, NSFW ltering, and blurring identiable faces). This results in 1.2B unique images.\n4', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Deduplication. We apply the copy detection pipeline of Pizzi et al. (2022) to the uncurated data and\nremove near-duplicate images. This reduces redundancy and increases diversity among images. We also\nremove near-duplicates of images contained in the test or validation set of any benchmark used in this work.\nSelf-supervised image retrieval. We build our curated pretraining dataset by retrieving images from\nour uncurated data source that are close to images in our curated sources. In order to do this, we rst\ncompute an image embedding using a self-supervised ViT-H/16 network pretrained on ImageNet-22k, and\nuse cosine-similarity as a distance measure between images. Then, we perform k-means clustering of the\nuncurated data. Given a query dataset for retrieval, if it is large enough we retrieve N(typically 4) nearest\nneighbors for each query image. If it is small, we sample Mimages from the cluster corresponding to each\nquery image. We adjust NandMby visual inspection of the retrieval result.\nImplementation Details. The deduplication and retrieval stages of our pipeline rely on the Faiss li-\nbrary (Johnson et al., 2019) to eciently index and compute batch searches of nearest embeddings. In\nparticular, we heavily leverage its support for GPU-accelerated indices, using inverted le indices with prod-\nuct quantization codes (Jegou et al., 2010). The whole processing is distributed on a compute cluster of 20\nnodes equipped with 8 V100-32GB GPUs and takes less than two days to produce the LVD-142M dataset.\n4 Discriminative Self-supervised Pre-training\nWe learn our features with a discriminative self-supervised method that can be seen as a combination of\nDINO and iBOT losses with the centering of SwAV (Caron et al., 2020). We also add a regularizer to spread\nfeatures and a short high-resolution training phase. We rapidly introduce each of these approaches, but more\ndetails can be found in the related papers, or in our open-sourced code.\nImage-level objective (Caron et al., 2021). We consider the cross-entropy loss between the\nfeatures extracted from a student and a teacher network. Both features are coming from the class\ntoken of a ViT, obtained from dierent crops of the same image. We learn the parameters of the\nstudent and build the teacher with an exponential moving average of past iterates (He et al., 2020).\nPatch-level objective (Zhou et al., 2021). We randomly mask some of the input patches given\nto the student, but not to the teacher. We then add a cross-entropy loss between the patch features\nof both networks on each masked patch. This loss is combined with the image-level loss.\nUntying head weights between both objectives. We observe that tying the weights associated\nwith both objectives makes the model undert at the patch-level while overtting at the image-level.\nUntying these weights resolves this issue and improve the performances at both scales.\nSinkhorn-Knopp centering (Caron et al., 2020). Ruan et al. (2022) recommend to replace the\nteacher softmax-centering step of DINO and iBot by the Sinkhorn-Knopp (SK) batch normalization\nof SwAV (Caron et al., 2020). We run the Sinkhorn-Knopp algorithm steps for 3 iterations. For the\nstudent, we apply the softmax normalization.\nKoLeo regularizer (Sablayrolles et al., 2018). The KoLeo regularizer derives from the\nKozachenko-Leonenko dierential entropy estimator (see Beirlant et al. (1997); Delattre & Fournier\n(2017)) and encourages a uniform span of the features within a batch. Given a set of nvectors\n(x1, . . . , x n), it is dened asLkoleo =1\nnn\ni=1log(dn,i),where dn,i= min j=ixixjis the mini-\nmum distance between xiand any other point within the batch. We also 2-normalize the features\nbefore computing this regularizer.\nAdapting the resolution (Touvron et al., 2019). Increasing image resolution is key to pixel-\nlevel downstream tasks such as segmentation or detection, where small objects disappear at low\nresolutions. However, training at high resolution is time and memory demanding, and instead, we\nincrease the resolution of images to 518518during a short period at the end of pretraining.\n5', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': '5 Ecient implementation\nWe consider several improvements to train models at a larger scale. We train models on A100 GPUs using\nPyTorch 2.0. The code is also available along with the pretrained models used for feature extraction1.\nThe details of our models are in the appendix, Table 17. With the same hardware, compared to the iBOT\nimplementation, the DINOv2 code runs around 2faster using only 1/3of the memory.\nFast and memory-ecient attention. We implemented our own version of FlashAttention (Dao et al.,\n2022) to improve memory usage and speed on the self-attention layers. Our version is on par with or\nbetter than the original on all cases considered, while covering more use-cases and hardware. Due to the\nGPU hardware specics, the eciency is best when the embedding dimension per head is a multiple of\n64, and the matrix operations are even better when the full embedding dimension is a multiple of 256.\nAs a consequence, our ViT-g architecture slightly diers from the architecture proposed by Zhai et al.\n(2022) in order to maximize compute eciency, and we use an embedding dimension of 1536 with 24 heads\n(64 dim/head), rather than 1408 with 16 heads (88 dim/head). Our experiments did not show signicant\ndierences in nal accuracy, and our ViT-g backbone counts 1.1B parameters.\nNested tensors in self-attention. Our version also allows running in the same forward pass the global\ncrops and the local crops (that have dierent numbers of patch tokens), leading to signicant compute\neciency gains compared to using separate forward and backward passes as done in prior implementations.\nThe lower-level components of our setup are available in the xFormers library2(Lefaudeux et al. (2022)).\nEcient stochastic depth. We implement an improved version of stochastic depth (Huang et al., 2016)\nthat skips the computation of the dropped residuals rather than masking the result. This saves memory and\ncompute in proportion approximately equal to the drop rate, thanks to specic fused kernels. With high\ndrop rates ( d= 40%in this work), this allows a drastic improvement in compute eciency and memory\nusage. The implementation consists of randomly shuing the Bsamples over the batch dimension, and\nslicing the rst (1d)Bsamples for the computations in the block.\nFully-Sharded Data Parallel (FSDP). Minimizing our objective with the AdamW optimizer requires\n4 model replicas in oat32 precision  student, teacher, optimizer rst moments, optimizer second moments.\nThis sums to 16 GBof memory for a billion-parameter model such as our ViT-g. In order to reduce this\nmemory footprint per GPU, we split the model replicas across GPUs, i.e., sharding 16 GBacross GPUs\nusing the PyTorch implementation of FSDP. Consequently, the model size is not bounded by the memory of\na single GPU but by the total sum of GPU memory across compute nodes. The Pytorch implementation of\nFSDP brings a second advantage, which is to save on the cross-GPU communication costs: the weight shards\nare stored in oat32 precision as required by the optimizer, but broadcasting weights and reducing gradients\nis done in oat16 precision for the backbone (MLP heads gradients are reduced in oat32 to avoid training\ninstabilities). This leads to approximately 50% reduction in communication costs compared to the oat32\ngradient all-reduce operation used in DistributedDataParallel (DDP), which is used in other self-supervised\npretraining methods (Caron et al., 2021; Zhou et al., 2021). As a consequence, the training procedure\nscales more eciently than DDP with oat16 autocast when scaling the number of GPU nodes. Overall,\nPytorch-FSDP mixed-precision is superior to DDP with autocast in virtually all cases we encountered.\nModeldistillation. Mostofourtechnicalimprovementstothetrainingloopaimatimprovingthetraining\nof large models over large quantities of data. For smaller models, we distill them from our largest model,\nthe ViT-g, instead of training them from scratch. Knowledge distillation (Hinton et al., 2015) aims at\nreproducing the output of a large model with a smaller model by minimizing some distance between both\noutputs for a set of given inputs. Since our objective function is a form of distillation from the teacher\nnetwork to the student network, we leverage the same training loop with a few exceptions: we use a larger\nmodel as a frozen teacher, keep a spare EMA of the student that we use as our nal model, remove the\nmasking and stochastic depth, and, apply the iBOT loss on the two global crops. In our ablations, we\n1https://github.com/facebookresearch/dinov2\n2https://github.com/facebookresearch/xformers\n6', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'INet-1k k-NN INet-1k linear\niBOT 72.9 82.3\n+(our reproduction) 74.5 1.6 83.2 0.9\n+LayerScale, Stochastic Depth 75.4 0.9 82.0 1.2\n+128k prototypes 76.6 1.2 81.9 0.1\n+KoLeo 78.9 2.3 82.5 0.6\n+SwiGLU FFN 78.7 0.2 83.1 0.6\n+Patch size 14 78.9 0.2 83.5 0.4\n+Teacher momentum 0.994 79.4 0.5 83.6 0.1\n+Tweak warmup schedules 80.5 1.1 83.8 0.2\n+Batch size 3k 81.7 1.2 84.7 0.9\n+Sinkhorn-Knopp 81.7 = 84.7 =\n+Untying heads = DINOv2 82.0 0.3 84.5 0.2\nTable 1:Ablation study of the training dierences between iBOT and DINOv2. We optimize\nfor k-NN performance, as in our experience, the linear probe performance is lower-bounded by the k-NN\nperformance. Some modications, like LayerScale and a high Stochastic Depth (rate= 0.4), incur a decrease\nin linear probe performance, but have the benets of increasing the stability of training by avoiding NaN loss\nvalues during training. Overall, these modications allowed for the next set of improvements to be added.\nExperiments are run using the ViT-Large architecture on ImageNet-22k.\nobserve that this approach achieves better performance than training from scratch, even for a ViT-L. Our\ndistillation method ends up close to the one described by Duval et al. (2023), except we do not modify the\nloss terms for distillation and evaluate the EMA of the student.\n6 Ablation Studies\nWe present a set of ablations to empirically validate dierent components of our pipeline: the technical\nmodications described in Sec. 4, the pretraining data and the impact of model distillation. We consider\nvarious downstream tasks that are described in Sec. 7.\n6.1 Improved Training Recipe\nOur approach improves over the iBOT method by combining it with several existing components described\nin Sec. 4. To evaluate their importance, we train multiple models where we successively add components to\na baseline iBOT model. We report the Top-1 accuracy on the validation set of ImageNet-1k with a k-NN\nand a linear linear in Table 1. Generally, we observe that each component improves the performance on\neither k-NN or linear probing and even both in most cases. Only LayerScale and Stochastic Depth incur a\nperformance drop in linear probing but signicantly improve the training stability in our experience.\n6.2 Pretraining Data Source\nThe quality of features is directly related to the quality of the pretraining data. In this experiment, we\nprobe the impact of LVD-142M compared to ImageNet-22k, a commonly used pretraining dataset, or using\ndirectly raw and uncurated data. For the uncurated dataset, we randomly sample 142million images from\nthe same data source as LVD-142M. We train a ViT-g/14 on each dataset for the same number of iterations.\nWe also include a variant of ImageNet-22k obtained by removing the synsets of ImageNet-1k (INet-22k \\\nINet-1k) for completeness. We report the comparisons in Table 2.\nThe most salient observation is that training on a curated set of images works better on most benchmarks\nthan training on uncurated data. This conrms the benet of curating data, even in the case of self-\nsupervised pretraining. When compared with models trained on ImageNet-22k, training on LVD-142M is\nalso superior on all the benchmarks but ImageNet-1k. This conrms that training on a more diverse set of\n7', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Training Data INet-1k Im-A ADE-20k Oxford-M\nINet-22k 85.9 73.5 46.6 62.5\nINet-22k\\INet-1k 85.3 70.3 46.2 58.7\nUncurated data 83.3 59.4 48.5 54.3\nLVD-142M 85.8 73.9 47.7 64.6\nTable 2:Ablation of the source of pretraining data. We compare the INet-22k dataset that was used\nin iBOT to our dataset, LVD-142M. Each model is trained for the same number of iterations, that is smaller\nthan in our nal run. Pretraining on LVD-142M maintains the performance over INet-1k while leading to\nmodels that perform better in other domains.\nL H g848586\nImageNet-1k\nINet-22k\nLVD142M\nL H g747678\nImageNet-V2\nL H g5060\nImageNet-Sketch\nL H g9294\nFood101\nL H g8090\nCars\nL H g3540\nAmsterTime\nL H g2040\nOxford-H\nFigure 4: Model scale versus data scale. Evolution of performance as a function of model size for\ntwo dierent pretraining datasets: ImageNet-22k (14M images) and LVD-142M (142M images). The ViT-g\ntrained on LVD-142M surpasses the ViT-g trained on ImageNet-22k on most benchmarks.\nimages improves the quality of the features in domains that are not covered by this dataset. Overall, the\nconclusion of this ablation is that our dataset provides a good balance of dierent types of images that leads\nto the best performance overall.\n6.3 Model Size and Data\nWe quantify the importance of scaling data with the model size in Fig. 4. As the size of models grow, training\non LVD-142M becomes more benecial than training on ImageNet-22k. For instance, a ViT-g trained on\nLVD-142M matches the performance on ImageNet-1k of a model trained on ImageNet-22k while signicantly\noutperforming it on the other benchmarks.\n6.4 Loss Components\nWe validated the proposed technical improvements in Sec. 6.1 by adding them incrementally. This section\nanalyzes the performance hit observed if we ablate specic loss terms, starting from our best-performing\nmodel. We ablate the importance of the KoLeo loss and the impact of the masked image modeling term.\nFor both, we report performance on ImageNet-1k using a linear classier, ADE-20k segmentation using a\nlinear classier, and nearest-neighbor image retrieval on Oxford-M. Table 3a shows the impact of using the\nKoLeo loss. We see that the instance retrieval performance improves by more than 8%, conrming that this\nterm helps spread features in the output space. At the same time, the other metrics do not suer from this\nregularization. In Table 3b, we show the impact of using the masked image modeling term from iBOT. This\nterm is critical for dense prediction tasks, leading to almost 3%performance improvement.\n6.5 Impact of Knowledge Distillation\nFor small architectures, we distill larger models instead of training them from scratch. We use the distillation\nprocedure described in Sec. 5. We evaluate the eectiveness of this approach by comparing a ViT-L/14\ntrained from scratch with one distilled from a ViT-g/14 over 12 benchmarks in Fig. 5. We also report the\nperformance of the ViT-g/14 used for distillation as a topline. The distilled model outperforms the one\ntrained from scratch on 10 out of 12 benchmarks, validating our pretraining approach for small models.\n8', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'KoLeo INet-1k Im-A ADE-20k Oxford-M\n\x15 85.3 70.6 47.2 55.6\n 85.8 72.8 47.1 63.9\n(a) Koleo lossMIM INet-1k Im-A ADE-20k Oxford-M\n\x15 85.3 72.0 44.2 64.3\n 85.8 72.8 47.1 63.9\n(b) MIM objective in iBOT\nTable 3:(a)Eect of the KoLeo loss term. (b)Eect of the iBOT Masked Image Modeling (MIM) loss\nterm. Evaluation performed on ImageNet-{1k,A} (classication with linear probe, accuracy %), ADE-20k\n(segmentation with linear layer, mIoU) and Oxford-M (image retrieval, mAP). Each model is trained on the\nsame number of iterations, that is smaller than our nal run. The KoLeo loss term improves nearest-neighbor\nsearch tasks (e.g. retrieval), and the MIM loss improves patch-level tasks (e.g. segmentation).\nINet-1kFoodCarsiNat18\niNat21\nPlaces 205Oxford-H\nParis-H\nINet-A\nINet-RKitti\nNYUd\nViT-L/14 Scratch\nViT-L/14 Distill\nViT-g/14 Scratch\n84.5 86.3 86.592.894.394.7\n81.890.191.4\n77.880.481.6\n83.185.185.7\n66.067.367.5\n47.7 52.6 52.1\n77.6\n84.482.761.7\n71.3\n75.968.1\n74.1\n78.82.57\n2.5\n2.350.345\n0.333\n0.298\n(a) Comparison on individual metricsArch Method INet-1k Segm. Depth Classif.\nViT-g/14 Scratch 86.5 73.4 1.00 92.1\nViT-L/14 Scratch 84.5 72.2 1.10 90.2\nViT-L/14 Distill 86.3 73.3 1.08 91.2\nArch Method Finegr. Retriev. ARSketch Video\nViT-g/14 Scratch 78.3 75.2 77.0 69.3\nViT-L/14 Scratch 75.8 71.3 69.5 67.3\nViT-L/14 Distill 77.6 76.3 74.5 67.5\n(b) Averaged metrics on 8 vision tasks\nFigure 5: Eectiveness of knowledge distillation. Comparison between a ViT-L trained from scratch\nor distilled from DINOv2 using ViT-g/14. For reference, we also report the performance of the ViT-g/14\nteacher. We show that a ViT-L model distilled from a frozen ViT-g outperforms a the same model trained\nfrom scratch on all benchmarks, sometimes even outperforming the distillation target.\n6.6 Impact of Resolution\nWe measure the impact of changing the resolution during the pretraining on the performance of image and\npatch-level features. We consider models trained from scratch using a xed resolution of either 224224\nor416416, and a model trained from scratch at 224224, then resumed for 10k more iterations at\n416416. High-resolution training is compute-intensive, so we conduct this ablation on a small setup: a\nViT-L/16 trained on ImageNet1k. In Fig. 6, we report the performance of a linear probe on ImageNet-1k\nand ADE-20k, evaluated at various resolutions. The model trained on high-resolution images performs best\nacross resolutions, but this comes at a high cost: training at 416is approximate 3more compute-intensive\nthan training at 224. On the other hand, training at high resolution for only 10k iterations at the end of the\ntraining is almost as good and only requiring a fraction of the compute. As a consequence, we include this\nstep at the end of the training rather than training at a high resolution from scratch.\n7 Results\nIn this section, we present the empirical evaluation of our models on many image understanding tasks. We\nevaluate both global and local image representations, on category and instance-level recognition, semantic\nsegmentation, monocular depth prediction, and action recognition. We detail the list of benchmarks in\nAppendix C. The goal of this evaluation is twofold. First, we show that our self-supervised features outper-\n9', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': '224 336 512 640 768\nresolution78798081828384Accuracy\nImageNet-1k\n224 336 512 640 768\nresolution3941434547mIoU\nADE-20K\n224\n416\n224416\nFigure 6: Role of resolution. Performance of ViT-L/16 trained on ImageNet-1k at xed resolution (224\nand 416) or trained at 224 then 416 for a short duration (224 416). We train linear classiers on top of\nfrozen features at dierent resolutions and report Top-1 accuracy on ImageNet and mIoU on ADE-20k. We\nobserve that performing SSL training at high resolution for a short duration achieve behavior and results\nclose to training at the same high resolution for the full training, at a fraction of the cost.\nform the current state of the art by a very large margin. Second, we show that they match, or surpass the\nperformance of weakly-supervised ones on a substantial number of tasks.\nBaselines. In our comparisons, we use two kinds of models as baselines. We compare to the best performing\nself-supervised models that are openly available. First, we run our evaluations for MAE (He et al., 2021),\nDINO (Caron et al., 2021), SEERv2 (Goyal et al., 2022a), MSN (Assran et al., 2022), EsViT (Li et al.,\n2022a), Mugs (Zhou et al., 2022) and iBOT (Zhou et al., 2021). When several architectural variants were\nproposed for a given method, we report results for the one that leads to best top-1 accuracy on ImageNet-1k.\nSecond, we report performance of open-source weakly-supervised models such as CLIP (Radford et al., 2021),\nOpenCLIP (Ilharco et al., 2021), and SWAG (Singh et al., 2022). When evaluating models on ImageNet-1k,\nwe report the performance for each of the aforementioned methods. For all other evaluations, we report\nthe four best-performing models amongst SSL ones. Also, for reference, we report the best performing\nOpenCLIP-G for weakly-supervised ones.\n7.1 ImageNet Classication\nAs a rst evaluation, we probe the quality of the holistic image representation produced by the model on the\nImageNet-1k classication dataset. We evaluate the quality of features by training a simple classier over a\nfrozen backbone, and do not perform netuning of the backbone weights. Following previous work, we use\na linear model for simplicity, ensuring a reproducible evaluation, despite the fact that classes may not be\nlinearly separable. Because most SSL methods were developped using ImageNet-1k validation performance\nas a debugging signal, we also report the top-1 accuracy on ImageNet-ReaL and ImageNet-V2. In order\nto report this additional validation performance, for all models, we run the evaluation with our code. We\ncompare our frozen features to the best publicly available SSL features in Table 4, regardless of architecture\nor pretraining data. We see the components proposed in this work lead to a very signicant improvement\n(+4.2%) over the previous state of the art (iBOT ViT-L/16 trained on ImageNet-22k) on linear evaluation.\nAt the same time, we also see that the performance increase on the alternative test sets is larger for our\nmethod, indicating stronger generalization. We describe details of our linear evaluation in Appendix B.3.\nHow far are we from weakly-supervised models? We also want to validate that our features are com-\npetitive with state-of-the-art open-source weakly supervised models. To this end, we compare on ImageNet-\n1k, using the linear evaluation, to three o-the-shelf methods with several architectural variants. For all\nmodels, we run the linear evaluation using our code, after making sure that our numbers match those re-\nported in technical reports and papers. We show the result of this evaluation in Table 4. We see that our\nbackbone, surpases the performance of OpenCLIP with a ViT-G/14 architecture ( +0.3%) and EVA-CLIP\nwith a ViT-g/14 ( +0.1%). At the same time, we also observe that our performance on the ImageNet-V2 test\n10', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}]: %s
2023-12-07 10:17:38,261 - INFO - Check the results [{'page_content': 'DINOv2: Learning Robust Visual Features\nwithout Supervision\nMaxime Oquab, Timothe Darcet, Tho Moutakanni,\nHuy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza,\nFrancisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba,\nRussell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat,\nVasu Sharma, Gabriel Synnaeve, Hu Xu, Herv Jegou, Julien Mairal1,\nPatrick Labatut, Armand Joulin, Piotr Bojanowski\nMeta AI Research1Inria\ncore teamequal contribution\nAbstract\nThe recent breakthroughs in natural language processing for model pretraining on large\nquantities of data have opened the way for similar foundation models in computer vision.\nThese models could greatly simplify the use of images in any system by producing all-\npurpose visual features, i.e., features that work across image distributions and tasks without\nnetuning. This work shows that existing pretraining methods, especially self-supervised\nmethods, can produce such features if trained on enough curated data from diverse sources.\nWe revisit existing approaches and combine dierent techniques to scale our pretraining in\nterms of data and model size. Most of the technical contributions aim at accelerating and\nstabilizing the training at scale. In terms of data, we propose an automatic pipeline to build\na dedicated, diverse, and curated image dataset instead of uncurated data, as typically done\nin the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy\net al., 2020) with 1B parameters and distill it into a series of smaller models that surpass\nthe best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the\nbenchmarks at image and pixel levels.\n1 Introduction\nLearning task-agnostic pretrained representations have become the standard in Natural Language Process-\ning (NLP) (Radford et al.; Rael et al., 2020; Chowdhery et al., 2022; Homann et al., 2022; Touvron et al.,\n2023). One can use these features as they are, i.e., without ne-tuning, and achieve performances on down-\nstream tasks that are signicantly better than those produced by task-specic models (Brown et al., 2020).\nThis success has been fueled by pretraining on large quantities of raw text using pretext objectives, such as\nlanguage modeling (Radford et al., 2017) or word vectors (Devlin et al., 2018), that require no supervision.\nFollowing this paradigm shift in NLP, we expect similar foundation models to appear in computer vi-\nsion (Bommasani et al., 2021). These models should generate visual features that work out of the box on\nany task, both at the image level, e.g., image classication, and pixel level, e.g., segmentation. Most promis-\ning eorts towards these foundation models focus on text-guided pretraining, i.e., using a form of textual\nsupervision to guide the training of the features (Joulin et al., 2016; Mahajan et al., 2018; Radford et al.,\n2021). This form of text-guided pretraining limits the information that can be retained about the image\nsince captions only approximate the rich information in images, and complex pixel-level information may\nAll the authors are aliated to Meta, except Julien Mairal who is aliated to Inria. Timothe Darcet and Pierre Fernandez\nhave a co-aliation with Inria. Tho Moutakanni has a co-aliation with Universit Paris Saclay. Alaaeldin El-Nouby has a\nco-aliation with Inria and ENS-PSL. Correspondence: {qas, timdarcet, theomoutakanni, ajoulin, bojanowski}@meta.com\n1arXiv:2304.07193v1  [cs.CV]  14 Apr 2023', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 1: Visualization of the rst PCA components. We compute a PCA between the patches of the\nimages from the same column (a, b, c and d) and show their rst 3 components. Each component is matched\nto a dierent color channel. Same parts are matched between related images despite changes of pose, style\nor even objects. Background is removed by thresholding the rst PCA component.\nnot surface with this supervision. Furthermore, these image encoders require aligned text-image corpora and\nhence, do not oer the exibility of their text counterparts, that is, to learn from raw data alone.\nAn alternative to text-guided pretraining is self-supervised learning (Caron et al., 2018; Chen et al., 2020;\nHe et al., 2021) where features are learned from images alone. These approaches are conceptually closer to\npretext tasks such as language modeling and can capture information at the image and pixel level (Caron\net al., 2021). However, despite their potential to learn all-purposed features, most of the advances in\nself-supervised learning were made in the context of pretraining on a small curated dataset, ImageNet-\n1k (Russakovsky et al., 2015). Some eorts on scaling these approaches beyond ImageNet-1k have been\nattempted (Caron et al., 2019; Goyal et al., 2021; 2022a), but they focused on uncurated datasets, which\ntypically lead to a signicant drop in the quality of the features. This is explained by the lack of control\nover the data quality and diversity, which are essential to produce good features.\nIn this work, we explore if self-supervised learning has the potential to learn all-purposed visual features if\npretrained on a large quantity of curated data. We revisit existing discriminative self-supervised approaches\nthat learn features at both the image and patch level, such as iBOT (Zhou et al., 2021), and we reconsider\nsomeoftheirdesignchoicesunderthelensofalargerdataset. Mostofourtechnicalcontributionsaretailored\ntoward stabilizing and accelerating discriminative self-supervised learning when scaling in model and data\nsizes. These improvements make our approach around 2 faster and require 3 less memory than similar\ndiscriminative self-supervised methods, allowing us to leverage longer training with larger batch sizes.\nRegarding pretraining data, we have built an automatic pipeline to lter and rebalance datasets from an\nextensive collection of uncurated images. This pipeline is inspired by pipelines used in NLP (Wenzek et al.,\n2019), where data similarities are used instead of external metadata and do not require manual annotation.\nA major diculty when dealing with images in the wild is to rebalance concepts and avoid overtting on a\nfew dominant modes. In this work, a naive clustering approach works reasonably well to resolve this issue.\nWe gathered a small but diverse corpus of 142M images to validate our approach.\nFinally, we provide a variety of pretrained visual models, called DINOv2, trained with dierent Vision\nTransformers (ViT) (Dosovitskiy et al., 2016) architectures on our data. We release all the models and\nthe code to retrain DINOv2 on any data. We validate the quality of DINOv2 on various computer vision\nbenchmarks at both image and pixel levels as we scale them, as summarized in Fig. 2. We conclude that self-\n2', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': '101010111012\nflops7578818487Accuracy\nInet-1k\n101010111012\nflops40485664mIoU\nSegmentation\n101010111012\nflops0.91.21.51.8 R-MSE\nMonocular Depth \n101010111012\nflops80848892Accuracy\nClassification\n101010111012\nflops4856647280Accuracy\nFinegrained Classification\n101010111012\nflops30456075mAP\nInstance Retrieval\n101010111012\nflops4050607080Accuracy\nImageNet-{A,R,Sketch}\n101010111012\nflops5055606570Accuracy\nVideo Understanding\nSSL\nWSL\nDINOv2Figure 2: Evolution of performance when scaling in parameters. We show performance on eight\ntypes of vision tasks, as presented in Sec. 7, and average metrics with each type. Features are extracted\nfrom our self-supervised encoders, DINOv2 (dark blue), and we compare them with self-supervised methods\n(pale orange), as well as weakly-supervised methods (dark pink). We report the best-performing weakly-\nsupervised models performance as a dashed horizontal line. Our family of models drastically improves over\nthe previous state of the art in self-supervised learning and reaches performance comparable with weakly-\nsupervised features. See Sec. 7 for a detailed analysis.\nsupervised pretraining alone is a good candidate for learning transferable frozen features that are competitive\nwith the best openly available weakly-supervised models.\n2 Related Work\nIntra-image self-supervised training. A rst family of self-supervised methods focuses on pretext tasks\nbuilt from the image, i.e., extracting a signal from the image to be predicted from the rest of the image.\nThis idea has become prevalent with the work of Doersch et al. (2015), where they train by predicting the\ncontext of a given patch. Many other pretext tasks were introduced based on re-colorizing images (Zhang\net al., 2016), predicting transformations (Gidaris et al., 2018), inpainting (Pathak et al., 2016) or patch\nre-ordering (Noroozi & Favaro, 2016; Misra & Maaten, 2020). Recently, the emergence of patch-based\narchitectures, like ViTs, has led to a revisit of inpainting for pre-training (He et al., 2021; Bao et al., 2021;\nEl-Nouby et al., 2021), potentially in feature space (Assran et al., 2023; Baevski et al., 2022). Of particular\ninterest, He et al. (2021) show that a masked auto-encoder (MAE) learns features that provide substantial\nimprovements when netuned on downstream tasks. This property of MAEs has been further validated\non video (Tong et al., 2022), audio (Xu et al., 2022), and across other modalities (Girdhar et al., 2022).\nHowever, their features require supervised netuning, while our features perform well out of the box.\nDiscriminativeself-supervisedlearning. Thesecondlineofwork, closertoours, isusingdiscriminative\nsignals between images or groups of images to learn features. This family of methods has roots in early\ndeep learning work (Hadsell et al., 2006) but became popular with the emergence of instance classication\nmethods (Dosovitskiy et al., 2014; Bojanowski & Joulin, 2017; Wu et al., 2018). Several improvements\nwere made based either on instance-level objectives (Hna et al., 2019; He et al., 2020; Chen & He, 2020;\nChen et al., 2020; Grill et al., 2020; Caron et al., 2021) or clustering (Caron et al., 2018; Asano et al.,\n2020; Caron et al., 2020). These methods provide performant frozen features on standard benchmarks like\nImageNet (Russakovsky et al., 2015), but they are hard to scale to larger model sizes (Chen et al., 2021). In\n3', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Uncur ated Data \nAugment ed Cur ated Data\nCurated Data\n Embedding\n Deduplication\n RetrievalFigure 3: Overview of our data processing pipeline. Images from curated and uncurated data sources\nare rst mapped to embeddings. Uncurated images are then deduplicated before being matched to curated\nimages. The resulting combination augments the initial dataset through a self-supervised retrieval system.\nthis work, we revisit the training of these approaches in the context of large pretraining datasets and models.\nIn particular, we build on top of Zhou et al. (2021) that we nd particularly suited for scaling.\nScaling self-supervised pretraining. A growing body of work has focused on the scaling abilities of\nself-supervised learning in terms of data and model size (Caron et al., 2019; Goyal et al., 2019; Tian et al.,\n2021; Goyal et al., 2022a). Most of these works use large quantities of uncurated data to train models\nwithout supervision. They show evidence that discriminative methods scale with data, but because of the\npoor quality of the pretraining data, most of the results are obtained by netuning the features. Of particular\ninterest, Goyal et al. (2021) have also shown that these methods benet from scaling in model size given\nenough pretrained data. This line of work questions the ability of self-supervised methods to work on any\ndata while we focus on producing the best pretrained encoders.\nAutomatic data curation. Our dataset construction borrows from the image retrieval community (Wein-\nzaepfeletal.,2021;Radenovietal.,2018b;Bermanetal.,2019;Douzeetal.,2009;Toliasetal.,2015;Revaud\net al., 2019). In particular, the use of retrieval to augment the training set has been studied in the context\nof semi-supervised learning (Yalniz et al., 2019). Similarly, others have used hashtags or other metadata to\nlter uncurated datasets (Mahajan et al., 2018; Radford et al., 2021). Unlike this work, we use no metadata\nnor supervision to lter images and leverage visual similarity between images. Our approach is inspired by\ntext curation pipelines (Wenzek et al., 2019), where a language model is trained on Wikipedia to score texts\nextracted from an uncurated source.\n3 Data Processing\nWe assemble our curated LVD-142M dataset by retrieving, from a large pool of uncurated data, images that\nare close to those in several curated datasets. We describe below the main components in our data pipeline\nincluding the curated/uncurated data sources, the image deduplication step and the retrieval system. Our\npipeline does not require any metadata or text and directly works with images, as shown in Fig. 3. We refer\nthe reader to appendix A for more details on our approach.\nData sources. Our selection of curated datasets is detailed in the appendix (Table 15) and contains\nImageNet-22k, the train split of ImageNet-1k, Google Landmarks and several ne-grained datasets. For the\nuncurated data source, we collect a raw unltered dataset of images from a publicly available repository of\ncrawled web data. From each web page in the repository, we extract URL links of images from <img>tags.\nWe discards URLs that are unsafe or restricted by domains, and post-process the downloaded images (PCA\nhash deduplication, NSFW ltering, and blurring identiable faces). This results in 1.2B unique images.\n4', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Deduplication. We apply the copy detection pipeline of Pizzi et al. (2022) to the uncurated data and\nremove near-duplicate images. This reduces redundancy and increases diversity among images. We also\nremove near-duplicates of images contained in the test or validation set of any benchmark used in this work.\nSelf-supervised image retrieval. We build our curated pretraining dataset by retrieving images from\nour uncurated data source that are close to images in our curated sources. In order to do this, we rst\ncompute an image embedding using a self-supervised ViT-H/16 network pretrained on ImageNet-22k, and\nuse cosine-similarity as a distance measure between images. Then, we perform k-means clustering of the\nuncurated data. Given a query dataset for retrieval, if it is large enough we retrieve N(typically 4) nearest\nneighbors for each query image. If it is small, we sample Mimages from the cluster corresponding to each\nquery image. We adjust NandMby visual inspection of the retrieval result.\nImplementation Details. The deduplication and retrieval stages of our pipeline rely on the Faiss li-\nbrary (Johnson et al., 2019) to eciently index and compute batch searches of nearest embeddings. In\nparticular, we heavily leverage its support for GPU-accelerated indices, using inverted le indices with prod-\nuct quantization codes (Jegou et al., 2010). The whole processing is distributed on a compute cluster of 20\nnodes equipped with 8 V100-32GB GPUs and takes less than two days to produce the LVD-142M dataset.\n4 Discriminative Self-supervised Pre-training\nWe learn our features with a discriminative self-supervised method that can be seen as a combination of\nDINO and iBOT losses with the centering of SwAV (Caron et al., 2020). We also add a regularizer to spread\nfeatures and a short high-resolution training phase. We rapidly introduce each of these approaches, but more\ndetails can be found in the related papers, or in our open-sourced code.\nImage-level objective (Caron et al., 2021). We consider the cross-entropy loss between the\nfeatures extracted from a student and a teacher network. Both features are coming from the class\ntoken of a ViT, obtained from dierent crops of the same image. We learn the parameters of the\nstudent and build the teacher with an exponential moving average of past iterates (He et al., 2020).\nPatch-level objective (Zhou et al., 2021). We randomly mask some of the input patches given\nto the student, but not to the teacher. We then add a cross-entropy loss between the patch features\nof both networks on each masked patch. This loss is combined with the image-level loss.\nUntying head weights between both objectives. We observe that tying the weights associated\nwith both objectives makes the model undert at the patch-level while overtting at the image-level.\nUntying these weights resolves this issue and improve the performances at both scales.\nSinkhorn-Knopp centering (Caron et al., 2020). Ruan et al. (2022) recommend to replace the\nteacher softmax-centering step of DINO and iBot by the Sinkhorn-Knopp (SK) batch normalization\nof SwAV (Caron et al., 2020). We run the Sinkhorn-Knopp algorithm steps for 3 iterations. For the\nstudent, we apply the softmax normalization.\nKoLeo regularizer (Sablayrolles et al., 2018). The KoLeo regularizer derives from the\nKozachenko-Leonenko dierential entropy estimator (see Beirlant et al. (1997); Delattre & Fournier\n(2017)) and encourages a uniform span of the features within a batch. Given a set of nvectors\n(x1, . . . , x n), it is dened asLkoleo =1\nnn\ni=1log(dn,i),where dn,i= min j=ixixjis the mini-\nmum distance between xiand any other point within the batch. We also 2-normalize the features\nbefore computing this regularizer.\nAdapting the resolution (Touvron et al., 2019). Increasing image resolution is key to pixel-\nlevel downstream tasks such as segmentation or detection, where small objects disappear at low\nresolutions. However, training at high resolution is time and memory demanding, and instead, we\nincrease the resolution of images to 518518during a short period at the end of pretraining.\n5', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': '5 Ecient implementation\nWe consider several improvements to train models at a larger scale. We train models on A100 GPUs using\nPyTorch 2.0. The code is also available along with the pretrained models used for feature extraction1.\nThe details of our models are in the appendix, Table 17. With the same hardware, compared to the iBOT\nimplementation, the DINOv2 code runs around 2faster using only 1/3of the memory.\nFast and memory-ecient attention. We implemented our own version of FlashAttention (Dao et al.,\n2022) to improve memory usage and speed on the self-attention layers. Our version is on par with or\nbetter than the original on all cases considered, while covering more use-cases and hardware. Due to the\nGPU hardware specics, the eciency is best when the embedding dimension per head is a multiple of\n64, and the matrix operations are even better when the full embedding dimension is a multiple of 256.\nAs a consequence, our ViT-g architecture slightly diers from the architecture proposed by Zhai et al.\n(2022) in order to maximize compute eciency, and we use an embedding dimension of 1536 with 24 heads\n(64 dim/head), rather than 1408 with 16 heads (88 dim/head). Our experiments did not show signicant\ndierences in nal accuracy, and our ViT-g backbone counts 1.1B parameters.\nNested tensors in self-attention. Our version also allows running in the same forward pass the global\ncrops and the local crops (that have dierent numbers of patch tokens), leading to signicant compute\neciency gains compared to using separate forward and backward passes as done in prior implementations.\nThe lower-level components of our setup are available in the xFormers library2(Lefaudeux et al. (2022)).\nEcient stochastic depth. We implement an improved version of stochastic depth (Huang et al., 2016)\nthat skips the computation of the dropped residuals rather than masking the result. This saves memory and\ncompute in proportion approximately equal to the drop rate, thanks to specic fused kernels. With high\ndrop rates ( d= 40%in this work), this allows a drastic improvement in compute eciency and memory\nusage. The implementation consists of randomly shuing the Bsamples over the batch dimension, and\nslicing the rst (1d)Bsamples for the computations in the block.\nFully-Sharded Data Parallel (FSDP). Minimizing our objective with the AdamW optimizer requires\n4 model replicas in oat32 precision  student, teacher, optimizer rst moments, optimizer second moments.\nThis sums to 16 GBof memory for a billion-parameter model such as our ViT-g. In order to reduce this\nmemory footprint per GPU, we split the model replicas across GPUs, i.e., sharding 16 GBacross GPUs\nusing the PyTorch implementation of FSDP. Consequently, the model size is not bounded by the memory of\na single GPU but by the total sum of GPU memory across compute nodes. The Pytorch implementation of\nFSDP brings a second advantage, which is to save on the cross-GPU communication costs: the weight shards\nare stored in oat32 precision as required by the optimizer, but broadcasting weights and reducing gradients\nis done in oat16 precision for the backbone (MLP heads gradients are reduced in oat32 to avoid training\ninstabilities). This leads to approximately 50% reduction in communication costs compared to the oat32\ngradient all-reduce operation used in DistributedDataParallel (DDP), which is used in other self-supervised\npretraining methods (Caron et al., 2021; Zhou et al., 2021). As a consequence, the training procedure\nscales more eciently than DDP with oat16 autocast when scaling the number of GPU nodes. Overall,\nPytorch-FSDP mixed-precision is superior to DDP with autocast in virtually all cases we encountered.\nModeldistillation. Mostofourtechnicalimprovementstothetrainingloopaimatimprovingthetraining\nof large models over large quantities of data. For smaller models, we distill them from our largest model,\nthe ViT-g, instead of training them from scratch. Knowledge distillation (Hinton et al., 2015) aims at\nreproducing the output of a large model with a smaller model by minimizing some distance between both\noutputs for a set of given inputs. Since our objective function is a form of distillation from the teacher\nnetwork to the student network, we leverage the same training loop with a few exceptions: we use a larger\nmodel as a frozen teacher, keep a spare EMA of the student that we use as our nal model, remove the\nmasking and stochastic depth, and, apply the iBOT loss on the two global crops. In our ablations, we\n1https://github.com/facebookresearch/dinov2\n2https://github.com/facebookresearch/xformers\n6', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'INet-1k k-NN INet-1k linear\niBOT 72.9 82.3\n+(our reproduction) 74.5 1.6 83.2 0.9\n+LayerScale, Stochastic Depth 75.4 0.9 82.0 1.2\n+128k prototypes 76.6 1.2 81.9 0.1\n+KoLeo 78.9 2.3 82.5 0.6\n+SwiGLU FFN 78.7 0.2 83.1 0.6\n+Patch size 14 78.9 0.2 83.5 0.4\n+Teacher momentum 0.994 79.4 0.5 83.6 0.1\n+Tweak warmup schedules 80.5 1.1 83.8 0.2\n+Batch size 3k 81.7 1.2 84.7 0.9\n+Sinkhorn-Knopp 81.7 = 84.7 =\n+Untying heads = DINOv2 82.0 0.3 84.5 0.2\nTable 1:Ablation study of the training dierences between iBOT and DINOv2. We optimize\nfor k-NN performance, as in our experience, the linear probe performance is lower-bounded by the k-NN\nperformance. Some modications, like LayerScale and a high Stochastic Depth (rate= 0.4), incur a decrease\nin linear probe performance, but have the benets of increasing the stability of training by avoiding NaN loss\nvalues during training. Overall, these modications allowed for the next set of improvements to be added.\nExperiments are run using the ViT-Large architecture on ImageNet-22k.\nobserve that this approach achieves better performance than training from scratch, even for a ViT-L. Our\ndistillation method ends up close to the one described by Duval et al. (2023), except we do not modify the\nloss terms for distillation and evaluate the EMA of the student.\n6 Ablation Studies\nWe present a set of ablations to empirically validate dierent components of our pipeline: the technical\nmodications described in Sec. 4, the pretraining data and the impact of model distillation. We consider\nvarious downstream tasks that are described in Sec. 7.\n6.1 Improved Training Recipe\nOur approach improves over the iBOT method by combining it with several existing components described\nin Sec. 4. To evaluate their importance, we train multiple models where we successively add components to\na baseline iBOT model. We report the Top-1 accuracy on the validation set of ImageNet-1k with a k-NN\nand a linear linear in Table 1. Generally, we observe that each component improves the performance on\neither k-NN or linear probing and even both in most cases. Only LayerScale and Stochastic Depth incur a\nperformance drop in linear probing but signicantly improve the training stability in our experience.\n6.2 Pretraining Data Source\nThe quality of features is directly related to the quality of the pretraining data. In this experiment, we\nprobe the impact of LVD-142M compared to ImageNet-22k, a commonly used pretraining dataset, or using\ndirectly raw and uncurated data. For the uncurated dataset, we randomly sample 142million images from\nthe same data source as LVD-142M. We train a ViT-g/14 on each dataset for the same number of iterations.\nWe also include a variant of ImageNet-22k obtained by removing the synsets of ImageNet-1k (INet-22k \\\nINet-1k) for completeness. We report the comparisons in Table 2.\nThe most salient observation is that training on a curated set of images works better on most benchmarks\nthan training on uncurated data. This conrms the benet of curating data, even in the case of self-\nsupervised pretraining. When compared with models trained on ImageNet-22k, training on LVD-142M is\nalso superior on all the benchmarks but ImageNet-1k. This conrms that training on a more diverse set of\n7', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Training Data INet-1k Im-A ADE-20k Oxford-M\nINet-22k 85.9 73.5 46.6 62.5\nINet-22k\\INet-1k 85.3 70.3 46.2 58.7\nUncurated data 83.3 59.4 48.5 54.3\nLVD-142M 85.8 73.9 47.7 64.6\nTable 2:Ablation of the source of pretraining data. We compare the INet-22k dataset that was used\nin iBOT to our dataset, LVD-142M. Each model is trained for the same number of iterations, that is smaller\nthan in our nal run. Pretraining on LVD-142M maintains the performance over INet-1k while leading to\nmodels that perform better in other domains.\nL H g848586\nImageNet-1k\nINet-22k\nLVD142M\nL H g747678\nImageNet-V2\nL H g5060\nImageNet-Sketch\nL H g9294\nFood101\nL H g8090\nCars\nL H g3540\nAmsterTime\nL H g2040\nOxford-H\nFigure 4: Model scale versus data scale. Evolution of performance as a function of model size for\ntwo dierent pretraining datasets: ImageNet-22k (14M images) and LVD-142M (142M images). The ViT-g\ntrained on LVD-142M surpasses the ViT-g trained on ImageNet-22k on most benchmarks.\nimages improves the quality of the features in domains that are not covered by this dataset. Overall, the\nconclusion of this ablation is that our dataset provides a good balance of dierent types of images that leads\nto the best performance overall.\n6.3 Model Size and Data\nWe quantify the importance of scaling data with the model size in Fig. 4. As the size of models grow, training\non LVD-142M becomes more benecial than training on ImageNet-22k. For instance, a ViT-g trained on\nLVD-142M matches the performance on ImageNet-1k of a model trained on ImageNet-22k while signicantly\noutperforming it on the other benchmarks.\n6.4 Loss Components\nWe validated the proposed technical improvements in Sec. 6.1 by adding them incrementally. This section\nanalyzes the performance hit observed if we ablate specic loss terms, starting from our best-performing\nmodel. We ablate the importance of the KoLeo loss and the impact of the masked image modeling term.\nFor both, we report performance on ImageNet-1k using a linear classier, ADE-20k segmentation using a\nlinear classier, and nearest-neighbor image retrieval on Oxford-M. Table 3a shows the impact of using the\nKoLeo loss. We see that the instance retrieval performance improves by more than 8%, conrming that this\nterm helps spread features in the output space. At the same time, the other metrics do not suer from this\nregularization. In Table 3b, we show the impact of using the masked image modeling term from iBOT. This\nterm is critical for dense prediction tasks, leading to almost 3%performance improvement.\n6.5 Impact of Knowledge Distillation\nFor small architectures, we distill larger models instead of training them from scratch. We use the distillation\nprocedure described in Sec. 5. We evaluate the eectiveness of this approach by comparing a ViT-L/14\ntrained from scratch with one distilled from a ViT-g/14 over 12 benchmarks in Fig. 5. We also report the\nperformance of the ViT-g/14 used for distillation as a topline. The distilled model outperforms the one\ntrained from scratch on 10 out of 12 benchmarks, validating our pretraining approach for small models.\n8', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'KoLeo INet-1k Im-A ADE-20k Oxford-M\n\x15 85.3 70.6 47.2 55.6\n 85.8 72.8 47.1 63.9\n(a) Koleo lossMIM INet-1k Im-A ADE-20k Oxford-M\n\x15 85.3 72.0 44.2 64.3\n 85.8 72.8 47.1 63.9\n(b) MIM objective in iBOT\nTable 3:(a)Eect of the KoLeo loss term. (b)Eect of the iBOT Masked Image Modeling (MIM) loss\nterm. Evaluation performed on ImageNet-{1k,A} (classication with linear probe, accuracy %), ADE-20k\n(segmentation with linear layer, mIoU) and Oxford-M (image retrieval, mAP). Each model is trained on the\nsame number of iterations, that is smaller than our nal run. The KoLeo loss term improves nearest-neighbor\nsearch tasks (e.g. retrieval), and the MIM loss improves patch-level tasks (e.g. segmentation).\nINet-1kFoodCarsiNat18\niNat21\nPlaces 205Oxford-H\nParis-H\nINet-A\nINet-RKitti\nNYUd\nViT-L/14 Scratch\nViT-L/14 Distill\nViT-g/14 Scratch\n84.5 86.3 86.592.894.394.7\n81.890.191.4\n77.880.481.6\n83.185.185.7\n66.067.367.5\n47.7 52.6 52.1\n77.6\n84.482.761.7\n71.3\n75.968.1\n74.1\n78.82.57\n2.5\n2.350.345\n0.333\n0.298\n(a) Comparison on individual metricsArch Method INet-1k Segm. Depth Classif.\nViT-g/14 Scratch 86.5 73.4 1.00 92.1\nViT-L/14 Scratch 84.5 72.2 1.10 90.2\nViT-L/14 Distill 86.3 73.3 1.08 91.2\nArch Method Finegr. Retriev. ARSketch Video\nViT-g/14 Scratch 78.3 75.2 77.0 69.3\nViT-L/14 Scratch 75.8 71.3 69.5 67.3\nViT-L/14 Distill 77.6 76.3 74.5 67.5\n(b) Averaged metrics on 8 vision tasks\nFigure 5: Eectiveness of knowledge distillation. Comparison between a ViT-L trained from scratch\nor distilled from DINOv2 using ViT-g/14. For reference, we also report the performance of the ViT-g/14\nteacher. We show that a ViT-L model distilled from a frozen ViT-g outperforms a the same model trained\nfrom scratch on all benchmarks, sometimes even outperforming the distillation target.\n6.6 Impact of Resolution\nWe measure the impact of changing the resolution during the pretraining on the performance of image and\npatch-level features. We consider models trained from scratch using a xed resolution of either 224224\nor416416, and a model trained from scratch at 224224, then resumed for 10k more iterations at\n416416. High-resolution training is compute-intensive, so we conduct this ablation on a small setup: a\nViT-L/16 trained on ImageNet1k. In Fig. 6, we report the performance of a linear probe on ImageNet-1k\nand ADE-20k, evaluated at various resolutions. The model trained on high-resolution images performs best\nacross resolutions, but this comes at a high cost: training at 416is approximate 3more compute-intensive\nthan training at 224. On the other hand, training at high resolution for only 10k iterations at the end of the\ntraining is almost as good and only requiring a fraction of the compute. As a consequence, we include this\nstep at the end of the training rather than training at a high resolution from scratch.\n7 Results\nIn this section, we present the empirical evaluation of our models on many image understanding tasks. We\nevaluate both global and local image representations, on category and instance-level recognition, semantic\nsegmentation, monocular depth prediction, and action recognition. We detail the list of benchmarks in\nAppendix C. The goal of this evaluation is twofold. First, we show that our self-supervised features outper-\n9', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': '224 336 512 640 768\nresolution78798081828384Accuracy\nImageNet-1k\n224 336 512 640 768\nresolution3941434547mIoU\nADE-20K\n224\n416\n224416\nFigure 6: Role of resolution. Performance of ViT-L/16 trained on ImageNet-1k at xed resolution (224\nand 416) or trained at 224 then 416 for a short duration (224 416). We train linear classiers on top of\nfrozen features at dierent resolutions and report Top-1 accuracy on ImageNet and mIoU on ADE-20k. We\nobserve that performing SSL training at high resolution for a short duration achieve behavior and results\nclose to training at the same high resolution for the full training, at a fraction of the cost.\nform the current state of the art by a very large margin. Second, we show that they match, or surpass the\nperformance of weakly-supervised ones on a substantial number of tasks.\nBaselines. In our comparisons, we use two kinds of models as baselines. We compare to the best performing\nself-supervised models that are openly available. First, we run our evaluations for MAE (He et al., 2021),\nDINO (Caron et al., 2021), SEERv2 (Goyal et al., 2022a), MSN (Assran et al., 2022), EsViT (Li et al.,\n2022a), Mugs (Zhou et al., 2022) and iBOT (Zhou et al., 2021). When several architectural variants were\nproposed for a given method, we report results for the one that leads to best top-1 accuracy on ImageNet-1k.\nSecond, we report performance of open-source weakly-supervised models such as CLIP (Radford et al., 2021),\nOpenCLIP (Ilharco et al., 2021), and SWAG (Singh et al., 2022). When evaluating models on ImageNet-1k,\nwe report the performance for each of the aforementioned methods. For all other evaluations, we report\nthe four best-performing models amongst SSL ones. Also, for reference, we report the best performing\nOpenCLIP-G for weakly-supervised ones.\n7.1 ImageNet Classication\nAs a rst evaluation, we probe the quality of the holistic image representation produced by the model on the\nImageNet-1k classication dataset. We evaluate the quality of features by training a simple classier over a\nfrozen backbone, and do not perform netuning of the backbone weights. Following previous work, we use\na linear model for simplicity, ensuring a reproducible evaluation, despite the fact that classes may not be\nlinearly separable. Because most SSL methods were developped using ImageNet-1k validation performance\nas a debugging signal, we also report the top-1 accuracy on ImageNet-ReaL and ImageNet-V2. In order\nto report this additional validation performance, for all models, we run the evaluation with our code. We\ncompare our frozen features to the best publicly available SSL features in Table 4, regardless of architecture\nor pretraining data. We see the components proposed in this work lead to a very signicant improvement\n(+4.2%) over the previous state of the art (iBOT ViT-L/16 trained on ImageNet-22k) on linear evaluation.\nAt the same time, we also see that the performance increase on the alternative test sets is larger for our\nmethod, indicating stronger generalization. We describe details of our linear evaluation in Appendix B.3.\nHow far are we from weakly-supervised models? We also want to validate that our features are com-\npetitive with state-of-the-art open-source weakly supervised models. To this end, we compare on ImageNet-\n1k, using the linear evaluation, to three o-the-shelf methods with several architectural variants. For all\nmodels, we run the linear evaluation using our code, after making sure that our numbers match those re-\nported in technical reports and papers. We show the result of this evaluation in Table 4. We see that our\nbackbone, surpases the performance of OpenCLIP with a ViT-G/14 architecture ( +0.3%) and EVA-CLIP\nwith a ViT-g/14 ( +0.1%). At the same time, we also observe that our performance on the ImageNet-V2 test\n10', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}]: %s
2023-12-07 10:17:39,939 - INFO - Check the data that is being passed [{'page_content': 'kNN linear\nMethod Arch. Data Text sup. val val ReaL V2\nWeakly supervised\nCLIP ViT-L/14 WIT-400M  79.8 84.3 88.1 75.3\nCLIP ViT-L/14 336WIT-400M  80.5 85.3 88.8 75.8\nSWAG ViT-H/14 IG3.6B  82.6 85.7 88.7 77.6\nOpenCLIP ViT-H/14 LAION  81.7 84.4 88.4 75.5\nOpenCLIP ViT-G/14 LAION  83.2 86.2 89.4 77.2\nEVA-CLIP ViT-g/14 custom 83.5 86.4 89.3 77.4\nSelf-supervised\nMAE ViT-H/14 INet-1k \x15 49.4 76.6 83.3 64.8\nDINO ViT-S/8 INet-1k \x15 78.6 79.2 85.5 68.2\nSEERv2 RG10B IG2B \x15  79.8  \nMSN ViT-L/7 INet-1k \x15 79.2 80.7 86.0 69.7\nEsViT Swin-B/W=14 INet-1k \x15 79.4 81.3 87.0 70.4\nMugs ViT-L/16 INet-1k \x15 80.2 82.1 86.9 70.8\niBOT ViT-L/16 INet-22k \x15 72.9 82.3 87.5 72.4\nDINOv2ViT-S/14 LVD-142M \x15 79.0 81.1 86.6 70.9\nViT-B/14 LVD-142M \x15 82.1 84.5 88.3 75.1\nViT-L/14 LVD-142M \x15 83.5 86.3 89.5 78.0\nViT-g/14 LVD-142M \x15 83.5 86.5 89.6 78.4\nTable4:LinearevaluationonImageNet-1koffrozenpretrainedfeatures. WereportTop-1accuracy\non the validation set for publicly available models trained on public or private data, and with or without\ntext supervision (text sup.). For reference, we also report the kNN performance on the validation set. We\ncompare across any possible architectures (Arch.), at resolution 224224unless stated otherwise. The\ndataset used for training EVA-CLIP is a custom mixture, see paper for details (Fang et al., 2023).\nset is signicantly better ( +1.1%versus EVA-CLIP), indicating better generalization. For the remainder of\nthis section, we report OpenCLIP-G as a reference for weakly-supervised models.\nCan we netune the encoders? We question if the ability of our models to produce high quality frozen\nfeatures impact their performance when netuned with supervision on a specic dataset. While this is not\ncore to this paper, this experiment is indicative of whether we have involuntarily specialized our models\nto the setting of linear evaluations of frozen features. To run this sanity check, we apply the netuning\npipeline from Touvron et al. (2022), without tweaking hyper-parameters. In Table 5, we show that the\nTop-1 accuracy on the validation set of ImageNet-1k improves by more than +2%when the backbone is\nnetuned. This is true both when using models at resolution 224and448. Further gains can be obtained by\ntuning the hyper-parameters of the netuning, but this is beyond the goal of this sanity check. Nonetheless,\nour best netuned performance ( 88.9%) is only a couple of percent below ( 2.2%) the absolute state of the\narts ( 91.1%), obtained by Chen et al. (2023). As DINOv2 leads to features that are strong in both the linear\nand netuning settings, a strong property of our approach is that netuning is optional .\nRobustness analysis. To complement our study, and probe the generalization of our features, we evaluate\nour ImageNet-1k models trained with linear classication heads on domain generalization benchmarks. We\nuse the best performing linear classier as described above and simply run inference on those benchmarks.\nPlease note that most results in the litterature are obtained with models that are netuned end-to-end on\nImageNet-1k. We show the result of this experiment in Table 6. When comparing with state-of-the-art SSL\nmethods, our models shows drastically better robustness ( +29.6%on A, +22.1%on R and +23.0%on Sketch\n11', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Arch. Res. Linear Finetuned \nViT-g/14224 86.5 88.5 +2.0\n448 86.7 88.9 +2.2\nTable 5: Supervised netuning on ImageNet-1k. We use the pipeline of Touvron et al. (2022) to\nnetune our encoders on ImageNet-1k at resolutions 224224or448448. We compare with the accuracy\nobtained with linear probing and observe only modest improvements with ne-tuning: this suggests that\nDINOv2 features already perform well out-of-the-box.\nMethod Arch Data Im-A Im-R Im-C Sketch\nOpenCLIP ViT-G/14 LAION 63.8 87.8 45.366.4\nMAE ViT-H/14 INet-1k 10.2 34.4 61.4 21.9\nDINO ViT-B/8 INet-1k 23.9 37.0 56.6 25.5\niBOT ViT-L/16 INet-22k 41.5 51.0 43.9 38.5\nDINOv2ViT-S/14 LVD-142M 33.5 53.7 54.4 41.2\nViT-B/14 LVD-142M 55.1 63.3 42.7 50.6\nViT-L/14 LVD-142M 71.3 74.4 31.5 59.3\nViT-g/14 LVD-142M 75.978.828.2 62.5\nTable 6:Domain Generalization with a linear probe on top of frozen features at a resolution of 224.\nHigher numbers are better for all benchmarks except Im-C.\ncompared to iBOT). Our model also improves upon the best weakly-supervised model on ImageNet-A while\nlagging behind on R and Sketch.\n7.2 Additional Image and Video classication Benchmarks\nIn this section we study the generalization of our features on downstream classication benchmarks. We\nconsider two sets of evaluations in that context. On one hand, we use large and negrained datasets such\nas iNaturalist and Places205. On the other, we use the 12 image classication tasks originally proposed\nin SimCLR (Chen et al., 2020). For iNaturalist 2018, iNaturalist 2021, and Places205, we train a linear\nclassier with data augmentations as in Sec. 7.1 We report top-1 accuracy for those three datasets in Table 7.\nInterestingly, our model signicantly outperforms OpenCLIP ViT-G/14 on both variants of iNaturalist\n(+8.6%and+9.7%for 2018 and 2021 respectively), and lags slightly behind on Places 205 ( 2.3%).\nIn a second set of evaluations, we measure the performance of our model on video action recognition even\nthough our features were not trained on videos.. We evaluated features on three datasets, namely UCF-\n101 (Soomro et al., 2012), Kinetics-400 (Kay et al., 2017) and Something-Something v2 (Goyal et al., 2017).\nFor this evaluation, we pick 8evenly spaced frames in the video and train a linear classier on the average\nof the features for UCF and K-400. For SSv2, we opt for concatenation to retain more temporal information\nthan with feature averaging. For each dataset, we measure average accuracy and report the results in\nTable 7. We see that amongst self-supervised approaches, our model clearly sets a new state of the art.\nMoreover, our model matches the accuracy of the OpenCLIP features on UCF and Kinetics ( +0.1%and\n+0.5%respectively) and clearly outperforms them on SSv2 ( +2.5%). This is particularly interesting, as\nSSv2 requires a much richer understanding of the video frames.\nFinally, in Table 8, we compare selected frozen features on 12 transfer classication benchmarks initially\nproposed by Chen et al. (2020). This benchmark covers scenes, objects (food, cars, planes), and textures.\nWe replace the Birdsnap dataset with CUB because the former was not publicly available in its entirety. We\nfollow the experimental protocol as outlined by Chen et al. (2020), namely training a logistic regression on\nprecomputed features. Our model signicantly outperforms state-of-the-art SSL models, with most notable\ndierences on Stanford Cars ( +14.8%versus DINO ViT-B/8) and FGVC Aircraft ( +14.8%versus iBOT\n12', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Image classication Video classication\nFeature Arch iNat2018 iNat2021 Places205 K400 UCF-101 SSv2\nOpenCLIP ViT-G/14 73.0 76.0 69.8 78.3 90.7 35.8\nMAE ViT-H/14 31.0 32.3 52.4 54.2 70.6 29.2\nDINO ViT-B/8 59.6 68.3 60.4 64.5 85.0 32.6\niBOT ViT-L/16 66.3 74.6 64.4 72.6 88.6 38.7\nDINOv2ViT-S/14 69 74.2 62.9 67.8 87 33.1\nViT-B/14 76.4 81.1 66.2 73.2 89.1 34.4\nViT-L/14 80.4 85.1 67.3 76.3 90.5 35.6\nViT-g/14 81.6 85.7 67.5 78.4 91.2 38.3\nTable 7:Linear evaluation on other image and video classication. The image benchmarks contain\na large quantity of ne-grained examples about objects or scenes. The video benchmarks cover action\nclassication and human-object interaction. All the features are frozen with a linear probe on top.\nFeature Arch Food C10 C100 SUN Cars Aircr VOC DTD Pets Cal101 Flowers CUB Avg\nOpenCLIP ViT-G/14 94.5 98.7 91.0 84.0 96.1 80.289.3 86.0 95.798.199.5 89.9 91.9\nMAE ViT-H/14 78.4 96.1 83.9 63.9 56.1 63.4 84.3 75.4 89.4 95.9 92.3 57.2 78.0\nDINO ViT-B/8 85.1 97.2 86.9 70.3 76.6 70.6 86.7 79.6 93.2 95.4 97.6 81.7 85.1\niBOT ViT-L/16 91.0 99.0 92.8 75.6 71.8 72.4 89.0 80.7 87.7 97.5 99.6 82.1 86.6\nDINOv2ViT-S/14 89.1 97.7 87.5 74.4 81.6 74.0 87.8 80.6 95.1 97.0 99.6 88.1 87.7\nViT-B/14 92.8 98.7 91.3 77.3 88.2 79.4 88.2 83.3 96.2 96.1 99.6 89.6 90.1\nViT-L/14 94.3 99.3 93.4 78.7 90.1 81.5 88.3 84.0 96.6 97.5 99.7 90.5 91.2\nViT-g/14 94.7 99.5 94.4 78.7 91.4 87.289.0 84.5 96.797.699.7 91.6 92.1\nTable 8:Linear evaluation of frozen features on ne-grained benchmarks. Accuracy on 12 bench-\nmarks covering objects, scenes and textures following the evaluation protocol proposed in Chen et al. (2020).\nViT-L/16). Even though these benchmarks favor text-guided pretraining, our features are still competitive\nwith OpenCLIP on most classication benchmarks, with the exception of a few datasets, especially SUN\n(5.3%) and Cars (4.7%).\n7.3 Instance Recognition\nIn this experiment, we probe our model on the task of instance-level recognition using a non-parametric\napproach. Images from a database are ranked according to their cosine similarity with a query image. We\nevaluated our model and compare to baselines on Paris and Oxford, that are landmark recognition bench-\nmarks. We also evaluated on Met, a dataset of artworks from the Metropolitan museum, and AmsterTime,\ncontaining street view images matched to archival images of Amsterdam. We measure performance by com-\nputing the mean average precision and report our results in Table 9. We see that our features signicantly\noutperform both SSL ( +41%mAP on Oxford-Hard), and weakly-supervised ( +34%mAP on Oxford-Hard)\nones. Itisinterestingtoseethatourfeaturesperformwellacrosstaskgranularities, bothatthecategory-level\nand instance-level. This is a desirable property for strong o-the-shelf computer vision features.\n7.4 Dense Recognition Tasks\nWe probe the quality of patch-level features extracted from our network on several dense downstream tasks.\nWeconsidersemanticimagesegmentationandmonoculardepthestimationinseveralsettingsandweconduct\nevaluations on multiple datasets for each.\n13', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Oxford Paris Met AmsterTime\nFeature Arch M H M H GAP GAP- ACC mAP\nOpenCLIP ViT-G/14 50.7 19.7 79.2 60.2 6.5 23.9 34.4 24.6\nMAE ViT-H/14 11.7 2.2 19.9 4.7 7.5 23.5 30.5 4.2\nDINO ViT-B/8 40.1 13.7 65.3 35.3 17.1 37.7 43.9 24.6\niBOT ViT-L/16 39.0 12.7 70.7 47.0 25.1 54.8 58.2 26.7\nDINOv2ViT-S/14 68.8 43.2 84.6 68.5 29.4 54.3 57.7 43.5\nViT-B/14 72.9 49.5 90.3 78.6 36.7 63.5 66.1 45.6\nViT-L/14 75.1 54.0 92.7 83.5 40.0 68.9 71.6 50.0\nViT-g/14 73.6 52.3 92.1 82.6 36.8 73.6 76.5 46.7\nTable 9:Evaluation of frozen features on instance-level recognition. We consider 4 dierent bench-\nmarks and report their main metrics.\nADE20k CityScapes Pascal VOC\n(62.9) (86.9) (89.0)\nMethod Arch. lin. +ms lin. +ms lin. +ms\nOpenCLIP ViT-G/14 39.3 46.0 60.3 70.3 71.4 79.2\nMAE ViT-H/14 33.3 30.7 58.4 61.0 67.6 63.3\nDINO ViT-B/8 31.8 35.2 56.9 66.2 66.4 75.6\niBOT ViT-L/16 44.6 47.5 64.8 74.5 82.3 84.3\nDINOv2ViT-S/14 44.3 47.2 66.6 77.1 81.1 82.6\nViT-B/14 47.3 51.3 69.4 80.0 82.5 84.9\nViT-L/14 47.7 53.1 70.3 80.9 82.1 86.0\nViT-g/14 49.053.071.3 81.0 83.0 86.2\nTable 10: Semantic segmentation on ADE20K, CityScapes and Pascal VOC with frozen features\nand a linear classier (lin.) and with multiscale (+ms). The absolute state of the art  from Wang et al.\n(2022), Liu et al. (2021) and Chen et al. (2018) respectively  are mentioned at the top of the Table. For\nreference, using the Mask2Former pipeline (Steiner et al., 2021) with a ViT-Adapter (Chen et al., 2022) on\ntop of our frozen ViT-g/14 backbone gives 60.2 mIoU on ADE-20k.\nSemantic segmentation. For our semantic segmentation evaluation, we consider two dierent setups.\nLinear: a linear layer is trained to predict class logits from a patch tokens. It is used to produce a low-\nresolution logit map (eg 32x32 for a model with patch size 16), which is then upsampled to full resolution\n(512x512) to obtain a segmentation map. This procedure is extremely simple but cannot easily produce\nhigh-resolution segmentations. +ms: a boosted version of the linear setup. We concatenate the patch\ntokens of the 4 last layers, use a larger image resolution of 640, and use multiscale test-time augmentations\nto improve the predictions. We report the performance of our model variants as well as the baselines on\nthree datasets under the two setups in Table 10.\nOur models show very good performance on all datasets and for all setups. Interestingly, our evaluation\nusing+msis on par with fully netuning MAE with an Upernet decoder ( 53.0versus 53.6mIoU). This is\nsurprising because we use a signicantly simpler predictor. Also, our best model, when evaluated using the\nboosted recipe, almost matches the state of the art on Pascal VOC ( 86.2versus 89.0mIoU).\nIn a nal experiment, we freeze our backbone, and plug it into a ViT-Adapter Chen et al. (2022) with a\nMask2former head (Cheng et al., 2022). We tune the weights of the adapter and head, but keep the backbone\nfrozen: only a fraction of the weights are tuned, keeping the training procedure lightweight. We reach 60.2\nmIoU on ADE20k, close to the competitive state of the art, standing at 62.9 mIoU (Wang et al., 2022).\n14', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'NYUd KITTI NYUd SUN RGB-D\n(0.330) (2.10) (0.421)\nMethod Arch. lin. 1 lin. 4 DPT lin. 1 lin. 4 DPT lin. 1 lin. 4 DPT\nOpenCLIP ViT-G/14 0.541 0.510 0.414 3.57 3.21 2.56 0.537 0.476 0.408\nMAE ViT-H/14 0.517 0.483 0.415 3.66 3.26 2.59 0.545 0.523 0.506\nDINO ViT-B/8 0.555 0.539 0.492 3.81 3.56 2.74 0.553 0.541 0.520\niBOT ViT-L/16 0.417 0.387 0.358 3.31 3.07 2.55 0.447 0.435 0.426\nDINOv2ViT-S/14 0.449 0.417 0.356 3.10 2.86 2.34 0.477 0.431 0.409\nViT-B/14 0.399 0.362 0.317 2.90 2.59 2.23 0.448 0.400 0.377\nViT-L/14 0.384 0.333 0.293 2.78 2.50 2.14 0.429 0.396 0.360\nViT-g/14 0.344 0.298 0.279 2.62 2.35 2.11 0.402 0.362 0.338\nTable 11: Depth estimation with frozen features . We report performance when training a linear\nclassier on top of one (lin. 1) or four (lin. 4) transformer layers, as well, as the DPT decoder (DPT) of\nRanftl et al. (2021). We report the RMSE metric on the 3 datasets. Lower is better. For reference, we\nreport state-of-the-art results taken from Li et al. (2022b) on each benchmark on top of the Table.\nDepth estimation. In this experiment, we evaluate our patch-level features on three monocular depth\nestimation benchmarks: NYUd, KITTI and zero-shot transfer from NYUd to SUN3d. We follow the evalu-\nation protocol of Li et al. (2022b). We consider three dierent setups for this evaluation. lin. 1: we extract\nthe last layer of the frozen transformer and concatenate the [CLS]token to each patch token. Then we\nbi-linearly upsample the tokens by a factor of 4 to increase the resolution. Finally we train a simple linear\nlayer using a classication loss by dividing the depth prediction range in 256 uniformly distributed bins and\nuse a linear normalization following Bhat et al. (2021). lin. 4: we use the same protocol that we use with\none layer, but concatenate the tokens from layers l={3,6,9,12}for ViT-S/B, l={5,12,18,24}for ViT-L,\nandl={10,20,30,40}for ViT-g. DPT: we use the DPT decoder (Ranftl et al., 2021) on top of our frozen\nmodels and setup a regression task. We scale the size of the head following the dimension of the features for\neach architecture. We show results for all baselines, all datasets and all setups in Table 11.\nFrom this table, we see that our features clearly surpass the best SSL and WSL features available. It\nis interesting to see that iBOT features extracted from a ViT-L outperform the ones of OpenCLIP with\na ViT-G. This observation supports the intuition that caption-based feature learning fails to learn subtle\npatterns like this one. Also, our model, with the DPT decoder and frozen backbone, matches or exceeds\nthe performance of the recent work of Li et al. (2022b). Finally, the out-of-domain generalization result on\nSUN-RGBd shows that our features allow very good transfer between domains. A depth prediction module\ntrained on indoor scenes from NYUd generalizes pretty well to the outdoor examples of SUN-RGBd.\n7.5 Qualitative Results\nIn this nal section of the empirical evaluation of our features, we propose a few qualitative analyses.\nSemantic Segmentation and Depth Estimation. We show some qualitative results for our dense\nprediction evaluations: segmentation on ADE20K in Fig. 7 and depth estimation on NYUd, KITTI and\nSUN RGB-D in Fig. 7. We compare DINOv2 with OpenCLIP with a linear classier on each dataset. While\nnot perfect, the linear segmentation model using our DINOv2 backbone produces good results and behaves\nmuch better than the OpenCLIP one under this evaluation setup. Indeed, the segmentation mask produced\nby OpenCLIP-G shows many artifacts and disconnected components. The qualitative results on depth\nestimation clearly illustrate the quantitative gap between OpenCLIP and DINOv2. These results highlight\nthat our features, as well as the features extracted from OpenCLIP, are able to linearly separate complex\ninformation such as depth, even though neither was trained with this type of information. However, our\nfeatures lead to a much smoother depth estimation, with less artifacts. Some objects such as the chair on\nthe SUN RGB-D image are completely ignored by OpenCLIP and correctly positioned using our features.\n15', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 7: Segmentation and depth estimation with linear classiers. Examples from ADE20K,\nNYUd, SUN RGB-D and KITTI with a linear probe on frozen OpenCLIP-G and DINOv2-g features.\nFigure 8:Examples of out-of-distribution examples with frozen DINOv2-g features and a linear probe.\nOut-of-distribution generalization. We show a few examples of applying the depth prediction and\nsegmentation linear classiers to out-of-distribution examples in Fig. 8. The qualitative results support our\nclaim that our features transfer between domains. The quality of the depth and segmentation predicted for\npictures of animals, or paintings is very good, even though the domains are very dierent.\nPCA of patch features. We show the results of the principal component analysis (PCA) performed on\nthe patch features extracted by our model. We keep only patches with a positive value after we threshold\nthe rst component. This procedure turns out to separate the images main object from the background. We\ncompute a second PCA on the remaining patches across three images depicting the same category. We color\nthe three rst components with three dierent colors and present the results in Fig. 1 and 9. There are two\ninteresting observations: rst, our unsupervised foreground / background detector, based on detecting the\n16', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 9:Morevisualization oftherst PCAcomponents. We compute the PCA between the patches\nfrom all of the images and show their rst 3 components. Each component corresponds to a specic color\nchannel. Same parts are matched between related images depsite changes of pose, style or even objects.\nBackground is removed by removing patches with a negative score of the rst PCA component.\nhighest variance direction, performs very well and is capable of delineating the boundary of the main object\nin the picture. Second, the other components correspond to "parts" of objects and match well for images of\nthe same category. This is an emerging property  our model was not trained to parse parts of objects.\nPatch matching. Finally, we explore what type of information our patch-level features contain by match-\ning them across images. We start by detecting the foreground object using the procedure described above.\nThen, we compute the euclidean distance between patch features extracted from two images and map them\nby solving an assignment problem. In order to reduce the number of matches, we then apply a non-maximum\nsuppression to keep only the salient ones. In Fig. 10, we show some examples of such matchings.\nWe observe that the features seem to capture information about semantic regions that serve similar purpose\nin dierent objects or animals. For instance, the wing of a plane matches the wing of a bird. We also observe\nthat the model is robust to style (image versus drawing), and to large variation of poses (see the elephant).\n8 Fairness and Bias Analysis\nWe conduct two fairness evaluations of our models. We probe for geographical fairness and potential harmful\nlabel associations. For both evaluations, we experiment with our largest ViT-g model.\n8.1 Geographical Fairness\nWe evaluate geographical fairness on the Dollar Street dataset introduced in De Vries et al. (2019) using\nthe evaluation protocol of Goyal et al. (2022b). This benchmark compares performance across countries and\nincome levels. It contains 16,073 images from 289 households across 54 countries. The task is to recognize\n94 concepts that vary visually between households based on income or location. In Table 12, we compare\nour model with SEERv2 (Goyal et al., 2022a), a model trained on a geographically diverse set of images.\nOur model is slightly fairer across regions and incomes than the SEERv2 model and signicantly better than\n17', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 10: Matching across images. We match patch-level features between images from dierent do-\nmains, poses and even objects that share similar semantic information. This exhibits the ability of our model\nto transfer across domains and understand relations between similar parts of dierent objects.\nIncome buckets Regions\nMethod Arch. Data low medium high Africa Asia Americas Europe\nSEERv2 RG-10B IG-1B 59.7 78.5 86.6 65.9 76.3 81.1 85.6\nDINOv2 ViT-g/14 LVD-142M 67.4 83.3 90.5 74.0 81.6 86.2 89.7\nTable 12: Geographical fairness and diversity analysis across income buckets and regions.\nthe supervised baseline reported by Goyal et al. (2022a). However, we still observe a signicant dierence\nbetween regions, particularly in Africa, where our model performance drops by 25.7% compared to Europe.\nThisshowthatourmodelisstillbiasedtowardWesterncountries. Similarly, ourmodelperformssignicantly\nbetter on high-income households than low-income ones, with a dierence of 31.7%. Despite improvements,\nwe observe signicant biases in our models toward wealthy households from Western countries.\n8.2 Gender, Skintones and Age\nIn a second set of evaluations, we question how our model classies images of people of dierent gender, skin\ntone, and age (all self-reported). We follow the protocol of Goyal et al. (2022b), where we train a multiclass\nclassier on a subset of 619 classes of ImageNet-22k. We group the 619 classes into four broader categories:\nHuman, Possibly Human, Non-Human, or Crime. Non-Human and Crime are considered harmful. Using\nthis classier, we run inference on 2955 images from the Casual Conversations dataset (Hazirbas et al., 2021)\nand keep all labels in the top-5 that are assigned a probability of 0.1 or more. Because of that, we can assign\n18', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Gender Skintone Age Groups\nModel Assoc.female\ndarkerfemale\nlightermale\ndarkermale\nlighter18-30 30-45 45-70 70+\nSEER Non-Human 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nRG-10B Crime 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nHuman 94.9 95.8 86.6 79.0 90.5 88.3 91.9 82.3\nPossibly-Human 13.6 6.7 65.0 60.2 32.8 37.2 29.4 6.5\nDINOv2 Non-Human 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nViT-g/14 Crime 0.0 0.0 0.2 0.0 0.0 0.1 0.0 0.0\nHuman 97.3 97.7 86.1 84.0 91.2 90.2 93.2 88.7\nPossibly-Human 15.8 17.2 52.2 48.1 35.3 37.3 23.0 9.7\nTable 13: Label association fairness evaluation across gender, skintones and age groups. We\nfollow the protocol proposed by Goyal et al. (2022b) with a slight modication. Instead of netuning the\nbackbone, we simply learn a linear classier on the subset of 619 classes of ImageNet-22k.\nModel toGPU TypeGPU PowerGPU-hours PUETotal power Carbon emitted\nReproduce consumption consumption (tCO 2eq)\nDINOv2-g A100-40GB 400W 22,016 1.1 9.7 MWh 3.7\nTable 14: Carbon footprint of reproducing DINOv2. We report the potential carbon emission of\nreproducing DINOv2-g when assuming a power consumption for the A100-40GB of 400W, a PUE of 1.1 and\ncarbon intensity factor of 0.385 kg CO 2e per KWh.\nmultiple classes to each image. We make one modication to the original evaluation protocol: we do not\nbackpropagate gradients to the backbone and keep it frozen. We compare our model to SEERv2 in Table 13.\nOur model often classies images of all groups as Human without large deviations across skin tones. Neither\nSEERv2 nor DINOv2 predict harmful labels from the Non-Human or Crime meta-categories (except for two\ninstances where the background contains bars visually similar to prison bars). We see that our model triggers\nthe Possibly-Human classes often. This class is constructed from objects in ImageNet-22k that are often\nrelated to Humans, such as Scarf, Glasses, or Beard. Our model often predicts the Possibly-Human class\nfor men because of the prevalence of the Beard class. No clear pattern indicates a bias against a particular\ngroup in this study. While this is encouraging, we also acknowledge that a more thorough evaluation of\nbiases may reveal aws in our model.\n9 Estimating the Environmental Impact of Training our Models\nTraining foundation models consumes a signicant amount of energy, resulting in carbon dioxide emissions.\nPatterson et al. (2021) propose a methodology to report an estimation of the carbon emitted during the\ntraining of a model based on the specics of the data center and its power grid. This computation informs\nthe design of the data center used for the training of models and the choice of location for data centers.\nThis methodology requires to know the specics of the data center used for training, which can be complex\nwhen multiple data centers are involved over time. Additionally, these specics are most often not in the\ncontrol of the AI practitioner, and hence, this methodology is less helpful when practioners make technical\ndecisions about future trainings. Instead, in this section, we follow an alternative that reports the potential\ncarbon emission of retraining a similar model in an average data center located in the US. This methodology\nwas used in previous work in natural language processing (Strubell et al., 2019; Touvron et al., 2023) to\nestablish an apple-to-apple comparison between pretraining schemes. More precisely, we x the value of all\nexogenous variables, i.e., the Power Usage Eectiveness (PUE) and carbon intensity factor of a power grid\nto the same values as in Touvron et al. (2023), that is, a PUE of 1.1 and the carbon intensity factor to the\n19', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'US average of 0.385 kg CO 2eq/KWh. We use the same formula as in Patterson et al. (2021) to estimate the\npotential energy consumption and the carbon emission. For the power consumption of an A100-80GB, we\ntake the thermal design power for NVLink systems, which is 400W. We report the potential carbon emission\nof retraining a DINOv2 ViT-g in Table 14. For comparison, retraining an OpenCLIP ViT-L or OpenCLIP\nViT-G would require 22.4 MWh and 118.9 MWh, respectively, if run in the same data center. This is 10 \nmore carbon emission. Note that this comparison is not fair to them, since they also train a text encoder in\nparallel, and we thus do not report them in the table. However, it gives a reasonable guidelines for those who\nare interested in training only visual features: in this context, training a self-supervised model is preferable\nin terms of carbon emission. Training a text-guided model still makes sense when planning to reuse the text\nencoder.\nCarbon footprint of the whole project. Additionally, we estimate the footprint of the whole project\nto be between 0.5k and 1k tCO 2eq using the same grid as presented above. This carbon footprint represents\nin the order of 200k GPU-days. The primary sources of emissions are the self-supervised pre-trainings of\nthe models. For example, a single pre-training of a ViT-g model (22k GPU-hours) emits 3.7 tons of CO 2eq,\nwhile a netuning on ImageNet-1k (1k GPU-hours) emits 0.2 tons. This estimate only considers the GPUs\nelectricity consumption and ignores other emissions, such as their manufacturing and disposal.\n10 Future work and Discussion\nIn this work, we present DINOv2, a new series of image encoders pretrained on large curated data with no\nsupervision. This is the rst SSL work on image data that leads to visual features that close the performance\ngap with (weakly) supervised alternatives across a wide range of benchmarks and without the need for\nnetuning. A few properties emerge from these models, such as an understanding of object parts and scene\ngeometry regardless of the image domains. We expect that more of these properties will emerge at larger\nscales of models and data, akin to instruction emergence in large language models, and plan to continue\nscaling along these axes. This paper also demonstrates that these visual features are compatible with\nclassiers as simple as linear layers - meaning the underlying information is readily available . In future work,\nwe plan to leverage this ability to train a a language-enabled AI system that can process visual features as\nif they were word tokens, and extract the required information to ground the system.\nAcknowledgments.\nWe thank Mathilde Caron for initial discussions that led to this work. We thank Olivia Joulin for the horse\ndrawing used in Fig. 10. We also thank the rest of FAIR and Meta AI for feedback on this work through\nthe entire project.\nReferences\nYuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous clustering\nand representation learning. In ICLR, 2020.\nMahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand\nJoulin, Michael Rabbat, and Nicolas Ballas. Masked siamese networks for label-ecient learning. arXiv\npreprint arXiv:2204.07141 , 2022.\nMahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann\nLeCun, and Nicolas Ballas. Self-supervised learning from images with a joint-embedding predictive archi-\ntecture.arXiv preprint arXiv:2301.08243 , 2023.\nAlexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. Data2vec: A general\nframework for self-supervised learning in speech, vision and language. arXiv preprint arXiv:2202.03555 ,\n2022.\nHangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint\narXiv:2106.08254 , 2021.\n20', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}]: %s
2023-12-07 10:17:39,939 - INFO - Check the results [{'page_content': 'kNN linear\nMethod Arch. Data Text sup. val val ReaL V2\nWeakly supervised\nCLIP ViT-L/14 WIT-400M  79.8 84.3 88.1 75.3\nCLIP ViT-L/14 336WIT-400M  80.5 85.3 88.8 75.8\nSWAG ViT-H/14 IG3.6B  82.6 85.7 88.7 77.6\nOpenCLIP ViT-H/14 LAION  81.7 84.4 88.4 75.5\nOpenCLIP ViT-G/14 LAION  83.2 86.2 89.4 77.2\nEVA-CLIP ViT-g/14 custom 83.5 86.4 89.3 77.4\nSelf-supervised\nMAE ViT-H/14 INet-1k \x15 49.4 76.6 83.3 64.8\nDINO ViT-S/8 INet-1k \x15 78.6 79.2 85.5 68.2\nSEERv2 RG10B IG2B \x15  79.8  \nMSN ViT-L/7 INet-1k \x15 79.2 80.7 86.0 69.7\nEsViT Swin-B/W=14 INet-1k \x15 79.4 81.3 87.0 70.4\nMugs ViT-L/16 INet-1k \x15 80.2 82.1 86.9 70.8\niBOT ViT-L/16 INet-22k \x15 72.9 82.3 87.5 72.4\nDINOv2ViT-S/14 LVD-142M \x15 79.0 81.1 86.6 70.9\nViT-B/14 LVD-142M \x15 82.1 84.5 88.3 75.1\nViT-L/14 LVD-142M \x15 83.5 86.3 89.5 78.0\nViT-g/14 LVD-142M \x15 83.5 86.5 89.6 78.4\nTable4:LinearevaluationonImageNet-1koffrozenpretrainedfeatures. WereportTop-1accuracy\non the validation set for publicly available models trained on public or private data, and with or without\ntext supervision (text sup.). For reference, we also report the kNN performance on the validation set. We\ncompare across any possible architectures (Arch.), at resolution 224224unless stated otherwise. The\ndataset used for training EVA-CLIP is a custom mixture, see paper for details (Fang et al., 2023).\nset is signicantly better ( +1.1%versus EVA-CLIP), indicating better generalization. For the remainder of\nthis section, we report OpenCLIP-G as a reference for weakly-supervised models.\nCan we netune the encoders? We question if the ability of our models to produce high quality frozen\nfeatures impact their performance when netuned with supervision on a specic dataset. While this is not\ncore to this paper, this experiment is indicative of whether we have involuntarily specialized our models\nto the setting of linear evaluations of frozen features. To run this sanity check, we apply the netuning\npipeline from Touvron et al. (2022), without tweaking hyper-parameters. In Table 5, we show that the\nTop-1 accuracy on the validation set of ImageNet-1k improves by more than +2%when the backbone is\nnetuned. This is true both when using models at resolution 224and448. Further gains can be obtained by\ntuning the hyper-parameters of the netuning, but this is beyond the goal of this sanity check. Nonetheless,\nour best netuned performance ( 88.9%) is only a couple of percent below ( 2.2%) the absolute state of the\narts ( 91.1%), obtained by Chen et al. (2023). As DINOv2 leads to features that are strong in both the linear\nand netuning settings, a strong property of our approach is that netuning is optional .\nRobustness analysis. To complement our study, and probe the generalization of our features, we evaluate\nour ImageNet-1k models trained with linear classication heads on domain generalization benchmarks. We\nuse the best performing linear classier as described above and simply run inference on those benchmarks.\nPlease note that most results in the litterature are obtained with models that are netuned end-to-end on\nImageNet-1k. We show the result of this experiment in Table 6. When comparing with state-of-the-art SSL\nmethods, our models shows drastically better robustness ( +29.6%on A, +22.1%on R and +23.0%on Sketch\n11', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Arch. Res. Linear Finetuned \nViT-g/14224 86.5 88.5 +2.0\n448 86.7 88.9 +2.2\nTable 5: Supervised netuning on ImageNet-1k. We use the pipeline of Touvron et al. (2022) to\nnetune our encoders on ImageNet-1k at resolutions 224224or448448. We compare with the accuracy\nobtained with linear probing and observe only modest improvements with ne-tuning: this suggests that\nDINOv2 features already perform well out-of-the-box.\nMethod Arch Data Im-A Im-R Im-C Sketch\nOpenCLIP ViT-G/14 LAION 63.8 87.8 45.366.4\nMAE ViT-H/14 INet-1k 10.2 34.4 61.4 21.9\nDINO ViT-B/8 INet-1k 23.9 37.0 56.6 25.5\niBOT ViT-L/16 INet-22k 41.5 51.0 43.9 38.5\nDINOv2ViT-S/14 LVD-142M 33.5 53.7 54.4 41.2\nViT-B/14 LVD-142M 55.1 63.3 42.7 50.6\nViT-L/14 LVD-142M 71.3 74.4 31.5 59.3\nViT-g/14 LVD-142M 75.978.828.2 62.5\nTable 6:Domain Generalization with a linear probe on top of frozen features at a resolution of 224.\nHigher numbers are better for all benchmarks except Im-C.\ncompared to iBOT). Our model also improves upon the best weakly-supervised model on ImageNet-A while\nlagging behind on R and Sketch.\n7.2 Additional Image and Video classication Benchmarks\nIn this section we study the generalization of our features on downstream classication benchmarks. We\nconsider two sets of evaluations in that context. On one hand, we use large and negrained datasets such\nas iNaturalist and Places205. On the other, we use the 12 image classication tasks originally proposed\nin SimCLR (Chen et al., 2020). For iNaturalist 2018, iNaturalist 2021, and Places205, we train a linear\nclassier with data augmentations as in Sec. 7.1 We report top-1 accuracy for those three datasets in Table 7.\nInterestingly, our model signicantly outperforms OpenCLIP ViT-G/14 on both variants of iNaturalist\n(+8.6%and+9.7%for 2018 and 2021 respectively), and lags slightly behind on Places 205 ( 2.3%).\nIn a second set of evaluations, we measure the performance of our model on video action recognition even\nthough our features were not trained on videos.. We evaluated features on three datasets, namely UCF-\n101 (Soomro et al., 2012), Kinetics-400 (Kay et al., 2017) and Something-Something v2 (Goyal et al., 2017).\nFor this evaluation, we pick 8evenly spaced frames in the video and train a linear classier on the average\nof the features for UCF and K-400. For SSv2, we opt for concatenation to retain more temporal information\nthan with feature averaging. For each dataset, we measure average accuracy and report the results in\nTable 7. We see that amongst self-supervised approaches, our model clearly sets a new state of the art.\nMoreover, our model matches the accuracy of the OpenCLIP features on UCF and Kinetics ( +0.1%and\n+0.5%respectively) and clearly outperforms them on SSv2 ( +2.5%). This is particularly interesting, as\nSSv2 requires a much richer understanding of the video frames.\nFinally, in Table 8, we compare selected frozen features on 12 transfer classication benchmarks initially\nproposed by Chen et al. (2020). This benchmark covers scenes, objects (food, cars, planes), and textures.\nWe replace the Birdsnap dataset with CUB because the former was not publicly available in its entirety. We\nfollow the experimental protocol as outlined by Chen et al. (2020), namely training a logistic regression on\nprecomputed features. Our model signicantly outperforms state-of-the-art SSL models, with most notable\ndierences on Stanford Cars ( +14.8%versus DINO ViT-B/8) and FGVC Aircraft ( +14.8%versus iBOT\n12', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Image classication Video classication\nFeature Arch iNat2018 iNat2021 Places205 K400 UCF-101 SSv2\nOpenCLIP ViT-G/14 73.0 76.0 69.8 78.3 90.7 35.8\nMAE ViT-H/14 31.0 32.3 52.4 54.2 70.6 29.2\nDINO ViT-B/8 59.6 68.3 60.4 64.5 85.0 32.6\niBOT ViT-L/16 66.3 74.6 64.4 72.6 88.6 38.7\nDINOv2ViT-S/14 69 74.2 62.9 67.8 87 33.1\nViT-B/14 76.4 81.1 66.2 73.2 89.1 34.4\nViT-L/14 80.4 85.1 67.3 76.3 90.5 35.6\nViT-g/14 81.6 85.7 67.5 78.4 91.2 38.3\nTable 7:Linear evaluation on other image and video classication. The image benchmarks contain\na large quantity of ne-grained examples about objects or scenes. The video benchmarks cover action\nclassication and human-object interaction. All the features are frozen with a linear probe on top.\nFeature Arch Food C10 C100 SUN Cars Aircr VOC DTD Pets Cal101 Flowers CUB Avg\nOpenCLIP ViT-G/14 94.5 98.7 91.0 84.0 96.1 80.289.3 86.0 95.798.199.5 89.9 91.9\nMAE ViT-H/14 78.4 96.1 83.9 63.9 56.1 63.4 84.3 75.4 89.4 95.9 92.3 57.2 78.0\nDINO ViT-B/8 85.1 97.2 86.9 70.3 76.6 70.6 86.7 79.6 93.2 95.4 97.6 81.7 85.1\niBOT ViT-L/16 91.0 99.0 92.8 75.6 71.8 72.4 89.0 80.7 87.7 97.5 99.6 82.1 86.6\nDINOv2ViT-S/14 89.1 97.7 87.5 74.4 81.6 74.0 87.8 80.6 95.1 97.0 99.6 88.1 87.7\nViT-B/14 92.8 98.7 91.3 77.3 88.2 79.4 88.2 83.3 96.2 96.1 99.6 89.6 90.1\nViT-L/14 94.3 99.3 93.4 78.7 90.1 81.5 88.3 84.0 96.6 97.5 99.7 90.5 91.2\nViT-g/14 94.7 99.5 94.4 78.7 91.4 87.289.0 84.5 96.797.699.7 91.6 92.1\nTable 8:Linear evaluation of frozen features on ne-grained benchmarks. Accuracy on 12 bench-\nmarks covering objects, scenes and textures following the evaluation protocol proposed in Chen et al. (2020).\nViT-L/16). Even though these benchmarks favor text-guided pretraining, our features are still competitive\nwith OpenCLIP on most classication benchmarks, with the exception of a few datasets, especially SUN\n(5.3%) and Cars (4.7%).\n7.3 Instance Recognition\nIn this experiment, we probe our model on the task of instance-level recognition using a non-parametric\napproach. Images from a database are ranked according to their cosine similarity with a query image. We\nevaluated our model and compare to baselines on Paris and Oxford, that are landmark recognition bench-\nmarks. We also evaluated on Met, a dataset of artworks from the Metropolitan museum, and AmsterTime,\ncontaining street view images matched to archival images of Amsterdam. We measure performance by com-\nputing the mean average precision and report our results in Table 9. We see that our features signicantly\noutperform both SSL ( +41%mAP on Oxford-Hard), and weakly-supervised ( +34%mAP on Oxford-Hard)\nones. Itisinterestingtoseethatourfeaturesperformwellacrosstaskgranularities, bothatthecategory-level\nand instance-level. This is a desirable property for strong o-the-shelf computer vision features.\n7.4 Dense Recognition Tasks\nWe probe the quality of patch-level features extracted from our network on several dense downstream tasks.\nWeconsidersemanticimagesegmentationandmonoculardepthestimationinseveralsettingsandweconduct\nevaluations on multiple datasets for each.\n13', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Oxford Paris Met AmsterTime\nFeature Arch M H M H GAP GAP- ACC mAP\nOpenCLIP ViT-G/14 50.7 19.7 79.2 60.2 6.5 23.9 34.4 24.6\nMAE ViT-H/14 11.7 2.2 19.9 4.7 7.5 23.5 30.5 4.2\nDINO ViT-B/8 40.1 13.7 65.3 35.3 17.1 37.7 43.9 24.6\niBOT ViT-L/16 39.0 12.7 70.7 47.0 25.1 54.8 58.2 26.7\nDINOv2ViT-S/14 68.8 43.2 84.6 68.5 29.4 54.3 57.7 43.5\nViT-B/14 72.9 49.5 90.3 78.6 36.7 63.5 66.1 45.6\nViT-L/14 75.1 54.0 92.7 83.5 40.0 68.9 71.6 50.0\nViT-g/14 73.6 52.3 92.1 82.6 36.8 73.6 76.5 46.7\nTable 9:Evaluation of frozen features on instance-level recognition. We consider 4 dierent bench-\nmarks and report their main metrics.\nADE20k CityScapes Pascal VOC\n(62.9) (86.9) (89.0)\nMethod Arch. lin. +ms lin. +ms lin. +ms\nOpenCLIP ViT-G/14 39.3 46.0 60.3 70.3 71.4 79.2\nMAE ViT-H/14 33.3 30.7 58.4 61.0 67.6 63.3\nDINO ViT-B/8 31.8 35.2 56.9 66.2 66.4 75.6\niBOT ViT-L/16 44.6 47.5 64.8 74.5 82.3 84.3\nDINOv2ViT-S/14 44.3 47.2 66.6 77.1 81.1 82.6\nViT-B/14 47.3 51.3 69.4 80.0 82.5 84.9\nViT-L/14 47.7 53.1 70.3 80.9 82.1 86.0\nViT-g/14 49.053.071.3 81.0 83.0 86.2\nTable 10: Semantic segmentation on ADE20K, CityScapes and Pascal VOC with frozen features\nand a linear classier (lin.) and with multiscale (+ms). The absolute state of the art  from Wang et al.\n(2022), Liu et al. (2021) and Chen et al. (2018) respectively  are mentioned at the top of the Table. For\nreference, using the Mask2Former pipeline (Steiner et al., 2021) with a ViT-Adapter (Chen et al., 2022) on\ntop of our frozen ViT-g/14 backbone gives 60.2 mIoU on ADE-20k.\nSemantic segmentation. For our semantic segmentation evaluation, we consider two dierent setups.\nLinear: a linear layer is trained to predict class logits from a patch tokens. It is used to produce a low-\nresolution logit map (eg 32x32 for a model with patch size 16), which is then upsampled to full resolution\n(512x512) to obtain a segmentation map. This procedure is extremely simple but cannot easily produce\nhigh-resolution segmentations. +ms: a boosted version of the linear setup. We concatenate the patch\ntokens of the 4 last layers, use a larger image resolution of 640, and use multiscale test-time augmentations\nto improve the predictions. We report the performance of our model variants as well as the baselines on\nthree datasets under the two setups in Table 10.\nOur models show very good performance on all datasets and for all setups. Interestingly, our evaluation\nusing+msis on par with fully netuning MAE with an Upernet decoder ( 53.0versus 53.6mIoU). This is\nsurprising because we use a signicantly simpler predictor. Also, our best model, when evaluated using the\nboosted recipe, almost matches the state of the art on Pascal VOC ( 86.2versus 89.0mIoU).\nIn a nal experiment, we freeze our backbone, and plug it into a ViT-Adapter Chen et al. (2022) with a\nMask2former head (Cheng et al., 2022). We tune the weights of the adapter and head, but keep the backbone\nfrozen: only a fraction of the weights are tuned, keeping the training procedure lightweight. We reach 60.2\nmIoU on ADE20k, close to the competitive state of the art, standing at 62.9 mIoU (Wang et al., 2022).\n14', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'NYUd KITTI NYUd SUN RGB-D\n(0.330) (2.10) (0.421)\nMethod Arch. lin. 1 lin. 4 DPT lin. 1 lin. 4 DPT lin. 1 lin. 4 DPT\nOpenCLIP ViT-G/14 0.541 0.510 0.414 3.57 3.21 2.56 0.537 0.476 0.408\nMAE ViT-H/14 0.517 0.483 0.415 3.66 3.26 2.59 0.545 0.523 0.506\nDINO ViT-B/8 0.555 0.539 0.492 3.81 3.56 2.74 0.553 0.541 0.520\niBOT ViT-L/16 0.417 0.387 0.358 3.31 3.07 2.55 0.447 0.435 0.426\nDINOv2ViT-S/14 0.449 0.417 0.356 3.10 2.86 2.34 0.477 0.431 0.409\nViT-B/14 0.399 0.362 0.317 2.90 2.59 2.23 0.448 0.400 0.377\nViT-L/14 0.384 0.333 0.293 2.78 2.50 2.14 0.429 0.396 0.360\nViT-g/14 0.344 0.298 0.279 2.62 2.35 2.11 0.402 0.362 0.338\nTable 11: Depth estimation with frozen features . We report performance when training a linear\nclassier on top of one (lin. 1) or four (lin. 4) transformer layers, as well, as the DPT decoder (DPT) of\nRanftl et al. (2021). We report the RMSE metric on the 3 datasets. Lower is better. For reference, we\nreport state-of-the-art results taken from Li et al. (2022b) on each benchmark on top of the Table.\nDepth estimation. In this experiment, we evaluate our patch-level features on three monocular depth\nestimation benchmarks: NYUd, KITTI and zero-shot transfer from NYUd to SUN3d. We follow the evalu-\nation protocol of Li et al. (2022b). We consider three dierent setups for this evaluation. lin. 1: we extract\nthe last layer of the frozen transformer and concatenate the [CLS]token to each patch token. Then we\nbi-linearly upsample the tokens by a factor of 4 to increase the resolution. Finally we train a simple linear\nlayer using a classication loss by dividing the depth prediction range in 256 uniformly distributed bins and\nuse a linear normalization following Bhat et al. (2021). lin. 4: we use the same protocol that we use with\none layer, but concatenate the tokens from layers l={3,6,9,12}for ViT-S/B, l={5,12,18,24}for ViT-L,\nandl={10,20,30,40}for ViT-g. DPT: we use the DPT decoder (Ranftl et al., 2021) on top of our frozen\nmodels and setup a regression task. We scale the size of the head following the dimension of the features for\neach architecture. We show results for all baselines, all datasets and all setups in Table 11.\nFrom this table, we see that our features clearly surpass the best SSL and WSL features available. It\nis interesting to see that iBOT features extracted from a ViT-L outperform the ones of OpenCLIP with\na ViT-G. This observation supports the intuition that caption-based feature learning fails to learn subtle\npatterns like this one. Also, our model, with the DPT decoder and frozen backbone, matches or exceeds\nthe performance of the recent work of Li et al. (2022b). Finally, the out-of-domain generalization result on\nSUN-RGBd shows that our features allow very good transfer between domains. A depth prediction module\ntrained on indoor scenes from NYUd generalizes pretty well to the outdoor examples of SUN-RGBd.\n7.5 Qualitative Results\nIn this nal section of the empirical evaluation of our features, we propose a few qualitative analyses.\nSemantic Segmentation and Depth Estimation. We show some qualitative results for our dense\nprediction evaluations: segmentation on ADE20K in Fig. 7 and depth estimation on NYUd, KITTI and\nSUN RGB-D in Fig. 7. We compare DINOv2 with OpenCLIP with a linear classier on each dataset. While\nnot perfect, the linear segmentation model using our DINOv2 backbone produces good results and behaves\nmuch better than the OpenCLIP one under this evaluation setup. Indeed, the segmentation mask produced\nby OpenCLIP-G shows many artifacts and disconnected components. The qualitative results on depth\nestimation clearly illustrate the quantitative gap between OpenCLIP and DINOv2. These results highlight\nthat our features, as well as the features extracted from OpenCLIP, are able to linearly separate complex\ninformation such as depth, even though neither was trained with this type of information. However, our\nfeatures lead to a much smoother depth estimation, with less artifacts. Some objects such as the chair on\nthe SUN RGB-D image are completely ignored by OpenCLIP and correctly positioned using our features.\n15', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 7: Segmentation and depth estimation with linear classiers. Examples from ADE20K,\nNYUd, SUN RGB-D and KITTI with a linear probe on frozen OpenCLIP-G and DINOv2-g features.\nFigure 8:Examples of out-of-distribution examples with frozen DINOv2-g features and a linear probe.\nOut-of-distribution generalization. We show a few examples of applying the depth prediction and\nsegmentation linear classiers to out-of-distribution examples in Fig. 8. The qualitative results support our\nclaim that our features transfer between domains. The quality of the depth and segmentation predicted for\npictures of animals, or paintings is very good, even though the domains are very dierent.\nPCA of patch features. We show the results of the principal component analysis (PCA) performed on\nthe patch features extracted by our model. We keep only patches with a positive value after we threshold\nthe rst component. This procedure turns out to separate the images main object from the background. We\ncompute a second PCA on the remaining patches across three images depicting the same category. We color\nthe three rst components with three dierent colors and present the results in Fig. 1 and 9. There are two\ninteresting observations: rst, our unsupervised foreground / background detector, based on detecting the\n16', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 9:Morevisualization oftherst PCAcomponents. We compute the PCA between the patches\nfrom all of the images and show their rst 3 components. Each component corresponds to a specic color\nchannel. Same parts are matched between related images depsite changes of pose, style or even objects.\nBackground is removed by removing patches with a negative score of the rst PCA component.\nhighest variance direction, performs very well and is capable of delineating the boundary of the main object\nin the picture. Second, the other components correspond to "parts" of objects and match well for images of\nthe same category. This is an emerging property  our model was not trained to parse parts of objects.\nPatch matching. Finally, we explore what type of information our patch-level features contain by match-\ning them across images. We start by detecting the foreground object using the procedure described above.\nThen, we compute the euclidean distance between patch features extracted from two images and map them\nby solving an assignment problem. In order to reduce the number of matches, we then apply a non-maximum\nsuppression to keep only the salient ones. In Fig. 10, we show some examples of such matchings.\nWe observe that the features seem to capture information about semantic regions that serve similar purpose\nin dierent objects or animals. For instance, the wing of a plane matches the wing of a bird. We also observe\nthat the model is robust to style (image versus drawing), and to large variation of poses (see the elephant).\n8 Fairness and Bias Analysis\nWe conduct two fairness evaluations of our models. We probe for geographical fairness and potential harmful\nlabel associations. For both evaluations, we experiment with our largest ViT-g model.\n8.1 Geographical Fairness\nWe evaluate geographical fairness on the Dollar Street dataset introduced in De Vries et al. (2019) using\nthe evaluation protocol of Goyal et al. (2022b). This benchmark compares performance across countries and\nincome levels. It contains 16,073 images from 289 households across 54 countries. The task is to recognize\n94 concepts that vary visually between households based on income or location. In Table 12, we compare\nour model with SEERv2 (Goyal et al., 2022a), a model trained on a geographically diverse set of images.\nOur model is slightly fairer across regions and incomes than the SEERv2 model and signicantly better than\n17', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 10: Matching across images. We match patch-level features between images from dierent do-\nmains, poses and even objects that share similar semantic information. This exhibits the ability of our model\nto transfer across domains and understand relations between similar parts of dierent objects.\nIncome buckets Regions\nMethod Arch. Data low medium high Africa Asia Americas Europe\nSEERv2 RG-10B IG-1B 59.7 78.5 86.6 65.9 76.3 81.1 85.6\nDINOv2 ViT-g/14 LVD-142M 67.4 83.3 90.5 74.0 81.6 86.2 89.7\nTable 12: Geographical fairness and diversity analysis across income buckets and regions.\nthe supervised baseline reported by Goyal et al. (2022a). However, we still observe a signicant dierence\nbetween regions, particularly in Africa, where our model performance drops by 25.7% compared to Europe.\nThisshowthatourmodelisstillbiasedtowardWesterncountries. Similarly, ourmodelperformssignicantly\nbetter on high-income households than low-income ones, with a dierence of 31.7%. Despite improvements,\nwe observe signicant biases in our models toward wealthy households from Western countries.\n8.2 Gender, Skintones and Age\nIn a second set of evaluations, we question how our model classies images of people of dierent gender, skin\ntone, and age (all self-reported). We follow the protocol of Goyal et al. (2022b), where we train a multiclass\nclassier on a subset of 619 classes of ImageNet-22k. We group the 619 classes into four broader categories:\nHuman, Possibly Human, Non-Human, or Crime. Non-Human and Crime are considered harmful. Using\nthis classier, we run inference on 2955 images from the Casual Conversations dataset (Hazirbas et al., 2021)\nand keep all labels in the top-5 that are assigned a probability of 0.1 or more. Because of that, we can assign\n18', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Gender Skintone Age Groups\nModel Assoc.female\ndarkerfemale\nlightermale\ndarkermale\nlighter18-30 30-45 45-70 70+\nSEER Non-Human 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nRG-10B Crime 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nHuman 94.9 95.8 86.6 79.0 90.5 88.3 91.9 82.3\nPossibly-Human 13.6 6.7 65.0 60.2 32.8 37.2 29.4 6.5\nDINOv2 Non-Human 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nViT-g/14 Crime 0.0 0.0 0.2 0.0 0.0 0.1 0.0 0.0\nHuman 97.3 97.7 86.1 84.0 91.2 90.2 93.2 88.7\nPossibly-Human 15.8 17.2 52.2 48.1 35.3 37.3 23.0 9.7\nTable 13: Label association fairness evaluation across gender, skintones and age groups. We\nfollow the protocol proposed by Goyal et al. (2022b) with a slight modication. Instead of netuning the\nbackbone, we simply learn a linear classier on the subset of 619 classes of ImageNet-22k.\nModel toGPU TypeGPU PowerGPU-hours PUETotal power Carbon emitted\nReproduce consumption consumption (tCO 2eq)\nDINOv2-g A100-40GB 400W 22,016 1.1 9.7 MWh 3.7\nTable 14: Carbon footprint of reproducing DINOv2. We report the potential carbon emission of\nreproducing DINOv2-g when assuming a power consumption for the A100-40GB of 400W, a PUE of 1.1 and\ncarbon intensity factor of 0.385 kg CO 2e per KWh.\nmultiple classes to each image. We make one modication to the original evaluation protocol: we do not\nbackpropagate gradients to the backbone and keep it frozen. We compare our model to SEERv2 in Table 13.\nOur model often classies images of all groups as Human without large deviations across skin tones. Neither\nSEERv2 nor DINOv2 predict harmful labels from the Non-Human or Crime meta-categories (except for two\ninstances where the background contains bars visually similar to prison bars). We see that our model triggers\nthe Possibly-Human classes often. This class is constructed from objects in ImageNet-22k that are often\nrelated to Humans, such as Scarf, Glasses, or Beard. Our model often predicts the Possibly-Human class\nfor men because of the prevalence of the Beard class. No clear pattern indicates a bias against a particular\ngroup in this study. While this is encouraging, we also acknowledge that a more thorough evaluation of\nbiases may reveal aws in our model.\n9 Estimating the Environmental Impact of Training our Models\nTraining foundation models consumes a signicant amount of energy, resulting in carbon dioxide emissions.\nPatterson et al. (2021) propose a methodology to report an estimation of the carbon emitted during the\ntraining of a model based on the specics of the data center and its power grid. This computation informs\nthe design of the data center used for the training of models and the choice of location for data centers.\nThis methodology requires to know the specics of the data center used for training, which can be complex\nwhen multiple data centers are involved over time. Additionally, these specics are most often not in the\ncontrol of the AI practitioner, and hence, this methodology is less helpful when practioners make technical\ndecisions about future trainings. Instead, in this section, we follow an alternative that reports the potential\ncarbon emission of retraining a similar model in an average data center located in the US. This methodology\nwas used in previous work in natural language processing (Strubell et al., 2019; Touvron et al., 2023) to\nestablish an apple-to-apple comparison between pretraining schemes. More precisely, we x the value of all\nexogenous variables, i.e., the Power Usage Eectiveness (PUE) and carbon intensity factor of a power grid\nto the same values as in Touvron et al. (2023), that is, a PUE of 1.1 and the carbon intensity factor to the\n19', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'US average of 0.385 kg CO 2eq/KWh. We use the same formula as in Patterson et al. (2021) to estimate the\npotential energy consumption and the carbon emission. For the power consumption of an A100-80GB, we\ntake the thermal design power for NVLink systems, which is 400W. We report the potential carbon emission\nof retraining a DINOv2 ViT-g in Table 14. For comparison, retraining an OpenCLIP ViT-L or OpenCLIP\nViT-G would require 22.4 MWh and 118.9 MWh, respectively, if run in the same data center. This is 10 \nmore carbon emission. Note that this comparison is not fair to them, since they also train a text encoder in\nparallel, and we thus do not report them in the table. However, it gives a reasonable guidelines for those who\nare interested in training only visual features: in this context, training a self-supervised model is preferable\nin terms of carbon emission. Training a text-guided model still makes sense when planning to reuse the text\nencoder.\nCarbon footprint of the whole project. Additionally, we estimate the footprint of the whole project\nto be between 0.5k and 1k tCO 2eq using the same grid as presented above. This carbon footprint represents\nin the order of 200k GPU-days. The primary sources of emissions are the self-supervised pre-trainings of\nthe models. For example, a single pre-training of a ViT-g model (22k GPU-hours) emits 3.7 tons of CO 2eq,\nwhile a netuning on ImageNet-1k (1k GPU-hours) emits 0.2 tons. This estimate only considers the GPUs\nelectricity consumption and ignores other emissions, such as their manufacturing and disposal.\n10 Future work and Discussion\nIn this work, we present DINOv2, a new series of image encoders pretrained on large curated data with no\nsupervision. This is the rst SSL work on image data that leads to visual features that close the performance\ngap with (weakly) supervised alternatives across a wide range of benchmarks and without the need for\nnetuning. A few properties emerge from these models, such as an understanding of object parts and scene\ngeometry regardless of the image domains. We expect that more of these properties will emerge at larger\nscales of models and data, akin to instruction emergence in large language models, and plan to continue\nscaling along these axes. This paper also demonstrates that these visual features are compatible with\nclassiers as simple as linear layers - meaning the underlying information is readily available . In future work,\nwe plan to leverage this ability to train a a language-enabled AI system that can process visual features as\nif they were word tokens, and extract the required information to ground the system.\nAcknowledgments.\nWe thank Mathilde Caron for initial discussions that led to this work. We thank Olivia Joulin for the horse\ndrawing used in Fig. 10. We also thank the rest of FAIR and Meta AI for feedback on this work through\nthe entire project.\nReferences\nYuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous clustering\nand representation learning. In ICLR, 2020.\nMahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand\nJoulin, Michael Rabbat, and Nicolas Ballas. Masked siamese networks for label-ecient learning. arXiv\npreprint arXiv:2204.07141 , 2022.\nMahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann\nLeCun, and Nicolas Ballas. Self-supervised learning from images with a joint-embedding predictive archi-\ntecture.arXiv preprint arXiv:2301.08243 , 2023.\nAlexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. Data2vec: A general\nframework for self-supervised learning in speech, vision and language. arXiv preprint arXiv:2202.03555 ,\n2022.\nHangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint\narXiv:2106.08254 , 2021.\n20', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}]: %s
2023-12-07 10:17:41,687 - INFO - Check the data that is being passed [{'page_content': 'Jan Beirlant, Edward J Dudewicz, Lszl Gyr, Edward C Van der Meulen, et al. Nonparametric entropy\nestimation: An overview. International Journal of Mathematical and Statistical Sciences , 6(1):1739, 1997.\nMaxim Berman, Herv Jgou, Vedaldi Andrea, Iasonas Kokkinos, and Matthijs Douze. MultiGrain: a unied\nimage embedding for classes and instances. arXiv preprint arXiv:1902.05509 , 2019.\nLucas Beyer, Olivier J Hna, Alexander Kolesnikov, Xiaohua Zhai, and Aron van den Oord. Are we done\nwith imagenet? arXiv preprint arXiv:2006.07159 , 2020.\nShariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. AdaBins: Depth estimation using adaptive\nbins. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . IEEE, jun\n2021. doi: 10.1109/cvpr46437.2021.00400. URL https://doi.org/10.1109%2Fcvpr46437.2021.00400 .\nPiotr Bojanowski and Armand Joulin. Unsupervised learning by predicting noise. In ICML, 2017.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of\nfoundation models. arXiv preprint arXiv:2108.07258 , 2021.\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101  mining discriminative components\nwith random forests. In European Conference on Computer Vision , 2014.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\npreprint arXiv:2005.14165 , 2020.\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised\nlearning of visual features. In ECCV, 2018.\nMathilde Caron, Piotr Bojanowski, Julien Mairal, and Armand Joulin. Unsupervised pre-training of image\nfeatures on non-curated data. In ICCV, 2019.\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsuper-\nvised learning of visual features by contrasting cluster assignments. In NeurIPS , 2020.\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv Jgou, Julien Mairal, Piotr Bojanowski, and Armand\nJoulin. Emerging properties in self-supervised vision transformers. arXiv preprint arXiv:2104.14294 , 2021.\nLiang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schro, and Hartwig Adam. Encoder-decoder\nwith atrous separable convolution for semantic image segmentation. In Proceedings of the European con-\nference on computer vision (ECCV) , pp. 801818, 2018.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Georey Hinton. A simple framework for contrastive\nlearning of visual representations. preprint arXiv:2002.05709 , 2020.\nXiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi\nDong, Thang Luong, Cho-Jui Hsieh, et al. Symbolic discovery of optimization algorithms. arXiv preprint\narXiv:2302.06675 , 2023.\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. preprint arXiv:2011.10566 ,\n2020.\nXinleiChen, SainingXie, andKaimingHe. Anempiricalstudyoftrainingself-supervisedvisiontransformers.\narXiv preprint arXiv:2104.02057 , 2021.\nZhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer\nadapter for dense predictions. arXiv preprint arXiv:2205.08534 , 2022.\nBowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention\nmask transformer for universal image segmentation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pp. 12901299, 2022.\n21', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language\nmodeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.\nM. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In\nProceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , 2014.\nMarius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,\nUwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understand-\ning. InProceedings of the IEEE conference on computer vision and pattern recognition , pp. 32133223,\n2016.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R. Flashattention: Fast and memory-\necient exact attention with io-awareness. arXiv preprint arXiv:2205.14135 , 2022.\nTerrance De Vries, Ishan Misra, Changhan Wang, and Laurens Van der Maaten. Does object recognition\nworkforeveryone? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition\nworkshops , pp. 5259, 2019.\nSylvain Delattre and Nicolas Fournier. On the kozachenkoleonenko entropy estimator. Journal of Statistical\nPlanning and Inference , 185:6993, 2017.\nJacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert: Pre-trainingofdeepbidirectional\ntransformers for language understanding. preprint arXiv:1810.04805 , 2018.\nJosip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander Kolesnikov,\nJoan Puigcerver, Matthias Minderer, Alexander DAmour, Dan Moldovan, et al. On robustness and\ntransferabilityofconvolutionalneuralnetworks. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pp. 1645816468, 2021.\nCarl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context\nprediction. In ICCV, 2015.\nAlexey Dosovitskiy, Jost Tobias Springenberg, Martin A. Riedmiller, and Thomas Brox. Discriminative\nunsupervised feature learning with convolutional neural networks. CoRR, abs/1406.6909, 2014. URL\nhttp://arxiv.org/abs/1406.6909 .\nAlexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Dis-\ncriminative unsupervised feature learning with exemplar convolutional neural networks. TPAMI, 2016.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-\nterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. preprint arXiv:2010.11929 , 2020.\nMatthijs Douze, Herv Jgou, Harsimrat Sandhawalia, Laurent Amsaleg, and Cordelia Schmid. Evaluation\nof gist descriptors for web-scale image search. In CIVR, 2009.\nQuentin Duval, Ishan Misra, and Nicolas Ballas. A simple recipe for competitive low-compute self supervised\nvision models. arXiv preprint arXiv:2301.09451 , 2023.\nAlaaeldin El-Nouby, Gautier Izacard, Hugo Touvron, Ivan Laptev, Herv Jegou, and Edouard Grave. Are\nlarge-scale datasets necessary for self-supervised pre-training? arXiv preprint arXiv:2112.10740 , 2021.\nM. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal\nvisual object classes challenge: A retrospective. International Journal of Computer Vision , 111(1):98136,\nJanuary 2015.\nYuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang,\nand Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. 2023.\n22', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples:\nAn incremental bayesian approach tested on 101 object categories. In 2004 conference on computer vision\nand pattern recognition workshop , pp. 178178. IEEE, 2004.\nAndreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti\ndataset. The International Journal of Robotics Research , 32(11):12311237, 2013.\nSpyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting\nimage rotations, 2018.\nRohit Girdhar, Alaaeldin El-Nouby, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Is-\nhan Misra. Omnimae: Single model masked pretraining on images and videos. arXiv preprint\narXiv:2206.08356 , 2022.\nPriya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. Scaling and benchmarking self-supervised\nvisual representation learning. In ICCV, 2019.\nPriya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat Singh,\nVitaliy Liptchinsky, Ishan Misra, Armand Joulin, et al. Self-supervised pretraining of visual features in\nthe wild. preprint arXiv:2103.01988 , 2021.\nPriya Goyal, Quentin Duval, Isaac Seessel, Mathilde Caron, Mannat Singh, Ishan Misra, Levent Sagun,\nArmand Joulin, and Piotr Bojanowski. Vision models are more robust and fair when pretrained on\nuncurated images without supervision. arXiv preprint arXiv:2202.08360 , 2022a.\nPriya Goyal, Adriana Romero Soriano, Caner Hazirbas, Levent Sagun, and Nicolas Usunier. Fairness in-\ndicators for systematic assessments of visual feature extractors. In 2022 ACM Conference on Fairness,\nAccountability, and Transparency , pp. 7088, 2022b.\nRaghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, He-\nuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The" something\nsomething" video database for learning and evaluating visual common sense. In Proceedings of the IEEE\ninternational conference on computer vision , pp. 58425850, 2017.\nJean-Bastien Grill, Florian Strub, Florent Altch, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya,\nCarl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray\nKavukcuoglu, Rmi Munos, and Michal Valko. Bootstrap your own latent: A new approach to self-\nsupervised learning. In NeurIPS , 2020.\nRaia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping.\nInCVPR, 2006.\nCanerHazirbas, JoannaBitton, BrianDolhansky, JacquelinePan, AlbertGordo, andCristianCantonFerrer.\nTowards measuring fairness in ai: the casual conversations dataset. IEEE Transactions on Biometrics,\nBehavior, and Identity Science , 4(3):324332, 2021.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised\nvisual representation learning. In CVPR, 2020.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollr, and Ross Girshick. Masked autoencoders\nare scalable vision learners. arXiv preprint arXiv:2111.06377 , 2021.\nOlivierJHna, AravindSrinivas, JereyDeFauw, AliRazavi, CarlDoersch, SMEslami, andAaronvanden\nOord. Data-ecientimagerecognitionwithcontrastivepredictivecoding. preprint arXiv:1905.09272 ,2019.\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions\nand perturbations. In International Conference on Learning Representations , 2019.\n23', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai,\nTyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-\nof-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pp. 83408349, 2021.\nGeorey Hinton, Oriol Vinyals, and Je Dean. Distilling the knowledge in a neural network. preprint\narXiv:1503.02531 , 2015.\nJordan Homann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal\nlarge language models. arXiv preprint arXiv:2203.15556 , 2022.\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochas-\ntic depth. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands,\nOctober 1114, 2016, Proceedings, Part IV 14 , pp. 646661. Springer, 2016.\nGabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal\nDave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig\nSchmidt. Openclip. 2021.\nHerve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. IEEE\ntransactions on pattern analysis and machine intelligence , 33(1), 2010.\nJe Johnson, Matthijs Douze, and Herv Jgou. Billion-scale similarity search with GPUs. IEEE Transac-\ntions on Big Data , 7(3):535547, 2019.\nArmand Joulin, Laurens Van Der Maaten, Allan Jabri, and Nicolas Vasilache. Learning visual features from\nlarge weakly supervised data. In ECCV, 2016.\nWill Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio\nViola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv\npreprint arXiv:1705.06950 , 2017.\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for ne-grained catego-\nrization. In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13) , Sydney,\nAustralia, 2013.\nAlex Krizhevsky, Georey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\nBenjaminLefaudeux, FranciscoMassa, DianaLiskovich, WenhanXiong, VittorioCaggiano, SeanNaren, Min\nXu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza. xformers: A modular\nand hackable transformer modelling library. https://github.com/facebookresearch/xformers , 2022.\nChunyuan Li, Jianwei Yang, Pengchuan Zhang, Mei Gao, Bin Xiao, Xiyang Dai, Lu Yuan, and Jianfeng\nGao. Ecient self-supervised vision transformers for representation learning. In ICLR, 2022a.\nZhenyu Li, Xuyang Wang, Xianming Liu, and Junjun Jiang. Binsformer: Revisiting adaptive bins for\nmonocular depth estimation. arXiv preprint arXiv:2204.00987 , 2022b.\nHuajun Liu, Fuqiang Liu, Xinyi Fan, and Dong Huang. Polarized self-attention: towards high-quality pixel-\nwise regression. arXiv preprint arXiv:2107.00782 , 2021.\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin\nBharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In ECCV,\n2018.\nS. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classication of aircraft.\nTechnical report, 2013.\n24', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. In\nCVPR, 2020.\nMaria-Elena Nilsback and Andrew Zisserman. Automated ower classication over a large number of classes.\nInIndian Conference on Computer Vision, Graphics and Image Processing , Dec 2008.\nMehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles.\nIn Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (eds.), Computer Vision  ECCV 2016 , pp.\n6984, Cham, 2016. Springer International Publishing. ISBN 978-3-319-46466-4.\nOmkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In IEEE\nConference on Computer Vision and Pattern Recognition , 2012.\nDeepak Pathak, Philipp Krhenbhl, Je Donahue, Trevor Darrell, and Alexei Efros. Context encoders:\nFeature learning by inpainting. In CVPR, 2016.\nDavid Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David\nSo, Maud Texier, and Je Dean. Carbon emissions and large neural network training. arXiv preprint\narXiv:2104.10350 , 2021.\nEd Pizzi, Sreya Dutta Roy, Sugosh Nagavara Ravindra, Priya Goyal, and Matthijs Douze. A self-supervised\ndescriptor for image copy detection. arXiv preprint arXiv:2202.10261 , 2022.\nFilip Radenovi, Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondej Chum. Revisiting oxford and\nparis: Large-scale image retrieval benchmarking. In CVPR, 2018a.\nFilip Radenovi, Giorgos Tolias, and Ondej Chum. Fine-tuning cnn image retrieval with no human anno-\ntation.IEEE transactions on pattern analysis and machine intelligence , 2018b.\nAlec Radford, Jerey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models\nare unsupervised multitask learners.\nAlec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to generate reviews and discovering sentiment.\narXiv preprint arXiv:1704.01444 , 2017.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In International Conference on Machine Learning , pp. 87488763. PMLR,\n2021.\nColin Rael, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unied text-to-text transformer.\nJ. Mach. Learn. Res. , 21(140):167, 2020.\nRen Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Pro-\nceedings of the IEEE/CVF International Conference on Computer Vision , pp. 1217912188, 2021.\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classiers generalize\nto imagenet? In International Conference on Machine Learning , pp. 53895400. PMLR, 2019.\nJerome Revaud, Jon Almazn, Rafael S Rezende, and Cesar Roberto de Souza. Learning with average\nprecision: Training image retrieval with a listwise loss. In ICCV, 2019.\nYangjun Ruan, Saurabh Singh, Warren Morningstar, Alexander A Alemi, Sergey Ioe, Ian Fischer, and\nJoshua V Dillon. Weighted ensemble self-supervised learning. arXiv preprint arXiv:2211.09981 , 2022.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej\nKarpathy, Aditya Khosla, Michael Bernstein, Alexander C Berg, and Li Fei-Fei. Imagenet large scale\nvisual recognition challenge. IJCV, 2015.\n25', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, and Herv Jgou. Spreading vectors for similarity\nsearch.arXiv preprint arXiv:1806.03198 , 2018.\nNoam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202 , 2020.\nNathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support\ninference from rgbd images. In European conference on computer vision , pp. 746760. Springer, 2012.\nMannat Singh, Laura Gustafson, Aaron Adcock, Vinicius de Freitas Reis, Bugra Gedik, Raj Prateek\nKosaraju, Dhruv Mahajan, Ross Girshick, Piotr Dollr, and Laurens van der Maaten. Revisiting Weakly\nSupervised Pre-Training of Visual Perception Models. In CVPR, 2022.\nShuranSong, SamuelPLichtenberg, andJianxiongXiao. Sunrgb-d: Argb-dsceneunderstandingbenchmark\nsuite. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 567576,\n2015.\nKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes\nfrom videos in the wild. arXiv preprint arXiv:1212.0402 , 2012.\nAndreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer.\nHow to train your vit? data, augmentation, and regularization in vision transformers. arXiv preprint\narXiv:2106.10270 , 2021.\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning\nin nlp.arXiv preprint arXiv:1906.02243 , 2019.\nYonglong Tian, Olivier J Hena, and Aron van den Oord. Divide and contrast: Self-supervised learning\nfrom uncurated data. In Proceedings of the IEEE/CVF International Conference on Computer Vision ,\npp. 1006310074, 2021.\nGiorgos Tolias, Ronan Sicre, and Herv Jgou. Particular object retrieval with integral max-pooling of cnn\nactivations. arXiv preprint arXiv:1511.05879 , 2015.\nZhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-ecient\nlearners for self-supervised video pre-training. arXiv preprint arXiv:2203.12602 , 2022.\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv Jgou. Fixing the train-test resolution discrep-\nancy. In NeurIPS , 2019.\nHugo Touvron, Matthieu Cord, and Herv Jgou. Deit iii: Revenge of the vit. arXiv preprint\narXiv:2204.07118 , 2022.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix,\nBaptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. Llama: Open and ecient foundation language models. arXiv preprint\narXiv:2302.13971 , 2023.\nGrant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro\nPerona, and Serge Belongie. The inaturalist species classication and detection dataset. In CVPR, 2018.\nGrant Van Horn, Elijah Cole, Sara Beery, Kimberly Wilber, Serge Belongie, and Oisin Mac Aodha. Bench-\nmarking representation learning for natural world image collections. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition , pp. 1288412893, 2021.\nWenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei\nLu, Hongsheng Li, et al. Internimage: Exploring large-scale vision foundation models with deformable\nconvolutions. arXiv preprint arXiv:2211.05778 , 2022.\nXiaolong Wang, Allan Jabri, and Alexei A Efros. Learning correspondence from the cycle-consistency of\ntime. In CVPR, 2019.\n26', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Philippe Weinzaepfel, Thomas Lucas, Diane Larlus, and Yannis Kalantidis. Learning super-features for\nimage retrieval. In International Conference on Learning Representations , 2021.\nP. Welinder, S. Branson, T. Mita, C. Wah, F. Schro, S. Belongie, and P. Perona. Caltech-UCSD Birds 200.\nTechnical Report CNS-TR-2010-001, California Institute of Technology, 2010.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmn, Armand\nJoulin, and Edouard Grave. Ccnet: Extracting high quality monolingual datasets from web crawl data.\narXiv preprint arXiv:1911.00359 , 2019.\nZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric\ninstance discrimination. In CVPR, 2018.\nJ. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recognition from\nabbey to zoo. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition ,\npp. 34853492, June 2010. doi: 10.1109/CVPR.2010.5539970.\nHuXu,JunchengLi,AlexeiBaevski,MichaelAuli,WojciechGaluba,FlorianMetze,ChristophFeichtenhofer,\net al. Masked autoencoders that listen. arXiv preprint arXiv:2207.06405 , 2022.\nI Zeki Yalniz, Herv Jgou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. Billion-scale semi-supervised\nlearning for image classication. arXiv preprint arXiv:1905.00546 , 2019.\nBurak Yildiz, Seyran Khademi, Ronald Maria Siebes, and Jan van Gemert. Amstertime: A visual place\nrecognition benchmark dataset for severe domain shift. arXiv preprint arXiv:2203.16291 , 2022.\nNikolaos-Antonios Ypsilantis, Noa Garcia, Guangxing Han, Sarah Ibrahimi, Nanne Van Noord, and Giorgos\nTolias. The met dataset: Instance-level recognition for artworks. In Thirty-fth Conference on Neural\nInformation Processing Systems Datasets and Benchmarks Track (Round 2) , 2021.\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 1210412113,\n2022.\nRichard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016.\nBolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep features\nfor scene recognition using places database. In NeurIPS , 2014.\nBolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing\nthroughade20kdataset. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npp. 633641, 2017.\nJinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert\npre-training with online tokenizer. arXiv preprint arXiv:2111.07832 , 2021.\nPan Zhou, Yichen Zhou, Chenyang Si, Weihao Yu, Teck Khim Ng, and Shuicheng Yan. Mugs: A multi-\ngranular self-supervised learning framework. arXiv preprint arXiv:2203.14415 , 2022.\n27', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'A Data Processing\nA.1 Data selection\nOur selection of datasets for building LVD-142M is detailed in Tab. 15. This collection is intended to provide\nimages covering well various downstream vision tasks both for image-level and dense recognition.\nA.2 Image similarity\nWe employ cosine similarity to compare image features (whether ours or feature generated for deduplication)\nwith the following similarity function m:\nm(s, r) =cosine-similarity (f(s), f(r)) =f(s).f(r)\nf(s)2f(r)2\nwhere sandrare a pair of images to compare and fis the model generating features.\nA.3 Deduplication\nSelf-deduplication. To deduplicate our uncurated data source of 1.3B images, we compute and use the\nembeddings generated by Pizzi et al. (2022) and retrieve the k= 64nearest neighbors of each image (using\ncosine similarity). Considering only neighbors with a similarity >0.6, we extract the connected components\nof the associated k-NN graph thanks to a scalable disjoint set data structure implementation. We then only\nkeep one representative for each component of duplicate images. This results in a self-deduplicated data\nsource of 1.1B images.\nRelativededuplication Toreduceredundancyandalsoproperlyevaluatetheperformanceofourfeatures,\nwe discard remaining images of our self-deduplicated data source that are too similar to train and test splits\nof our evaluation datasets. To achieve this, we apply a similar procedure as for self-deduplication, with a\nstricter similarity >0.45, this time identifying the duplicate components (if any) to which each reference\nimage belong and discarding it entirely. This results in a self- and relatively-deduplicated data source of\n744M images.\nA.4 Retrieval\nWe employ two approaches to augment dataset via retrieval: sample-based and cluster-based. The rst one,\nsample-based, applies to datasets larger than 1M images and consists in collecting a xed number kof nearest\nimages for each sample image of the dataset to retrieve, eectively trying to multiply by kthe size of the\ndataset. We use k= 4for Google Landmarks v2 and ImageNet-22k but a larger k= 32to make this specic\nretrieval a core part of our LVD-142M dataset. For smaller datasets, the second approach, cluster-based,\nconsists in rst clustering our uncurated data source into 100,000separate clusters thanks to a distributed\nk-means implementation. Each cluster should capture dierent types of image concept and contents. We\nthen pick 10,000images from each cluster associated with more than 3images of the retrieved dataset. As\nthis can result in a very large number of retrieved images for some dataset, we restrict such retrievals to a\nmaximum of 1M images to maintain the balance between the dierent datasets within LVD-142M.\nB Implementation Details\nB.1 Unsupervised pre-training\nFor unsupervised pre-training we build on the DINO and iBOT codebases. We use hyperparameters shown\nin Table 16, ViT architectures described in Table 17.\nKoLeo regularization. We apply the KoLeo regularizer with a weight of 0.1 between the class tokens of\nthe rst global crop, for all samples within a GPU without cross-communication for this step.\n28', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Task Dataset / Split Images Retrieval Retrieved Final\nclassication ImageNet-22k /  14,197,086 as is  14,197,086\nclassication ImageNet-22k /  14,197,086 sample 56,788,344 56,788,344\nclassication ImageNet-1k / train 1,281,167 sample 40,997,344 40,997,344\nne-grained classif. Caltech 101 / train 3,030 cluster 2,630,000 1,000,000\nne-grained classif. CUB-200-2011 / train 5,994 cluster 1,300,000 1,000,000\nne-grained classif. DTD / train1 1,880 cluster 1,580,000 1,000,000\nne-grained classif. FGVC-Aircraft / train 3,334 cluster 1,170,000 1,000,000\nne-grained classif. Flowers-102 / train 1,020 cluster 1,060,000 1,000,000\nne-grained classif. Food-101 / train 75,750 cluster 21,670,000 1,000,000\nne-grained classif. Oxford-IIIT Pet / trainval 3,680 cluster 2,750,000 1,000,000\nne-grained classif. Stanford Cars / train 8,144 cluster 7,220,000 1,000,000\nne-grained classif. SUN397 / train1 19,850 cluster 18,950,000 1,000,000\nne-grained classif. Pascal VOC 2007 / train 2,501 cluster 1,010,000 1,000,000\nsegmentation ADE20K / train 20,210 cluster 20,720,000 1,000,000\nsegmentation Cityscapes / train 2,975 cluster 1,390,000 1,000,000\nsegmentation Pascal VOC 2012 (seg.) / trainaug 1,464 cluster 10,140,000 1,000,000\ndepth estimation Mapillary SLS / train 1,434,262 as is  1,434,262\ndepth estimation KITTI / train (Eigen) 23,158 cluster 3,700,000 1,000,000\ndepth estimation NYU Depth V2 / train 24,231 cluster 10,850,000 1,000,000\ndepth estimation SUN RGB-D / train 4,829 cluster 4,870,000 1,000,000\nretrieval Google Landmarks v2 / train (clean) 1,580,470 as is  1,580,470\nretrieval Google Landmarks v2 / train (clean) 1,580,470 sample 6,321,880 6,321,880\nretrieval AmsterTime / new 1,231 cluster 960,000 960,000\nretrieval AmsterTime / old 1,231 cluster 830,000 830,000\nretrieval Met / train 397,121 cluster 62,860,000 1,000,000\nretrieval Revisiting Oxford / base 4,993 cluster 3,680,000 1,000,000\nretrieval Revisiting Paris / base 6,322 cluster 3,660,000 1,000,000\n142,109,386\nTable 15: Composition of our LVD-142M dataset. We report the list of datasets and associated splits\nused to build the dataset, how they were included (as is without retrieval or via sample-based or cluster-based\nretrieval). For retrievals, we indicate the actual number of retrieved images and the nal number included\nin the dataset.\n29', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Arch. Drop-rate LR Batch size\nDINOv2-S (distilled) ViT-S/14 0 1e-3 2048\nDINOv2-B (distilled) ViT-B/14 0 1e-3 2048\nDINOv2-L (distilled) ViT-L/14 0 1e-3 2048\nDINOv2-L (from scratch) ViT-L/14 0.4 3.5e-4 3072\nDINOv2-g (from scratch) ViT-g/14 0.4 3.5e-4 3072\nTable 16: Training hyperparameters for DINOv2-S, DINOv2-B, DINOv2-L and DINOv2-g. All\nmodels run for 625k iterations with optimizer AdamW, an initial LayerScale value of 1e-5, a weight decay\ncosine schedule from 0.04 to 0.2, a learning rate warmup of 100k iterations, a teacher momentum cosine\nschedule from 0.994 to 1, and we train in oat16 precision in all cases (except for the DINO heads where we\nreduce the gradients in oat32).\nArch. Embed dim Heads Blocks FFN layer\nViT-S/14 (distilled) 384 6 12 MLP\nViT-B/14 (distilled) 768 12 18 MLP\nViT-L/14 (distilled) 1024 16 24 MLP\nViT-L/14 (from scratch) 1024 16 24 SwiGLU\nViT-g/14 (from scratch) 1536 24 40 SwiGLU\nTable 17: Architecture details of the ViT-S/B/L/g networks used in this work. We use MLP\nfeed-forward networks for distilled models, and SwiGLU (Shazeer, 2020) when training from scratch.\nEMA update for the teacher. The teacher is initialized with the same state as the student, and is an\nexponential moving average of the student network, with a momentum value in [0.994, 1.0] following a cosine\nschedule. It is updated at the end of every training step.\nB.2 High-Resolution adaptation\nWe initialise the model with the pretrained weights then train it for 10k iterations with the same procedure\nas the original pretraining. All the schedules are kept the same as in the original training, but compressed\nto t in 10k iterations. All the hyperparameters are kept the same as in the rst pretraining, except the\nbase learning rate which is reduced.\nB.3 Linear probing evaluation\nFor linear probing we dene 3 evaluation parameters: the learning rate, how many output layers we use,\nwhether we concatenate the average-pooled patch token features with the class token (or use only the\nclass token). We train our linear layer with SGD for 12500 iterations, using random-resized-crop data\naugmentation, and perform the following grid search:\nlearning rate in{0.0001,0.0002,0.0005,0.001,0.002,0.005,0.01,0.02,0.05,0.1,0.2,0.3,0.5}\noutput layers in{1,4}\nconcatenate average-pooled tokens in {yes, no}\nWe then report the highest accuracy value obtained on the validation set as is common practice. Note that\nthis grid search is not expensive, because at each iteration we perform inference on the backbone only once,\nthen feed the output to all linear classiers (each performing a single matrix multiplication).\nC List of benchmarks used for evaluations\nWe show in Table 18 the list of benchmarks and datasets used for evaluation.\n30', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Dataset name Task Citation\nImageNet-1k Image Classication (Russakovsky et al., 2015)\nImageNet-V2 Image Classication (Recht et al., 2019)\nImageNet-ReaL Image Classication (Beyer et al., 2020)\nImageNet-A Image Classication (Djolonga et al., 2021)\nImageNet-C Image Classication (Hendrycks & Dietterich, 2019)\nImageNet-Rendition Image Classication (Hendrycks et al., 2021)\nImageNet-Sketch Image Classication (Wang et al., 2019)\nFood-101 Image Classication (Bossard et al., 2014)\nCIFAR-10 Image Classication (Krizhevsky et al., 2009)\nCIFAR-100 Image Classication (Krizhevsky et al., 2009)\nSUN397 Image Classication (Xiao et al., 2010)\nStanfordCars Image Classication (Krause et al., 2013)\nFGVC-Aircraft Image Classication (Maji et al., 2013)\nPascal VOC 2007 Image Classication (Everingham et al., 2015)\nDescribable Textures Image Classication (Cimpoi et al., 2014)\nOxford Pets Image Classication (Parkhi et al., 2012)\nCaltech101 Image Classication (Fei-Fei et al., 2004)\nOxford Flowers Image Classication (Nilsback & Zisserman, 2008)\nCUB200 Image Classication (Welinder et al., 2010)\niNaturalist 2018 Image Classication (Van Horn et al., 2018)\niNaturalist 2021 Image Classication (Van Horn et al., 2021)\nPlaces-205 Image Classication (Zhou et al., 2014)\nUCF101 Video Classication (Soomro et al., 2012)\nKinetics-400 Video Classication (Kay et al., 2017)\nSomething-Something-V2 Video Classication (Goyal et al., 2017)\nRevisiting-Paris Image Retrieval (Radenovi et al., 2018a)\nRevisiting-Oxford Image Retrieval (Radenovi et al., 2018a)\nMet Image Retrieval (Ypsilantis et al., 2021)\nAmstertime Image Retrieval (Yildiz et al., 2022)\nADE20k Image Segmentation (Zhou et al., 2017)\nCityscapes Image Segmentation (Cordts et al., 2016)\nPascal VOC 2012 Image Segmentation (Everingham et al., 2015)\nNYU-Depth V2 Monocular Depth Estimation (Silberman et al., 2012)\nKITTI Monocular Depth Estimation (Geiger et al., 2013)\nSUN-RGBD Monocular Depth Estimation (Song et al., 2015)\nDollarStreet Fairness Analysis (De Vries et al., 2019)\nCasual Conversations Fairness Analysis (Hazirbas et al., 2021)\nTable 18: List of datasets used for evaluation.\n31', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}]: %s
2023-12-07 10:17:41,687 - INFO - Check the results [{'page_content': 'Jan Beirlant, Edward J Dudewicz, Lszl Gyr, Edward C Van der Meulen, et al. Nonparametric entropy\nestimation: An overview. International Journal of Mathematical and Statistical Sciences , 6(1):1739, 1997.\nMaxim Berman, Herv Jgou, Vedaldi Andrea, Iasonas Kokkinos, and Matthijs Douze. MultiGrain: a unied\nimage embedding for classes and instances. arXiv preprint arXiv:1902.05509 , 2019.\nLucas Beyer, Olivier J Hna, Alexander Kolesnikov, Xiaohua Zhai, and Aron van den Oord. Are we done\nwith imagenet? arXiv preprint arXiv:2006.07159 , 2020.\nShariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. AdaBins: Depth estimation using adaptive\nbins. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . IEEE, jun\n2021. doi: 10.1109/cvpr46437.2021.00400. URL https://doi.org/10.1109%2Fcvpr46437.2021.00400 .\nPiotr Bojanowski and Armand Joulin. Unsupervised learning by predicting noise. In ICML, 2017.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of\nfoundation models. arXiv preprint arXiv:2108.07258 , 2021.\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101  mining discriminative components\nwith random forests. In European Conference on Computer Vision , 2014.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\npreprint arXiv:2005.14165 , 2020.\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised\nlearning of visual features. In ECCV, 2018.\nMathilde Caron, Piotr Bojanowski, Julien Mairal, and Armand Joulin. Unsupervised pre-training of image\nfeatures on non-curated data. In ICCV, 2019.\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsuper-\nvised learning of visual features by contrasting cluster assignments. In NeurIPS , 2020.\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv Jgou, Julien Mairal, Piotr Bojanowski, and Armand\nJoulin. Emerging properties in self-supervised vision transformers. arXiv preprint arXiv:2104.14294 , 2021.\nLiang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schro, and Hartwig Adam. Encoder-decoder\nwith atrous separable convolution for semantic image segmentation. In Proceedings of the European con-\nference on computer vision (ECCV) , pp. 801818, 2018.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Georey Hinton. A simple framework for contrastive\nlearning of visual representations. preprint arXiv:2002.05709 , 2020.\nXiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi\nDong, Thang Luong, Cho-Jui Hsieh, et al. Symbolic discovery of optimization algorithms. arXiv preprint\narXiv:2302.06675 , 2023.\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. preprint arXiv:2011.10566 ,\n2020.\nXinleiChen, SainingXie, andKaimingHe. Anempiricalstudyoftrainingself-supervisedvisiontransformers.\narXiv preprint arXiv:2104.02057 , 2021.\nZhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer\nadapter for dense predictions. arXiv preprint arXiv:2205.08534 , 2022.\nBowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention\nmask transformer for universal image segmentation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pp. 12901299, 2022.\n21', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language\nmodeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.\nM. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In\nProceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , 2014.\nMarius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,\nUwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understand-\ning. InProceedings of the IEEE conference on computer vision and pattern recognition , pp. 32133223,\n2016.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R. Flashattention: Fast and memory-\necient exact attention with io-awareness. arXiv preprint arXiv:2205.14135 , 2022.\nTerrance De Vries, Ishan Misra, Changhan Wang, and Laurens Van der Maaten. Does object recognition\nworkforeveryone? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition\nworkshops , pp. 5259, 2019.\nSylvain Delattre and Nicolas Fournier. On the kozachenkoleonenko entropy estimator. Journal of Statistical\nPlanning and Inference , 185:6993, 2017.\nJacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert: Pre-trainingofdeepbidirectional\ntransformers for language understanding. preprint arXiv:1810.04805 , 2018.\nJosip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander Kolesnikov,\nJoan Puigcerver, Matthias Minderer, Alexander DAmour, Dan Moldovan, et al. On robustness and\ntransferabilityofconvolutionalneuralnetworks. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pp. 1645816468, 2021.\nCarl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context\nprediction. In ICCV, 2015.\nAlexey Dosovitskiy, Jost Tobias Springenberg, Martin A. Riedmiller, and Thomas Brox. Discriminative\nunsupervised feature learning with convolutional neural networks. CoRR, abs/1406.6909, 2014. URL\nhttp://arxiv.org/abs/1406.6909 .\nAlexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Dis-\ncriminative unsupervised feature learning with exemplar convolutional neural networks. TPAMI, 2016.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-\nterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. preprint arXiv:2010.11929 , 2020.\nMatthijs Douze, Herv Jgou, Harsimrat Sandhawalia, Laurent Amsaleg, and Cordelia Schmid. Evaluation\nof gist descriptors for web-scale image search. In CIVR, 2009.\nQuentin Duval, Ishan Misra, and Nicolas Ballas. A simple recipe for competitive low-compute self supervised\nvision models. arXiv preprint arXiv:2301.09451 , 2023.\nAlaaeldin El-Nouby, Gautier Izacard, Hugo Touvron, Ivan Laptev, Herv Jegou, and Edouard Grave. Are\nlarge-scale datasets necessary for self-supervised pre-training? arXiv preprint arXiv:2112.10740 , 2021.\nM. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal\nvisual object classes challenge: A retrospective. International Journal of Computer Vision , 111(1):98136,\nJanuary 2015.\nYuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang,\nand Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. 2023.\n22', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples:\nAn incremental bayesian approach tested on 101 object categories. In 2004 conference on computer vision\nand pattern recognition workshop , pp. 178178. IEEE, 2004.\nAndreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti\ndataset. The International Journal of Robotics Research , 32(11):12311237, 2013.\nSpyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting\nimage rotations, 2018.\nRohit Girdhar, Alaaeldin El-Nouby, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Is-\nhan Misra. Omnimae: Single model masked pretraining on images and videos. arXiv preprint\narXiv:2206.08356 , 2022.\nPriya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. Scaling and benchmarking self-supervised\nvisual representation learning. In ICCV, 2019.\nPriya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat Singh,\nVitaliy Liptchinsky, Ishan Misra, Armand Joulin, et al. Self-supervised pretraining of visual features in\nthe wild. preprint arXiv:2103.01988 , 2021.\nPriya Goyal, Quentin Duval, Isaac Seessel, Mathilde Caron, Mannat Singh, Ishan Misra, Levent Sagun,\nArmand Joulin, and Piotr Bojanowski. Vision models are more robust and fair when pretrained on\nuncurated images without supervision. arXiv preprint arXiv:2202.08360 , 2022a.\nPriya Goyal, Adriana Romero Soriano, Caner Hazirbas, Levent Sagun, and Nicolas Usunier. Fairness in-\ndicators for systematic assessments of visual feature extractors. In 2022 ACM Conference on Fairness,\nAccountability, and Transparency , pp. 7088, 2022b.\nRaghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, He-\nuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The" something\nsomething" video database for learning and evaluating visual common sense. In Proceedings of the IEEE\ninternational conference on computer vision , pp. 58425850, 2017.\nJean-Bastien Grill, Florian Strub, Florent Altch, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya,\nCarl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray\nKavukcuoglu, Rmi Munos, and Michal Valko. Bootstrap your own latent: A new approach to self-\nsupervised learning. In NeurIPS , 2020.\nRaia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping.\nInCVPR, 2006.\nCanerHazirbas, JoannaBitton, BrianDolhansky, JacquelinePan, AlbertGordo, andCristianCantonFerrer.\nTowards measuring fairness in ai: the casual conversations dataset. IEEE Transactions on Biometrics,\nBehavior, and Identity Science , 4(3):324332, 2021.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised\nvisual representation learning. In CVPR, 2020.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollr, and Ross Girshick. Masked autoencoders\nare scalable vision learners. arXiv preprint arXiv:2111.06377 , 2021.\nOlivierJHna, AravindSrinivas, JereyDeFauw, AliRazavi, CarlDoersch, SMEslami, andAaronvanden\nOord. Data-ecientimagerecognitionwithcontrastivepredictivecoding. preprint arXiv:1905.09272 ,2019.\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions\nand perturbations. In International Conference on Learning Representations , 2019.\n23', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai,\nTyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-\nof-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pp. 83408349, 2021.\nGeorey Hinton, Oriol Vinyals, and Je Dean. Distilling the knowledge in a neural network. preprint\narXiv:1503.02531 , 2015.\nJordan Homann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal\nlarge language models. arXiv preprint arXiv:2203.15556 , 2022.\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochas-\ntic depth. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands,\nOctober 1114, 2016, Proceedings, Part IV 14 , pp. 646661. Springer, 2016.\nGabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal\nDave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig\nSchmidt. Openclip. 2021.\nHerve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. IEEE\ntransactions on pattern analysis and machine intelligence , 33(1), 2010.\nJe Johnson, Matthijs Douze, and Herv Jgou. Billion-scale similarity search with GPUs. IEEE Transac-\ntions on Big Data , 7(3):535547, 2019.\nArmand Joulin, Laurens Van Der Maaten, Allan Jabri, and Nicolas Vasilache. Learning visual features from\nlarge weakly supervised data. In ECCV, 2016.\nWill Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio\nViola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv\npreprint arXiv:1705.06950 , 2017.\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for ne-grained catego-\nrization. In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13) , Sydney,\nAustralia, 2013.\nAlex Krizhevsky, Georey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\nBenjaminLefaudeux, FranciscoMassa, DianaLiskovich, WenhanXiong, VittorioCaggiano, SeanNaren, Min\nXu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza. xformers: A modular\nand hackable transformer modelling library. https://github.com/facebookresearch/xformers , 2022.\nChunyuan Li, Jianwei Yang, Pengchuan Zhang, Mei Gao, Bin Xiao, Xiyang Dai, Lu Yuan, and Jianfeng\nGao. Ecient self-supervised vision transformers for representation learning. In ICLR, 2022a.\nZhenyu Li, Xuyang Wang, Xianming Liu, and Junjun Jiang. Binsformer: Revisiting adaptive bins for\nmonocular depth estimation. arXiv preprint arXiv:2204.00987 , 2022b.\nHuajun Liu, Fuqiang Liu, Xinyi Fan, and Dong Huang. Polarized self-attention: towards high-quality pixel-\nwise regression. arXiv preprint arXiv:2107.00782 , 2021.\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin\nBharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In ECCV,\n2018.\nS. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classication of aircraft.\nTechnical report, 2013.\n24', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. In\nCVPR, 2020.\nMaria-Elena Nilsback and Andrew Zisserman. Automated ower classication over a large number of classes.\nInIndian Conference on Computer Vision, Graphics and Image Processing , Dec 2008.\nMehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles.\nIn Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (eds.), Computer Vision  ECCV 2016 , pp.\n6984, Cham, 2016. Springer International Publishing. ISBN 978-3-319-46466-4.\nOmkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In IEEE\nConference on Computer Vision and Pattern Recognition , 2012.\nDeepak Pathak, Philipp Krhenbhl, Je Donahue, Trevor Darrell, and Alexei Efros. Context encoders:\nFeature learning by inpainting. In CVPR, 2016.\nDavid Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David\nSo, Maud Texier, and Je Dean. Carbon emissions and large neural network training. arXiv preprint\narXiv:2104.10350 , 2021.\nEd Pizzi, Sreya Dutta Roy, Sugosh Nagavara Ravindra, Priya Goyal, and Matthijs Douze. A self-supervised\ndescriptor for image copy detection. arXiv preprint arXiv:2202.10261 , 2022.\nFilip Radenovi, Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondej Chum. Revisiting oxford and\nparis: Large-scale image retrieval benchmarking. In CVPR, 2018a.\nFilip Radenovi, Giorgos Tolias, and Ondej Chum. Fine-tuning cnn image retrieval with no human anno-\ntation.IEEE transactions on pattern analysis and machine intelligence , 2018b.\nAlec Radford, Jerey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models\nare unsupervised multitask learners.\nAlec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to generate reviews and discovering sentiment.\narXiv preprint arXiv:1704.01444 , 2017.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In International Conference on Machine Learning , pp. 87488763. PMLR,\n2021.\nColin Rael, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unied text-to-text transformer.\nJ. Mach. Learn. Res. , 21(140):167, 2020.\nRen Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Pro-\nceedings of the IEEE/CVF International Conference on Computer Vision , pp. 1217912188, 2021.\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classiers generalize\nto imagenet? In International Conference on Machine Learning , pp. 53895400. PMLR, 2019.\nJerome Revaud, Jon Almazn, Rafael S Rezende, and Cesar Roberto de Souza. Learning with average\nprecision: Training image retrieval with a listwise loss. In ICCV, 2019.\nYangjun Ruan, Saurabh Singh, Warren Morningstar, Alexander A Alemi, Sergey Ioe, Ian Fischer, and\nJoshua V Dillon. Weighted ensemble self-supervised learning. arXiv preprint arXiv:2211.09981 , 2022.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej\nKarpathy, Aditya Khosla, Michael Bernstein, Alexander C Berg, and Li Fei-Fei. Imagenet large scale\nvisual recognition challenge. IJCV, 2015.\n25', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, and Herv Jgou. Spreading vectors for similarity\nsearch.arXiv preprint arXiv:1806.03198 , 2018.\nNoam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202 , 2020.\nNathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support\ninference from rgbd images. In European conference on computer vision , pp. 746760. Springer, 2012.\nMannat Singh, Laura Gustafson, Aaron Adcock, Vinicius de Freitas Reis, Bugra Gedik, Raj Prateek\nKosaraju, Dhruv Mahajan, Ross Girshick, Piotr Dollr, and Laurens van der Maaten. Revisiting Weakly\nSupervised Pre-Training of Visual Perception Models. In CVPR, 2022.\nShuranSong, SamuelPLichtenberg, andJianxiongXiao. Sunrgb-d: Argb-dsceneunderstandingbenchmark\nsuite. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 567576,\n2015.\nKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes\nfrom videos in the wild. arXiv preprint arXiv:1212.0402 , 2012.\nAndreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer.\nHow to train your vit? data, augmentation, and regularization in vision transformers. arXiv preprint\narXiv:2106.10270 , 2021.\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning\nin nlp.arXiv preprint arXiv:1906.02243 , 2019.\nYonglong Tian, Olivier J Hena, and Aron van den Oord. Divide and contrast: Self-supervised learning\nfrom uncurated data. In Proceedings of the IEEE/CVF International Conference on Computer Vision ,\npp. 1006310074, 2021.\nGiorgos Tolias, Ronan Sicre, and Herv Jgou. Particular object retrieval with integral max-pooling of cnn\nactivations. arXiv preprint arXiv:1511.05879 , 2015.\nZhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-ecient\nlearners for self-supervised video pre-training. arXiv preprint arXiv:2203.12602 , 2022.\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv Jgou. Fixing the train-test resolution discrep-\nancy. In NeurIPS , 2019.\nHugo Touvron, Matthieu Cord, and Herv Jgou. Deit iii: Revenge of the vit. arXiv preprint\narXiv:2204.07118 , 2022.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix,\nBaptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. Llama: Open and ecient foundation language models. arXiv preprint\narXiv:2302.13971 , 2023.\nGrant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro\nPerona, and Serge Belongie. The inaturalist species classication and detection dataset. In CVPR, 2018.\nGrant Van Horn, Elijah Cole, Sara Beery, Kimberly Wilber, Serge Belongie, and Oisin Mac Aodha. Bench-\nmarking representation learning for natural world image collections. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition , pp. 1288412893, 2021.\nWenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei\nLu, Hongsheng Li, et al. Internimage: Exploring large-scale vision foundation models with deformable\nconvolutions. arXiv preprint arXiv:2211.05778 , 2022.\nXiaolong Wang, Allan Jabri, and Alexei A Efros. Learning correspondence from the cycle-consistency of\ntime. In CVPR, 2019.\n26', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Philippe Weinzaepfel, Thomas Lucas, Diane Larlus, and Yannis Kalantidis. Learning super-features for\nimage retrieval. In International Conference on Learning Representations , 2021.\nP. Welinder, S. Branson, T. Mita, C. Wah, F. Schro, S. Belongie, and P. Perona. Caltech-UCSD Birds 200.\nTechnical Report CNS-TR-2010-001, California Institute of Technology, 2010.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmn, Armand\nJoulin, and Edouard Grave. Ccnet: Extracting high quality monolingual datasets from web crawl data.\narXiv preprint arXiv:1911.00359 , 2019.\nZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric\ninstance discrimination. In CVPR, 2018.\nJ. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recognition from\nabbey to zoo. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition ,\npp. 34853492, June 2010. doi: 10.1109/CVPR.2010.5539970.\nHuXu,JunchengLi,AlexeiBaevski,MichaelAuli,WojciechGaluba,FlorianMetze,ChristophFeichtenhofer,\net al. Masked autoencoders that listen. arXiv preprint arXiv:2207.06405 , 2022.\nI Zeki Yalniz, Herv Jgou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. Billion-scale semi-supervised\nlearning for image classication. arXiv preprint arXiv:1905.00546 , 2019.\nBurak Yildiz, Seyran Khademi, Ronald Maria Siebes, and Jan van Gemert. Amstertime: A visual place\nrecognition benchmark dataset for severe domain shift. arXiv preprint arXiv:2203.16291 , 2022.\nNikolaos-Antonios Ypsilantis, Noa Garcia, Guangxing Han, Sarah Ibrahimi, Nanne Van Noord, and Giorgos\nTolias. The met dataset: Instance-level recognition for artworks. In Thirty-fth Conference on Neural\nInformation Processing Systems Datasets and Benchmarks Track (Round 2) , 2021.\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 1210412113,\n2022.\nRichard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016.\nBolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep features\nfor scene recognition using places database. In NeurIPS , 2014.\nBolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing\nthroughade20kdataset. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npp. 633641, 2017.\nJinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert\npre-training with online tokenizer. arXiv preprint arXiv:2111.07832 , 2021.\nPan Zhou, Yichen Zhou, Chenyang Si, Weihao Yu, Teck Khim Ng, and Shuicheng Yan. Mugs: A multi-\ngranular self-supervised learning framework. arXiv preprint arXiv:2203.14415 , 2022.\n27', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'A Data Processing\nA.1 Data selection\nOur selection of datasets for building LVD-142M is detailed in Tab. 15. This collection is intended to provide\nimages covering well various downstream vision tasks both for image-level and dense recognition.\nA.2 Image similarity\nWe employ cosine similarity to compare image features (whether ours or feature generated for deduplication)\nwith the following similarity function m:\nm(s, r) =cosine-similarity (f(s), f(r)) =f(s).f(r)\nf(s)2f(r)2\nwhere sandrare a pair of images to compare and fis the model generating features.\nA.3 Deduplication\nSelf-deduplication. To deduplicate our uncurated data source of 1.3B images, we compute and use the\nembeddings generated by Pizzi et al. (2022) and retrieve the k= 64nearest neighbors of each image (using\ncosine similarity). Considering only neighbors with a similarity >0.6, we extract the connected components\nof the associated k-NN graph thanks to a scalable disjoint set data structure implementation. We then only\nkeep one representative for each component of duplicate images. This results in a self-deduplicated data\nsource of 1.1B images.\nRelativededuplication Toreduceredundancyandalsoproperlyevaluatetheperformanceofourfeatures,\nwe discard remaining images of our self-deduplicated data source that are too similar to train and test splits\nof our evaluation datasets. To achieve this, we apply a similar procedure as for self-deduplication, with a\nstricter similarity >0.45, this time identifying the duplicate components (if any) to which each reference\nimage belong and discarding it entirely. This results in a self- and relatively-deduplicated data source of\n744M images.\nA.4 Retrieval\nWe employ two approaches to augment dataset via retrieval: sample-based and cluster-based. The rst one,\nsample-based, applies to datasets larger than 1M images and consists in collecting a xed number kof nearest\nimages for each sample image of the dataset to retrieve, eectively trying to multiply by kthe size of the\ndataset. We use k= 4for Google Landmarks v2 and ImageNet-22k but a larger k= 32to make this specic\nretrieval a core part of our LVD-142M dataset. For smaller datasets, the second approach, cluster-based,\nconsists in rst clustering our uncurated data source into 100,000separate clusters thanks to a distributed\nk-means implementation. Each cluster should capture dierent types of image concept and contents. We\nthen pick 10,000images from each cluster associated with more than 3images of the retrieved dataset. As\nthis can result in a very large number of retrieved images for some dataset, we restrict such retrievals to a\nmaximum of 1M images to maintain the balance between the dierent datasets within LVD-142M.\nB Implementation Details\nB.1 Unsupervised pre-training\nFor unsupervised pre-training we build on the DINO and iBOT codebases. We use hyperparameters shown\nin Table 16, ViT architectures described in Table 17.\nKoLeo regularization. We apply the KoLeo regularizer with a weight of 0.1 between the class tokens of\nthe rst global crop, for all samples within a GPU without cross-communication for this step.\n28', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Task Dataset / Split Images Retrieval Retrieved Final\nclassication ImageNet-22k /  14,197,086 as is  14,197,086\nclassication ImageNet-22k /  14,197,086 sample 56,788,344 56,788,344\nclassication ImageNet-1k / train 1,281,167 sample 40,997,344 40,997,344\nne-grained classif. Caltech 101 / train 3,030 cluster 2,630,000 1,000,000\nne-grained classif. CUB-200-2011 / train 5,994 cluster 1,300,000 1,000,000\nne-grained classif. DTD / train1 1,880 cluster 1,580,000 1,000,000\nne-grained classif. FGVC-Aircraft / train 3,334 cluster 1,170,000 1,000,000\nne-grained classif. Flowers-102 / train 1,020 cluster 1,060,000 1,000,000\nne-grained classif. Food-101 / train 75,750 cluster 21,670,000 1,000,000\nne-grained classif. Oxford-IIIT Pet / trainval 3,680 cluster 2,750,000 1,000,000\nne-grained classif. Stanford Cars / train 8,144 cluster 7,220,000 1,000,000\nne-grained classif. SUN397 / train1 19,850 cluster 18,950,000 1,000,000\nne-grained classif. Pascal VOC 2007 / train 2,501 cluster 1,010,000 1,000,000\nsegmentation ADE20K / train 20,210 cluster 20,720,000 1,000,000\nsegmentation Cityscapes / train 2,975 cluster 1,390,000 1,000,000\nsegmentation Pascal VOC 2012 (seg.) / trainaug 1,464 cluster 10,140,000 1,000,000\ndepth estimation Mapillary SLS / train 1,434,262 as is  1,434,262\ndepth estimation KITTI / train (Eigen) 23,158 cluster 3,700,000 1,000,000\ndepth estimation NYU Depth V2 / train 24,231 cluster 10,850,000 1,000,000\ndepth estimation SUN RGB-D / train 4,829 cluster 4,870,000 1,000,000\nretrieval Google Landmarks v2 / train (clean) 1,580,470 as is  1,580,470\nretrieval Google Landmarks v2 / train (clean) 1,580,470 sample 6,321,880 6,321,880\nretrieval AmsterTime / new 1,231 cluster 960,000 960,000\nretrieval AmsterTime / old 1,231 cluster 830,000 830,000\nretrieval Met / train 397,121 cluster 62,860,000 1,000,000\nretrieval Revisiting Oxford / base 4,993 cluster 3,680,000 1,000,000\nretrieval Revisiting Paris / base 6,322 cluster 3,660,000 1,000,000\n142,109,386\nTable 15: Composition of our LVD-142M dataset. We report the list of datasets and associated splits\nused to build the dataset, how they were included (as is without retrieval or via sample-based or cluster-based\nretrieval). For retrievals, we indicate the actual number of retrieved images and the nal number included\nin the dataset.\n29', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Arch. Drop-rate LR Batch size\nDINOv2-S (distilled) ViT-S/14 0 1e-3 2048\nDINOv2-B (distilled) ViT-B/14 0 1e-3 2048\nDINOv2-L (distilled) ViT-L/14 0 1e-3 2048\nDINOv2-L (from scratch) ViT-L/14 0.4 3.5e-4 3072\nDINOv2-g (from scratch) ViT-g/14 0.4 3.5e-4 3072\nTable 16: Training hyperparameters for DINOv2-S, DINOv2-B, DINOv2-L and DINOv2-g. All\nmodels run for 625k iterations with optimizer AdamW, an initial LayerScale value of 1e-5, a weight decay\ncosine schedule from 0.04 to 0.2, a learning rate warmup of 100k iterations, a teacher momentum cosine\nschedule from 0.994 to 1, and we train in oat16 precision in all cases (except for the DINO heads where we\nreduce the gradients in oat32).\nArch. Embed dim Heads Blocks FFN layer\nViT-S/14 (distilled) 384 6 12 MLP\nViT-B/14 (distilled) 768 12 18 MLP\nViT-L/14 (distilled) 1024 16 24 MLP\nViT-L/14 (from scratch) 1024 16 24 SwiGLU\nViT-g/14 (from scratch) 1536 24 40 SwiGLU\nTable 17: Architecture details of the ViT-S/B/L/g networks used in this work. We use MLP\nfeed-forward networks for distilled models, and SwiGLU (Shazeer, 2020) when training from scratch.\nEMA update for the teacher. The teacher is initialized with the same state as the student, and is an\nexponential moving average of the student network, with a momentum value in [0.994, 1.0] following a cosine\nschedule. It is updated at the end of every training step.\nB.2 High-Resolution adaptation\nWe initialise the model with the pretrained weights then train it for 10k iterations with the same procedure\nas the original pretraining. All the schedules are kept the same as in the original training, but compressed\nto t in 10k iterations. All the hyperparameters are kept the same as in the rst pretraining, except the\nbase learning rate which is reduced.\nB.3 Linear probing evaluation\nFor linear probing we dene 3 evaluation parameters: the learning rate, how many output layers we use,\nwhether we concatenate the average-pooled patch token features with the class token (or use only the\nclass token). We train our linear layer with SGD for 12500 iterations, using random-resized-crop data\naugmentation, and perform the following grid search:\nlearning rate in{0.0001,0.0002,0.0005,0.001,0.002,0.005,0.01,0.02,0.05,0.1,0.2,0.3,0.5}\noutput layers in{1,4}\nconcatenate average-pooled tokens in {yes, no}\nWe then report the highest accuracy value obtained on the validation set as is common practice. Note that\nthis grid search is not expensive, because at each iteration we perform inference on the backbone only once,\nthen feed the output to all linear classiers (each performing a single matrix multiplication).\nC List of benchmarks used for evaluations\nWe show in Table 18 the list of benchmarks and datasets used for evaluation.\n30', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Dataset name Task Citation\nImageNet-1k Image Classication (Russakovsky et al., 2015)\nImageNet-V2 Image Classication (Recht et al., 2019)\nImageNet-ReaL Image Classication (Beyer et al., 2020)\nImageNet-A Image Classication (Djolonga et al., 2021)\nImageNet-C Image Classication (Hendrycks & Dietterich, 2019)\nImageNet-Rendition Image Classication (Hendrycks et al., 2021)\nImageNet-Sketch Image Classication (Wang et al., 2019)\nFood-101 Image Classication (Bossard et al., 2014)\nCIFAR-10 Image Classication (Krizhevsky et al., 2009)\nCIFAR-100 Image Classication (Krizhevsky et al., 2009)\nSUN397 Image Classication (Xiao et al., 2010)\nStanfordCars Image Classication (Krause et al., 2013)\nFGVC-Aircraft Image Classication (Maji et al., 2013)\nPascal VOC 2007 Image Classication (Everingham et al., 2015)\nDescribable Textures Image Classication (Cimpoi et al., 2014)\nOxford Pets Image Classication (Parkhi et al., 2012)\nCaltech101 Image Classication (Fei-Fei et al., 2004)\nOxford Flowers Image Classication (Nilsback & Zisserman, 2008)\nCUB200 Image Classication (Welinder et al., 2010)\niNaturalist 2018 Image Classication (Van Horn et al., 2018)\niNaturalist 2021 Image Classication (Van Horn et al., 2021)\nPlaces-205 Image Classication (Zhou et al., 2014)\nUCF101 Video Classication (Soomro et al., 2012)\nKinetics-400 Video Classication (Kay et al., 2017)\nSomething-Something-V2 Video Classication (Goyal et al., 2017)\nRevisiting-Paris Image Retrieval (Radenovi et al., 2018a)\nRevisiting-Oxford Image Retrieval (Radenovi et al., 2018a)\nMet Image Retrieval (Ypsilantis et al., 2021)\nAmstertime Image Retrieval (Yildiz et al., 2022)\nADE20k Image Segmentation (Zhou et al., 2017)\nCityscapes Image Segmentation (Cordts et al., 2016)\nPascal VOC 2012 Image Segmentation (Everingham et al., 2015)\nNYU-Depth V2 Monocular Depth Estimation (Silberman et al., 2012)\nKITTI Monocular Depth Estimation (Geiger et al., 2013)\nSUN-RGBD Monocular Depth Estimation (Song et al., 2015)\nDollarStreet Fairness Analysis (De Vries et al., 2019)\nCasual Conversations Fairness Analysis (Hazirbas et al., 2021)\nTable 18: List of datasets used for evaluation.\n31', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}]: %s
2023-12-07 11:00:01,093 - INFO - Received requests to /inference endpoint
2023-12-07 11:00:01,194 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 11:00:01,195 - INFO - Received request: {'username': 'amin', 'prompt': 'give me a brief of the paper', 'memory': True, 'conversation_number': 2, 'AI_assistance': False, 'collection_name': 'genAI', 'llm_model': 'Llama_13b'}
2023-12-07 11:00:06,521 - INFO - Processed the request successfully
2023-12-07 11:00:40,583 - INFO - Received requests to /inference endpoint
2023-12-07 11:00:40,684 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 11:00:40,684 - INFO - Received request: {'username': 'amin', 'prompt': 'Hi how are you doing?', 'memory': True, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-12-07 11:00:46,181 - INFO - Processed the request successfully
2023-12-07 11:01:09,755 - INFO - Received requests to /inference endpoint
2023-12-07 11:01:09,856 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 11:01:09,857 - INFO - Received request: {'username': 'amin', 'prompt': 'give me a summary of the paper', 'memory': True, 'conversation_number': 2, 'AI_assistance': False, 'collection_name': 'genAI', 'llm_model': 'Llama_70b'}
2023-12-07 11:02:43,734 - INFO - Processed the request successfully
2023-12-07 11:03:02,857 - INFO - collection delete: {'success': 'Class amin_\tgenAI has been removed'}: %s
2023-12-07 11:03:09,285 - INFO - collection delete: {'success': 'Class amin_\tsdee has been removed'}: %s
2023-12-07 11:03:15,959 - INFO - collection delete: {'success': 'Class amin_cd has been removed'}: %s
2023-12-07 11:03:26,362 - INFO - collection delete: {'success': 'Class amin_sdee has been removed'}: %s
2023-12-07 11:03:32,158 - INFO - collection delete: {'success': 'Class amin_genAI has been removed'}: %s
2023-12-07 11:03:35,918 - INFO - collection delete: {'success': 'Class amin_rr has been removed'}: %s
2023-12-07 11:03:39,837 - INFO - collection delete: {'success': 'Class amin_file has been removed'}: %s
2023-12-07 11:03:44,517 - INFO - collection delete: {'success': 'Class amin_web has been removed'}: %s
2023-12-07 11:03:49,924 - INFO - collection delete: {'success': 'Class amin_document has been removed'}: %s
2023-12-07 11:03:54,188 - INFO - collection delete: {'success': 'Class amin_video has been removed'}: %s
2023-12-07 11:04:01,826 - INFO - checking the request/ username='amin' class_name='paper' mode='create_collection' vectorDB_type='Weaviate' file_path=None: %s
2023-12-07 11:04:01,904 - INFO - checkpoint 1
2023-12-07 11:04:01,904 - INFO - checkpoint 2 amin: %s
2023-12-07 11:04:01,904 - INFO - checkpoint 2 amin_paper: %s
2023-12-07 11:04:01,927 - INFO - class name added successfully to database
2023-12-07 11:04:01,927 - INFO - success: class paper created for user amin
2023-12-07 11:04:21,627 - INFO - request received username='amin' class_name='paper' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/b411b22ab5006a5c': %s
2023-12-07 11:04:22,559 - INFO - actors creation successful [Actor(WeaviateEmbedder, d55105037d427a4bca3cd49701000000), Actor(WeaviateEmbedder, 555d836fcc0ae02c69101bc501000000), Actor(WeaviateEmbedder, 9c0695adf39668ae9d3195be01000000)]: %s
2023-12-07 11:04:22,560 - INFO - check 1st step of ray was successful
2023-12-07 11:04:22,560 - INFO - check if ray was successful:
2023-12-07 11:04:22,560 - INFO - check weaviate add data, 
2023-12-07 11:04:22,560 - INFO - request processed successfully username='amin' class_name='paper' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/b411b22ab5006a5c': %s
2023-12-07 11:04:24,140 - INFO - Check the data that is being passed [{'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nZhuosheng Zhang1Aston Zhang2Mu Li2Hai Zhao1George Karypis2Alex Smola2\nAbstract\nLarge language models (LLMs) have shown im-\npressive performance on complex reasoning by\nleveraging chain-of-thought (CoT) prompting to\ngenerate intermediate reasoning chains as the ra-\ntionale to infer the answer. However, existing\nCoT studies have focused on the language modal-\nity. We propose Multimodal-CoT that incorpo-\nrates language (text) and vision (images) modal-\nities into a two-stage framework that separates\nrationale generation and answer inference. In this\nway, answer inference can leverage better gen-\nerated rationales that are based on multimodal\ninformation. With Multimodal-CoT, our model\nunder 1 billion parameters outperforms the previ-\nous state-of-the-art LLM (GPT-3.5) by 16 percent-\nage points (75.17% 91.68% accuracy) and even\nsurpasses human performance on the ScienceQA\nbenchmark. Code is publicly available.1\n1. Introduction\nImagine reading a textbook with no gures or tables. Our\nability to knowledge acquisition is greatly strengthened by\njointly modeling diverse data modalities, such as vision, lan-\nguage, and audio. Recently, large language models (LLMs)\n(Brown et al., 2020; Thoppilan et al., 2022; Rae et al., 2021;\nChowdhery et al., 2022) have shown impressive perfor-\nmance in complex reasoning by generating intermediate\nreasoning steps before inferring the answer. The intriguing\ntechnique is called chain-of-thought (CoT) reasoning (Wei\net al., 2022b; Kojima et al., 2022; Zhang et al., 2022).\nHowever, existing studies related to CoT reasoning are\nlargely isolated in the language modality (Wang et al.,\n2022b; Zhou et al., 2022; Lu et al., 2022b; Fu et al., 2022),\nwith little consideration of multimodal scenarios. To elicit\nCoT reasoning in multimodality, we advocate a Multimodal-\n1Shanghai Jiao Tong University2Amazon Web Services.\nCorrespondence to: Zhuosheng Zhang (work done at Ama-\nzon Web Services) <zhangzs@sjtu.edu.cn >, Aston Zhang\n<az@astonzhang.com >.\n1https://github.com/amazon-science/mm-cot\nOptions:(B) salty(A) softOutputQuestion:Whichpropertydothesetwoobjectshaveincommon?Context: Select the better answer.\nRationale:Lookateachobject.Foreachobject,decideifithasthatproperty.Potatochipshaveasaltytaste.Bothobjectsaresalty.Asoftobjectchangesshapewhenyousqueezeit.Thefriesaresoft,butthecrackerisnot.Thepropertythatbothobjectshaveincommonissalty.Answer:Theansweris(B).VisionLanguageInputFigure 1. Example of the multimodal CoT task.\nCoT paradigm. Given the inputs in different modalities,\nMultimodal-CoT decomposes multi-step problems into in-\ntermediate reasoning steps (rationale) and then infers the\nanswer. Since vision and language are the most popular\nmodalities, we focus on those two modalities in this work.\nAn example is shown in Figure 1. In general, there are two\nways to elicit Multimodal-CoT reasoning as follows: (i)\nprompting LLMs and (ii) ne-tuning small models.2\nThe most immediate way to perform Multimodal-CoT is to\ntransform the input of different modalities into one modality\nand prompt LLMs to perform CoT. For example, it is possi-\nble to extract the caption of an image by a captioning model\nand then concatenate the caption with the original language\ninput to be fed into LLMs (Lu et al., 2022a). However, there\nis severe information loss in the captioning process; thus,\nusing the captions (as opposed to vision features) may suffer\nfrom a lack of mutual synergy in the representation space\nof different modalities.\nTo facilitate the interaction between modalities, another\npotential solution is to ne-tune smaller language models\n(LMs) by fusing multimodal features (Zhang et al., 2023).\nAs this approach allows the exibility of adjusting model\narchitectures to incorporate multimodal features, we study\nne-tuning models in this work instead of prompting LLMs.\nThe key challenge is that language models under 100 billion\nparameters tend to generate hallucinated rationales that mis-\nlead the answer inference (Ho et al., 2022; Magister et al.,\n2In this work, we refer to small models as models with less\nthan 1 billion parameters (hereinafter dubbed as 1B-models).arXiv:2302.00923v4  [cs.CL]  17 Feb 2023', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nTable 1. Typical CoT techniques (FT: ne-tuning; KD: knowledge distillation). Segment 1: in-context learning techniques; Segment 2:\nne-tuning techniques. To the best of our knowledge, our work is the rst to study CoT reasoning in different modalities. Besides, we\nfocus on 1B-models, without relying on the outputs of LLMs.\nModels Mutimodal w/o LLM Model / Engine Training CoT Role CoT Source\nZero-Shot-CoT (Kojima et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Template\nFew-Shot-CoT (Wei et al., 2022b) \x17 \x17 PaLM (540B) ICL Reasoning Hand-crafted\nSelf-Consistency-CoT (Wang et al., 2022a) \x17 \x17 Codex (175B) ICL Reasoning Hand-crafted\nLeast-to-Most Prompting (Zhou et al., 2022) \x17 \x17 Codex (175B) ICL Reasoning Hand-crafted\nRetrieval-CoT (Zhang et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Auto-generated\nPromptPG-CoT (Lu et al., 2022b) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Hand-crafted\nAuto-CoT (Zhang et al., 2022) \x17 \x17 Codex (175B) ICL Reasoning Auto-generated\nComplexity-CoT (Fu et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Hand-crafted\nFew-Shot-PoT (Chen et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Hand-crafted\nUniedQA (Lu et al., 2022a) \x17  T5 (770M) FT Explanation Crawled\nFine-Tuned T5 XXL (Magister et al., 2022) \x17 \x17 T5 (11B) KD Reasoning LLM-generated\nFine-Tune-CoT (Ho et al., 2022) \x17 \x17 GPT-3 (6.7B) KD Reasoning LLM-generated\nMultimodal-CoT (our work)   T5 (770M) FT Reasoning Crawled\n2022; Ji et al., 2022).\nTo mitigate the challenge of hallucination, we propose\nMultimodal-CoT that incorporates language (text) and vi-\nsion (images) modalities into a two-stage framework that\nseparates rationale generation and answer inference. In\nthis way, answer inference can leverage better generated\nrationales that are based on multimodal information. Our\nexperiments are conducted on the ScienceQA benchmark\n(Lu et al., 2022a), which is the latest multimodal reasoning\nbenchmark with annotated reasoning chains. Experimental\nresults show that our method surpasses the previous state-of-\nthe-art GPT-3.5 model by +16% (75.17% 91.68%) on the\nbenchmark. Our contributions are summarized as follows:\n(i) To the best of our knowledge, this work is the rst to\nstudy CoT reasoning in different modalities.\n(ii) We propose a two-stage framework by ne-tuning lan-\nguage models to fuse vision and language representations\nto perform Multimodal-CoT. The model is able to generate\ninformative rationales to facilitate inferring nal answers.\n(iii) Our method achieves new state-of-the-art performance\non the ScienceQA benchmark, outperforming accuracy of\nGPT-3.5 by 16% and even surpassing human performance.\n2. Background\nThis section reviews recent progress of eliciting CoT rea-\nsoning by prompting and ne-tuning language models.\n2.1. CoT Reasoning with LLMs\nRecently, CoT has been widely used to elicit the multi-step\nreasoning abilities of LLMs (Wei et al., 2022b). Concretely,\nCoT techniques encourage the LLM to generate intermedi-\nate reasoning chains for solving a problem. Studies have\nshown that LLMs can perform CoT reasoning with two ma-\njor paradigms of techniques: Zero-Shot-CoT (Kojima et al.,2022) and Few-Shot-CoT (Wei et al., 2022b; Zhang et al.,\n2022). For Zero-Shot-CoT, Kojima et al. (2022) showed that\nLLMs are decent zero-shot reasoners by adding a prompt\nlike Lets think step by step after the test question to in-\nvoke CoT reasoning. For Few-Shot-CoT, a few step-by-step\nreasoning demonstrations are used as conditions for infer-\nence. Each demonstration has a question and a reasoning\nchain that leads to the nal answer. The demonstrations are\ncommonly obtained by hand-crafting or automatic gener-\nation. The corresponding techniques are thus referred to\nas Manual-CoT (Wei et al., 2022b) and Auto-CoT (Zhang\net al., 2022).\nWith effective demonstrations, Few-Shot-CoT often\nachieves stronger performance than Zero-Shot-CoT and has\nattracted more research interest. Therefore, most recent\nstudies focused on how to improve Few-Shot-CoT. Those\nstudies are categorized into two major research lines: (i)\noptimizing the demonstrations; (ii) optimizing the reasoning\nchains. Table 1 compares typical CoT techniques.\nOptimizing Demonstrations The performance of Few-\nShot-CoT relies on the quality of demonstrations. As re-\nported in Wei et al. (2022b), using demonstrations written\nby different annotators results in dramatic accuracy dispar-\nity in a symbolic reasoning task. Beyond hand-crafting the\ndemonstrations, recent studies have investigated ways to op-\ntimize the demonstration selection process. Notably, Rubin\net al. (2022) retrieved the semantically similar demonstra-\ntions with the test instance. However, this approach shows\na degraded performance when there are mistakes in the rea-\nsoning chains (Zhang et al., 2022). To address the limitation,\nZhang et al. (2022) found that the key is the diversity of\ndemonstration questions and proposed Auto-CoT: (i) par-\ntition questions of a given dataset into a few clusters; (ii)\nsample a representative question from each cluster and gen-\nerate its reasoning chain using Zero-Shot-CoT with simple\nheuristics. In addition, reinforcement learning (RL) and', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\ncomplexity-based selection strategies were also proposed\nto obtain effective demonstrations. Fu et al. (2022) chose\nexamples with complex reasoning chains (i.e., with more\nreasoning steps) as the demonstrations. Lu et al. (2022b)\ntrained an agent to nd optimal in-context examples from\na candidate pool and maximize the prediction rewards on\ngiven training examples when interacting with GPT-3.5.\nOptimizing Reasoning Chains A notable way to opti-\nmize reasoning chains is problem decomposition. Zhou\net al. (2022) proposed least-to-most prompting to decom-\npose complex problems into sub-problems and then solve\nthese sub-problems sequentially. As a result, solving a\ngiven sub-problem is facilitated by the answers to previ-\nously solved sub-problems. Similarly, Khot et al. (2022)\nused diverse decomposition structures and designed differ-\nent prompts to answer each sub-question. In addition to\nprompting the reasoning chains as natural language texts,\nChen et al. (2022) proposed program-of-thoughts (PoT),\nwhich modeled the reasoning process as a program and\nprompted LLMs to derive the answer by executing the gen-\nerated programs. Another trend is to vote over multiple\nreasoning paths for a test question. Wang et al. (2022a)\nintroduced a self-consistency decoding strategy to sample\nmultiple outputs of LLMs and then took a majority over\nthe nal answers. Wang et al. (2022b) and Li et al. (2022b)\nintroduced randomness in the input space to produce more\ndiverse outputs for voting.\n2.2. Eliciting CoT Reasoning by Fine-Tuning Models\nA recent interest is eliciting CoT reasoning by ne-tuning\nlanguage models. Lu et al. (2022a) ne-tuned the encoder-\ndecoder T5 model on a large-scale dataset with CoT annota-\ntions. However, a dramatic performance decline is observed\nwhen using CoT to infer the answer, i.e., generating the rea-\nsoning chain before the answer (reasoning). Instead, CoT\nis only used as an explanation after the answer. Magister\net al. (2022) and Ho et al. (2022) employed knowledge\ndistillation by ne-tuning a student model on the chain-of-\nthought outputs generated by a larger teacher model. The\nproposed methods showed performance gains in arithmetic,\ncommonsense, and symbolic reasoning tasks.\nThere is a key challenge in training 1B-models to be CoT\nreasoners. As observed by Wei et al. (2022b), models un-\nder 100 billion parameters tend to produce illogical CoT\nthat leads to wrong answers. In other words, it might be\nharder for 1B-models to generate effective CoT than directly\ngenerating the answer. It becomes even more challenging\nin a multimodal setting where answering the question also\nrequires understanding the multimodal inputs. In the follow-\ning part, we will explore the challenge of Multimodal-CoT\nand investigate how to perform effective multi-step reason-\ning.3. Challenge of Multimodal-CoT\nExisting studies have suggested that the CoT reasoning abil-\nity may emerge in language models at a certain scale, e.g.,\nover 100 billion parameters (Wei et al., 2022a). However,\nit remains an unresolved challenge to elicit such reasoning\nabilities in 1B-models, let alone in the multimodal scenario.\nThis work focuses on 1B-models as they can be ne-tuned\nand deployed with consumer-grade GPUs (e.g., 32G mem-\nory). In this section, we will investigate why 1B-models\nfail at CoT reasoning and study how to design an effective\napproach to overcome the challenge.\n3.1. Towards the Role of CoT\nTo begin with, we ne-tune a text-only baseline for CoT rea-\nsoning on the ScienceQA benchmark (Lu et al., 2022a).\nFollowing Lu et al. (2022a), we adopt UniedQA Base\n(Khashabi et al., 2020) as the backbone language model.3\nOur task is modeled as a text generation problem, where the\nmodel takes the textual information as the input and gener-\nates the output sequence that consists of the rationale and\nthe answer. As an example shown in Figure 1, the model\ntakes the concatenation of tokens of the question text (Q),\nthe context text (C), and multiple options (M) as the input.\nTo study the effect of CoT, we compare the performance\nwith three variants: (i) No-CoT which predicts the answer\ndirectly (QCMA); (ii) Reasoning where answer inference\nis conditioned to the rationale (QCM RA); (iii) Explana-\ntion where the rationale is used for explaining the answer\ninference (QCMAR).\nTable 2. Effects of CoT in the one-stage setting.\nMethod Format Accuracy\nNo-CoT QCM A 80.40\nReasoning QCM RA 67.86\nExplanation QCM AR 69.77\nSurprisingly, we observe a 12.54% accuracy decrease\n(80.40%67.86%) if the model predicts rationales before\nanswers (QCMRA). The results imply that the rationales\nmight not necessarily contribute to predicting the right an-\nswer. A similar phenomenon was observed in Lu et al.\n(2022a), where the plausible reason might be that the model\nexceeds the maximum token limits before obtaining the\nrequired answer or stops generating the prediction early.\nHowever, we nd that the maximum length of the gener-\nated outputs (RA) is always less than 400 tokens, which\nis below the length limit of language models (i.e., 512 in\nUniedQA Base). Therefore, it deserves a more in-depth\ninvestigation into why the rationales harm answer inference.\n3UniedQA (Khashabi et al., 2020) is adopted as it is the best\nne-tuning model in Lu et al. (2022a). Model information and\nimplementation details are presented in Appendix B.1.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nGeneratedRationale:Magnetscanpullorpushoneachotherwithouttouching.Whenmagnetsattract,theypulltogether.Whenmagnetsrepel,theypushapart.Whetheramagnetattractsorrepelsothermagnetsdependsonthepositionsofitspoles,orends.Everymagnethastwopoles,callednorthandsouth.Herearesomeexamplesofmagnets.ThenorthpoleofeachmagnetismarkedN,andthesouthpoleismarkedS.Ifdifferentpolesareclosesttoeachother,themagnetsattract.Themagnetsinthepairbelowattract.Ifthesamepolesareclosesttoeachother,themagnetsrepel.Themagnetsinbothpairsbelowrepel.Willthesemagnetsattractorrepel?Tofindout,lookatwhichpolesareclosesttoeachother.Thesouthpoleofonemagnetisclosesttothesouthpoleoftheothermagnet.Polesthatarethesamerepel.So,thesemagnetswillrepeleachother.Answer:Theansweris(B).Options:(B) repel(A) attractProblem\nBaselineQuestion:Willthesemagnetsattractorrepeleachother?Context:Twomagnetsareplacedasshown.Hint:Magnetsthatattractpulltogether.Magnetsthatrepelpushapart.GoldRationale:Magnetscanpullorpushoneachotherwithouttouching.Whenmagnetsattract,theypulltogether.Whenmagnetsrepel,theypushapart.Whetheramagnetattractsorrepelsothermagnetsdependsonthepositionsofitspoles,orends.Everymagnethastwopoles,callednorthandsouth.Herearesomeexamplesofmagnets.ThenorthpoleofeachmagnetismarkedN,andthesouthpoleismarkedS.Ifdifferentpolesareclosesttoeachother,themagnetsattract.Themagnetsinthepairbelowattract.Ifthesamepolesareclosesttoeachother,themagnetsrepel.Themagnetsinbothpairsbelowrepel.Willthesemagnetsattractorrepel?Tofindout,lookatwhichpolesareclosesttoeachother.Thenorthpoleofonemagnetisclosesttothesouthpoleoftheothermagnet.Polesthataredifferentattract.So,thesemagnetswillattracteachother.Answer:Theansweris(A).GeneratedRationale:Magnetscanpullorpushoneachotherwithouttouching.Whenmagnetsattract,theypulltogether.Whenmagnetsrepel,theypushapart.Whetheramagnetattractsorrepelsothermagnetsdependsonthepositionsofitspoles,orends.Everymagnethastwopoles,callednorthandsouth.Herearesomeexamplesofmagnets.ThenorthpoleofeachmagnetismarkedN,andthesouthpoleismarkedS.Ifdifferentpolesareclosesttoeachother,themagnetsattract.Themagnetsinthepairbelowattract.Ifthesamepolesareclosesttoeachother,themagnetsrepel.Themagnetsinbothpairsbelowrepel.Willthesemagnetsattractorrepel?Tofindout,lookatwhichpolesareclosesttoeachother.Thenorthpoleofonemagnetisclosesttothesouthpoleoftheothermagnet.Polesthataredifferentattract.So,thesemagnetswillattracteachother.Answer:Theansweris(A).+ Vision Features\nVision\nFigure 2. Example of the two-stage framework without vision features (baseline) and with vision features (ours) for generating rationales\nand predicting answers. The upper part presents the problem details with a gold rationale, and the lower part shows the outputs of the\nbaseline and our method incorporated with vision features. We observe that the baseline fails to predict the right answer due to the\nmisleading by hallucinated rationales. More examples are shown in Appendix A.1.\n3.2. Misleading by Hallucinated Rationales\nTo dive into how the rationales affect the answer prediction,\nwe separate the CoT problem into two stages, rationale\ngeneration andanswer inference . We report the RougeL\nscore and accuracy for the rationale generation and answer\ninference, respectively. Table 3 shows the results based\non the two-stage framework. Although the two-stage base-\nline model achieves a 91.76 RougeL score of the rationale\ngeneration, the answer inference accuracy is only 70.53%.\nCompared with the QCM A variant (80.40%) in Table 2,\nthe result shows that the generated rationale in the two-stage\nframework does not improve answer accuracy.\nTable 3. Two-stage setting of (i) rationale generation (RougeL) and\n(ii) answer inference (Accuracy).\nMethod (i) QCM R (ii) QCMRA\nTwo-Stage Framework 91.76 70.53\nw/ Captions 91.85 71.12\nw/ Vision Features 96.97 84.91\nThen, we randomly sample 50 error cases and nd that the\nmodel tends to generate hallucinated rationales that mislead\nthe answer inference. As an example shown in Figure 2, the\nmodel (left part) hallucinates that,  The south pole of one\nmagnet is closest to the south pole of the other magnet , due\nto the lack of reference to the vision content. We nd that\nsuch mistakes occur at a ratio of 64% among the error cases\nOthers(36%)Resolved (62.5%)Unresolved(37.5%)Hallucination(64%)(a) ratio of hallucination mistakes(b) correction rate w/ vision features  Figure 3. The ratio of hallucination mistakes (a) and correction\nrate w/ vision features (b).\n(Figure 3(a)).\n3.3. Multimodality Contributes to Effective Rationales\nWe speculate that such a phenomenon of hallucination is\ndue to a lack of necessary vision contexts for performing\neffective Multimodal-CoT. To inject vision information, a\nsimple way is to transform the paired image into a caption\n(Lu et al., 2022a) and then append the caption in the input of\nboth stages. However, as shown in Table 3, using captions\nonly yields marginal performance gains ( 0.59%). Then,\nwe explore an advanced technique by incorporating vision\nfeatures into the language model. Concretely, we feed the\npaired image to the DETR model (Carion et al., 2020) to\nextract vision features. Then we fuse the vision features', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nVisionLanguageRationale GenerationQuestion:Whichpropertydothesetwoobjectshaveincommon?Context: Select the better answer.Lookateachobject.Foreachobject,decideifithasthatproperty.Potatochipshaveasaltytaste.Bothobjectsaresalty.Asoftobjectchangesshapewhenyousqueezeit.Thefriesaresoft,butthecrackerisnot.Thepropertythatbothobjectshaveincommonissalty.\nOptions:(B) salty(A) softRationaleAnswer InferenceTheansweris(B).Answer\nFigure 4. Overview of our Multimodal-CoT framework. Multimodal-CoT consists of two stages: (i) rationale generation and (ii) answer\ninference. Both stages share the same model architecture but differ in the input and output. In the rst stage, we feed the model with\nlanguage and vision inputs to generate rationales. In the second stage, we append the original language input with the rationale generated\nfrom the rst stage. Then, we feed the updated language input with the original vision input to the model to infer the answer.\nwith the encoded language representations before feeding\nto the decoder (more details will be presented in Section\n4). Interestingly, with vision features, the RougeL score of\nthe rationale generation has boosted to 96.97% (QCM R),\nwhich correspondingly contributes to better answer accuracy\nof 84.91% (QCMR A). With those effective rationales,\nthe phenomenon of hallucination is mitigated  62.5%\nhallucination mistakes in Section 3.2 have been corrected\n(Figure 3(b)), as an example shown in Figure 2 (right part).4\nThe analysis so far compellingly shows that vision features\nare indeed benecial for generating effective rationales and\ncontributing to accurate answer inference. As the two-stage\nmethod (QCMRA) in Table 3 achieves better performance\nthan all the one-stage method in Table 2, we choose the two-\nstage method in our Multimodal-CoT framework.\n4. Multimodal-CoT\nBased on the observations and discussions in Section 3, we\npropose Multimodal-CoT to incorporate language (text) and\nvision (images) modalities into a two-stage framework. In\nthis section, we will rst overview the procedure of the\nframework and then elaborate on the technical design of the\nmodel architecture.\n4.1. Framework Overview\nMultimodal-CoT consists of two training stages: (i) ratio-\nnale generation and (ii) answer inference. Both stages share\nthe same model architecture but differ in the input Xand\noutputY. The overall procedure is illustrated in Figure 4.\nWe will take vision-language as an example to show how\nMultimodal-CoT works.\n4The left mistakes are mainly about map understanding, which\nrequires more advanced vision features. We will discuss them in\nSection 6.4.In the rationale generation stage, we feed the model with\nX={X1\nlanguage,Xvision}whereX1\nlanguage represents the lan-\nguage input in the rst stage and Xvision represents the vision\ninput, i.e., the image. For example, Xcan be instantiated as\na concatenation of question, context, and options of a multi-\nple choice reasoning problem (Lu et al., 2022a) as shown in\nFigure 4. The goal is to learn a rationale generation model\nR=F(X)whereRis the rationale.\nIn the answer inference stage, the rationale Ris appended\nto the original language input X1\nlanguage to construct the lan-\nguage input in the second stage, X2\nlanguage =X1\nlanguageR\nwheredenotes concatenation. Then, we feed the updated\ninputX={X2\nlanguage,Xvision}to the answer inference\nmodel to infer the nal answer A=F(X).\nIn both stages, we train two models with the same archi-\ntecture independently. They take the annotated elements\n(e.g.,XR,XRA, respectively) from the training\nset for supervised learning. During inference, given X, the\nrationales for the test sets are generated using the model\ntrained in the rst stage; they are used in the second stage\nfor answer inference.\n4.2. Model Architecture\nGiven the language input Xlanguage{X1\nlanguage,X2\nlanguage}\nand the vision input Xvision, we compute the probability of\ngenerating target text Y(either the rationale or the answer\nin Figure 4) of length Nby\np(Y|Xlanguage,Xvision ) =N\ni=1p(Yi|Xlanguage,Xvision,Y<i),\n(1)\nwherep(Yi|Xlanguage,Xvision,Y<i)is implemented with\na Transformer-based network (Vaswani et al., 2017). The\nnetwork has three major procedures: encoding, interaction,', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nAlgorithm 1 Multimodal-CoT\nInput: Language input X1\nlanguage , vision input Xvision\nOutput: Generated rationale R, inferred answer A\n1: Construct the input X={Xlanguage,X vision}\n2: Generate rationale R=F(X)using the model F()\n3:Append the rationale Rto the original language input\nX2\nlanguage =X1\nlanguageR.\n4: Construct new input X={X2\nlanguage,X vision}\n5:Infer the answer Aby conditioning on the new input, A=\nF(X).\n6:procedure F(X)\n7: Encode the language and vision inputs Hlanguage andHvision,\nrespectively\n8: Build the interaction between language and vision features\nby attention Hattn\nvision\n9: FuseHlanguage andHattn\nvision by a gated fusion mechanism to\nhaveHfuse\n10: FeedHfuseto the decoder to obtain the target prediction Y\n11: return Y\n12:end procedure\nand decoding. Specically, we feed the language text into\na Transformer encoder to obtain a textual representation,\nwhich is then interacted and fused with the vision represen-\ntation before being fed into the Transformer decoder.\nEncoding The modelF(X)takes both the language and\nvision inputs and obtains the text representation Hlanguage\nand the image feature Hvision by the following functions:\nHlanguage =LanguageEncoder (Xlanguage ), (2)\nHvision =WhVisionExtractor (Xvision ),(3)\nwhere LanguageEncoder( ) is implemented as a Trans-\nformer model. We use the hidden states of the last layer\nin the Transformer encoder as the language representation\nHlanguageRndwherendenotes the length of the lan-\nguage input, and dis the hidden dimension. Meanwhile,\nVisionExtractor() is used to vectorize the input image into\nvision features. Inspired by the recent success of Vision\nTransformers (Dosovitskiy et al., 2021), we fetch the patch-\nlevel features by off-the-shelf vision extraction models,5\nsuch as DETR (Carion et al., 2020). After obtaining the\npatch-level vision features, we apply a learnable projection\nmatrixWhto convert the shape of VisionExtractor (Xvision )\ninto that ofHlanguage ; thus we have HvisionRmdwhere\nmis the number of patches.\nInteraction After obtaining language and vision represen-\ntations, we use a single-head attention network to correlate\ntext tokens with image patches, where the query ( Q), key\n(K) and value (V) areHlanguage ,Hvision andHvision, respec-\n5The parameters of the vision extraction are frozen.tively. The attention output Hattn\nvisionRndis dened as:\nHattn\nvision =Softmax (QK\ndk)V, (4)\nwheredkis the same as the dimension of Hlanguage because\na single head is used.\nThen, we apply the gated fusion mechanism (Zhang et al.,\n2020; Wu et al., 2021; Li et al., 2022a) to fuse Hlanguage and\nHvision. The fused output HfuseRndis obtained by:\n=Sigmoid (WlHlanguage +WvHattn\nvision ),(5)\nHfuse = (1)Hlanguage +Hattn\nvision, (6)\nwhereWlandWvare learnable parameters.\nDecoding Finally, the fused output Hfuseis fed into the\nTransformer decoder to predict the target Y. The complete\nprocedure of Multimodal-CoT is shown in Algorithm 1.\n5. Experiments\nThis section will present the benchmark dataset, the imple-\nmentation of our technique, and the baselines for compar-\nisons. Then, we will report our main results and ndings.\n5.1. Dataset\nOur method is evaluated on the ScienceQA benchmark (Lu\net al., 2022a). ScienceQA is the rst large-scale multimodal\nscience question dataset that annotates the answers with de-\ntailed lectures and explanations. It contains 21k multimodal\nmultiple choice questions with rich domain diversity across\n3 subjects, 26 topics, 127 categories, and 379 skills. The\nbenchmark dataset is split into training, validation, and test\nsplits with 12726, 4241, and 4241 examples, respectively.\n5.2. Implementation\nThe following part presents the experimental settings of\nMultimodal-CoT and the baseline methods.\nExperimental Settings As the Multimodal-CoT task re-\nquires generating the reasoning chains and leveraging the\nvision features, we use the T5 encoder-decoder architec-\nture (Raffel et al., 2020). Specically, we adopt UniedQA\n(Khashabi et al., 2020) to initialize our models in the two\nstages because it achieves the best ne-tuning results in\nLu et al. (2022a). To verify the generality of our approach\nacross different LMs, we also employ FLAN-T5 (Chung\net al., 2022) as the backbone in Section 6.3. As using im-\nage captions does not yield signicant performance gains in\nSection 3.3, we did not use the captions. We ne-tune the\nmodels up to 20 epochs, with a learning rate of 5e-5. The', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}]: %s
2023-12-07 11:04:24,141 - INFO - Check the results [{'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nZhuosheng Zhang1Aston Zhang2Mu Li2Hai Zhao1George Karypis2Alex Smola2\nAbstract\nLarge language models (LLMs) have shown im-\npressive performance on complex reasoning by\nleveraging chain-of-thought (CoT) prompting to\ngenerate intermediate reasoning chains as the ra-\ntionale to infer the answer. However, existing\nCoT studies have focused on the language modal-\nity. We propose Multimodal-CoT that incorpo-\nrates language (text) and vision (images) modal-\nities into a two-stage framework that separates\nrationale generation and answer inference. In this\nway, answer inference can leverage better gen-\nerated rationales that are based on multimodal\ninformation. With Multimodal-CoT, our model\nunder 1 billion parameters outperforms the previ-\nous state-of-the-art LLM (GPT-3.5) by 16 percent-\nage points (75.17% 91.68% accuracy) and even\nsurpasses human performance on the ScienceQA\nbenchmark. Code is publicly available.1\n1. Introduction\nImagine reading a textbook with no gures or tables. Our\nability to knowledge acquisition is greatly strengthened by\njointly modeling diverse data modalities, such as vision, lan-\nguage, and audio. Recently, large language models (LLMs)\n(Brown et al., 2020; Thoppilan et al., 2022; Rae et al., 2021;\nChowdhery et al., 2022) have shown impressive perfor-\nmance in complex reasoning by generating intermediate\nreasoning steps before inferring the answer. The intriguing\ntechnique is called chain-of-thought (CoT) reasoning (Wei\net al., 2022b; Kojima et al., 2022; Zhang et al., 2022).\nHowever, existing studies related to CoT reasoning are\nlargely isolated in the language modality (Wang et al.,\n2022b; Zhou et al., 2022; Lu et al., 2022b; Fu et al., 2022),\nwith little consideration of multimodal scenarios. To elicit\nCoT reasoning in multimodality, we advocate a Multimodal-\n1Shanghai Jiao Tong University2Amazon Web Services.\nCorrespondence to: Zhuosheng Zhang (work done at Ama-\nzon Web Services) <zhangzs@sjtu.edu.cn >, Aston Zhang\n<az@astonzhang.com >.\n1https://github.com/amazon-science/mm-cot\nOptions:(B) salty(A) softOutputQuestion:Whichpropertydothesetwoobjectshaveincommon?Context: Select the better answer.\nRationale:Lookateachobject.Foreachobject,decideifithasthatproperty.Potatochipshaveasaltytaste.Bothobjectsaresalty.Asoftobjectchangesshapewhenyousqueezeit.Thefriesaresoft,butthecrackerisnot.Thepropertythatbothobjectshaveincommonissalty.Answer:Theansweris(B).VisionLanguageInputFigure 1. Example of the multimodal CoT task.\nCoT paradigm. Given the inputs in different modalities,\nMultimodal-CoT decomposes multi-step problems into in-\ntermediate reasoning steps (rationale) and then infers the\nanswer. Since vision and language are the most popular\nmodalities, we focus on those two modalities in this work.\nAn example is shown in Figure 1. In general, there are two\nways to elicit Multimodal-CoT reasoning as follows: (i)\nprompting LLMs and (ii) ne-tuning small models.2\nThe most immediate way to perform Multimodal-CoT is to\ntransform the input of different modalities into one modality\nand prompt LLMs to perform CoT. For example, it is possi-\nble to extract the caption of an image by a captioning model\nand then concatenate the caption with the original language\ninput to be fed into LLMs (Lu et al., 2022a). However, there\nis severe information loss in the captioning process; thus,\nusing the captions (as opposed to vision features) may suffer\nfrom a lack of mutual synergy in the representation space\nof different modalities.\nTo facilitate the interaction between modalities, another\npotential solution is to ne-tune smaller language models\n(LMs) by fusing multimodal features (Zhang et al., 2023).\nAs this approach allows the exibility of adjusting model\narchitectures to incorporate multimodal features, we study\nne-tuning models in this work instead of prompting LLMs.\nThe key challenge is that language models under 100 billion\nparameters tend to generate hallucinated rationales that mis-\nlead the answer inference (Ho et al., 2022; Magister et al.,\n2In this work, we refer to small models as models with less\nthan 1 billion parameters (hereinafter dubbed as 1B-models).arXiv:2302.00923v4  [cs.CL]  17 Feb 2023', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nTable 1. Typical CoT techniques (FT: ne-tuning; KD: knowledge distillation). Segment 1: in-context learning techniques; Segment 2:\nne-tuning techniques. To the best of our knowledge, our work is the rst to study CoT reasoning in different modalities. Besides, we\nfocus on 1B-models, without relying on the outputs of LLMs.\nModels Mutimodal w/o LLM Model / Engine Training CoT Role CoT Source\nZero-Shot-CoT (Kojima et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Template\nFew-Shot-CoT (Wei et al., 2022b) \x17 \x17 PaLM (540B) ICL Reasoning Hand-crafted\nSelf-Consistency-CoT (Wang et al., 2022a) \x17 \x17 Codex (175B) ICL Reasoning Hand-crafted\nLeast-to-Most Prompting (Zhou et al., 2022) \x17 \x17 Codex (175B) ICL Reasoning Hand-crafted\nRetrieval-CoT (Zhang et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Auto-generated\nPromptPG-CoT (Lu et al., 2022b) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Hand-crafted\nAuto-CoT (Zhang et al., 2022) \x17 \x17 Codex (175B) ICL Reasoning Auto-generated\nComplexity-CoT (Fu et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Hand-crafted\nFew-Shot-PoT (Chen et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Hand-crafted\nUniedQA (Lu et al., 2022a) \x17  T5 (770M) FT Explanation Crawled\nFine-Tuned T5 XXL (Magister et al., 2022) \x17 \x17 T5 (11B) KD Reasoning LLM-generated\nFine-Tune-CoT (Ho et al., 2022) \x17 \x17 GPT-3 (6.7B) KD Reasoning LLM-generated\nMultimodal-CoT (our work)   T5 (770M) FT Reasoning Crawled\n2022; Ji et al., 2022).\nTo mitigate the challenge of hallucination, we propose\nMultimodal-CoT that incorporates language (text) and vi-\nsion (images) modalities into a two-stage framework that\nseparates rationale generation and answer inference. In\nthis way, answer inference can leverage better generated\nrationales that are based on multimodal information. Our\nexperiments are conducted on the ScienceQA benchmark\n(Lu et al., 2022a), which is the latest multimodal reasoning\nbenchmark with annotated reasoning chains. Experimental\nresults show that our method surpasses the previous state-of-\nthe-art GPT-3.5 model by +16% (75.17% 91.68%) on the\nbenchmark. Our contributions are summarized as follows:\n(i) To the best of our knowledge, this work is the rst to\nstudy CoT reasoning in different modalities.\n(ii) We propose a two-stage framework by ne-tuning lan-\nguage models to fuse vision and language representations\nto perform Multimodal-CoT. The model is able to generate\ninformative rationales to facilitate inferring nal answers.\n(iii) Our method achieves new state-of-the-art performance\non the ScienceQA benchmark, outperforming accuracy of\nGPT-3.5 by 16% and even surpassing human performance.\n2. Background\nThis section reviews recent progress of eliciting CoT rea-\nsoning by prompting and ne-tuning language models.\n2.1. CoT Reasoning with LLMs\nRecently, CoT has been widely used to elicit the multi-step\nreasoning abilities of LLMs (Wei et al., 2022b). Concretely,\nCoT techniques encourage the LLM to generate intermedi-\nate reasoning chains for solving a problem. Studies have\nshown that LLMs can perform CoT reasoning with two ma-\njor paradigms of techniques: Zero-Shot-CoT (Kojima et al.,2022) and Few-Shot-CoT (Wei et al., 2022b; Zhang et al.,\n2022). For Zero-Shot-CoT, Kojima et al. (2022) showed that\nLLMs are decent zero-shot reasoners by adding a prompt\nlike Lets think step by step after the test question to in-\nvoke CoT reasoning. For Few-Shot-CoT, a few step-by-step\nreasoning demonstrations are used as conditions for infer-\nence. Each demonstration has a question and a reasoning\nchain that leads to the nal answer. The demonstrations are\ncommonly obtained by hand-crafting or automatic gener-\nation. The corresponding techniques are thus referred to\nas Manual-CoT (Wei et al., 2022b) and Auto-CoT (Zhang\net al., 2022).\nWith effective demonstrations, Few-Shot-CoT often\nachieves stronger performance than Zero-Shot-CoT and has\nattracted more research interest. Therefore, most recent\nstudies focused on how to improve Few-Shot-CoT. Those\nstudies are categorized into two major research lines: (i)\noptimizing the demonstrations; (ii) optimizing the reasoning\nchains. Table 1 compares typical CoT techniques.\nOptimizing Demonstrations The performance of Few-\nShot-CoT relies on the quality of demonstrations. As re-\nported in Wei et al. (2022b), using demonstrations written\nby different annotators results in dramatic accuracy dispar-\nity in a symbolic reasoning task. Beyond hand-crafting the\ndemonstrations, recent studies have investigated ways to op-\ntimize the demonstration selection process. Notably, Rubin\net al. (2022) retrieved the semantically similar demonstra-\ntions with the test instance. However, this approach shows\na degraded performance when there are mistakes in the rea-\nsoning chains (Zhang et al., 2022). To address the limitation,\nZhang et al. (2022) found that the key is the diversity of\ndemonstration questions and proposed Auto-CoT: (i) par-\ntition questions of a given dataset into a few clusters; (ii)\nsample a representative question from each cluster and gen-\nerate its reasoning chain using Zero-Shot-CoT with simple\nheuristics. In addition, reinforcement learning (RL) and', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\ncomplexity-based selection strategies were also proposed\nto obtain effective demonstrations. Fu et al. (2022) chose\nexamples with complex reasoning chains (i.e., with more\nreasoning steps) as the demonstrations. Lu et al. (2022b)\ntrained an agent to nd optimal in-context examples from\na candidate pool and maximize the prediction rewards on\ngiven training examples when interacting with GPT-3.5.\nOptimizing Reasoning Chains A notable way to opti-\nmize reasoning chains is problem decomposition. Zhou\net al. (2022) proposed least-to-most prompting to decom-\npose complex problems into sub-problems and then solve\nthese sub-problems sequentially. As a result, solving a\ngiven sub-problem is facilitated by the answers to previ-\nously solved sub-problems. Similarly, Khot et al. (2022)\nused diverse decomposition structures and designed differ-\nent prompts to answer each sub-question. In addition to\nprompting the reasoning chains as natural language texts,\nChen et al. (2022) proposed program-of-thoughts (PoT),\nwhich modeled the reasoning process as a program and\nprompted LLMs to derive the answer by executing the gen-\nerated programs. Another trend is to vote over multiple\nreasoning paths for a test question. Wang et al. (2022a)\nintroduced a self-consistency decoding strategy to sample\nmultiple outputs of LLMs and then took a majority over\nthe nal answers. Wang et al. (2022b) and Li et al. (2022b)\nintroduced randomness in the input space to produce more\ndiverse outputs for voting.\n2.2. Eliciting CoT Reasoning by Fine-Tuning Models\nA recent interest is eliciting CoT reasoning by ne-tuning\nlanguage models. Lu et al. (2022a) ne-tuned the encoder-\ndecoder T5 model on a large-scale dataset with CoT annota-\ntions. However, a dramatic performance decline is observed\nwhen using CoT to infer the answer, i.e., generating the rea-\nsoning chain before the answer (reasoning). Instead, CoT\nis only used as an explanation after the answer. Magister\net al. (2022) and Ho et al. (2022) employed knowledge\ndistillation by ne-tuning a student model on the chain-of-\nthought outputs generated by a larger teacher model. The\nproposed methods showed performance gains in arithmetic,\ncommonsense, and symbolic reasoning tasks.\nThere is a key challenge in training 1B-models to be CoT\nreasoners. As observed by Wei et al. (2022b), models un-\nder 100 billion parameters tend to produce illogical CoT\nthat leads to wrong answers. In other words, it might be\nharder for 1B-models to generate effective CoT than directly\ngenerating the answer. It becomes even more challenging\nin a multimodal setting where answering the question also\nrequires understanding the multimodal inputs. In the follow-\ning part, we will explore the challenge of Multimodal-CoT\nand investigate how to perform effective multi-step reason-\ning.3. Challenge of Multimodal-CoT\nExisting studies have suggested that the CoT reasoning abil-\nity may emerge in language models at a certain scale, e.g.,\nover 100 billion parameters (Wei et al., 2022a). However,\nit remains an unresolved challenge to elicit such reasoning\nabilities in 1B-models, let alone in the multimodal scenario.\nThis work focuses on 1B-models as they can be ne-tuned\nand deployed with consumer-grade GPUs (e.g., 32G mem-\nory). In this section, we will investigate why 1B-models\nfail at CoT reasoning and study how to design an effective\napproach to overcome the challenge.\n3.1. Towards the Role of CoT\nTo begin with, we ne-tune a text-only baseline for CoT rea-\nsoning on the ScienceQA benchmark (Lu et al., 2022a).\nFollowing Lu et al. (2022a), we adopt UniedQA Base\n(Khashabi et al., 2020) as the backbone language model.3\nOur task is modeled as a text generation problem, where the\nmodel takes the textual information as the input and gener-\nates the output sequence that consists of the rationale and\nthe answer. As an example shown in Figure 1, the model\ntakes the concatenation of tokens of the question text (Q),\nthe context text (C), and multiple options (M) as the input.\nTo study the effect of CoT, we compare the performance\nwith three variants: (i) No-CoT which predicts the answer\ndirectly (QCMA); (ii) Reasoning where answer inference\nis conditioned to the rationale (QCM RA); (iii) Explana-\ntion where the rationale is used for explaining the answer\ninference (QCMAR).\nTable 2. Effects of CoT in the one-stage setting.\nMethod Format Accuracy\nNo-CoT QCM A 80.40\nReasoning QCM RA 67.86\nExplanation QCM AR 69.77\nSurprisingly, we observe a 12.54% accuracy decrease\n(80.40%67.86%) if the model predicts rationales before\nanswers (QCMRA). The results imply that the rationales\nmight not necessarily contribute to predicting the right an-\nswer. A similar phenomenon was observed in Lu et al.\n(2022a), where the plausible reason might be that the model\nexceeds the maximum token limits before obtaining the\nrequired answer or stops generating the prediction early.\nHowever, we nd that the maximum length of the gener-\nated outputs (RA) is always less than 400 tokens, which\nis below the length limit of language models (i.e., 512 in\nUniedQA Base). Therefore, it deserves a more in-depth\ninvestigation into why the rationales harm answer inference.\n3UniedQA (Khashabi et al., 2020) is adopted as it is the best\nne-tuning model in Lu et al. (2022a). Model information and\nimplementation details are presented in Appendix B.1.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nGeneratedRationale:Magnetscanpullorpushoneachotherwithouttouching.Whenmagnetsattract,theypulltogether.Whenmagnetsrepel,theypushapart.Whetheramagnetattractsorrepelsothermagnetsdependsonthepositionsofitspoles,orends.Everymagnethastwopoles,callednorthandsouth.Herearesomeexamplesofmagnets.ThenorthpoleofeachmagnetismarkedN,andthesouthpoleismarkedS.Ifdifferentpolesareclosesttoeachother,themagnetsattract.Themagnetsinthepairbelowattract.Ifthesamepolesareclosesttoeachother,themagnetsrepel.Themagnetsinbothpairsbelowrepel.Willthesemagnetsattractorrepel?Tofindout,lookatwhichpolesareclosesttoeachother.Thesouthpoleofonemagnetisclosesttothesouthpoleoftheothermagnet.Polesthatarethesamerepel.So,thesemagnetswillrepeleachother.Answer:Theansweris(B).Options:(B) repel(A) attractProblem\nBaselineQuestion:Willthesemagnetsattractorrepeleachother?Context:Twomagnetsareplacedasshown.Hint:Magnetsthatattractpulltogether.Magnetsthatrepelpushapart.GoldRationale:Magnetscanpullorpushoneachotherwithouttouching.Whenmagnetsattract,theypulltogether.Whenmagnetsrepel,theypushapart.Whetheramagnetattractsorrepelsothermagnetsdependsonthepositionsofitspoles,orends.Everymagnethastwopoles,callednorthandsouth.Herearesomeexamplesofmagnets.ThenorthpoleofeachmagnetismarkedN,andthesouthpoleismarkedS.Ifdifferentpolesareclosesttoeachother,themagnetsattract.Themagnetsinthepairbelowattract.Ifthesamepolesareclosesttoeachother,themagnetsrepel.Themagnetsinbothpairsbelowrepel.Willthesemagnetsattractorrepel?Tofindout,lookatwhichpolesareclosesttoeachother.Thenorthpoleofonemagnetisclosesttothesouthpoleoftheothermagnet.Polesthataredifferentattract.So,thesemagnetswillattracteachother.Answer:Theansweris(A).GeneratedRationale:Magnetscanpullorpushoneachotherwithouttouching.Whenmagnetsattract,theypulltogether.Whenmagnetsrepel,theypushapart.Whetheramagnetattractsorrepelsothermagnetsdependsonthepositionsofitspoles,orends.Everymagnethastwopoles,callednorthandsouth.Herearesomeexamplesofmagnets.ThenorthpoleofeachmagnetismarkedN,andthesouthpoleismarkedS.Ifdifferentpolesareclosesttoeachother,themagnetsattract.Themagnetsinthepairbelowattract.Ifthesamepolesareclosesttoeachother,themagnetsrepel.Themagnetsinbothpairsbelowrepel.Willthesemagnetsattractorrepel?Tofindout,lookatwhichpolesareclosesttoeachother.Thenorthpoleofonemagnetisclosesttothesouthpoleoftheothermagnet.Polesthataredifferentattract.So,thesemagnetswillattracteachother.Answer:Theansweris(A).+ Vision Features\nVision\nFigure 2. Example of the two-stage framework without vision features (baseline) and with vision features (ours) for generating rationales\nand predicting answers. The upper part presents the problem details with a gold rationale, and the lower part shows the outputs of the\nbaseline and our method incorporated with vision features. We observe that the baseline fails to predict the right answer due to the\nmisleading by hallucinated rationales. More examples are shown in Appendix A.1.\n3.2. Misleading by Hallucinated Rationales\nTo dive into how the rationales affect the answer prediction,\nwe separate the CoT problem into two stages, rationale\ngeneration andanswer inference . We report the RougeL\nscore and accuracy for the rationale generation and answer\ninference, respectively. Table 3 shows the results based\non the two-stage framework. Although the two-stage base-\nline model achieves a 91.76 RougeL score of the rationale\ngeneration, the answer inference accuracy is only 70.53%.\nCompared with the QCM A variant (80.40%) in Table 2,\nthe result shows that the generated rationale in the two-stage\nframework does not improve answer accuracy.\nTable 3. Two-stage setting of (i) rationale generation (RougeL) and\n(ii) answer inference (Accuracy).\nMethod (i) QCM R (ii) QCMRA\nTwo-Stage Framework 91.76 70.53\nw/ Captions 91.85 71.12\nw/ Vision Features 96.97 84.91\nThen, we randomly sample 50 error cases and nd that the\nmodel tends to generate hallucinated rationales that mislead\nthe answer inference. As an example shown in Figure 2, the\nmodel (left part) hallucinates that,  The south pole of one\nmagnet is closest to the south pole of the other magnet , due\nto the lack of reference to the vision content. We nd that\nsuch mistakes occur at a ratio of 64% among the error cases\nOthers(36%)Resolved (62.5%)Unresolved(37.5%)Hallucination(64%)(a) ratio of hallucination mistakes(b) correction rate w/ vision features  Figure 3. The ratio of hallucination mistakes (a) and correction\nrate w/ vision features (b).\n(Figure 3(a)).\n3.3. Multimodality Contributes to Effective Rationales\nWe speculate that such a phenomenon of hallucination is\ndue to a lack of necessary vision contexts for performing\neffective Multimodal-CoT. To inject vision information, a\nsimple way is to transform the paired image into a caption\n(Lu et al., 2022a) and then append the caption in the input of\nboth stages. However, as shown in Table 3, using captions\nonly yields marginal performance gains ( 0.59%). Then,\nwe explore an advanced technique by incorporating vision\nfeatures into the language model. Concretely, we feed the\npaired image to the DETR model (Carion et al., 2020) to\nextract vision features. Then we fuse the vision features', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nVisionLanguageRationale GenerationQuestion:Whichpropertydothesetwoobjectshaveincommon?Context: Select the better answer.Lookateachobject.Foreachobject,decideifithasthatproperty.Potatochipshaveasaltytaste.Bothobjectsaresalty.Asoftobjectchangesshapewhenyousqueezeit.Thefriesaresoft,butthecrackerisnot.Thepropertythatbothobjectshaveincommonissalty.\nOptions:(B) salty(A) softRationaleAnswer InferenceTheansweris(B).Answer\nFigure 4. Overview of our Multimodal-CoT framework. Multimodal-CoT consists of two stages: (i) rationale generation and (ii) answer\ninference. Both stages share the same model architecture but differ in the input and output. In the rst stage, we feed the model with\nlanguage and vision inputs to generate rationales. In the second stage, we append the original language input with the rationale generated\nfrom the rst stage. Then, we feed the updated language input with the original vision input to the model to infer the answer.\nwith the encoded language representations before feeding\nto the decoder (more details will be presented in Section\n4). Interestingly, with vision features, the RougeL score of\nthe rationale generation has boosted to 96.97% (QCM R),\nwhich correspondingly contributes to better answer accuracy\nof 84.91% (QCMR A). With those effective rationales,\nthe phenomenon of hallucination is mitigated  62.5%\nhallucination mistakes in Section 3.2 have been corrected\n(Figure 3(b)), as an example shown in Figure 2 (right part).4\nThe analysis so far compellingly shows that vision features\nare indeed benecial for generating effective rationales and\ncontributing to accurate answer inference. As the two-stage\nmethod (QCMRA) in Table 3 achieves better performance\nthan all the one-stage method in Table 2, we choose the two-\nstage method in our Multimodal-CoT framework.\n4. Multimodal-CoT\nBased on the observations and discussions in Section 3, we\npropose Multimodal-CoT to incorporate language (text) and\nvision (images) modalities into a two-stage framework. In\nthis section, we will rst overview the procedure of the\nframework and then elaborate on the technical design of the\nmodel architecture.\n4.1. Framework Overview\nMultimodal-CoT consists of two training stages: (i) ratio-\nnale generation and (ii) answer inference. Both stages share\nthe same model architecture but differ in the input Xand\noutputY. The overall procedure is illustrated in Figure 4.\nWe will take vision-language as an example to show how\nMultimodal-CoT works.\n4The left mistakes are mainly about map understanding, which\nrequires more advanced vision features. We will discuss them in\nSection 6.4.In the rationale generation stage, we feed the model with\nX={X1\nlanguage,Xvision}whereX1\nlanguage represents the lan-\nguage input in the rst stage and Xvision represents the vision\ninput, i.e., the image. For example, Xcan be instantiated as\na concatenation of question, context, and options of a multi-\nple choice reasoning problem (Lu et al., 2022a) as shown in\nFigure 4. The goal is to learn a rationale generation model\nR=F(X)whereRis the rationale.\nIn the answer inference stage, the rationale Ris appended\nto the original language input X1\nlanguage to construct the lan-\nguage input in the second stage, X2\nlanguage =X1\nlanguageR\nwheredenotes concatenation. Then, we feed the updated\ninputX={X2\nlanguage,Xvision}to the answer inference\nmodel to infer the nal answer A=F(X).\nIn both stages, we train two models with the same archi-\ntecture independently. They take the annotated elements\n(e.g.,XR,XRA, respectively) from the training\nset for supervised learning. During inference, given X, the\nrationales for the test sets are generated using the model\ntrained in the rst stage; they are used in the second stage\nfor answer inference.\n4.2. Model Architecture\nGiven the language input Xlanguage{X1\nlanguage,X2\nlanguage}\nand the vision input Xvision, we compute the probability of\ngenerating target text Y(either the rationale or the answer\nin Figure 4) of length Nby\np(Y|Xlanguage,Xvision ) =N\ni=1p(Yi|Xlanguage,Xvision,Y<i),\n(1)\nwherep(Yi|Xlanguage,Xvision,Y<i)is implemented with\na Transformer-based network (Vaswani et al., 2017). The\nnetwork has three major procedures: encoding, interaction,', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nAlgorithm 1 Multimodal-CoT\nInput: Language input X1\nlanguage , vision input Xvision\nOutput: Generated rationale R, inferred answer A\n1: Construct the input X={Xlanguage,X vision}\n2: Generate rationale R=F(X)using the model F()\n3:Append the rationale Rto the original language input\nX2\nlanguage =X1\nlanguageR.\n4: Construct new input X={X2\nlanguage,X vision}\n5:Infer the answer Aby conditioning on the new input, A=\nF(X).\n6:procedure F(X)\n7: Encode the language and vision inputs Hlanguage andHvision,\nrespectively\n8: Build the interaction between language and vision features\nby attention Hattn\nvision\n9: FuseHlanguage andHattn\nvision by a gated fusion mechanism to\nhaveHfuse\n10: FeedHfuseto the decoder to obtain the target prediction Y\n11: return Y\n12:end procedure\nand decoding. Specically, we feed the language text into\na Transformer encoder to obtain a textual representation,\nwhich is then interacted and fused with the vision represen-\ntation before being fed into the Transformer decoder.\nEncoding The modelF(X)takes both the language and\nvision inputs and obtains the text representation Hlanguage\nand the image feature Hvision by the following functions:\nHlanguage =LanguageEncoder (Xlanguage ), (2)\nHvision =WhVisionExtractor (Xvision ),(3)\nwhere LanguageEncoder( ) is implemented as a Trans-\nformer model. We use the hidden states of the last layer\nin the Transformer encoder as the language representation\nHlanguageRndwherendenotes the length of the lan-\nguage input, and dis the hidden dimension. Meanwhile,\nVisionExtractor() is used to vectorize the input image into\nvision features. Inspired by the recent success of Vision\nTransformers (Dosovitskiy et al., 2021), we fetch the patch-\nlevel features by off-the-shelf vision extraction models,5\nsuch as DETR (Carion et al., 2020). After obtaining the\npatch-level vision features, we apply a learnable projection\nmatrixWhto convert the shape of VisionExtractor (Xvision )\ninto that ofHlanguage ; thus we have HvisionRmdwhere\nmis the number of patches.\nInteraction After obtaining language and vision represen-\ntations, we use a single-head attention network to correlate\ntext tokens with image patches, where the query ( Q), key\n(K) and value (V) areHlanguage ,Hvision andHvision, respec-\n5The parameters of the vision extraction are frozen.tively. The attention output Hattn\nvisionRndis dened as:\nHattn\nvision =Softmax (QK\ndk)V, (4)\nwheredkis the same as the dimension of Hlanguage because\na single head is used.\nThen, we apply the gated fusion mechanism (Zhang et al.,\n2020; Wu et al., 2021; Li et al., 2022a) to fuse Hlanguage and\nHvision. The fused output HfuseRndis obtained by:\n=Sigmoid (WlHlanguage +WvHattn\nvision ),(5)\nHfuse = (1)Hlanguage +Hattn\nvision, (6)\nwhereWlandWvare learnable parameters.\nDecoding Finally, the fused output Hfuseis fed into the\nTransformer decoder to predict the target Y. The complete\nprocedure of Multimodal-CoT is shown in Algorithm 1.\n5. Experiments\nThis section will present the benchmark dataset, the imple-\nmentation of our technique, and the baselines for compar-\nisons. Then, we will report our main results and ndings.\n5.1. Dataset\nOur method is evaluated on the ScienceQA benchmark (Lu\net al., 2022a). ScienceQA is the rst large-scale multimodal\nscience question dataset that annotates the answers with de-\ntailed lectures and explanations. It contains 21k multimodal\nmultiple choice questions with rich domain diversity across\n3 subjects, 26 topics, 127 categories, and 379 skills. The\nbenchmark dataset is split into training, validation, and test\nsplits with 12726, 4241, and 4241 examples, respectively.\n5.2. Implementation\nThe following part presents the experimental settings of\nMultimodal-CoT and the baseline methods.\nExperimental Settings As the Multimodal-CoT task re-\nquires generating the reasoning chains and leveraging the\nvision features, we use the T5 encoder-decoder architec-\nture (Raffel et al., 2020). Specically, we adopt UniedQA\n(Khashabi et al., 2020) to initialize our models in the two\nstages because it achieves the best ne-tuning results in\nLu et al. (2022a). To verify the generality of our approach\nacross different LMs, we also employ FLAN-T5 (Chung\net al., 2022) as the backbone in Section 6.3. As using im-\nage captions does not yield signicant performance gains in\nSection 3.3, we did not use the captions. We ne-tune the\nmodels up to 20 epochs, with a learning rate of 5e-5. The', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}]: %s
2023-12-07 11:04:25,696 - INFO - Check the data that is being passed [{'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nTable 4. Main results (%). Size = backbone model size. Question classes: NAT = natural science, SOC = social science, LAN = language\nscience, TXT = text context, IMG = image context, NO = no context, G1-6 = grades 1-6, G7-12 = grades 7-12. Results except ours are\ntaken from Lu et al. (2022a). Segment 1: Human performance; Segment 2: VQA baselines; Segment 3: UniedQA baselines; Segment 4:\nGPT-3.5 baselines; Segment 5: Our Multimodal-CoT results. Results in bold are the best performance.\nModel Size NAT SOC LAN TXT IMG NO G1-6 G7-12 Avg\nHuman -90.23 84.97 87.48 89.60 87.50 88.10 91.59 82.42 88.40\nMCAN (Yu et al., 2019) 95M 56.08 46.23 58.09 59.43 51.17 55.40 51.65 59.72 54.54\nTop-Down (Anderson et al., 2018) 70M 59.50 54.33 61.82 62.90 54.88 59.79 57.27 62.16 59.02\nBAN (Kim et al., 2018) 112M 60.88 46.57 66.64 62.61 52.60 65.51 56.83 63.94 59.37\nDFAF (Gao et al., 2019) 74M 64.03 48.82 63.55 65.88 54.49 64.11 57.12 67.17 60.72\nViLT (Kim et al., 2021) 113M 60.48 63.89 60.27 63.20 61.38 57.00 60.72 61.90 61.14\nPatch-TRM (Lu et al., 2021) 90M 65.19 46.79 65.55 66.96 55.28 64.95 58.04 67.50 61.42\nVisualBERT (Li et al., 2019) 111M 59.33 69.18 61.18 62.71 62.17 58.54 62.96 59.92 61.87\nUniedQA Base (Khashabi et al., 2020) 223M 68.16 69.18 74.91 63.78 61.38 77.84 72.98 65.00 70.12\nUniedQA Base w/ CoT (Lu et al., 2022a) 223M 71.00 76.04 78.91 66.42 66.53 81.81 77.06 68.82 74.11\nGPT-3.5 (Chen et al., 2020) 175B 74.64 69.74 76.00 74.44 67.28 77.42 76.80 68.89 73.97\nGPT-3.5 w/ CoT (Lu et al., 2022a) 175B 75.44 70.87 78.09 74.68 67.43 79.93 78.23 69.68 75.17\nMutimodal-CoT Base 223M 87.52 77.17 85.82 87.88 82.90 86.83 84.65 85.37 84.91\nMutimodal-CoT Large 738M 95.91 82.00 90.82 95.26 88.80 92.89 92.44 90.31 91.68\nTable 5. Ablation results of Multimodal-CoT.\nModel NAT SOC LAN TXT IMG NO G1-6 G7-12 Avg\nMultimodal-CoT 87.52 77.17 85.82 87.88 82.90 86.83 84.65 85.37 84.91\nw/o Two-Stage Framework 80.99 87.40 81.91 80.25 78.83 83.62 82.78 82.20 82.57\nw/o Vision Features 71.09 70.75 69.18 71.16 65.84 71.57 71.00 69.68 70.53\nmaximum input sequence length is 512. The batch sizes for\nthe base and large models are 16 and 8, respectively. Our\nexperiments are run on 4 NVIDIA Tesla V100 32G GPUs.\nBaseline Models Following Lu et al. (2022a), our base-\nlines include (i) Visual question answering (VQA) models\n(Anderson et al., 2018; Kim et al., 2018; Yu et al., 2019;\nGao et al., 2019; Kim et al., 2021; Lu et al., 2021; Li et al.,\n2019); (ii) Text-to-text LM models. (Khashabi et al., 2020);\n(iii) GPT-3.5 models (Chen et al., 2020). More details are\npresented in Appendix B.1.\n5.3. Main Results\nTable 4 shows the main results. Mutimodal-CoT Large out-\nperforms GPT-3.5 by 16.51% (75.17% 91.68%) and sur-\npasses human performance. Specically, among the 8\nquestion classes, Mutimodal-CoT Large achieves a 21.37%\n(67.43%88.80%) performance gain for the questions with\npaired images (IMG). Compared with existing UniedQA\nand GPT-3.5 methods that leverage image captions in the\ncontext to provide vision semantics, the results indicate that\nusing image features is more effective. In addition, our\ntwo-stage framework contributes to the superior results ac-\ncording to our ablation study results in Table 5. Overall,\nthe results verify the effectiveness of multimodality and the\npotential of achieving CoT reasoning with 1B-models via\nour two-stage framework.12345678910405060708090\nEpochAccuracyOne-stage Baseline One-stage Multimodal\nTwo-Stage Baseline Two-Stage Multimodal\nFigure 5. Accuracy curve of the No-CoT baseline and Multimodal-\nCoT variants across epochs.\n6. Analysis\nThe following analysis will investigate how Multimodal-\nCoT works and discuss contribution factors and limitations.\nWe use models under the base size for analysis unless\notherwise stated.\n6.1. Multimodality Boosts Convergence\nFigure 5 shows the evaluation accuracy curve of the baseline\nand Multimodal-CoT in different training epochs. One-\nstage is based on the QCM A input-output format as it', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nachieves the best performance in Table 2 and Two-stage\nis our two-stage framework. We nd that the two-stage\nmethods achieve relatively higher accuracy at the beginning\nthan the one-stage baselines that generate the answer directly\nwithout CoT. However, without the vision features, the two-\nstage baseline could not yield better results as the training\ngoes on due to the low-quality rationales (as observed in\nSection 3). In contrast, using vision features helps generate\nmore effective rationales that contribute to better answer\naccuracy in our two-stage multimodal variant.\n6.2. Using Different Vision Features\nDifferent vision features may affect the model performance.\nWe compare three widely-used types of vision features,\nCLIP (Radford et al., 2021), DETR (Carion et al., 2020),\nand ResNet (He et al., 2016). CLIP and DETR are patch-like\nfeatures where DETR is based on object detection. For the\nResNet features, we repeat the pooled features of ResNet-\n50 to the same length with the text sequence to imitate the\npatch-like features, where each patch is the same as the\npooled image features. More details of the vision features\nare presented in Appendix B.2.\nTable 6. Accuracy (%) of using different vision features.\nMethod One-stage Two-Stage\nw/ CLIP 81.21 84.81\nw/ DETR 82.57 84.91\nw/ ResNet 80.97 84.77\nTable 6 shows the comparative results of vision features. We\nobserve that using vision features generally achieves better\nperformance than the language only baseline. Specically,\nDETR achieves relatively better performance in general.\nTherefore, we use DETR by default in Multimodal-CoT.\n6.3. General Effectiveness Across Backbone Models\nTo test the generality of the benets of our approach to\nother backbone models, we alter the underlying LMs to\nother variants in different sizes or types. As shown in Table\n7, our approach is generally effective for the widely-used\nbackbone models.\nTable 7. Accuracy (%) with different backbone language models.\nMethod Size Language Only Mutimodal-CoT\nUniedQA Base 223M 80.40 84.91\nUniedQA Large 738M 83.60 91.68\nFLAN-T5 Base 248M 83.42 85.85\nFLAN-T5 Large 783M 85.19 93.02\n6.4. Error Analysis\nTo better understand the behavior of Multimodal-CoT and\nfacilitate future studies, we manually investigate randomly\nselected examples generated by our approach. Table 8 sum-marizes the categorization results generated by Multimodal-\nCoT. We randomly picked up 50 samples whose answers\nwere correct and 50 samples whose answers were incor-\nrect. The corresponding examples from each category are\npresented in Appendix C.\nTable 8. Categorization analysis of Multimodal-CoT.\nAnswer CoT Category Percentage (%)\nCorrectCoT is correct 90\nCoT is incorrect 10\nIncorrectCommonsense Mistake 82\nLogical Mistake 12\nCoT is correct 6\nWe nd that the correct samples (i.e., whose answers are cor-\nrect) contain a certain amount of incorrect chain-of-thought\n(10%). The results indicate that CoT may not always benet\nthe answer inference, and the model is robust to some extent\n it can predict the correct answer by ignoring incorrect\nrationales. For incorrect samples (i.e., whose answers are\nincorrect), commonsense mistake in the CoT is the most\nfrequent error type (88%). The model often makes com-\nmonsense mistakes when answering the questions requires\ncommonsense knowledge, e.g., understand maps and count-\ning numbers in the images (Figure 9), and utilizing the\nalphabet (Figure 10). The other type of mistake is a logical\nmistake (12%), with contradictions in the reasoning chains\n(Figure 11). In addition, there are cases with incorrect an-\nswers while their CoT are correct (6%) but might not be\nnecessarily related to answer options (Figure 12).\nThe analysis indicates that there are prospective directions\nfor future studies. It is possible to improve Multimodal-\nCoT by (i) incorporating more informative vision features\nand improving language-vision interaction to be capable of\nunderstanding maps and counting numbers; (ii) injecting\ncommonsense knowledge; (iii) applying a ltering mecha-\nnism, e.g., using only the effective CoT to infer the answer\nand get rid of irrelevant CoT.\n7. Conclusion\nWe formally study the problem of multimodal CoT. We pro-\npose Multimodal-CoT that incorporates language and vision\nmodalities into a two-stage framework that separates ratio-\nnale generation and answer inference, so answer inference\ncan leverage better generated rationales from multimodal in-\nformation. With Multimodal-CoT, we show that our method\nsurpasses GPT-3.5 by 16 percentage points in accuracy on\nthe ScienceQA benchmark. Our error analysis shows that\nit is the potential to leverage more effective vision features,\ninject commonsense knowledge, and apply ltering mecha-\nnisms to improve CoT reasoning in future studies.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nReferences\nAnderson, P., He, X., Buehler, C., Teney, D., Johnson, M.,\nGould, S., and Zhang, L. Bottom-up and top-down atten-\ntion for image captioning and visual question answering.\nIn2018 IEEE Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2018, Salt Lake City, UT, USA,\nJune 18-22, 2018 , pp. 60776086. IEEE Computer Soci-\nety, 2018. doi: 10.1109/CVPR.2018.00636.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,\nHenighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,\nJ., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,\nGray, S., Chess, B., Clark, J., Berner, C., McCandlish,\nS., Radford, A., Sutskever, I., and Amodei, D. Language\nmodels are few-shot learners. In Larochelle, H., Ranzato,\nM., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances\nin Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020.\nCarion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov,\nA., and Zagoruyko, S. End-to-end object detection with\ntransformers. In Computer VisionECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 2328, 2020,\nProceedings, Part I , pp. 213229, 2020.\nChen, T., Kornblith, S., Swersky, K., Norouzi, M., and\nHinton, G. E. Big self-supervised models are strong\nsemi-supervised learners. In Larochelle, H., Ranzato, M.,\nHadsell, R., Balcan, M., and Lin, H. (eds.), Advances\nin Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020.\nChen, W., Ma, X., Wang, X., and Cohen, W. W. Program\nof thoughts prompting: Disentangling computation from\nreasoning for numerical reasoning tasks. ArXiv preprint ,\nabs/2211.12588, 2022.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton,\nC., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,\nS., Maynez, J., Rao, A., Barnes, P., Tay, Y ., Shazeer,\nN., Prabhakaran, V ., Reif, E., Du, N., Hutchinson, B.,\nPope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G.,\nYin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S.,\nMichalewski, H., Garcia, X., Misra, V ., Robinson, K., Fe-\ndus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph,\nB., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal,\nS., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M.,\nLewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee,\nK., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O.,\nCatasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean,J., Petrov, S., and Fiedel, N. Palm: Scaling language mod-\neling with pathways. ArXiv preprint , abs/2204.02311,\n2022.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y .,\nFedus, W., Li, E., Wang, X., Dehghani, M., Brahma,\nS., et al. Scaling instruction-netuned language models.\narXiv preprint arXiv:2210.11416 , 2022.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\nHeigold, G., Gelly, S., et al. An image is worth 16x16\nwords: Transformers for image recognition at scale. In\nThe International Conference on Learning Representa-\ntions (ICLR) , 2021.\nFu, Y ., Peng, H., Sabharwal, A., Clark, P., and Khot, T.\nComplexity-based prompting for multi-step reasoning.\nArXiv preprint , abs/2210.00720, 2022.\nGao, P., Jiang, Z., You, H., Lu, P., Hoi, S. C. H., Wang, X.,\nand Li, H. Dynamic fusion with intra- and inter-modality\nattention ow for visual question answering. In IEEE\nConference on Computer Vision and Pattern Recognition,\nCVPR 2019, Long Beach, CA, USA, June 16-20, 2019 , pp.\n66396648. Computer Vision Foundation / IEEE, 2019.\ndoi: 10.1109/CVPR.2019.00680.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\nlearning for image recognition. In 2016 IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2016,\nLas Vegas, NV, USA, June 27-30, 2016 , pp. 770778.\nIEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.\n90.\nHo, N., Schmid, L., and Yun, S.-Y . Large language models\nare reasoning teachers. arXiv preprint arXiv:2212.10071 ,\n2022.\nJi, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y ., Ishii, E.,\nBang, Y ., Madotto, A., and Fung, P. Survey of halluci-\nnation in natural language generation. ACM Computing\nSurveys , 2022.\nKhashabi, D., Min, S., Khot, T., Sabharwal, A., Tafjord,\nO., Clark, P., and Hajishirzi, H. UNIFIEDQA: Crossing\nformat boundaries with a single QA system. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2020 , pp. 18961907, Online, 2020. Association\nfor Computational Linguistics. doi: 10.18653/v1/2020.\nndings-emnlp.171.\nKhot, T., Trivedi, H., Finlayson, M., Fu, Y ., Richardson, K.,\nClark, P., and Sabharwal, A. Decomposed prompting:\nA modular approach for solving complex tasks. ArXiv\npreprint , abs/2210.02406, 2022.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nKim, J., Jun, J., and Zhang, B. Bilinear attention networks.\nIn Bengio, S., Wallach, H. M., Larochelle, H., Grauman,\nK., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances\nin Neural Information Processing Systems 31: Annual\nConference on Neural Information Processing Systems\n2018, NeurIPS 2018, December 3-8, 2018, Montr eal,\nCanada , pp. 15711581, 2018.\nKim, W., Son, B., and Kim, I. Vilt: Vision-and-language\ntransformer without convolution or region supervision.\nInProceedings of the 38th International Conference on\nMachine Learning (ICML) , pp. 55835594, 2021.\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y ., and Iwasawa,\nY . Large language models are zero-shot reasoners. ArXiv\npreprint , abs/2205.11916, 2022.\nLi, B., Lv, C., Zhou, Z., Zhou, T., Xiao, T., Ma, A., and Zhu,\nJ. On vision features in multimodal machine translation.\nInProceedings of the 60th Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: Long\nPapers) , pp. 63276337, 2022a.\nLi, L. H., Yatskar, M., Yin, D., Hsieh, C.-J., and Chang,\nK.-W. Visualbert: A simple and performant baseline for\nvision and language. ArXiv preprint , abs/1908.03557,\n2019.\nLi, Y ., Lin, Z., Zhang, S., Fu, Q., Chen, B., Lou, J.-G., and\nChen, W. On the advance of making language models\nbetter reasoners. ArXiv preprint , abs/2206.02336, 2022b.\nLu, P., Qiu, L., Chen, J., Xia, T., Zhao, Y ., Zhang, W., Yu,\nZ., Liang, X., and Zhu, S.-C. Iconqa: A new benchmark\nfor abstract diagram understanding and visual language\nreasoning. In The 35th Conference on Neural Information\nProcessing Systems (NeurIPS) Track on Datasets and\nBenchmarks , 2021.\nLu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu,\nS.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to\nexplain: Multimodal reasoning via thought chains for sci-\nence question answering. ArXiv preprint , abs/2209.09513,\n2022a.\nLu, P., Qiu, L., Chang, K.-W., Wu, Y . N., Zhu, S.-C., Ra-\njpurohit, T., Clark, P., and Kalyan, A. Dynamic prompt\nlearning via policy gradient for semi-structured mathemat-\nical reasoning. ArXiv preprint , abs/2209.14610, 2022b.\nMagister, L. C., Mallinson, J., Adamek, J., Malmi, E., and\nSeveryn, A. Teaching small language models to reason.\nArXiv preprint , abs/2212.08410, 2022.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\net al. Learning transferable visual models from natural\nlanguage supervision. In International Conference on\nMachine Learning , pp. 87488763. PMLR, 2021.Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J.,\nSong, F., Aslanides, J., Henderson, S., Ring, R., Young,\nS., Rutherford, E., Hennigan, T., Menick, J., Cassirer, A.,\nPowell, R., Driessche, G. v. d., Hendricks, L. A., Rauh,\nM., Huang, P.-S., Glaese, A., Welbl, J., Dathathri, S.,\nHuang, S., Uesato, J., Mellor, J., Higgins, I., Creswell,\nA., McAleese, N., Wu, A., Elsen, E., Jayakumar, S.,\nBuchatskaya, E., Budden, D., Sutherland, E., Simonyan,\nK., Paganini, M., Sifre, L., Martens, L., Li, X. L., Kun-\ncoro, A., Nematzadeh, A., Gribovskaya, E., Donato, D.,\nLazaridou, A., Mensch, A., Lespiau, J.-B., Tsimpoukelli,\nM., Grigorev, N., Fritz, D., Sottiaux, T., Pajarskas, M.,\nPohlen, T., Gong, Z., Toyama, D., dAutume, C. d. M.,\nLi, Y ., Terzi, T., Mikulik, V ., Babuschkin, I., Clark, A.,\nCasas, D. d. L., Guy, A., Jones, C., Bradbury, J., Johnson,\nM., Hechtman, B., Weidinger, L., Gabriel, I., Isaac, W.,\nLockhart, E., Osindero, S., Rimell, L., Dyer, C., Vinyals,\nO., Ayoub, K., Stanway, J., Bennett, L., Hassabis, D.,\nKavukcuoglu, K., and Irving, G. Scaling language mod-\nels: Methods, analysis & insights from training gopher.\nArXiv preprint , abs/2112.11446, 2021.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring the\nlimits of transfer learning with a unied text-to-text trans-\nformer. Journal of Machine Learning Research (JMLR) ,\n21:167, 2020.\nRubin, O., Herzig, J., and Berant, J. Learning to re-\ntrieve prompts for in-context learning. In Proceedings\nof the 2022 Conference of the North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies , pp. 26552671, 2022. doi:\n10.18653/v1/2022.naacl-main.191.\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kul-\nshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L.,\nDu, Y ., Li, Y ., Lee, H., Zheng, H. S., Ghafouri, A., Mene-\ngali, M., Huang, Y ., Krikun, M., Lepikhin, D., Qin, J.,\nChen, D., Xu, Y ., Chen, Z., Roberts, A., Bosma, M.,\nZhao, V ., Zhou, Y ., Chang, C.-C., Krivokon, I., Rusch,\nW., Pickett, M., Srinivasan, P., Man, L., Meier-Hellstern,\nK., Morris, M. R., Doshi, T., Santos, R. D., Duke, T.,\nSoraker, J., Zevenbergen, B., Prabhakaran, V ., Diaz, M.,\nHutchinson, B., Olson, K., Molina, A., Hoffman-John, E.,\nLee, J., Aroyo, L., Rajakumar, R., Butryna, A., Lamm,\nM., Kuzmina, V ., Fenton, J., Cohen, A., Bernstein, R.,\nKurzweil, R., Aguera-Arcas, B., Cui, C., Croak, M., Chi,\nE., and Le, Q. Lamda: Language models for dialog\napplications. ArXiv preprint , abs/2201.08239, 2022.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need. In Guyon, I., von Luxburg, U., Bengio,\nS., Wallach, H. M., Fergus, R., Vishwanathan, S. V . N.,\nand Garnett, R. (eds.), Advances in Neural Information', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nProcessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA , pp. 59986008, 2017.\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E.,\nand Zhou, D. Self-consistency improves chain of\nthought reasoning in language models. ArXiv preprint ,\nabs/2203.11171, 2022a.\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and\nZhou, D. Rationale-augmented ensembles in language\nmodels. ArXiv preprint , abs/2207.00747, 2022b.\nWei, J., Tay, Y ., Bommasani, R., Raffel, C., Zoph, B.,\nBorgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Met-\nzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang,\nP., Dean, J., and Fedus, W. Emergent abilities of large\nlanguage models. Transactions on Machine Learning\nResearch , 2022a. Survey Certication.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E.,\nLe, Q., and Zhou, D. Chain of thought prompting elic-\nits reasoning in large language models. ArXiv preprint ,\nabs/2201.11903, 2022b.\nWu, Z., Kong, L., Bi, W., Li, X., and Kao, B. Good for\nmisconceived reasons: An empirical revisiting on the\nneed for visual context in multimodal machine transla-\ntion. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers) , pp. 61536166, Online,\n2021. Association for Computational Linguistics. doi:\n10.18653/v1/2021.acl-long.480.\nYu, Z., Yu, J., Cui, Y ., Tao, D., and Tian, Q. Deep modu-\nlar co-attention networks for visual question answering.\nInIEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2019, Long Beach, CA, USA, June\n16-20, 2019 , pp. 62816290. Computer Vision Founda-\ntion / IEEE, 2019. doi: 10.1109/CVPR.2019.00644.\nZhang, Z., Chen, K., Wang, R., Utiyama, M., Sumita, E., Li,\nZ., and Zhao, H. Neural machine translation with univer-\nsal visual representation. In 8th International Conference\non Learning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020 . OpenReview.net, 2020.\nZhang, Z., Zhang, A., Li, M., and Smola, A. Automatic\nchain of thought prompting in large language models.\nArXiv preprint , abs/2210.03493, 2022.\nZhang, Z., Chen, K., Wang, R., Utiyama, M., Sumita, E., Li,\nZ., and Zhao, H. Universal multimodal representation for\nlanguage understanding. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence , pp. 118, 2023. doi:\n10.1109/TPAMI.2023.3234170.Zhou, D., Sch arli, N., Hou, L., Wei, J., Scales, N., Wang,\nX., Schuurmans, D., Bousquet, O., Le, Q., and Chi, E.\nLeast-to-most prompting enables complex reasoning in\nlarge language models. ArXiv preprint , abs/2205.10625,\n2022.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nA. Extended analysis for the challenge of Multimodal-CoT\nA.1. More Examples of Misleading by Hallucinated Rationales\nAccording to our case studies (Section 3.2), we nd that the baseline tends to generate hallucinated rationales. We provide\nfurther examples as shown in Figure 6.\nOptions:(B) lichen(A) mushroomProblem\nBaselineQuestion:Basedonthearrows,whichofthefollowingorganismsisaconsumer?Context:BelowisafoodwebfromatundraecosysteminNunavut,aterritoryinNorthernCanada.Afoodwebmodelshowthemattereatenbyorganismsmovesthroughanecosystem.Thearrowsinafoodwebrepresenthowmattermovesbetweenorganismsinanecosystem.GoldRationale:AfoodwebisamodelAnorganisminafoodwebcanhavemorethanonearrowpointingfromit.Thisshowsthattheorganismiseatenbymorethanoneotherorganisminthefoodweb.Anorganisminafoodwebcanalsohavemorethanonearrowpointingtoit.Thisshowsthattheorganismeatsmorethanoneotherorganisminthefoodweb.Consumerseatotherorganisms.So,therearearrowsinafoodwebthatpointfromotherorganismstoconsumers.Themushroomhasarrowspointingtoitfromthebarren-groundcaribouandthegrizzlybear.So,themushroomisaconsumer.Thelichendoesnothaveanyarrowspointingtoit.So,thelichenisaproducer,notaconsumer.Answer:Theansweris(A).Rationale:AfoodwebisamodelThisshowsthattheorganismiseatenbymorethanoneotherorganisminthefoodweb.Anorganisminafoodwebcanalsohavemorethanonearrowpointingtoit.Thisshowsthattheorganismeatsmorethanoneotherorganisminthefoodweb.Consumerseatproducers.So,inafoodweb,consumershavearrowspointingtothemfromproducers.Themushroomhasanarrowpointingtoitfromthemushroom.Themushroomisnotaconsumer,sothemushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Answer:Theansweris(B).Rationale:AfoodwebisamodelAnorganisminafoodwebcanhavemorethanonearrowpointingfromit.Thisshowsthattheorganismiseatenbymorethanoneotherorganisminthefoodweb.Anorganisminafoodwebcanalsohavemorethanonearrowpointingtoit.Thisshowsthattheorganismeatsmorethanoneotherorganisminthefoodweb.Consumerseatproducersorganisms.So,inarearrowsafoodwebthatpointfromotherorganismstoconsumers.Themushroomhasarrowspointingtoitfromthen-browmushroomibou.thelichenzzlybear.So,themushroomisaconsumer.Thelichenhasnothaveanyarrowspointingtoit.So,themushroomisaconsumer.Lichenisnotaconsumer.Answer:Theansweris(B).Vision\n+ Vision Features\nOptions:(B) sample A(A) neither; the samples have the same temperatureProblem\nBaselineQuestion:Comparetheaveragekineticenergiesoftheparticlesineachsample.Whichsamplehasthehighertemperature?Context:Thediagramsbelowshowtwopuresamplesofgasinidenticalclosed,rigidcontainers.Eachcoloredballrepresentsonegasparticle.Bothsampleshavethesamenumberofparticles.GoldRationale:Thetemperatureofasubstancedependsontheaveragekineticenergyoftheparticlesinthesubstance.Thehighertheaveragekineticenergyoftheparticles,thehigherthetemperatureofthesubstance.Thekineticenergyofaparticleisdeterminedbyitsmassandspeed.Forapuresubstance,thegreaterthemassofeachparticleinthesubstanceandthehighertheaveragespeedoftheparticles,thehighertheiraveragekineticenergy.EachparticleinsampleAhasmoremassthaneachparticleinsampleB.TheparticlesinsampleAalso+ Vision FeaturesVision(C) sample B\nhaveahigheraveragespeedthantheparticlesinsampleB.So,theparticlesinsampleAhaveahigheraveragekineticenergythantheparticlesinsampleB.BecausetheparticlesinsampleAhavethehigheraveragekineticenergy,sampleAmusthavethehighertemperature.Answer:Theansweris(B).Rationale:Thetemperatureofasubstancedependsontheaveragekineticenergyoftheparticlesinthesubstance.Thehighertheaveragekineticenergyoftheparticles,thehigherthetemperatureofthesubstance.Thekineticenergyofaparticleisdeterminedbyitsmassandspeed.Forapuresubstance,thegreaterthemassofeachparticleinthesubstanceandthehighertheaveragespeedoftheparticles,thehighertheiraveragekineticenergy.EachparticleinsampleBhasmoremassthaneachparticleinsampleA.TheparticlesinsampleBalsohaveahigheraveragespeedthantheparticlesinsampleA.So,theparticlesinsampleBhaveahigheraveragekineticenergythantheparticlesinsampleA.BecausetheparticlesinsampleBhavethehigheraveragekineticenergy,sampleBmusthavethehighertemperature.Answer:Theansweris(C).Rationale:Thetemperatureofasubstancedependsontheaveragekineticenergyoftheparticlesinthesubstance.Thehighertheaveragekineticenergyoftheparticles,thehigherthetemperatureofthesubstance.Thekineticenergyofaparticleisdeterminedbyitsmassandspeed.Forapuresubstance,thegreaterthemassofeachparticleinthesubstanceandthehighertheaveragespeedoftheparticles,thehighertheiraveragekineticenergy.EachparticleinsampleAhasmoremassthaneachparticleinsampleB.TheparticlesinsampleAalsohaveahigheraveragespeedthantheparticlesinsampleB.So,theparticlesinsampleAhaveahigheraveragekineticenergythantheparticlesinsampleB.BecausetheparticlesinsampleAhavethehigheraveragekineticenergy,sampleAmusthavethehighertemperature.Answer:Theansweris(B).\nFigure 6. Examples of the two-stage framework without vision features (baseline) and with vision features (ours) for generating rationales\nand predicting answers. The upper part presents the problem details, and the lower part shows the outputs of the baseline and our method.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}]: %s
2023-12-07 11:04:25,697 - INFO - Check the results [{'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nTable 4. Main results (%). Size = backbone model size. Question classes: NAT = natural science, SOC = social science, LAN = language\nscience, TXT = text context, IMG = image context, NO = no context, G1-6 = grades 1-6, G7-12 = grades 7-12. Results except ours are\ntaken from Lu et al. (2022a). Segment 1: Human performance; Segment 2: VQA baselines; Segment 3: UniedQA baselines; Segment 4:\nGPT-3.5 baselines; Segment 5: Our Multimodal-CoT results. Results in bold are the best performance.\nModel Size NAT SOC LAN TXT IMG NO G1-6 G7-12 Avg\nHuman -90.23 84.97 87.48 89.60 87.50 88.10 91.59 82.42 88.40\nMCAN (Yu et al., 2019) 95M 56.08 46.23 58.09 59.43 51.17 55.40 51.65 59.72 54.54\nTop-Down (Anderson et al., 2018) 70M 59.50 54.33 61.82 62.90 54.88 59.79 57.27 62.16 59.02\nBAN (Kim et al., 2018) 112M 60.88 46.57 66.64 62.61 52.60 65.51 56.83 63.94 59.37\nDFAF (Gao et al., 2019) 74M 64.03 48.82 63.55 65.88 54.49 64.11 57.12 67.17 60.72\nViLT (Kim et al., 2021) 113M 60.48 63.89 60.27 63.20 61.38 57.00 60.72 61.90 61.14\nPatch-TRM (Lu et al., 2021) 90M 65.19 46.79 65.55 66.96 55.28 64.95 58.04 67.50 61.42\nVisualBERT (Li et al., 2019) 111M 59.33 69.18 61.18 62.71 62.17 58.54 62.96 59.92 61.87\nUniedQA Base (Khashabi et al., 2020) 223M 68.16 69.18 74.91 63.78 61.38 77.84 72.98 65.00 70.12\nUniedQA Base w/ CoT (Lu et al., 2022a) 223M 71.00 76.04 78.91 66.42 66.53 81.81 77.06 68.82 74.11\nGPT-3.5 (Chen et al., 2020) 175B 74.64 69.74 76.00 74.44 67.28 77.42 76.80 68.89 73.97\nGPT-3.5 w/ CoT (Lu et al., 2022a) 175B 75.44 70.87 78.09 74.68 67.43 79.93 78.23 69.68 75.17\nMutimodal-CoT Base 223M 87.52 77.17 85.82 87.88 82.90 86.83 84.65 85.37 84.91\nMutimodal-CoT Large 738M 95.91 82.00 90.82 95.26 88.80 92.89 92.44 90.31 91.68\nTable 5. Ablation results of Multimodal-CoT.\nModel NAT SOC LAN TXT IMG NO G1-6 G7-12 Avg\nMultimodal-CoT 87.52 77.17 85.82 87.88 82.90 86.83 84.65 85.37 84.91\nw/o Two-Stage Framework 80.99 87.40 81.91 80.25 78.83 83.62 82.78 82.20 82.57\nw/o Vision Features 71.09 70.75 69.18 71.16 65.84 71.57 71.00 69.68 70.53\nmaximum input sequence length is 512. The batch sizes for\nthe base and large models are 16 and 8, respectively. Our\nexperiments are run on 4 NVIDIA Tesla V100 32G GPUs.\nBaseline Models Following Lu et al. (2022a), our base-\nlines include (i) Visual question answering (VQA) models\n(Anderson et al., 2018; Kim et al., 2018; Yu et al., 2019;\nGao et al., 2019; Kim et al., 2021; Lu et al., 2021; Li et al.,\n2019); (ii) Text-to-text LM models. (Khashabi et al., 2020);\n(iii) GPT-3.5 models (Chen et al., 2020). More details are\npresented in Appendix B.1.\n5.3. Main Results\nTable 4 shows the main results. Mutimodal-CoT Large out-\nperforms GPT-3.5 by 16.51% (75.17% 91.68%) and sur-\npasses human performance. Specically, among the 8\nquestion classes, Mutimodal-CoT Large achieves a 21.37%\n(67.43%88.80%) performance gain for the questions with\npaired images (IMG). Compared with existing UniedQA\nand GPT-3.5 methods that leverage image captions in the\ncontext to provide vision semantics, the results indicate that\nusing image features is more effective. In addition, our\ntwo-stage framework contributes to the superior results ac-\ncording to our ablation study results in Table 5. Overall,\nthe results verify the effectiveness of multimodality and the\npotential of achieving CoT reasoning with 1B-models via\nour two-stage framework.12345678910405060708090\nEpochAccuracyOne-stage Baseline One-stage Multimodal\nTwo-Stage Baseline Two-Stage Multimodal\nFigure 5. Accuracy curve of the No-CoT baseline and Multimodal-\nCoT variants across epochs.\n6. Analysis\nThe following analysis will investigate how Multimodal-\nCoT works and discuss contribution factors and limitations.\nWe use models under the base size for analysis unless\notherwise stated.\n6.1. Multimodality Boosts Convergence\nFigure 5 shows the evaluation accuracy curve of the baseline\nand Multimodal-CoT in different training epochs. One-\nstage is based on the QCM A input-output format as it', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nachieves the best performance in Table 2 and Two-stage\nis our two-stage framework. We nd that the two-stage\nmethods achieve relatively higher accuracy at the beginning\nthan the one-stage baselines that generate the answer directly\nwithout CoT. However, without the vision features, the two-\nstage baseline could not yield better results as the training\ngoes on due to the low-quality rationales (as observed in\nSection 3). In contrast, using vision features helps generate\nmore effective rationales that contribute to better answer\naccuracy in our two-stage multimodal variant.\n6.2. Using Different Vision Features\nDifferent vision features may affect the model performance.\nWe compare three widely-used types of vision features,\nCLIP (Radford et al., 2021), DETR (Carion et al., 2020),\nand ResNet (He et al., 2016). CLIP and DETR are patch-like\nfeatures where DETR is based on object detection. For the\nResNet features, we repeat the pooled features of ResNet-\n50 to the same length with the text sequence to imitate the\npatch-like features, where each patch is the same as the\npooled image features. More details of the vision features\nare presented in Appendix B.2.\nTable 6. Accuracy (%) of using different vision features.\nMethod One-stage Two-Stage\nw/ CLIP 81.21 84.81\nw/ DETR 82.57 84.91\nw/ ResNet 80.97 84.77\nTable 6 shows the comparative results of vision features. We\nobserve that using vision features generally achieves better\nperformance than the language only baseline. Specically,\nDETR achieves relatively better performance in general.\nTherefore, we use DETR by default in Multimodal-CoT.\n6.3. General Effectiveness Across Backbone Models\nTo test the generality of the benets of our approach to\nother backbone models, we alter the underlying LMs to\nother variants in different sizes or types. As shown in Table\n7, our approach is generally effective for the widely-used\nbackbone models.\nTable 7. Accuracy (%) with different backbone language models.\nMethod Size Language Only Mutimodal-CoT\nUniedQA Base 223M 80.40 84.91\nUniedQA Large 738M 83.60 91.68\nFLAN-T5 Base 248M 83.42 85.85\nFLAN-T5 Large 783M 85.19 93.02\n6.4. Error Analysis\nTo better understand the behavior of Multimodal-CoT and\nfacilitate future studies, we manually investigate randomly\nselected examples generated by our approach. Table 8 sum-marizes the categorization results generated by Multimodal-\nCoT. We randomly picked up 50 samples whose answers\nwere correct and 50 samples whose answers were incor-\nrect. The corresponding examples from each category are\npresented in Appendix C.\nTable 8. Categorization analysis of Multimodal-CoT.\nAnswer CoT Category Percentage (%)\nCorrectCoT is correct 90\nCoT is incorrect 10\nIncorrectCommonsense Mistake 82\nLogical Mistake 12\nCoT is correct 6\nWe nd that the correct samples (i.e., whose answers are cor-\nrect) contain a certain amount of incorrect chain-of-thought\n(10%). The results indicate that CoT may not always benet\nthe answer inference, and the model is robust to some extent\n it can predict the correct answer by ignoring incorrect\nrationales. For incorrect samples (i.e., whose answers are\nincorrect), commonsense mistake in the CoT is the most\nfrequent error type (88%). The model often makes com-\nmonsense mistakes when answering the questions requires\ncommonsense knowledge, e.g., understand maps and count-\ning numbers in the images (Figure 9), and utilizing the\nalphabet (Figure 10). The other type of mistake is a logical\nmistake (12%), with contradictions in the reasoning chains\n(Figure 11). In addition, there are cases with incorrect an-\nswers while their CoT are correct (6%) but might not be\nnecessarily related to answer options (Figure 12).\nThe analysis indicates that there are prospective directions\nfor future studies. It is possible to improve Multimodal-\nCoT by (i) incorporating more informative vision features\nand improving language-vision interaction to be capable of\nunderstanding maps and counting numbers; (ii) injecting\ncommonsense knowledge; (iii) applying a ltering mecha-\nnism, e.g., using only the effective CoT to infer the answer\nand get rid of irrelevant CoT.\n7. Conclusion\nWe formally study the problem of multimodal CoT. We pro-\npose Multimodal-CoT that incorporates language and vision\nmodalities into a two-stage framework that separates ratio-\nnale generation and answer inference, so answer inference\ncan leverage better generated rationales from multimodal in-\nformation. With Multimodal-CoT, we show that our method\nsurpasses GPT-3.5 by 16 percentage points in accuracy on\nthe ScienceQA benchmark. Our error analysis shows that\nit is the potential to leverage more effective vision features,\ninject commonsense knowledge, and apply ltering mecha-\nnisms to improve CoT reasoning in future studies.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nReferences\nAnderson, P., He, X., Buehler, C., Teney, D., Johnson, M.,\nGould, S., and Zhang, L. Bottom-up and top-down atten-\ntion for image captioning and visual question answering.\nIn2018 IEEE Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2018, Salt Lake City, UT, USA,\nJune 18-22, 2018 , pp. 60776086. IEEE Computer Soci-\nety, 2018. doi: 10.1109/CVPR.2018.00636.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,\nHenighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,\nJ., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,\nGray, S., Chess, B., Clark, J., Berner, C., McCandlish,\nS., Radford, A., Sutskever, I., and Amodei, D. Language\nmodels are few-shot learners. In Larochelle, H., Ranzato,\nM., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances\nin Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020.\nCarion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov,\nA., and Zagoruyko, S. End-to-end object detection with\ntransformers. In Computer VisionECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 2328, 2020,\nProceedings, Part I , pp. 213229, 2020.\nChen, T., Kornblith, S., Swersky, K., Norouzi, M., and\nHinton, G. E. Big self-supervised models are strong\nsemi-supervised learners. In Larochelle, H., Ranzato, M.,\nHadsell, R., Balcan, M., and Lin, H. (eds.), Advances\nin Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020.\nChen, W., Ma, X., Wang, X., and Cohen, W. W. Program\nof thoughts prompting: Disentangling computation from\nreasoning for numerical reasoning tasks. ArXiv preprint ,\nabs/2211.12588, 2022.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton,\nC., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,\nS., Maynez, J., Rao, A., Barnes, P., Tay, Y ., Shazeer,\nN., Prabhakaran, V ., Reif, E., Du, N., Hutchinson, B.,\nPope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G.,\nYin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S.,\nMichalewski, H., Garcia, X., Misra, V ., Robinson, K., Fe-\ndus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph,\nB., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal,\nS., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M.,\nLewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee,\nK., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O.,\nCatasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean,J., Petrov, S., and Fiedel, N. Palm: Scaling language mod-\neling with pathways. ArXiv preprint , abs/2204.02311,\n2022.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y .,\nFedus, W., Li, E., Wang, X., Dehghani, M., Brahma,\nS., et al. Scaling instruction-netuned language models.\narXiv preprint arXiv:2210.11416 , 2022.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\nHeigold, G., Gelly, S., et al. An image is worth 16x16\nwords: Transformers for image recognition at scale. In\nThe International Conference on Learning Representa-\ntions (ICLR) , 2021.\nFu, Y ., Peng, H., Sabharwal, A., Clark, P., and Khot, T.\nComplexity-based prompting for multi-step reasoning.\nArXiv preprint , abs/2210.00720, 2022.\nGao, P., Jiang, Z., You, H., Lu, P., Hoi, S. C. H., Wang, X.,\nand Li, H. Dynamic fusion with intra- and inter-modality\nattention ow for visual question answering. In IEEE\nConference on Computer Vision and Pattern Recognition,\nCVPR 2019, Long Beach, CA, USA, June 16-20, 2019 , pp.\n66396648. Computer Vision Foundation / IEEE, 2019.\ndoi: 10.1109/CVPR.2019.00680.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\nlearning for image recognition. In 2016 IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2016,\nLas Vegas, NV, USA, June 27-30, 2016 , pp. 770778.\nIEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.\n90.\nHo, N., Schmid, L., and Yun, S.-Y . Large language models\nare reasoning teachers. arXiv preprint arXiv:2212.10071 ,\n2022.\nJi, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y ., Ishii, E.,\nBang, Y ., Madotto, A., and Fung, P. Survey of halluci-\nnation in natural language generation. ACM Computing\nSurveys , 2022.\nKhashabi, D., Min, S., Khot, T., Sabharwal, A., Tafjord,\nO., Clark, P., and Hajishirzi, H. UNIFIEDQA: Crossing\nformat boundaries with a single QA system. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2020 , pp. 18961907, Online, 2020. Association\nfor Computational Linguistics. doi: 10.18653/v1/2020.\nndings-emnlp.171.\nKhot, T., Trivedi, H., Finlayson, M., Fu, Y ., Richardson, K.,\nClark, P., and Sabharwal, A. Decomposed prompting:\nA modular approach for solving complex tasks. ArXiv\npreprint , abs/2210.02406, 2022.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nKim, J., Jun, J., and Zhang, B. Bilinear attention networks.\nIn Bengio, S., Wallach, H. M., Larochelle, H., Grauman,\nK., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances\nin Neural Information Processing Systems 31: Annual\nConference on Neural Information Processing Systems\n2018, NeurIPS 2018, December 3-8, 2018, Montr eal,\nCanada , pp. 15711581, 2018.\nKim, W., Son, B., and Kim, I. Vilt: Vision-and-language\ntransformer without convolution or region supervision.\nInProceedings of the 38th International Conference on\nMachine Learning (ICML) , pp. 55835594, 2021.\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y ., and Iwasawa,\nY . Large language models are zero-shot reasoners. ArXiv\npreprint , abs/2205.11916, 2022.\nLi, B., Lv, C., Zhou, Z., Zhou, T., Xiao, T., Ma, A., and Zhu,\nJ. On vision features in multimodal machine translation.\nInProceedings of the 60th Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: Long\nPapers) , pp. 63276337, 2022a.\nLi, L. H., Yatskar, M., Yin, D., Hsieh, C.-J., and Chang,\nK.-W. Visualbert: A simple and performant baseline for\nvision and language. ArXiv preprint , abs/1908.03557,\n2019.\nLi, Y ., Lin, Z., Zhang, S., Fu, Q., Chen, B., Lou, J.-G., and\nChen, W. On the advance of making language models\nbetter reasoners. ArXiv preprint , abs/2206.02336, 2022b.\nLu, P., Qiu, L., Chen, J., Xia, T., Zhao, Y ., Zhang, W., Yu,\nZ., Liang, X., and Zhu, S.-C. Iconqa: A new benchmark\nfor abstract diagram understanding and visual language\nreasoning. In The 35th Conference on Neural Information\nProcessing Systems (NeurIPS) Track on Datasets and\nBenchmarks , 2021.\nLu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu,\nS.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to\nexplain: Multimodal reasoning via thought chains for sci-\nence question answering. ArXiv preprint , abs/2209.09513,\n2022a.\nLu, P., Qiu, L., Chang, K.-W., Wu, Y . N., Zhu, S.-C., Ra-\njpurohit, T., Clark, P., and Kalyan, A. Dynamic prompt\nlearning via policy gradient for semi-structured mathemat-\nical reasoning. ArXiv preprint , abs/2209.14610, 2022b.\nMagister, L. C., Mallinson, J., Adamek, J., Malmi, E., and\nSeveryn, A. Teaching small language models to reason.\nArXiv preprint , abs/2212.08410, 2022.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\net al. Learning transferable visual models from natural\nlanguage supervision. In International Conference on\nMachine Learning , pp. 87488763. PMLR, 2021.Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J.,\nSong, F., Aslanides, J., Henderson, S., Ring, R., Young,\nS., Rutherford, E., Hennigan, T., Menick, J., Cassirer, A.,\nPowell, R., Driessche, G. v. d., Hendricks, L. A., Rauh,\nM., Huang, P.-S., Glaese, A., Welbl, J., Dathathri, S.,\nHuang, S., Uesato, J., Mellor, J., Higgins, I., Creswell,\nA., McAleese, N., Wu, A., Elsen, E., Jayakumar, S.,\nBuchatskaya, E., Budden, D., Sutherland, E., Simonyan,\nK., Paganini, M., Sifre, L., Martens, L., Li, X. L., Kun-\ncoro, A., Nematzadeh, A., Gribovskaya, E., Donato, D.,\nLazaridou, A., Mensch, A., Lespiau, J.-B., Tsimpoukelli,\nM., Grigorev, N., Fritz, D., Sottiaux, T., Pajarskas, M.,\nPohlen, T., Gong, Z., Toyama, D., dAutume, C. d. M.,\nLi, Y ., Terzi, T., Mikulik, V ., Babuschkin, I., Clark, A.,\nCasas, D. d. L., Guy, A., Jones, C., Bradbury, J., Johnson,\nM., Hechtman, B., Weidinger, L., Gabriel, I., Isaac, W.,\nLockhart, E., Osindero, S., Rimell, L., Dyer, C., Vinyals,\nO., Ayoub, K., Stanway, J., Bennett, L., Hassabis, D.,\nKavukcuoglu, K., and Irving, G. Scaling language mod-\nels: Methods, analysis & insights from training gopher.\nArXiv preprint , abs/2112.11446, 2021.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring the\nlimits of transfer learning with a unied text-to-text trans-\nformer. Journal of Machine Learning Research (JMLR) ,\n21:167, 2020.\nRubin, O., Herzig, J., and Berant, J. Learning to re-\ntrieve prompts for in-context learning. In Proceedings\nof the 2022 Conference of the North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies , pp. 26552671, 2022. doi:\n10.18653/v1/2022.naacl-main.191.\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kul-\nshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L.,\nDu, Y ., Li, Y ., Lee, H., Zheng, H. S., Ghafouri, A., Mene-\ngali, M., Huang, Y ., Krikun, M., Lepikhin, D., Qin, J.,\nChen, D., Xu, Y ., Chen, Z., Roberts, A., Bosma, M.,\nZhao, V ., Zhou, Y ., Chang, C.-C., Krivokon, I., Rusch,\nW., Pickett, M., Srinivasan, P., Man, L., Meier-Hellstern,\nK., Morris, M. R., Doshi, T., Santos, R. D., Duke, T.,\nSoraker, J., Zevenbergen, B., Prabhakaran, V ., Diaz, M.,\nHutchinson, B., Olson, K., Molina, A., Hoffman-John, E.,\nLee, J., Aroyo, L., Rajakumar, R., Butryna, A., Lamm,\nM., Kuzmina, V ., Fenton, J., Cohen, A., Bernstein, R.,\nKurzweil, R., Aguera-Arcas, B., Cui, C., Croak, M., Chi,\nE., and Le, Q. Lamda: Language models for dialog\napplications. ArXiv preprint , abs/2201.08239, 2022.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need. In Guyon, I., von Luxburg, U., Bengio,\nS., Wallach, H. M., Fergus, R., Vishwanathan, S. V . N.,\nand Garnett, R. (eds.), Advances in Neural Information', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nProcessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA , pp. 59986008, 2017.\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E.,\nand Zhou, D. Self-consistency improves chain of\nthought reasoning in language models. ArXiv preprint ,\nabs/2203.11171, 2022a.\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and\nZhou, D. Rationale-augmented ensembles in language\nmodels. ArXiv preprint , abs/2207.00747, 2022b.\nWei, J., Tay, Y ., Bommasani, R., Raffel, C., Zoph, B.,\nBorgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Met-\nzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang,\nP., Dean, J., and Fedus, W. Emergent abilities of large\nlanguage models. Transactions on Machine Learning\nResearch , 2022a. Survey Certication.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E.,\nLe, Q., and Zhou, D. Chain of thought prompting elic-\nits reasoning in large language models. ArXiv preprint ,\nabs/2201.11903, 2022b.\nWu, Z., Kong, L., Bi, W., Li, X., and Kao, B. Good for\nmisconceived reasons: An empirical revisiting on the\nneed for visual context in multimodal machine transla-\ntion. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers) , pp. 61536166, Online,\n2021. Association for Computational Linguistics. doi:\n10.18653/v1/2021.acl-long.480.\nYu, Z., Yu, J., Cui, Y ., Tao, D., and Tian, Q. Deep modu-\nlar co-attention networks for visual question answering.\nInIEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2019, Long Beach, CA, USA, June\n16-20, 2019 , pp. 62816290. Computer Vision Founda-\ntion / IEEE, 2019. doi: 10.1109/CVPR.2019.00644.\nZhang, Z., Chen, K., Wang, R., Utiyama, M., Sumita, E., Li,\nZ., and Zhao, H. Neural machine translation with univer-\nsal visual representation. In 8th International Conference\non Learning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020 . OpenReview.net, 2020.\nZhang, Z., Zhang, A., Li, M., and Smola, A. Automatic\nchain of thought prompting in large language models.\nArXiv preprint , abs/2210.03493, 2022.\nZhang, Z., Chen, K., Wang, R., Utiyama, M., Sumita, E., Li,\nZ., and Zhao, H. Universal multimodal representation for\nlanguage understanding. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence , pp. 118, 2023. doi:\n10.1109/TPAMI.2023.3234170.Zhou, D., Sch arli, N., Hou, L., Wei, J., Scales, N., Wang,\nX., Schuurmans, D., Bousquet, O., Le, Q., and Chi, E.\nLeast-to-most prompting enables complex reasoning in\nlarge language models. ArXiv preprint , abs/2205.10625,\n2022.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nA. Extended analysis for the challenge of Multimodal-CoT\nA.1. More Examples of Misleading by Hallucinated Rationales\nAccording to our case studies (Section 3.2), we nd that the baseline tends to generate hallucinated rationales. We provide\nfurther examples as shown in Figure 6.\nOptions:(B) lichen(A) mushroomProblem\nBaselineQuestion:Basedonthearrows,whichofthefollowingorganismsisaconsumer?Context:BelowisafoodwebfromatundraecosysteminNunavut,aterritoryinNorthernCanada.Afoodwebmodelshowthemattereatenbyorganismsmovesthroughanecosystem.Thearrowsinafoodwebrepresenthowmattermovesbetweenorganismsinanecosystem.GoldRationale:AfoodwebisamodelAnorganisminafoodwebcanhavemorethanonearrowpointingfromit.Thisshowsthattheorganismiseatenbymorethanoneotherorganisminthefoodweb.Anorganisminafoodwebcanalsohavemorethanonearrowpointingtoit.Thisshowsthattheorganismeatsmorethanoneotherorganisminthefoodweb.Consumerseatotherorganisms.So,therearearrowsinafoodwebthatpointfromotherorganismstoconsumers.Themushroomhasarrowspointingtoitfromthebarren-groundcaribouandthegrizzlybear.So,themushroomisaconsumer.Thelichendoesnothaveanyarrowspointingtoit.So,thelichenisaproducer,notaconsumer.Answer:Theansweris(A).Rationale:AfoodwebisamodelThisshowsthattheorganismiseatenbymorethanoneotherorganisminthefoodweb.Anorganisminafoodwebcanalsohavemorethanonearrowpointingtoit.Thisshowsthattheorganismeatsmorethanoneotherorganisminthefoodweb.Consumerseatproducers.So,inafoodweb,consumershavearrowspointingtothemfromproducers.Themushroomhasanarrowpointingtoitfromthemushroom.Themushroomisnotaconsumer,sothemushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Answer:Theansweris(B).Rationale:AfoodwebisamodelAnorganisminafoodwebcanhavemorethanonearrowpointingfromit.Thisshowsthattheorganismiseatenbymorethanoneotherorganisminthefoodweb.Anorganisminafoodwebcanalsohavemorethanonearrowpointingtoit.Thisshowsthattheorganismeatsmorethanoneotherorganisminthefoodweb.Consumerseatproducersorganisms.So,inarearrowsafoodwebthatpointfromotherorganismstoconsumers.Themushroomhasarrowspointingtoitfromthen-browmushroomibou.thelichenzzlybear.So,themushroomisaconsumer.Thelichenhasnothaveanyarrowspointingtoit.So,themushroomisaconsumer.Lichenisnotaconsumer.Answer:Theansweris(B).Vision\n+ Vision Features\nOptions:(B) sample A(A) neither; the samples have the same temperatureProblem\nBaselineQuestion:Comparetheaveragekineticenergiesoftheparticlesineachsample.Whichsamplehasthehighertemperature?Context:Thediagramsbelowshowtwopuresamplesofgasinidenticalclosed,rigidcontainers.Eachcoloredballrepresentsonegasparticle.Bothsampleshavethesamenumberofparticles.GoldRationale:Thetemperatureofasubstancedependsontheaveragekineticenergyoftheparticlesinthesubstance.Thehighertheaveragekineticenergyoftheparticles,thehigherthetemperatureofthesubstance.Thekineticenergyofaparticleisdeterminedbyitsmassandspeed.Forapuresubstance,thegreaterthemassofeachparticleinthesubstanceandthehighertheaveragespeedoftheparticles,thehighertheiraveragekineticenergy.EachparticleinsampleAhasmoremassthaneachparticleinsampleB.TheparticlesinsampleAalso+ Vision FeaturesVision(C) sample B\nhaveahigheraveragespeedthantheparticlesinsampleB.So,theparticlesinsampleAhaveahigheraveragekineticenergythantheparticlesinsampleB.BecausetheparticlesinsampleAhavethehigheraveragekineticenergy,sampleAmusthavethehighertemperature.Answer:Theansweris(B).Rationale:Thetemperatureofasubstancedependsontheaveragekineticenergyoftheparticlesinthesubstance.Thehighertheaveragekineticenergyoftheparticles,thehigherthetemperatureofthesubstance.Thekineticenergyofaparticleisdeterminedbyitsmassandspeed.Forapuresubstance,thegreaterthemassofeachparticleinthesubstanceandthehighertheaveragespeedoftheparticles,thehighertheiraveragekineticenergy.EachparticleinsampleBhasmoremassthaneachparticleinsampleA.TheparticlesinsampleBalsohaveahigheraveragespeedthantheparticlesinsampleA.So,theparticlesinsampleBhaveahigheraveragekineticenergythantheparticlesinsampleA.BecausetheparticlesinsampleBhavethehigheraveragekineticenergy,sampleBmusthavethehighertemperature.Answer:Theansweris(C).Rationale:Thetemperatureofasubstancedependsontheaveragekineticenergyoftheparticlesinthesubstance.Thehighertheaveragekineticenergyoftheparticles,thehigherthetemperatureofthesubstance.Thekineticenergyofaparticleisdeterminedbyitsmassandspeed.Forapuresubstance,thegreaterthemassofeachparticleinthesubstanceandthehighertheaveragespeedoftheparticles,thehighertheiraveragekineticenergy.EachparticleinsampleAhasmoremassthaneachparticleinsampleB.TheparticlesinsampleAalsohaveahigheraveragespeedthantheparticlesinsampleB.So,theparticlesinsampleAhaveahigheraveragekineticenergythantheparticlesinsampleB.BecausetheparticlesinsampleAhavethehigheraveragekineticenergy,sampleAmusthavethehighertemperature.Answer:Theansweris(B).\nFigure 6. Examples of the two-stage framework without vision features (baseline) and with vision features (ours) for generating rationales\nand predicting answers. The upper part presents the problem details, and the lower part shows the outputs of the baseline and our method.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}]: %s
2023-12-07 11:04:27,170 - INFO - Check the data that is being passed [{'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nA.2. Two-Stage Training Performance with Different Sizes of LMs.\nIn Section 3, we obverse that incorporating vision features helps generate more effective rationales, thus leading to improved\nanswer accuracy. Besides incorporating vision features, it is possible to scale the LM size to mitigate the issue of incorrect\nrationales. Figure 7 shows the answer accuracy with UniedQA Base and UniedQA Large . When using a larger LM, the\naccuracy of the baseline (w/o vision features) is boosted. The result indicates that scaling the LM is possible to mitigate the\nissue of incorrect rationales. However, the performance is still much inferior to using vision features. The result further\nveries the effectiveness of our Multimodal-CoT with different sizes of LMs.\nbase large6580100\n70.5384.9182.9791.68Accuracy (%)w/o Vision Modality w/ Vision Modality\nFigure 7. Answer accuracy with different sizes of LMs.\nB. Experimental Details\nB.1. Baseline Methods\nFollowing Lu et al. (2022a), our baselines include three types of methods:\n(i) Visual question answering (VQA) models (Yu et al., 2019; Anderson et al., 2018; Kim et al., 2018; Gao et al., 2019; Lu\net al., 2021; Li et al., 2019). The VQA baselines take the question, the context, and choices as the textual input, take the\nimage as the vision input, and predict the score distribution over choice candidates via a linear classier.\n(ii) Text-to-text LM models. UniedQA (Khashabi et al., 2020) is adopted as it is the best ne-tuning model in Lu et al.\n(2022a). UniedQA takes the textual information as the input and outputs the answer option. The image is converted into a\ncaption extracted by an image captioning model based on ViT and GPT-2.6UniedQA treats our task as a text generation\nproblem. In Lu et al. (2022a), it is trained to generate a target answer text, i.e., one of the candidate options. Then, the most\nsimilar option is selected as the nal prediction to evaluate the question answering accuracy.\n(iii) GPT-3.5 models (Chen et al., 2020) based on the text-davinci-002 engine. The inference is based on the few-shot\nprompting, where two in-context examples from the training set are concatenated before the test instance.\nFor UniedQA and GPT-3.5, CoT is applied after the answer (Lu et al., 2022a). Besides the above baselines, we develop a\nstronger baseline with a slight modication of the output format of UniedQA. Instead of predicting the answer texts, our\nbaseline directly predicts the choice, e.g., the answer is B . This setting helps our baseline achieve better results than the\nexisting UniedQA. Therefore, we use the stronger method as the language only baseline for analysis.\nB.2. Details of Vision Features\nIn Section 6.2, we compared four types of vision features, CLIP (Radford et al., 2021), DETR (Carion et al., 2020), and\nResNet (He et al., 2016). The specic models are: (i) CLIP: RN101;7(ii) DETR: detr resnet101 dc5;8(iii) ResNet: we use\n6https://huggingface.co/nlpconnect/vit-gpt2-image-captioning .\n7https://github.com/jianjieluo/OpenAI-CLIP-Feature .\n8https://github.com/facebookresearch/detr .', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nthe averaged pooled features of a pre-trained ResNet50 CNN. Table 9 presents the dimension of the vision features (after the\nfunction VisionExtractor( ) in Eq. 3). For ResNet-50, we repeat the pooled features of ResNet-50 to the same length as the\ntext sequence to imitate the patch-like features, where each patch is the same as the pooled image features.\nTable 9. Dimension of vision features\nMethod Dimension\nCLIP (49, 2048)\nDETR (100, 256)\nResNet (512, 2048)\nC. Examples of Case Studies\nTo better understand the behavior of Multimodal-CoT, we manually investigate randomly selected examples generated by\nour approach. Table 8 summarizes the categorization results generated by Multimodal-CoT. We randomly picked up 50\nsamples whose prediction results were correct and 50 samples whose prediction results were incorrect.\nWe nd that the correct samples contain a certain amount of incorrect chain-of-thought. As shown in Figure 8(b), the model\ngenerates the incorrect rationale,  Animals cannot their food by digesting other organisms  but the predicted answer is\ncorrect. The result indicates that CoT may not always benet the answer inference, and the model is robust to some extent \nit can predict the correct answer by ignoring incorrect rationales.\nFor incorrect samples, commonsense mistake is the most frequent error type. The model also makes commonsense mistakes\nwhen answering the questions requires commonsense knowledge, e.g., understand maps and counting numbers in the\nimages (Figure 9), and utilizing the alphabet (Figure 10). The other type of mistake is the logical mistake, where there are\ncontradictions in the reasoning chains (Figure 11). In addition, there are cases that the CoT is correct but might not be\nnecessarily related to answer options; thus the model chooses the incorrect answer.\nThe analysis indicates that there are prospective directions for future studies. On the one hand, it is possible to improve the\nquality of CoT by (i) using more ne-grained interaction of language and vision features; and (ii) injecting commonsense\nknowledge. On the other hand, applying a ltering mechanism to using only the effective CoT to infer the answer and\neliminate irrelevant CoT.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Birds,mammals,fish,reptiles,andamphibiansaregroupsofanimals.Theanimalsineachgrouphavetraitsincommon.Scientistssortanimalsintogroupsbasedontraitstheyhaveincommon.Thisprocessiscalledclassification.Atigersharkisafish.Itlivesunderwater.Ithasfins,notlimbs.Tigersharkscannocturnal.Theymeansthattheyareactivemostlyatnight.Ared-tailedhawkisabird.Ithasfeathers,twowings,andabeak.Red-tailedhawkslivetheirbeteethakstopushforpre.birds,and,repamptilesAnswer:Theansweris(B).Options:(A) red-tailed hawkProblem\nPredictionQuestion:Selectthebirdbelow.Context:Birdshavefeathers,twowings,andabeak.Aperegrinefalconisanexampleofabird.Rationale:Birds,mammals,fish,reptiles,andamphibiansaregroupsofanimals.Theanimalsineachgrouphavetraitsincommon.Scientistssortanimalsintogroupsbasedontraitstheyhaveincommon.Thisprocessiscalledclassification.Atigersharkisafish.Itlivesunderwater.Ithasfins,notlimbs.Tigersharkscannocturnal.Theymeansthattheyareactivemostlyatnight.Ared-tailedhawkisabird.Ithasfeathers,twowings,andabeak.Red-tailedhawkslivetheirbeteethakstopushforpre.birds,and,repamptilesAnswer:Theansweris(A).Vision(B) tiger shark\n(a)CoTiscorrect\nRationale:Leopardthepast,scientistsclassifiedlivingorganismsintotwogroups:plantsandanimals.Overthepast300years,scientistshavediscoveredmanymoretypesoforganisms.Today,manyscientistsclassifyorganismsintosixbroadgroups,calledkingdoms.Organismsineachkingdomhavespecifictraits.Thetablebelowshowssometraitsusedtodescribeeachkingdom.|Bacteria|Archaea|Protists|Fungi|Animals|Plants.Howmanycellsdotheyhave?|one|one|oneormany|oneormany|many|many.Dotheircellshaveanucleus?|no|no|yes|yes|yes|yesnCantheircellsmakefood?|somespeciescan|somespeciescan|somespeciescan|no|no|yesLeoparduswiediiisananimal.Animalcellscannotmaketheirownfood.Animalscannottheirfoodbydigestingotherorganisms.Answer:Theansweris(B).Options:(A) noProblem\nPredictionQuestion:CanLeoparduswiediicellsmaketheirownfood?Context:ThisorganismisLeoparduswiedii.Itisamemberoftheanimalkingdom.Leoparduswiediiiscommonlycalledamargay.MargaysliveintheforestinCentralandSouthAmerica.Margaysarenocturnal.Anocturnalanimalsleepsmostofthedayandisawakeatnight.Rationale:Inthepast,scientistsclassifiedlivingorganismsintotwogroups:plantsandanimals.Overthepast300years,scientistshavediscoveredmanymoretypesoforganisms.Today,manyscientistsclassifyorganismsintosixbroadgroups,calledkingdoms.Organismsineachkingdomhavespecifictraits.Thetablebelowshowssometraitsusedtodescribeeachkingdom.|Bacteria|Archaea|Protists|Fungi|Animals|Plants.Howmanycellsdotheyhave?|one|one|oneormany|oneormany|many|manyDotheircellshaveanucleus?|no|no|yes|yes|yes|yesCantheircellsmakefood?|somespeciescan|somespeciescan|somespeciescan|no|no|yesLeoparduswiediiisananimal.Animalcellscannotmaketheirownfood.Animalsgettheirfoodbydigestingotherorganisms.Answer:Theansweris(A).Vision(B) yes\n(b)CoTisincorrect\nFigure 8. Examples of answers are correct while the CoT is correct (a) or incorrect (b).', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Oceansarehugebodiesofsaltwater.Theworldhasfiveoceans.Alloftheoceansareconnected,makingoneworldocean.ThisistheIndianOcean.Answer:Theansweris(B).Options:(A) the Atlantic OceanProblem\nPredictionQuestion:Whichoceanishighlighted?Context:N/ARationale:Oceansarehugebodiesofsaltwater.Theworldhasfiveoceans.Alloftheoceansareconnected,makingoneworldocean.ThisisthePacificOcean.Answer:Theansweris(C).Vision(B) the Indian Ocean\n(a)Mistakesaboutmaps.\nRationale:Inasolution,soluteparticlesmoveandspreadthroughoutthesolvent.Thediagrambelowshowshowasolutioncanchangeovertime.SoluteparticlesmovefromtheareawheretheyareatahigherconcentrationtotheareawheretheyareatalowerconcentrationLookatthediagramagain.Itshowsyouhowthesolutionchangedduringtheprocessofdiffusion.Beforethesoluteparticlesreachedequilibrium,therewere5soluteparticlesontheleftsideofthemembraneand4soluteparticlesontherightsideofthemembrane.Whenthesoluteparticlesreachedequilibrium,therewere5soluteparticlesoneachsideofthemembrane.Therewas1moresoluteparticleontherightsideofthemembranethanbefore.So,forthesoluteparticlestoreachequilibrium,moresoluteparticlesmusthavemovedacrossthemembranetotherightthantotheright.Answer:Theansweris(B).Options:(A) to the left than to the rightProblem\nPredictionQuestion:Completethetexttodescribethediagram.Soluteparticlesmovedinbothdirectionsacrossthepermeablemembrane.Butmoresoluteparticlesmovedacrossthemembrane().Whentherewasanequalconcentrationonbothsides,theparticlesreachedequilibrium.Context:Thediagrambelowshowsasolutionwithonesolute.Eachsoluteparticleisrepresentedbyagreenball.Thesolutionfillsaclosedcontainerthatisdividedinhalfbyamembrane.Themembrane,representedbyadottedline,ispermeabletothesoluteparticles.Thediagramshowshowthesolutioncanchangeovertimeduringtheprocessofdiffusion.Rationale:Inasolution,soluteparticlesmoveandspreadthroughoutthesolvent.Thediagrambelowshowshowasolutioncanchangeovertime.SoluteparticlesmovefromtheareawheretheyareatahigherconcentrationtotheareawheretheyareatalowerconcentrationLookatthediagramagain.Itshowsyouhowthesolutionchangedduringtheprocessofdiffusion.Beforethesoluteparticlesreachedequilibrium,therewere3soluteparticlesontheleftsideofthemembraneand5soluteparticlesontherightsideofthemembrane.Whenthesoluteparticlesreachedequilibrium,therewere4soluteparticlesoneachsideofthemembrane.Therewas1moresoluteparticleontheleftsideofthemembranethanbefore.So,forthesoluteparticlestoreachequilibrium,moresoluteparticlesmusthavemovedacrossthemembranetotheleftthantotheright.Answer:Theansweris(A).Vision\n(b)Mistakesaboutcountingnumbersintheimage.(C) the Pacific Ocean(D) the Southern Ocean\n(B) to the right than to the left\nFigure 9. Examples of commonsense mistakes about understanding maps and counting numbers.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Guidewordsappearoneachpageofadictionary.Theytellyouthefirstwordandlastwordonthepage.Theotherwordsonthepagecomebetweentheguidewordsinalphabeticalorder.Toputwordsinalphabeticalorder,puttheminorderbytheirfirstletters.Ifthefirstlettersarethesame,lookatthesecondletters.Ifthesecondlettersarethesame,lookatthethirdletters,andsoon.Ifonewordisshorter,andtherearenomoreletterstocompare,thentheshorterwordcomesfirstinalphabeticalorder.Forexample,becomesbeforebed.Putthewordsinalphabeticalorder.Sincehardshipisbetweentheguidewordshelping-hunter,itwouldbefoundonthatpage.Answer:Theansweris(B).Options:(A) hospitalProblem\nPredictionQuestion:Whichwordwouldyoufindonadictionarypagewiththefollowingguidewords?helping-hunterContext:Thediagrambelowshowsasolutionwithonesolute.Eachsoluteparticleisrepresentedbyagreenball.Thesolutionfillsaclosedcontainerthatisdividedinhalfbyamembrane.Themembrane,representedbyadottedline,ispermeabletothesoluteparticles.Thediagramshowshowthesolutioncanchangeovertimeduringtheprocessofdiffusion.Rationale:Guidewordsappearoneachpageofadictionary.Theytellyouthefirstwordandlastwordonthepage.Theotherwordsonthepagecomebetweentheguidewordsinalphabeticalorder.Toputwordsinalphabeticalorder,puttheminorderbytheirfirstletters.Ifthefirstlettersarethesame,lookatthesecondletters.Ifthesecondlettersarethesame,lookatthethirdletters,andsoon.Ifonewordisshorter,andtherearenomoreletterstocompare,thentheshorterwordcomesfirstinalphabeticalorder.Forexample,becomesbeforebed.Putthewordsinalphabeticalorder.Sincehospitalisbetweentheguidewordshelping-hunter,itwouldbefoundonthatpage.Answer:Theansweris(A).(B) hardshipVisionN/A\nFigure 10. Examples of commonsense mistakes about utilizing alphabet.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': "Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Measurementsarewrittenwithbothanumberandaunit.Theunitcomesafterthenumber.Theunitshowswhatthenumbermeans.TemperaturecanbewrittenwithunitsofdegreesFahrenheit(F)orCelsius(C).Usethelistbelowtocomparethetwounits.212F|Waterboils|100C98.6F|Bodytemperature|37C68F|Roomtemperature|20C32F|Waterfreezes|0C.Thebetterestimateforthetemperatureoftheaironahotdayis36F.36Fistoohot.Answer:Theansweris(A).Options:(A) 36FProblem\nPredictionQuestion:Whatisthetemperatureoftheaironahotday?Context:Selectthebetterestimate.Rationale:Measurementsarewrittenwithbothanumberandaunit.Theunitcomesafterthenumber.Theunitshowswhatthenumbermeans.TemperaturecanbewrittenwithunitsofdegreesFahrenheit(F)orCelsius(C).Usethelistbelowtocomparethetwounits.212F|Waterboils|100C98.6F|Bodytemperature|37C68F|Roomtemperature|20C32F|Waterfreezes|0C.Thebetterestimateforthetemperatureoftheaironahotdayis36C.36Fistoocold.Answer:Theansweris(B).(B) 36CVisionN/A\nRationale::Asentenceisagroupofwordsthatexpressesacompletethought.ThebandI'minhasbeenrehearsingdailybecausewehaveaconcertintwoweeks.Arun-onsentenceisformedwhentwosentencesareruntogether,joinedbyjustacommaorbynopunctuationatall.Ifonlyacommaisused,therun-oniscalledacommaspliceThebandI'minhasbeenrehearsingdailysincewehaveaconcertintwoweeks.Thisisnotarun-onsentence.Itisformedacommaspliceformedfromtwosentencesruntogether,joinedwithoutjustacomma.Right-handednessiscontrolledbygenetics,interestinglyenough,thesamegenesarealsoresponsibleforthebrainbecomingmorespecializedatcertaintasks.Answer:Theansweris(B).Options:(A) yesProblem\nPredictionQuestion:Isthisarun-onsentence?Right-handednessiscontrolledbygenetics,interestinglyenough,thesamegenesarealsoresponsibleforthebrainbecomingmorespecializedatcertaintasks.Context:N/ARationale:Asentenceisagroupofwordsthatexpressesacompletethought.ThebandI'minhasbeenrehearsingdailybecausewehaveaconcertintwoweeks.Arun-onsentenceisformedwhentwosentencesareruntogether,joinedbyjustacommaorbynopunctuationatall.Ifonlyacommaisused,therun-oniscalledacommaspliceThebandI'minhasbeenrehearsingdailysincewehaveaconcertintwoweeks.Thisisarun-onsentence.Itisacommaspliceformedfromtwosentencesruntogether,joinedbyjustacomma.Right-handednessiscontrolledbygenetics,interestinglyenough,thesamegenesarealsoresponsibleforthebrainbecomingmorespecializedatcertaintasks.Answer:Theansweris(A).(B) noVisionN/A(a)Logicalmistakewherethemodelfailsatcomparisons.\n(b)Logicalmistakewherethethereisacontradictionintherationale.\nFigure 11. Examples of logical mistakes.", 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': "Multimodal Chain-of-Thought Reasoning in Language Models\nOptions:(A) black stripes on its skinProblem\nPredictionRationale:Thewayanorganismlooksoractsiscalledatrait.Scientistsusefossilstolearnmoreaboutthetraitsofancientorganisms.Fossilscanpreservetheremainsofbodypartsandactivities.Afossilofabodypart,suchasatailorawing,cantellyouwhatanorganismlookedlike.Afossilofanorganism'sactivities,suchasaburroworafootprint,cantellyouabouttheorganism'sbehavior.Herearethreeexamplesoffossilsandthetraitsthatyoucanobservefromthem:Thisisafossilofananimal.Thisfossiltellsyouthattheanimalhadaspiral-shapedshell.Thisisafossilofaplant.Thisfossiltellsyouthattheplanthadsmallleavesarrangedinabranchedpattern.Thisisafossilofananimal'sfootprint.Thisfossiltellsyouthattheanimalcouldwalkonland.Anorganism'sfossilmaynotshowalloftheorganism'straits.Thisisbecausemostbodypartsaredestroyedduringfossilformation.Whenanorganism'sbodyturnsintoafossil,onlyafewbodypartsareusuallypreserved.Answer:Theansweris(C).(B) large fins on its bodyVision(C) a long, thin bodyQuestion:WhichtraitdidPalaeopythonhave?Selectthetraityoucanobserveonthefossil.Context:ThispictureshowsafossilofanancientanimalcalledPalaeopython.Palaeopythonlivedintreesandcouldgrowmorethansixfeetlong.\nRationale:Thewayanorganismlooksoractsiscalledatrait.Scientistsusefossilstolearnmoreaboutthetraitsofancientorganisms.Fossilscanpreservetheremainsofbodypartsandactivities.Afossilofabodypart,suchasatailorawing,cantellyouwhatanorganismlookedlike.Afossilofanorganism'sactivities,suchasaburroworafootprint,cantellyouabouttheorganism'sbehavior.Herearethreeexamplesoffossilsandthetraitsthatyoucanobservefromthem:Thisisafossilofananimal.Thisfossiltellsyouthattheanimalhadaspiral-shapedshell.Thisisafossilofaplant.Thisfossiltellsyouthattheplanthadsmallleavesarrangedinabranchedpattern.Thisisafossilofananimal'sfootprint.Thisfossiltellsyouthattheanimalcouldwalkonland.Anorganism'sfossilmaynotshowalloftheorganism'straits.Thisisbecausemostbodypartsaredestroyedduringfossilformation.Whenanorganism'sbodyturnsintoafossil,onlyafewbodypartsareusuallypreserved.Answer:Theansweris(B).\nFigure 12. Examples of answers are incorrect while the CoT is correct.", 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}]: %s
2023-12-07 11:04:27,170 - INFO - Check the results [{'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nA.2. Two-Stage Training Performance with Different Sizes of LMs.\nIn Section 3, we obverse that incorporating vision features helps generate more effective rationales, thus leading to improved\nanswer accuracy. Besides incorporating vision features, it is possible to scale the LM size to mitigate the issue of incorrect\nrationales. Figure 7 shows the answer accuracy with UniedQA Base and UniedQA Large . When using a larger LM, the\naccuracy of the baseline (w/o vision features) is boosted. The result indicates that scaling the LM is possible to mitigate the\nissue of incorrect rationales. However, the performance is still much inferior to using vision features. The result further\nveries the effectiveness of our Multimodal-CoT with different sizes of LMs.\nbase large6580100\n70.5384.9182.9791.68Accuracy (%)w/o Vision Modality w/ Vision Modality\nFigure 7. Answer accuracy with different sizes of LMs.\nB. Experimental Details\nB.1. Baseline Methods\nFollowing Lu et al. (2022a), our baselines include three types of methods:\n(i) Visual question answering (VQA) models (Yu et al., 2019; Anderson et al., 2018; Kim et al., 2018; Gao et al., 2019; Lu\net al., 2021; Li et al., 2019). The VQA baselines take the question, the context, and choices as the textual input, take the\nimage as the vision input, and predict the score distribution over choice candidates via a linear classier.\n(ii) Text-to-text LM models. UniedQA (Khashabi et al., 2020) is adopted as it is the best ne-tuning model in Lu et al.\n(2022a). UniedQA takes the textual information as the input and outputs the answer option. The image is converted into a\ncaption extracted by an image captioning model based on ViT and GPT-2.6UniedQA treats our task as a text generation\nproblem. In Lu et al. (2022a), it is trained to generate a target answer text, i.e., one of the candidate options. Then, the most\nsimilar option is selected as the nal prediction to evaluate the question answering accuracy.\n(iii) GPT-3.5 models (Chen et al., 2020) based on the text-davinci-002 engine. The inference is based on the few-shot\nprompting, where two in-context examples from the training set are concatenated before the test instance.\nFor UniedQA and GPT-3.5, CoT is applied after the answer (Lu et al., 2022a). Besides the above baselines, we develop a\nstronger baseline with a slight modication of the output format of UniedQA. Instead of predicting the answer texts, our\nbaseline directly predicts the choice, e.g., the answer is B . This setting helps our baseline achieve better results than the\nexisting UniedQA. Therefore, we use the stronger method as the language only baseline for analysis.\nB.2. Details of Vision Features\nIn Section 6.2, we compared four types of vision features, CLIP (Radford et al., 2021), DETR (Carion et al., 2020), and\nResNet (He et al., 2016). The specic models are: (i) CLIP: RN101;7(ii) DETR: detr resnet101 dc5;8(iii) ResNet: we use\n6https://huggingface.co/nlpconnect/vit-gpt2-image-captioning .\n7https://github.com/jianjieluo/OpenAI-CLIP-Feature .\n8https://github.com/facebookresearch/detr .', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nthe averaged pooled features of a pre-trained ResNet50 CNN. Table 9 presents the dimension of the vision features (after the\nfunction VisionExtractor( ) in Eq. 3). For ResNet-50, we repeat the pooled features of ResNet-50 to the same length as the\ntext sequence to imitate the patch-like features, where each patch is the same as the pooled image features.\nTable 9. Dimension of vision features\nMethod Dimension\nCLIP (49, 2048)\nDETR (100, 256)\nResNet (512, 2048)\nC. Examples of Case Studies\nTo better understand the behavior of Multimodal-CoT, we manually investigate randomly selected examples generated by\nour approach. Table 8 summarizes the categorization results generated by Multimodal-CoT. We randomly picked up 50\nsamples whose prediction results were correct and 50 samples whose prediction results were incorrect.\nWe nd that the correct samples contain a certain amount of incorrect chain-of-thought. As shown in Figure 8(b), the model\ngenerates the incorrect rationale,  Animals cannot their food by digesting other organisms  but the predicted answer is\ncorrect. The result indicates that CoT may not always benet the answer inference, and the model is robust to some extent \nit can predict the correct answer by ignoring incorrect rationales.\nFor incorrect samples, commonsense mistake is the most frequent error type. The model also makes commonsense mistakes\nwhen answering the questions requires commonsense knowledge, e.g., understand maps and counting numbers in the\nimages (Figure 9), and utilizing the alphabet (Figure 10). The other type of mistake is the logical mistake, where there are\ncontradictions in the reasoning chains (Figure 11). In addition, there are cases that the CoT is correct but might not be\nnecessarily related to answer options; thus the model chooses the incorrect answer.\nThe analysis indicates that there are prospective directions for future studies. On the one hand, it is possible to improve the\nquality of CoT by (i) using more ne-grained interaction of language and vision features; and (ii) injecting commonsense\nknowledge. On the other hand, applying a ltering mechanism to using only the effective CoT to infer the answer and\neliminate irrelevant CoT.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Birds,mammals,fish,reptiles,andamphibiansaregroupsofanimals.Theanimalsineachgrouphavetraitsincommon.Scientistssortanimalsintogroupsbasedontraitstheyhaveincommon.Thisprocessiscalledclassification.Atigersharkisafish.Itlivesunderwater.Ithasfins,notlimbs.Tigersharkscannocturnal.Theymeansthattheyareactivemostlyatnight.Ared-tailedhawkisabird.Ithasfeathers,twowings,andabeak.Red-tailedhawkslivetheirbeteethakstopushforpre.birds,and,repamptilesAnswer:Theansweris(B).Options:(A) red-tailed hawkProblem\nPredictionQuestion:Selectthebirdbelow.Context:Birdshavefeathers,twowings,andabeak.Aperegrinefalconisanexampleofabird.Rationale:Birds,mammals,fish,reptiles,andamphibiansaregroupsofanimals.Theanimalsineachgrouphavetraitsincommon.Scientistssortanimalsintogroupsbasedontraitstheyhaveincommon.Thisprocessiscalledclassification.Atigersharkisafish.Itlivesunderwater.Ithasfins,notlimbs.Tigersharkscannocturnal.Theymeansthattheyareactivemostlyatnight.Ared-tailedhawkisabird.Ithasfeathers,twowings,andabeak.Red-tailedhawkslivetheirbeteethakstopushforpre.birds,and,repamptilesAnswer:Theansweris(A).Vision(B) tiger shark\n(a)CoTiscorrect\nRationale:Leopardthepast,scientistsclassifiedlivingorganismsintotwogroups:plantsandanimals.Overthepast300years,scientistshavediscoveredmanymoretypesoforganisms.Today,manyscientistsclassifyorganismsintosixbroadgroups,calledkingdoms.Organismsineachkingdomhavespecifictraits.Thetablebelowshowssometraitsusedtodescribeeachkingdom.|Bacteria|Archaea|Protists|Fungi|Animals|Plants.Howmanycellsdotheyhave?|one|one|oneormany|oneormany|many|many.Dotheircellshaveanucleus?|no|no|yes|yes|yes|yesnCantheircellsmakefood?|somespeciescan|somespeciescan|somespeciescan|no|no|yesLeoparduswiediiisananimal.Animalcellscannotmaketheirownfood.Animalscannottheirfoodbydigestingotherorganisms.Answer:Theansweris(B).Options:(A) noProblem\nPredictionQuestion:CanLeoparduswiediicellsmaketheirownfood?Context:ThisorganismisLeoparduswiedii.Itisamemberoftheanimalkingdom.Leoparduswiediiiscommonlycalledamargay.MargaysliveintheforestinCentralandSouthAmerica.Margaysarenocturnal.Anocturnalanimalsleepsmostofthedayandisawakeatnight.Rationale:Inthepast,scientistsclassifiedlivingorganismsintotwogroups:plantsandanimals.Overthepast300years,scientistshavediscoveredmanymoretypesoforganisms.Today,manyscientistsclassifyorganismsintosixbroadgroups,calledkingdoms.Organismsineachkingdomhavespecifictraits.Thetablebelowshowssometraitsusedtodescribeeachkingdom.|Bacteria|Archaea|Protists|Fungi|Animals|Plants.Howmanycellsdotheyhave?|one|one|oneormany|oneormany|many|manyDotheircellshaveanucleus?|no|no|yes|yes|yes|yesCantheircellsmakefood?|somespeciescan|somespeciescan|somespeciescan|no|no|yesLeoparduswiediiisananimal.Animalcellscannotmaketheirownfood.Animalsgettheirfoodbydigestingotherorganisms.Answer:Theansweris(A).Vision(B) yes\n(b)CoTisincorrect\nFigure 8. Examples of answers are correct while the CoT is correct (a) or incorrect (b).', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Oceansarehugebodiesofsaltwater.Theworldhasfiveoceans.Alloftheoceansareconnected,makingoneworldocean.ThisistheIndianOcean.Answer:Theansweris(B).Options:(A) the Atlantic OceanProblem\nPredictionQuestion:Whichoceanishighlighted?Context:N/ARationale:Oceansarehugebodiesofsaltwater.Theworldhasfiveoceans.Alloftheoceansareconnected,makingoneworldocean.ThisisthePacificOcean.Answer:Theansweris(C).Vision(B) the Indian Ocean\n(a)Mistakesaboutmaps.\nRationale:Inasolution,soluteparticlesmoveandspreadthroughoutthesolvent.Thediagrambelowshowshowasolutioncanchangeovertime.SoluteparticlesmovefromtheareawheretheyareatahigherconcentrationtotheareawheretheyareatalowerconcentrationLookatthediagramagain.Itshowsyouhowthesolutionchangedduringtheprocessofdiffusion.Beforethesoluteparticlesreachedequilibrium,therewere5soluteparticlesontheleftsideofthemembraneand4soluteparticlesontherightsideofthemembrane.Whenthesoluteparticlesreachedequilibrium,therewere5soluteparticlesoneachsideofthemembrane.Therewas1moresoluteparticleontherightsideofthemembranethanbefore.So,forthesoluteparticlestoreachequilibrium,moresoluteparticlesmusthavemovedacrossthemembranetotherightthantotheright.Answer:Theansweris(B).Options:(A) to the left than to the rightProblem\nPredictionQuestion:Completethetexttodescribethediagram.Soluteparticlesmovedinbothdirectionsacrossthepermeablemembrane.Butmoresoluteparticlesmovedacrossthemembrane().Whentherewasanequalconcentrationonbothsides,theparticlesreachedequilibrium.Context:Thediagrambelowshowsasolutionwithonesolute.Eachsoluteparticleisrepresentedbyagreenball.Thesolutionfillsaclosedcontainerthatisdividedinhalfbyamembrane.Themembrane,representedbyadottedline,ispermeabletothesoluteparticles.Thediagramshowshowthesolutioncanchangeovertimeduringtheprocessofdiffusion.Rationale:Inasolution,soluteparticlesmoveandspreadthroughoutthesolvent.Thediagrambelowshowshowasolutioncanchangeovertime.SoluteparticlesmovefromtheareawheretheyareatahigherconcentrationtotheareawheretheyareatalowerconcentrationLookatthediagramagain.Itshowsyouhowthesolutionchangedduringtheprocessofdiffusion.Beforethesoluteparticlesreachedequilibrium,therewere3soluteparticlesontheleftsideofthemembraneand5soluteparticlesontherightsideofthemembrane.Whenthesoluteparticlesreachedequilibrium,therewere4soluteparticlesoneachsideofthemembrane.Therewas1moresoluteparticleontheleftsideofthemembranethanbefore.So,forthesoluteparticlestoreachequilibrium,moresoluteparticlesmusthavemovedacrossthemembranetotheleftthantotheright.Answer:Theansweris(A).Vision\n(b)Mistakesaboutcountingnumbersintheimage.(C) the Pacific Ocean(D) the Southern Ocean\n(B) to the right than to the left\nFigure 9. Examples of commonsense mistakes about understanding maps and counting numbers.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Guidewordsappearoneachpageofadictionary.Theytellyouthefirstwordandlastwordonthepage.Theotherwordsonthepagecomebetweentheguidewordsinalphabeticalorder.Toputwordsinalphabeticalorder,puttheminorderbytheirfirstletters.Ifthefirstlettersarethesame,lookatthesecondletters.Ifthesecondlettersarethesame,lookatthethirdletters,andsoon.Ifonewordisshorter,andtherearenomoreletterstocompare,thentheshorterwordcomesfirstinalphabeticalorder.Forexample,becomesbeforebed.Putthewordsinalphabeticalorder.Sincehardshipisbetweentheguidewordshelping-hunter,itwouldbefoundonthatpage.Answer:Theansweris(B).Options:(A) hospitalProblem\nPredictionQuestion:Whichwordwouldyoufindonadictionarypagewiththefollowingguidewords?helping-hunterContext:Thediagrambelowshowsasolutionwithonesolute.Eachsoluteparticleisrepresentedbyagreenball.Thesolutionfillsaclosedcontainerthatisdividedinhalfbyamembrane.Themembrane,representedbyadottedline,ispermeabletothesoluteparticles.Thediagramshowshowthesolutioncanchangeovertimeduringtheprocessofdiffusion.Rationale:Guidewordsappearoneachpageofadictionary.Theytellyouthefirstwordandlastwordonthepage.Theotherwordsonthepagecomebetweentheguidewordsinalphabeticalorder.Toputwordsinalphabeticalorder,puttheminorderbytheirfirstletters.Ifthefirstlettersarethesame,lookatthesecondletters.Ifthesecondlettersarethesame,lookatthethirdletters,andsoon.Ifonewordisshorter,andtherearenomoreletterstocompare,thentheshorterwordcomesfirstinalphabeticalorder.Forexample,becomesbeforebed.Putthewordsinalphabeticalorder.Sincehospitalisbetweentheguidewordshelping-hunter,itwouldbefoundonthatpage.Answer:Theansweris(A).(B) hardshipVisionN/A\nFigure 10. Examples of commonsense mistakes about utilizing alphabet.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': "Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Measurementsarewrittenwithbothanumberandaunit.Theunitcomesafterthenumber.Theunitshowswhatthenumbermeans.TemperaturecanbewrittenwithunitsofdegreesFahrenheit(F)orCelsius(C).Usethelistbelowtocomparethetwounits.212F|Waterboils|100C98.6F|Bodytemperature|37C68F|Roomtemperature|20C32F|Waterfreezes|0C.Thebetterestimateforthetemperatureoftheaironahotdayis36F.36Fistoohot.Answer:Theansweris(A).Options:(A) 36FProblem\nPredictionQuestion:Whatisthetemperatureoftheaironahotday?Context:Selectthebetterestimate.Rationale:Measurementsarewrittenwithbothanumberandaunit.Theunitcomesafterthenumber.Theunitshowswhatthenumbermeans.TemperaturecanbewrittenwithunitsofdegreesFahrenheit(F)orCelsius(C).Usethelistbelowtocomparethetwounits.212F|Waterboils|100C98.6F|Bodytemperature|37C68F|Roomtemperature|20C32F|Waterfreezes|0C.Thebetterestimateforthetemperatureoftheaironahotdayis36C.36Fistoocold.Answer:Theansweris(B).(B) 36CVisionN/A\nRationale::Asentenceisagroupofwordsthatexpressesacompletethought.ThebandI'minhasbeenrehearsingdailybecausewehaveaconcertintwoweeks.Arun-onsentenceisformedwhentwosentencesareruntogether,joinedbyjustacommaorbynopunctuationatall.Ifonlyacommaisused,therun-oniscalledacommaspliceThebandI'minhasbeenrehearsingdailysincewehaveaconcertintwoweeks.Thisisnotarun-onsentence.Itisformedacommaspliceformedfromtwosentencesruntogether,joinedwithoutjustacomma.Right-handednessiscontrolledbygenetics,interestinglyenough,thesamegenesarealsoresponsibleforthebrainbecomingmorespecializedatcertaintasks.Answer:Theansweris(B).Options:(A) yesProblem\nPredictionQuestion:Isthisarun-onsentence?Right-handednessiscontrolledbygenetics,interestinglyenough,thesamegenesarealsoresponsibleforthebrainbecomingmorespecializedatcertaintasks.Context:N/ARationale:Asentenceisagroupofwordsthatexpressesacompletethought.ThebandI'minhasbeenrehearsingdailybecausewehaveaconcertintwoweeks.Arun-onsentenceisformedwhentwosentencesareruntogether,joinedbyjustacommaorbynopunctuationatall.Ifonlyacommaisused,therun-oniscalledacommaspliceThebandI'minhasbeenrehearsingdailysincewehaveaconcertintwoweeks.Thisisarun-onsentence.Itisacommaspliceformedfromtwosentencesruntogether,joinedbyjustacomma.Right-handednessiscontrolledbygenetics,interestinglyenough,thesamegenesarealsoresponsibleforthebrainbecomingmorespecializedatcertaintasks.Answer:Theansweris(A).(B) noVisionN/A(a)Logicalmistakewherethemodelfailsatcomparisons.\n(b)Logicalmistakewherethethereisacontradictionintherationale.\nFigure 11. Examples of logical mistakes.", 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': "Multimodal Chain-of-Thought Reasoning in Language Models\nOptions:(A) black stripes on its skinProblem\nPredictionRationale:Thewayanorganismlooksoractsiscalledatrait.Scientistsusefossilstolearnmoreaboutthetraitsofancientorganisms.Fossilscanpreservetheremainsofbodypartsandactivities.Afossilofabodypart,suchasatailorawing,cantellyouwhatanorganismlookedlike.Afossilofanorganism'sactivities,suchasaburroworafootprint,cantellyouabouttheorganism'sbehavior.Herearethreeexamplesoffossilsandthetraitsthatyoucanobservefromthem:Thisisafossilofananimal.Thisfossiltellsyouthattheanimalhadaspiral-shapedshell.Thisisafossilofaplant.Thisfossiltellsyouthattheplanthadsmallleavesarrangedinabranchedpattern.Thisisafossilofananimal'sfootprint.Thisfossiltellsyouthattheanimalcouldwalkonland.Anorganism'sfossilmaynotshowalloftheorganism'straits.Thisisbecausemostbodypartsaredestroyedduringfossilformation.Whenanorganism'sbodyturnsintoafossil,onlyafewbodypartsareusuallypreserved.Answer:Theansweris(C).(B) large fins on its bodyVision(C) a long, thin bodyQuestion:WhichtraitdidPalaeopythonhave?Selectthetraityoucanobserveonthefossil.Context:ThispictureshowsafossilofanancientanimalcalledPalaeopython.Palaeopythonlivedintreesandcouldgrowmorethansixfeetlong.\nRationale:Thewayanorganismlooksoractsiscalledatrait.Scientistsusefossilstolearnmoreaboutthetraitsofancientorganisms.Fossilscanpreservetheremainsofbodypartsandactivities.Afossilofabodypart,suchasatailorawing,cantellyouwhatanorganismlookedlike.Afossilofanorganism'sactivities,suchasaburroworafootprint,cantellyouabouttheorganism'sbehavior.Herearethreeexamplesoffossilsandthetraitsthatyoucanobservefromthem:Thisisafossilofananimal.Thisfossiltellsyouthattheanimalhadaspiral-shapedshell.Thisisafossilofaplant.Thisfossiltellsyouthattheplanthadsmallleavesarrangedinabranchedpattern.Thisisafossilofananimal'sfootprint.Thisfossiltellsyouthattheanimalcouldwalkonland.Anorganism'sfossilmaynotshowalloftheorganism'straits.Thisisbecausemostbodypartsaredestroyedduringfossilformation.Whenanorganism'sbodyturnsintoafossil,onlyafewbodypartsareusuallypreserved.Answer:Theansweris(B).\nFigure 12. Examples of answers are incorrect while the CoT is correct.", 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}]: %s
2023-12-07 11:05:15,512 - INFO - Received requests to /inference endpoint
2023-12-07 11:05:15,613 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 11:05:15,614 - INFO - Received request: {'username': 'amin', 'prompt': 'provide me with a brief of the multimodal paper', 'memory': True, 'conversation_number': 2, 'AI_assistance': False, 'collection_name': 'paper', 'llm_model': 'Llama_13b'}
2023-12-07 11:05:16,297 - ERROR - Error processing the request: CUDA out of memory. Tried to allocate 9.63 GiB (GPU 0; 79.15 GiB total capacity; 12.17 GiB already allocated; 6.68 GiB free; 14.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2023-12-07 11:07:24,969 - INFO - Created a temporary directory at /tmp/tmpc4ht3skb
2023-12-07 11:07:24,969 - INFO - Writing /tmp/tmpc4ht3skb/_remote_module_non_scriptable.py
2023-12-07 11:07:26,772 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-07 11:07:31,016 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-07 11:09:29,302 - INFO - Received requests to /inference endpoint
2023-12-07 11:09:29,403 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 11:09:29,403 - INFO - Received request: {'username': 'amin', 'prompt': 'give me  a summary of the multimodal paper.', 'memory': True, 'conversation_number': 2, 'AI_assistance': False, 'collection_name': 'paper', 'llm_model': 'Llama_13b'}
2023-12-07 11:10:17,286 - INFO - Processed the request successfully
2023-12-07 11:12:05,089 - INFO - Received requests to /inference endpoint
2023-12-07 11:12:05,190 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 11:12:05,190 - INFO - Received request: {'username': 'amin', 'prompt': 'Give me a summary of the paper', 'memory': False, 'conversation_number': 0, 'AI_assistance': False, 'collection_name': 'paper', 'llm_model': 'Llama_13b'}
2023-12-07 11:12:11,281 - INFO - Processed the request successfully
2023-12-07 11:12:40,279 - INFO - Received requests to /inference endpoint
2023-12-07 11:12:40,380 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 11:12:40,380 - INFO - Received request: {'username': 'amin', 'prompt': 'Where is the capital of the uk', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'paper', 'llm_model': 'Llama_13b'}
2023-12-07 11:12:46,206 - INFO - Processed the request successfully
2023-12-07 11:13:41,232 - INFO - request received username='amin' class_name='paper ' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/fea4c21b92fc08de': %s
2023-12-07 11:13:41,860 - INFO - actors creation successful [Actor(WeaviateEmbedder, 34edfad8c270d94e909ad5c201000000), Actor(WeaviateEmbedder, 890f822bab4c5b167f50312401000000), Actor(WeaviateEmbedder, fd8635f546cf87bf4f811b9b01000000)]: %s
2023-12-07 11:13:41,861 - INFO - check 1st step of ray was successful
2023-12-07 11:13:41,862 - INFO - check if ray was successful:
2023-12-07 11:13:41,862 - INFO - check weaviate add data, 
2023-12-07 11:13:41,862 - INFO - request processed successfully username='amin' class_name='paper ' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/fea4c21b92fc08de': %s
2023-12-07 11:13:43,198 - INFO - Check the data that is being passed [{'page_content': 'DINOv2: Learning Robust Visual Features\nwithout Supervision\nMaxime Oquab, Timothe Darcet, Tho Moutakanni,\nHuy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza,\nFrancisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba,\nRussell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat,\nVasu Sharma, Gabriel Synnaeve, Hu Xu, Herv Jegou, Julien Mairal1,\nPatrick Labatut, Armand Joulin, Piotr Bojanowski\nMeta AI Research1Inria\ncore teamequal contribution\nAbstract\nThe recent breakthroughs in natural language processing for model pretraining on large\nquantities of data have opened the way for similar foundation models in computer vision.\nThese models could greatly simplify the use of images in any system by producing all-\npurpose visual features, i.e., features that work across image distributions and tasks without\nnetuning. This work shows that existing pretraining methods, especially self-supervised\nmethods, can produce such features if trained on enough curated data from diverse sources.\nWe revisit existing approaches and combine dierent techniques to scale our pretraining in\nterms of data and model size. Most of the technical contributions aim at accelerating and\nstabilizing the training at scale. In terms of data, we propose an automatic pipeline to build\na dedicated, diverse, and curated image dataset instead of uncurated data, as typically done\nin the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy\net al., 2020) with 1B parameters and distill it into a series of smaller models that surpass\nthe best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the\nbenchmarks at image and pixel levels.\n1 Introduction\nLearning task-agnostic pretrained representations have become the standard in Natural Language Process-\ning (NLP) (Radford et al.; Rael et al., 2020; Chowdhery et al., 2022; Homann et al., 2022; Touvron et al.,\n2023). One can use these features as they are, i.e., without ne-tuning, and achieve performances on down-\nstream tasks that are signicantly better than those produced by task-specic models (Brown et al., 2020).\nThis success has been fueled by pretraining on large quantities of raw text using pretext objectives, such as\nlanguage modeling (Radford et al., 2017) or word vectors (Devlin et al., 2018), that require no supervision.\nFollowing this paradigm shift in NLP, we expect similar foundation models to appear in computer vi-\nsion (Bommasani et al., 2021). These models should generate visual features that work out of the box on\nany task, both at the image level, e.g., image classication, and pixel level, e.g., segmentation. Most promis-\ning eorts towards these foundation models focus on text-guided pretraining, i.e., using a form of textual\nsupervision to guide the training of the features (Joulin et al., 2016; Mahajan et al., 2018; Radford et al.,\n2021). This form of text-guided pretraining limits the information that can be retained about the image\nsince captions only approximate the rich information in images, and complex pixel-level information may\nAll the authors are aliated to Meta, except Julien Mairal who is aliated to Inria. Timothe Darcet and Pierre Fernandez\nhave a co-aliation with Inria. Tho Moutakanni has a co-aliation with Universit Paris Saclay. Alaaeldin El-Nouby has a\nco-aliation with Inria and ENS-PSL. Correspondence: {qas, timdarcet, theomoutakanni, ajoulin, bojanowski}@meta.com\n1arXiv:2304.07193v1  [cs.CV]  14 Apr 2023', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 1: Visualization of the rst PCA components. We compute a PCA between the patches of the\nimages from the same column (a, b, c and d) and show their rst 3 components. Each component is matched\nto a dierent color channel. Same parts are matched between related images despite changes of pose, style\nor even objects. Background is removed by thresholding the rst PCA component.\nnot surface with this supervision. Furthermore, these image encoders require aligned text-image corpora and\nhence, do not oer the exibility of their text counterparts, that is, to learn from raw data alone.\nAn alternative to text-guided pretraining is self-supervised learning (Caron et al., 2018; Chen et al., 2020;\nHe et al., 2021) where features are learned from images alone. These approaches are conceptually closer to\npretext tasks such as language modeling and can capture information at the image and pixel level (Caron\net al., 2021). However, despite their potential to learn all-purposed features, most of the advances in\nself-supervised learning were made in the context of pretraining on a small curated dataset, ImageNet-\n1k (Russakovsky et al., 2015). Some eorts on scaling these approaches beyond ImageNet-1k have been\nattempted (Caron et al., 2019; Goyal et al., 2021; 2022a), but they focused on uncurated datasets, which\ntypically lead to a signicant drop in the quality of the features. This is explained by the lack of control\nover the data quality and diversity, which are essential to produce good features.\nIn this work, we explore if self-supervised learning has the potential to learn all-purposed visual features if\npretrained on a large quantity of curated data. We revisit existing discriminative self-supervised approaches\nthat learn features at both the image and patch level, such as iBOT (Zhou et al., 2021), and we reconsider\nsomeoftheirdesignchoicesunderthelensofalargerdataset. Mostofourtechnicalcontributionsaretailored\ntoward stabilizing and accelerating discriminative self-supervised learning when scaling in model and data\nsizes. These improvements make our approach around 2 faster and require 3 less memory than similar\ndiscriminative self-supervised methods, allowing us to leverage longer training with larger batch sizes.\nRegarding pretraining data, we have built an automatic pipeline to lter and rebalance datasets from an\nextensive collection of uncurated images. This pipeline is inspired by pipelines used in NLP (Wenzek et al.,\n2019), where data similarities are used instead of external metadata and do not require manual annotation.\nA major diculty when dealing with images in the wild is to rebalance concepts and avoid overtting on a\nfew dominant modes. In this work, a naive clustering approach works reasonably well to resolve this issue.\nWe gathered a small but diverse corpus of 142M images to validate our approach.\nFinally, we provide a variety of pretrained visual models, called DINOv2, trained with dierent Vision\nTransformers (ViT) (Dosovitskiy et al., 2016) architectures on our data. We release all the models and\nthe code to retrain DINOv2 on any data. We validate the quality of DINOv2 on various computer vision\nbenchmarks at both image and pixel levels as we scale them, as summarized in Fig. 2. We conclude that self-\n2', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': '101010111012\nflops7578818487Accuracy\nInet-1k\n101010111012\nflops40485664mIoU\nSegmentation\n101010111012\nflops0.91.21.51.8 R-MSE\nMonocular Depth \n101010111012\nflops80848892Accuracy\nClassification\n101010111012\nflops4856647280Accuracy\nFinegrained Classification\n101010111012\nflops30456075mAP\nInstance Retrieval\n101010111012\nflops4050607080Accuracy\nImageNet-{A,R,Sketch}\n101010111012\nflops5055606570Accuracy\nVideo Understanding\nSSL\nWSL\nDINOv2Figure 2: Evolution of performance when scaling in parameters. We show performance on eight\ntypes of vision tasks, as presented in Sec. 7, and average metrics with each type. Features are extracted\nfrom our self-supervised encoders, DINOv2 (dark blue), and we compare them with self-supervised methods\n(pale orange), as well as weakly-supervised methods (dark pink). We report the best-performing weakly-\nsupervised models performance as a dashed horizontal line. Our family of models drastically improves over\nthe previous state of the art in self-supervised learning and reaches performance comparable with weakly-\nsupervised features. See Sec. 7 for a detailed analysis.\nsupervised pretraining alone is a good candidate for learning transferable frozen features that are competitive\nwith the best openly available weakly-supervised models.\n2 Related Work\nIntra-image self-supervised training. A rst family of self-supervised methods focuses on pretext tasks\nbuilt from the image, i.e., extracting a signal from the image to be predicted from the rest of the image.\nThis idea has become prevalent with the work of Doersch et al. (2015), where they train by predicting the\ncontext of a given patch. Many other pretext tasks were introduced based on re-colorizing images (Zhang\net al., 2016), predicting transformations (Gidaris et al., 2018), inpainting (Pathak et al., 2016) or patch\nre-ordering (Noroozi & Favaro, 2016; Misra & Maaten, 2020). Recently, the emergence of patch-based\narchitectures, like ViTs, has led to a revisit of inpainting for pre-training (He et al., 2021; Bao et al., 2021;\nEl-Nouby et al., 2021), potentially in feature space (Assran et al., 2023; Baevski et al., 2022). Of particular\ninterest, He et al. (2021) show that a masked auto-encoder (MAE) learns features that provide substantial\nimprovements when netuned on downstream tasks. This property of MAEs has been further validated\non video (Tong et al., 2022), audio (Xu et al., 2022), and across other modalities (Girdhar et al., 2022).\nHowever, their features require supervised netuning, while our features perform well out of the box.\nDiscriminativeself-supervisedlearning. Thesecondlineofwork, closertoours, isusingdiscriminative\nsignals between images or groups of images to learn features. This family of methods has roots in early\ndeep learning work (Hadsell et al., 2006) but became popular with the emergence of instance classication\nmethods (Dosovitskiy et al., 2014; Bojanowski & Joulin, 2017; Wu et al., 2018). Several improvements\nwere made based either on instance-level objectives (Hna et al., 2019; He et al., 2020; Chen & He, 2020;\nChen et al., 2020; Grill et al., 2020; Caron et al., 2021) or clustering (Caron et al., 2018; Asano et al.,\n2020; Caron et al., 2020). These methods provide performant frozen features on standard benchmarks like\nImageNet (Russakovsky et al., 2015), but they are hard to scale to larger model sizes (Chen et al., 2021). In\n3', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Uncur ated Data \nAugment ed Cur ated Data\nCurated Data\n Embedding\n Deduplication\n RetrievalFigure 3: Overview of our data processing pipeline. Images from curated and uncurated data sources\nare rst mapped to embeddings. Uncurated images are then deduplicated before being matched to curated\nimages. The resulting combination augments the initial dataset through a self-supervised retrieval system.\nthis work, we revisit the training of these approaches in the context of large pretraining datasets and models.\nIn particular, we build on top of Zhou et al. (2021) that we nd particularly suited for scaling.\nScaling self-supervised pretraining. A growing body of work has focused on the scaling abilities of\nself-supervised learning in terms of data and model size (Caron et al., 2019; Goyal et al., 2019; Tian et al.,\n2021; Goyal et al., 2022a). Most of these works use large quantities of uncurated data to train models\nwithout supervision. They show evidence that discriminative methods scale with data, but because of the\npoor quality of the pretraining data, most of the results are obtained by netuning the features. Of particular\ninterest, Goyal et al. (2021) have also shown that these methods benet from scaling in model size given\nenough pretrained data. This line of work questions the ability of self-supervised methods to work on any\ndata while we focus on producing the best pretrained encoders.\nAutomatic data curation. Our dataset construction borrows from the image retrieval community (Wein-\nzaepfeletal.,2021;Radenovietal.,2018b;Bermanetal.,2019;Douzeetal.,2009;Toliasetal.,2015;Revaud\net al., 2019). In particular, the use of retrieval to augment the training set has been studied in the context\nof semi-supervised learning (Yalniz et al., 2019). Similarly, others have used hashtags or other metadata to\nlter uncurated datasets (Mahajan et al., 2018; Radford et al., 2021). Unlike this work, we use no metadata\nnor supervision to lter images and leverage visual similarity between images. Our approach is inspired by\ntext curation pipelines (Wenzek et al., 2019), where a language model is trained on Wikipedia to score texts\nextracted from an uncurated source.\n3 Data Processing\nWe assemble our curated LVD-142M dataset by retrieving, from a large pool of uncurated data, images that\nare close to those in several curated datasets. We describe below the main components in our data pipeline\nincluding the curated/uncurated data sources, the image deduplication step and the retrieval system. Our\npipeline does not require any metadata or text and directly works with images, as shown in Fig. 3. We refer\nthe reader to appendix A for more details on our approach.\nData sources. Our selection of curated datasets is detailed in the appendix (Table 15) and contains\nImageNet-22k, the train split of ImageNet-1k, Google Landmarks and several ne-grained datasets. For the\nuncurated data source, we collect a raw unltered dataset of images from a publicly available repository of\ncrawled web data. From each web page in the repository, we extract URL links of images from <img>tags.\nWe discards URLs that are unsafe or restricted by domains, and post-process the downloaded images (PCA\nhash deduplication, NSFW ltering, and blurring identiable faces). This results in 1.2B unique images.\n4', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Deduplication. We apply the copy detection pipeline of Pizzi et al. (2022) to the uncurated data and\nremove near-duplicate images. This reduces redundancy and increases diversity among images. We also\nremove near-duplicates of images contained in the test or validation set of any benchmark used in this work.\nSelf-supervised image retrieval. We build our curated pretraining dataset by retrieving images from\nour uncurated data source that are close to images in our curated sources. In order to do this, we rst\ncompute an image embedding using a self-supervised ViT-H/16 network pretrained on ImageNet-22k, and\nuse cosine-similarity as a distance measure between images. Then, we perform k-means clustering of the\nuncurated data. Given a query dataset for retrieval, if it is large enough we retrieve N(typically 4) nearest\nneighbors for each query image. If it is small, we sample Mimages from the cluster corresponding to each\nquery image. We adjust NandMby visual inspection of the retrieval result.\nImplementation Details. The deduplication and retrieval stages of our pipeline rely on the Faiss li-\nbrary (Johnson et al., 2019) to eciently index and compute batch searches of nearest embeddings. In\nparticular, we heavily leverage its support for GPU-accelerated indices, using inverted le indices with prod-\nuct quantization codes (Jegou et al., 2010). The whole processing is distributed on a compute cluster of 20\nnodes equipped with 8 V100-32GB GPUs and takes less than two days to produce the LVD-142M dataset.\n4 Discriminative Self-supervised Pre-training\nWe learn our features with a discriminative self-supervised method that can be seen as a combination of\nDINO and iBOT losses with the centering of SwAV (Caron et al., 2020). We also add a regularizer to spread\nfeatures and a short high-resolution training phase. We rapidly introduce each of these approaches, but more\ndetails can be found in the related papers, or in our open-sourced code.\nImage-level objective (Caron et al., 2021). We consider the cross-entropy loss between the\nfeatures extracted from a student and a teacher network. Both features are coming from the class\ntoken of a ViT, obtained from dierent crops of the same image. We learn the parameters of the\nstudent and build the teacher with an exponential moving average of past iterates (He et al., 2020).\nPatch-level objective (Zhou et al., 2021). We randomly mask some of the input patches given\nto the student, but not to the teacher. We then add a cross-entropy loss between the patch features\nof both networks on each masked patch. This loss is combined with the image-level loss.\nUntying head weights between both objectives. We observe that tying the weights associated\nwith both objectives makes the model undert at the patch-level while overtting at the image-level.\nUntying these weights resolves this issue and improve the performances at both scales.\nSinkhorn-Knopp centering (Caron et al., 2020). Ruan et al. (2022) recommend to replace the\nteacher softmax-centering step of DINO and iBot by the Sinkhorn-Knopp (SK) batch normalization\nof SwAV (Caron et al., 2020). We run the Sinkhorn-Knopp algorithm steps for 3 iterations. For the\nstudent, we apply the softmax normalization.\nKoLeo regularizer (Sablayrolles et al., 2018). The KoLeo regularizer derives from the\nKozachenko-Leonenko dierential entropy estimator (see Beirlant et al. (1997); Delattre & Fournier\n(2017)) and encourages a uniform span of the features within a batch. Given a set of nvectors\n(x1, . . . , x n), it is dened asLkoleo =1\nnn\ni=1log(dn,i),where dn,i= min j=ixixjis the mini-\nmum distance between xiand any other point within the batch. We also 2-normalize the features\nbefore computing this regularizer.\nAdapting the resolution (Touvron et al., 2019). Increasing image resolution is key to pixel-\nlevel downstream tasks such as segmentation or detection, where small objects disappear at low\nresolutions. However, training at high resolution is time and memory demanding, and instead, we\nincrease the resolution of images to 518518during a short period at the end of pretraining.\n5', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': '5 Ecient implementation\nWe consider several improvements to train models at a larger scale. We train models on A100 GPUs using\nPyTorch 2.0. The code is also available along with the pretrained models used for feature extraction1.\nThe details of our models are in the appendix, Table 17. With the same hardware, compared to the iBOT\nimplementation, the DINOv2 code runs around 2faster using only 1/3of the memory.\nFast and memory-ecient attention. We implemented our own version of FlashAttention (Dao et al.,\n2022) to improve memory usage and speed on the self-attention layers. Our version is on par with or\nbetter than the original on all cases considered, while covering more use-cases and hardware. Due to the\nGPU hardware specics, the eciency is best when the embedding dimension per head is a multiple of\n64, and the matrix operations are even better when the full embedding dimension is a multiple of 256.\nAs a consequence, our ViT-g architecture slightly diers from the architecture proposed by Zhai et al.\n(2022) in order to maximize compute eciency, and we use an embedding dimension of 1536 with 24 heads\n(64 dim/head), rather than 1408 with 16 heads (88 dim/head). Our experiments did not show signicant\ndierences in nal accuracy, and our ViT-g backbone counts 1.1B parameters.\nNested tensors in self-attention. Our version also allows running in the same forward pass the global\ncrops and the local crops (that have dierent numbers of patch tokens), leading to signicant compute\neciency gains compared to using separate forward and backward passes as done in prior implementations.\nThe lower-level components of our setup are available in the xFormers library2(Lefaudeux et al. (2022)).\nEcient stochastic depth. We implement an improved version of stochastic depth (Huang et al., 2016)\nthat skips the computation of the dropped residuals rather than masking the result. This saves memory and\ncompute in proportion approximately equal to the drop rate, thanks to specic fused kernels. With high\ndrop rates ( d= 40%in this work), this allows a drastic improvement in compute eciency and memory\nusage. The implementation consists of randomly shuing the Bsamples over the batch dimension, and\nslicing the rst (1d)Bsamples for the computations in the block.\nFully-Sharded Data Parallel (FSDP). Minimizing our objective with the AdamW optimizer requires\n4 model replicas in oat32 precision  student, teacher, optimizer rst moments, optimizer second moments.\nThis sums to 16 GBof memory for a billion-parameter model such as our ViT-g. In order to reduce this\nmemory footprint per GPU, we split the model replicas across GPUs, i.e., sharding 16 GBacross GPUs\nusing the PyTorch implementation of FSDP. Consequently, the model size is not bounded by the memory of\na single GPU but by the total sum of GPU memory across compute nodes. The Pytorch implementation of\nFSDP brings a second advantage, which is to save on the cross-GPU communication costs: the weight shards\nare stored in oat32 precision as required by the optimizer, but broadcasting weights and reducing gradients\nis done in oat16 precision for the backbone (MLP heads gradients are reduced in oat32 to avoid training\ninstabilities). This leads to approximately 50% reduction in communication costs compared to the oat32\ngradient all-reduce operation used in DistributedDataParallel (DDP), which is used in other self-supervised\npretraining methods (Caron et al., 2021; Zhou et al., 2021). As a consequence, the training procedure\nscales more eciently than DDP with oat16 autocast when scaling the number of GPU nodes. Overall,\nPytorch-FSDP mixed-precision is superior to DDP with autocast in virtually all cases we encountered.\nModeldistillation. Mostofourtechnicalimprovementstothetrainingloopaimatimprovingthetraining\nof large models over large quantities of data. For smaller models, we distill them from our largest model,\nthe ViT-g, instead of training them from scratch. Knowledge distillation (Hinton et al., 2015) aims at\nreproducing the output of a large model with a smaller model by minimizing some distance between both\noutputs for a set of given inputs. Since our objective function is a form of distillation from the teacher\nnetwork to the student network, we leverage the same training loop with a few exceptions: we use a larger\nmodel as a frozen teacher, keep a spare EMA of the student that we use as our nal model, remove the\nmasking and stochastic depth, and, apply the iBOT loss on the two global crops. In our ablations, we\n1https://github.com/facebookresearch/dinov2\n2https://github.com/facebookresearch/xformers\n6', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'INet-1k k-NN INet-1k linear\niBOT 72.9 82.3\n+(our reproduction) 74.5 1.6 83.2 0.9\n+LayerScale, Stochastic Depth 75.4 0.9 82.0 1.2\n+128k prototypes 76.6 1.2 81.9 0.1\n+KoLeo 78.9 2.3 82.5 0.6\n+SwiGLU FFN 78.7 0.2 83.1 0.6\n+Patch size 14 78.9 0.2 83.5 0.4\n+Teacher momentum 0.994 79.4 0.5 83.6 0.1\n+Tweak warmup schedules 80.5 1.1 83.8 0.2\n+Batch size 3k 81.7 1.2 84.7 0.9\n+Sinkhorn-Knopp 81.7 = 84.7 =\n+Untying heads = DINOv2 82.0 0.3 84.5 0.2\nTable 1:Ablation study of the training dierences between iBOT and DINOv2. We optimize\nfor k-NN performance, as in our experience, the linear probe performance is lower-bounded by the k-NN\nperformance. Some modications, like LayerScale and a high Stochastic Depth (rate= 0.4), incur a decrease\nin linear probe performance, but have the benets of increasing the stability of training by avoiding NaN loss\nvalues during training. Overall, these modications allowed for the next set of improvements to be added.\nExperiments are run using the ViT-Large architecture on ImageNet-22k.\nobserve that this approach achieves better performance than training from scratch, even for a ViT-L. Our\ndistillation method ends up close to the one described by Duval et al. (2023), except we do not modify the\nloss terms for distillation and evaluate the EMA of the student.\n6 Ablation Studies\nWe present a set of ablations to empirically validate dierent components of our pipeline: the technical\nmodications described in Sec. 4, the pretraining data and the impact of model distillation. We consider\nvarious downstream tasks that are described in Sec. 7.\n6.1 Improved Training Recipe\nOur approach improves over the iBOT method by combining it with several existing components described\nin Sec. 4. To evaluate their importance, we train multiple models where we successively add components to\na baseline iBOT model. We report the Top-1 accuracy on the validation set of ImageNet-1k with a k-NN\nand a linear linear in Table 1. Generally, we observe that each component improves the performance on\neither k-NN or linear probing and even both in most cases. Only LayerScale and Stochastic Depth incur a\nperformance drop in linear probing but signicantly improve the training stability in our experience.\n6.2 Pretraining Data Source\nThe quality of features is directly related to the quality of the pretraining data. In this experiment, we\nprobe the impact of LVD-142M compared to ImageNet-22k, a commonly used pretraining dataset, or using\ndirectly raw and uncurated data. For the uncurated dataset, we randomly sample 142million images from\nthe same data source as LVD-142M. We train a ViT-g/14 on each dataset for the same number of iterations.\nWe also include a variant of ImageNet-22k obtained by removing the synsets of ImageNet-1k (INet-22k \\\nINet-1k) for completeness. We report the comparisons in Table 2.\nThe most salient observation is that training on a curated set of images works better on most benchmarks\nthan training on uncurated data. This conrms the benet of curating data, even in the case of self-\nsupervised pretraining. When compared with models trained on ImageNet-22k, training on LVD-142M is\nalso superior on all the benchmarks but ImageNet-1k. This conrms that training on a more diverse set of\n7', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Training Data INet-1k Im-A ADE-20k Oxford-M\nINet-22k 85.9 73.5 46.6 62.5\nINet-22k\\INet-1k 85.3 70.3 46.2 58.7\nUncurated data 83.3 59.4 48.5 54.3\nLVD-142M 85.8 73.9 47.7 64.6\nTable 2:Ablation of the source of pretraining data. We compare the INet-22k dataset that was used\nin iBOT to our dataset, LVD-142M. Each model is trained for the same number of iterations, that is smaller\nthan in our nal run. Pretraining on LVD-142M maintains the performance over INet-1k while leading to\nmodels that perform better in other domains.\nL H g848586\nImageNet-1k\nINet-22k\nLVD142M\nL H g747678\nImageNet-V2\nL H g5060\nImageNet-Sketch\nL H g9294\nFood101\nL H g8090\nCars\nL H g3540\nAmsterTime\nL H g2040\nOxford-H\nFigure 4: Model scale versus data scale. Evolution of performance as a function of model size for\ntwo dierent pretraining datasets: ImageNet-22k (14M images) and LVD-142M (142M images). The ViT-g\ntrained on LVD-142M surpasses the ViT-g trained on ImageNet-22k on most benchmarks.\nimages improves the quality of the features in domains that are not covered by this dataset. Overall, the\nconclusion of this ablation is that our dataset provides a good balance of dierent types of images that leads\nto the best performance overall.\n6.3 Model Size and Data\nWe quantify the importance of scaling data with the model size in Fig. 4. As the size of models grow, training\non LVD-142M becomes more benecial than training on ImageNet-22k. For instance, a ViT-g trained on\nLVD-142M matches the performance on ImageNet-1k of a model trained on ImageNet-22k while signicantly\noutperforming it on the other benchmarks.\n6.4 Loss Components\nWe validated the proposed technical improvements in Sec. 6.1 by adding them incrementally. This section\nanalyzes the performance hit observed if we ablate specic loss terms, starting from our best-performing\nmodel. We ablate the importance of the KoLeo loss and the impact of the masked image modeling term.\nFor both, we report performance on ImageNet-1k using a linear classier, ADE-20k segmentation using a\nlinear classier, and nearest-neighbor image retrieval on Oxford-M. Table 3a shows the impact of using the\nKoLeo loss. We see that the instance retrieval performance improves by more than 8%, conrming that this\nterm helps spread features in the output space. At the same time, the other metrics do not suer from this\nregularization. In Table 3b, we show the impact of using the masked image modeling term from iBOT. This\nterm is critical for dense prediction tasks, leading to almost 3%performance improvement.\n6.5 Impact of Knowledge Distillation\nFor small architectures, we distill larger models instead of training them from scratch. We use the distillation\nprocedure described in Sec. 5. We evaluate the eectiveness of this approach by comparing a ViT-L/14\ntrained from scratch with one distilled from a ViT-g/14 over 12 benchmarks in Fig. 5. We also report the\nperformance of the ViT-g/14 used for distillation as a topline. The distilled model outperforms the one\ntrained from scratch on 10 out of 12 benchmarks, validating our pretraining approach for small models.\n8', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'KoLeo INet-1k Im-A ADE-20k Oxford-M\n\x15 85.3 70.6 47.2 55.6\n 85.8 72.8 47.1 63.9\n(a) Koleo lossMIM INet-1k Im-A ADE-20k Oxford-M\n\x15 85.3 72.0 44.2 64.3\n 85.8 72.8 47.1 63.9\n(b) MIM objective in iBOT\nTable 3:(a)Eect of the KoLeo loss term. (b)Eect of the iBOT Masked Image Modeling (MIM) loss\nterm. Evaluation performed on ImageNet-{1k,A} (classication with linear probe, accuracy %), ADE-20k\n(segmentation with linear layer, mIoU) and Oxford-M (image retrieval, mAP). Each model is trained on the\nsame number of iterations, that is smaller than our nal run. The KoLeo loss term improves nearest-neighbor\nsearch tasks (e.g. retrieval), and the MIM loss improves patch-level tasks (e.g. segmentation).\nINet-1kFoodCarsiNat18\niNat21\nPlaces 205Oxford-H\nParis-H\nINet-A\nINet-RKitti\nNYUd\nViT-L/14 Scratch\nViT-L/14 Distill\nViT-g/14 Scratch\n84.5 86.3 86.592.894.394.7\n81.890.191.4\n77.880.481.6\n83.185.185.7\n66.067.367.5\n47.7 52.6 52.1\n77.6\n84.482.761.7\n71.3\n75.968.1\n74.1\n78.82.57\n2.5\n2.350.345\n0.333\n0.298\n(a) Comparison on individual metricsArch Method INet-1k Segm. Depth Classif.\nViT-g/14 Scratch 86.5 73.4 1.00 92.1\nViT-L/14 Scratch 84.5 72.2 1.10 90.2\nViT-L/14 Distill 86.3 73.3 1.08 91.2\nArch Method Finegr. Retriev. ARSketch Video\nViT-g/14 Scratch 78.3 75.2 77.0 69.3\nViT-L/14 Scratch 75.8 71.3 69.5 67.3\nViT-L/14 Distill 77.6 76.3 74.5 67.5\n(b) Averaged metrics on 8 vision tasks\nFigure 5: Eectiveness of knowledge distillation. Comparison between a ViT-L trained from scratch\nor distilled from DINOv2 using ViT-g/14. For reference, we also report the performance of the ViT-g/14\nteacher. We show that a ViT-L model distilled from a frozen ViT-g outperforms a the same model trained\nfrom scratch on all benchmarks, sometimes even outperforming the distillation target.\n6.6 Impact of Resolution\nWe measure the impact of changing the resolution during the pretraining on the performance of image and\npatch-level features. We consider models trained from scratch using a xed resolution of either 224224\nor416416, and a model trained from scratch at 224224, then resumed for 10k more iterations at\n416416. High-resolution training is compute-intensive, so we conduct this ablation on a small setup: a\nViT-L/16 trained on ImageNet1k. In Fig. 6, we report the performance of a linear probe on ImageNet-1k\nand ADE-20k, evaluated at various resolutions. The model trained on high-resolution images performs best\nacross resolutions, but this comes at a high cost: training at 416is approximate 3more compute-intensive\nthan training at 224. On the other hand, training at high resolution for only 10k iterations at the end of the\ntraining is almost as good and only requiring a fraction of the compute. As a consequence, we include this\nstep at the end of the training rather than training at a high resolution from scratch.\n7 Results\nIn this section, we present the empirical evaluation of our models on many image understanding tasks. We\nevaluate both global and local image representations, on category and instance-level recognition, semantic\nsegmentation, monocular depth prediction, and action recognition. We detail the list of benchmarks in\nAppendix C. The goal of this evaluation is twofold. First, we show that our self-supervised features outper-\n9', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': '224 336 512 640 768\nresolution78798081828384Accuracy\nImageNet-1k\n224 336 512 640 768\nresolution3941434547mIoU\nADE-20K\n224\n416\n224416\nFigure 6: Role of resolution. Performance of ViT-L/16 trained on ImageNet-1k at xed resolution (224\nand 416) or trained at 224 then 416 for a short duration (224 416). We train linear classiers on top of\nfrozen features at dierent resolutions and report Top-1 accuracy on ImageNet and mIoU on ADE-20k. We\nobserve that performing SSL training at high resolution for a short duration achieve behavior and results\nclose to training at the same high resolution for the full training, at a fraction of the cost.\nform the current state of the art by a very large margin. Second, we show that they match, or surpass the\nperformance of weakly-supervised ones on a substantial number of tasks.\nBaselines. In our comparisons, we use two kinds of models as baselines. We compare to the best performing\nself-supervised models that are openly available. First, we run our evaluations for MAE (He et al., 2021),\nDINO (Caron et al., 2021), SEERv2 (Goyal et al., 2022a), MSN (Assran et al., 2022), EsViT (Li et al.,\n2022a), Mugs (Zhou et al., 2022) and iBOT (Zhou et al., 2021). When several architectural variants were\nproposed for a given method, we report results for the one that leads to best top-1 accuracy on ImageNet-1k.\nSecond, we report performance of open-source weakly-supervised models such as CLIP (Radford et al., 2021),\nOpenCLIP (Ilharco et al., 2021), and SWAG (Singh et al., 2022). When evaluating models on ImageNet-1k,\nwe report the performance for each of the aforementioned methods. For all other evaluations, we report\nthe four best-performing models amongst SSL ones. Also, for reference, we report the best performing\nOpenCLIP-G for weakly-supervised ones.\n7.1 ImageNet Classication\nAs a rst evaluation, we probe the quality of the holistic image representation produced by the model on the\nImageNet-1k classication dataset. We evaluate the quality of features by training a simple classier over a\nfrozen backbone, and do not perform netuning of the backbone weights. Following previous work, we use\na linear model for simplicity, ensuring a reproducible evaluation, despite the fact that classes may not be\nlinearly separable. Because most SSL methods were developped using ImageNet-1k validation performance\nas a debugging signal, we also report the top-1 accuracy on ImageNet-ReaL and ImageNet-V2. In order\nto report this additional validation performance, for all models, we run the evaluation with our code. We\ncompare our frozen features to the best publicly available SSL features in Table 4, regardless of architecture\nor pretraining data. We see the components proposed in this work lead to a very signicant improvement\n(+4.2%) over the previous state of the art (iBOT ViT-L/16 trained on ImageNet-22k) on linear evaluation.\nAt the same time, we also see that the performance increase on the alternative test sets is larger for our\nmethod, indicating stronger generalization. We describe details of our linear evaluation in Appendix B.3.\nHow far are we from weakly-supervised models? We also want to validate that our features are com-\npetitive with state-of-the-art open-source weakly supervised models. To this end, we compare on ImageNet-\n1k, using the linear evaluation, to three o-the-shelf methods with several architectural variants. For all\nmodels, we run the linear evaluation using our code, after making sure that our numbers match those re-\nported in technical reports and papers. We show the result of this evaluation in Table 4. We see that our\nbackbone, surpases the performance of OpenCLIP with a ViT-G/14 architecture ( +0.3%) and EVA-CLIP\nwith a ViT-g/14 ( +0.1%). At the same time, we also observe that our performance on the ImageNet-V2 test\n10', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}]: %s
2023-12-07 11:13:43,199 - INFO - Check the results [{'page_content': 'DINOv2: Learning Robust Visual Features\nwithout Supervision\nMaxime Oquab, Timothe Darcet, Tho Moutakanni,\nHuy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza,\nFrancisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba,\nRussell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat,\nVasu Sharma, Gabriel Synnaeve, Hu Xu, Herv Jegou, Julien Mairal1,\nPatrick Labatut, Armand Joulin, Piotr Bojanowski\nMeta AI Research1Inria\ncore teamequal contribution\nAbstract\nThe recent breakthroughs in natural language processing for model pretraining on large\nquantities of data have opened the way for similar foundation models in computer vision.\nThese models could greatly simplify the use of images in any system by producing all-\npurpose visual features, i.e., features that work across image distributions and tasks without\nnetuning. This work shows that existing pretraining methods, especially self-supervised\nmethods, can produce such features if trained on enough curated data from diverse sources.\nWe revisit existing approaches and combine dierent techniques to scale our pretraining in\nterms of data and model size. Most of the technical contributions aim at accelerating and\nstabilizing the training at scale. In terms of data, we propose an automatic pipeline to build\na dedicated, diverse, and curated image dataset instead of uncurated data, as typically done\nin the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy\net al., 2020) with 1B parameters and distill it into a series of smaller models that surpass\nthe best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the\nbenchmarks at image and pixel levels.\n1 Introduction\nLearning task-agnostic pretrained representations have become the standard in Natural Language Process-\ning (NLP) (Radford et al.; Rael et al., 2020; Chowdhery et al., 2022; Homann et al., 2022; Touvron et al.,\n2023). One can use these features as they are, i.e., without ne-tuning, and achieve performances on down-\nstream tasks that are signicantly better than those produced by task-specic models (Brown et al., 2020).\nThis success has been fueled by pretraining on large quantities of raw text using pretext objectives, such as\nlanguage modeling (Radford et al., 2017) or word vectors (Devlin et al., 2018), that require no supervision.\nFollowing this paradigm shift in NLP, we expect similar foundation models to appear in computer vi-\nsion (Bommasani et al., 2021). These models should generate visual features that work out of the box on\nany task, both at the image level, e.g., image classication, and pixel level, e.g., segmentation. Most promis-\ning eorts towards these foundation models focus on text-guided pretraining, i.e., using a form of textual\nsupervision to guide the training of the features (Joulin et al., 2016; Mahajan et al., 2018; Radford et al.,\n2021). This form of text-guided pretraining limits the information that can be retained about the image\nsince captions only approximate the rich information in images, and complex pixel-level information may\nAll the authors are aliated to Meta, except Julien Mairal who is aliated to Inria. Timothe Darcet and Pierre Fernandez\nhave a co-aliation with Inria. Tho Moutakanni has a co-aliation with Universit Paris Saclay. Alaaeldin El-Nouby has a\nco-aliation with Inria and ENS-PSL. Correspondence: {qas, timdarcet, theomoutakanni, ajoulin, bojanowski}@meta.com\n1arXiv:2304.07193v1  [cs.CV]  14 Apr 2023', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 1: Visualization of the rst PCA components. We compute a PCA between the patches of the\nimages from the same column (a, b, c and d) and show their rst 3 components. Each component is matched\nto a dierent color channel. Same parts are matched between related images despite changes of pose, style\nor even objects. Background is removed by thresholding the rst PCA component.\nnot surface with this supervision. Furthermore, these image encoders require aligned text-image corpora and\nhence, do not oer the exibility of their text counterparts, that is, to learn from raw data alone.\nAn alternative to text-guided pretraining is self-supervised learning (Caron et al., 2018; Chen et al., 2020;\nHe et al., 2021) where features are learned from images alone. These approaches are conceptually closer to\npretext tasks such as language modeling and can capture information at the image and pixel level (Caron\net al., 2021). However, despite their potential to learn all-purposed features, most of the advances in\nself-supervised learning were made in the context of pretraining on a small curated dataset, ImageNet-\n1k (Russakovsky et al., 2015). Some eorts on scaling these approaches beyond ImageNet-1k have been\nattempted (Caron et al., 2019; Goyal et al., 2021; 2022a), but they focused on uncurated datasets, which\ntypically lead to a signicant drop in the quality of the features. This is explained by the lack of control\nover the data quality and diversity, which are essential to produce good features.\nIn this work, we explore if self-supervised learning has the potential to learn all-purposed visual features if\npretrained on a large quantity of curated data. We revisit existing discriminative self-supervised approaches\nthat learn features at both the image and patch level, such as iBOT (Zhou et al., 2021), and we reconsider\nsomeoftheirdesignchoicesunderthelensofalargerdataset. Mostofourtechnicalcontributionsaretailored\ntoward stabilizing and accelerating discriminative self-supervised learning when scaling in model and data\nsizes. These improvements make our approach around 2 faster and require 3 less memory than similar\ndiscriminative self-supervised methods, allowing us to leverage longer training with larger batch sizes.\nRegarding pretraining data, we have built an automatic pipeline to lter and rebalance datasets from an\nextensive collection of uncurated images. This pipeline is inspired by pipelines used in NLP (Wenzek et al.,\n2019), where data similarities are used instead of external metadata and do not require manual annotation.\nA major diculty when dealing with images in the wild is to rebalance concepts and avoid overtting on a\nfew dominant modes. In this work, a naive clustering approach works reasonably well to resolve this issue.\nWe gathered a small but diverse corpus of 142M images to validate our approach.\nFinally, we provide a variety of pretrained visual models, called DINOv2, trained with dierent Vision\nTransformers (ViT) (Dosovitskiy et al., 2016) architectures on our data. We release all the models and\nthe code to retrain DINOv2 on any data. We validate the quality of DINOv2 on various computer vision\nbenchmarks at both image and pixel levels as we scale them, as summarized in Fig. 2. We conclude that self-\n2', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': '101010111012\nflops7578818487Accuracy\nInet-1k\n101010111012\nflops40485664mIoU\nSegmentation\n101010111012\nflops0.91.21.51.8 R-MSE\nMonocular Depth \n101010111012\nflops80848892Accuracy\nClassification\n101010111012\nflops4856647280Accuracy\nFinegrained Classification\n101010111012\nflops30456075mAP\nInstance Retrieval\n101010111012\nflops4050607080Accuracy\nImageNet-{A,R,Sketch}\n101010111012\nflops5055606570Accuracy\nVideo Understanding\nSSL\nWSL\nDINOv2Figure 2: Evolution of performance when scaling in parameters. We show performance on eight\ntypes of vision tasks, as presented in Sec. 7, and average metrics with each type. Features are extracted\nfrom our self-supervised encoders, DINOv2 (dark blue), and we compare them with self-supervised methods\n(pale orange), as well as weakly-supervised methods (dark pink). We report the best-performing weakly-\nsupervised models performance as a dashed horizontal line. Our family of models drastically improves over\nthe previous state of the art in self-supervised learning and reaches performance comparable with weakly-\nsupervised features. See Sec. 7 for a detailed analysis.\nsupervised pretraining alone is a good candidate for learning transferable frozen features that are competitive\nwith the best openly available weakly-supervised models.\n2 Related Work\nIntra-image self-supervised training. A rst family of self-supervised methods focuses on pretext tasks\nbuilt from the image, i.e., extracting a signal from the image to be predicted from the rest of the image.\nThis idea has become prevalent with the work of Doersch et al. (2015), where they train by predicting the\ncontext of a given patch. Many other pretext tasks were introduced based on re-colorizing images (Zhang\net al., 2016), predicting transformations (Gidaris et al., 2018), inpainting (Pathak et al., 2016) or patch\nre-ordering (Noroozi & Favaro, 2016; Misra & Maaten, 2020). Recently, the emergence of patch-based\narchitectures, like ViTs, has led to a revisit of inpainting for pre-training (He et al., 2021; Bao et al., 2021;\nEl-Nouby et al., 2021), potentially in feature space (Assran et al., 2023; Baevski et al., 2022). Of particular\ninterest, He et al. (2021) show that a masked auto-encoder (MAE) learns features that provide substantial\nimprovements when netuned on downstream tasks. This property of MAEs has been further validated\non video (Tong et al., 2022), audio (Xu et al., 2022), and across other modalities (Girdhar et al., 2022).\nHowever, their features require supervised netuning, while our features perform well out of the box.\nDiscriminativeself-supervisedlearning. Thesecondlineofwork, closertoours, isusingdiscriminative\nsignals between images or groups of images to learn features. This family of methods has roots in early\ndeep learning work (Hadsell et al., 2006) but became popular with the emergence of instance classication\nmethods (Dosovitskiy et al., 2014; Bojanowski & Joulin, 2017; Wu et al., 2018). Several improvements\nwere made based either on instance-level objectives (Hna et al., 2019; He et al., 2020; Chen & He, 2020;\nChen et al., 2020; Grill et al., 2020; Caron et al., 2021) or clustering (Caron et al., 2018; Asano et al.,\n2020; Caron et al., 2020). These methods provide performant frozen features on standard benchmarks like\nImageNet (Russakovsky et al., 2015), but they are hard to scale to larger model sizes (Chen et al., 2021). In\n3', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Uncur ated Data \nAugment ed Cur ated Data\nCurated Data\n Embedding\n Deduplication\n RetrievalFigure 3: Overview of our data processing pipeline. Images from curated and uncurated data sources\nare rst mapped to embeddings. Uncurated images are then deduplicated before being matched to curated\nimages. The resulting combination augments the initial dataset through a self-supervised retrieval system.\nthis work, we revisit the training of these approaches in the context of large pretraining datasets and models.\nIn particular, we build on top of Zhou et al. (2021) that we nd particularly suited for scaling.\nScaling self-supervised pretraining. A growing body of work has focused on the scaling abilities of\nself-supervised learning in terms of data and model size (Caron et al., 2019; Goyal et al., 2019; Tian et al.,\n2021; Goyal et al., 2022a). Most of these works use large quantities of uncurated data to train models\nwithout supervision. They show evidence that discriminative methods scale with data, but because of the\npoor quality of the pretraining data, most of the results are obtained by netuning the features. Of particular\ninterest, Goyal et al. (2021) have also shown that these methods benet from scaling in model size given\nenough pretrained data. This line of work questions the ability of self-supervised methods to work on any\ndata while we focus on producing the best pretrained encoders.\nAutomatic data curation. Our dataset construction borrows from the image retrieval community (Wein-\nzaepfeletal.,2021;Radenovietal.,2018b;Bermanetal.,2019;Douzeetal.,2009;Toliasetal.,2015;Revaud\net al., 2019). In particular, the use of retrieval to augment the training set has been studied in the context\nof semi-supervised learning (Yalniz et al., 2019). Similarly, others have used hashtags or other metadata to\nlter uncurated datasets (Mahajan et al., 2018; Radford et al., 2021). Unlike this work, we use no metadata\nnor supervision to lter images and leverage visual similarity between images. Our approach is inspired by\ntext curation pipelines (Wenzek et al., 2019), where a language model is trained on Wikipedia to score texts\nextracted from an uncurated source.\n3 Data Processing\nWe assemble our curated LVD-142M dataset by retrieving, from a large pool of uncurated data, images that\nare close to those in several curated datasets. We describe below the main components in our data pipeline\nincluding the curated/uncurated data sources, the image deduplication step and the retrieval system. Our\npipeline does not require any metadata or text and directly works with images, as shown in Fig. 3. We refer\nthe reader to appendix A for more details on our approach.\nData sources. Our selection of curated datasets is detailed in the appendix (Table 15) and contains\nImageNet-22k, the train split of ImageNet-1k, Google Landmarks and several ne-grained datasets. For the\nuncurated data source, we collect a raw unltered dataset of images from a publicly available repository of\ncrawled web data. From each web page in the repository, we extract URL links of images from <img>tags.\nWe discards URLs that are unsafe or restricted by domains, and post-process the downloaded images (PCA\nhash deduplication, NSFW ltering, and blurring identiable faces). This results in 1.2B unique images.\n4', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Deduplication. We apply the copy detection pipeline of Pizzi et al. (2022) to the uncurated data and\nremove near-duplicate images. This reduces redundancy and increases diversity among images. We also\nremove near-duplicates of images contained in the test or validation set of any benchmark used in this work.\nSelf-supervised image retrieval. We build our curated pretraining dataset by retrieving images from\nour uncurated data source that are close to images in our curated sources. In order to do this, we rst\ncompute an image embedding using a self-supervised ViT-H/16 network pretrained on ImageNet-22k, and\nuse cosine-similarity as a distance measure between images. Then, we perform k-means clustering of the\nuncurated data. Given a query dataset for retrieval, if it is large enough we retrieve N(typically 4) nearest\nneighbors for each query image. If it is small, we sample Mimages from the cluster corresponding to each\nquery image. We adjust NandMby visual inspection of the retrieval result.\nImplementation Details. The deduplication and retrieval stages of our pipeline rely on the Faiss li-\nbrary (Johnson et al., 2019) to eciently index and compute batch searches of nearest embeddings. In\nparticular, we heavily leverage its support for GPU-accelerated indices, using inverted le indices with prod-\nuct quantization codes (Jegou et al., 2010). The whole processing is distributed on a compute cluster of 20\nnodes equipped with 8 V100-32GB GPUs and takes less than two days to produce the LVD-142M dataset.\n4 Discriminative Self-supervised Pre-training\nWe learn our features with a discriminative self-supervised method that can be seen as a combination of\nDINO and iBOT losses with the centering of SwAV (Caron et al., 2020). We also add a regularizer to spread\nfeatures and a short high-resolution training phase. We rapidly introduce each of these approaches, but more\ndetails can be found in the related papers, or in our open-sourced code.\nImage-level objective (Caron et al., 2021). We consider the cross-entropy loss between the\nfeatures extracted from a student and a teacher network. Both features are coming from the class\ntoken of a ViT, obtained from dierent crops of the same image. We learn the parameters of the\nstudent and build the teacher with an exponential moving average of past iterates (He et al., 2020).\nPatch-level objective (Zhou et al., 2021). We randomly mask some of the input patches given\nto the student, but not to the teacher. We then add a cross-entropy loss between the patch features\nof both networks on each masked patch. This loss is combined with the image-level loss.\nUntying head weights between both objectives. We observe that tying the weights associated\nwith both objectives makes the model undert at the patch-level while overtting at the image-level.\nUntying these weights resolves this issue and improve the performances at both scales.\nSinkhorn-Knopp centering (Caron et al., 2020). Ruan et al. (2022) recommend to replace the\nteacher softmax-centering step of DINO and iBot by the Sinkhorn-Knopp (SK) batch normalization\nof SwAV (Caron et al., 2020). We run the Sinkhorn-Knopp algorithm steps for 3 iterations. For the\nstudent, we apply the softmax normalization.\nKoLeo regularizer (Sablayrolles et al., 2018). The KoLeo regularizer derives from the\nKozachenko-Leonenko dierential entropy estimator (see Beirlant et al. (1997); Delattre & Fournier\n(2017)) and encourages a uniform span of the features within a batch. Given a set of nvectors\n(x1, . . . , x n), it is dened asLkoleo =1\nnn\ni=1log(dn,i),where dn,i= min j=ixixjis the mini-\nmum distance between xiand any other point within the batch. We also 2-normalize the features\nbefore computing this regularizer.\nAdapting the resolution (Touvron et al., 2019). Increasing image resolution is key to pixel-\nlevel downstream tasks such as segmentation or detection, where small objects disappear at low\nresolutions. However, training at high resolution is time and memory demanding, and instead, we\nincrease the resolution of images to 518518during a short period at the end of pretraining.\n5', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': '5 Ecient implementation\nWe consider several improvements to train models at a larger scale. We train models on A100 GPUs using\nPyTorch 2.0. The code is also available along with the pretrained models used for feature extraction1.\nThe details of our models are in the appendix, Table 17. With the same hardware, compared to the iBOT\nimplementation, the DINOv2 code runs around 2faster using only 1/3of the memory.\nFast and memory-ecient attention. We implemented our own version of FlashAttention (Dao et al.,\n2022) to improve memory usage and speed on the self-attention layers. Our version is on par with or\nbetter than the original on all cases considered, while covering more use-cases and hardware. Due to the\nGPU hardware specics, the eciency is best when the embedding dimension per head is a multiple of\n64, and the matrix operations are even better when the full embedding dimension is a multiple of 256.\nAs a consequence, our ViT-g architecture slightly diers from the architecture proposed by Zhai et al.\n(2022) in order to maximize compute eciency, and we use an embedding dimension of 1536 with 24 heads\n(64 dim/head), rather than 1408 with 16 heads (88 dim/head). Our experiments did not show signicant\ndierences in nal accuracy, and our ViT-g backbone counts 1.1B parameters.\nNested tensors in self-attention. Our version also allows running in the same forward pass the global\ncrops and the local crops (that have dierent numbers of patch tokens), leading to signicant compute\neciency gains compared to using separate forward and backward passes as done in prior implementations.\nThe lower-level components of our setup are available in the xFormers library2(Lefaudeux et al. (2022)).\nEcient stochastic depth. We implement an improved version of stochastic depth (Huang et al., 2016)\nthat skips the computation of the dropped residuals rather than masking the result. This saves memory and\ncompute in proportion approximately equal to the drop rate, thanks to specic fused kernels. With high\ndrop rates ( d= 40%in this work), this allows a drastic improvement in compute eciency and memory\nusage. The implementation consists of randomly shuing the Bsamples over the batch dimension, and\nslicing the rst (1d)Bsamples for the computations in the block.\nFully-Sharded Data Parallel (FSDP). Minimizing our objective with the AdamW optimizer requires\n4 model replicas in oat32 precision  student, teacher, optimizer rst moments, optimizer second moments.\nThis sums to 16 GBof memory for a billion-parameter model such as our ViT-g. In order to reduce this\nmemory footprint per GPU, we split the model replicas across GPUs, i.e., sharding 16 GBacross GPUs\nusing the PyTorch implementation of FSDP. Consequently, the model size is not bounded by the memory of\na single GPU but by the total sum of GPU memory across compute nodes. The Pytorch implementation of\nFSDP brings a second advantage, which is to save on the cross-GPU communication costs: the weight shards\nare stored in oat32 precision as required by the optimizer, but broadcasting weights and reducing gradients\nis done in oat16 precision for the backbone (MLP heads gradients are reduced in oat32 to avoid training\ninstabilities). This leads to approximately 50% reduction in communication costs compared to the oat32\ngradient all-reduce operation used in DistributedDataParallel (DDP), which is used in other self-supervised\npretraining methods (Caron et al., 2021; Zhou et al., 2021). As a consequence, the training procedure\nscales more eciently than DDP with oat16 autocast when scaling the number of GPU nodes. Overall,\nPytorch-FSDP mixed-precision is superior to DDP with autocast in virtually all cases we encountered.\nModeldistillation. Mostofourtechnicalimprovementstothetrainingloopaimatimprovingthetraining\nof large models over large quantities of data. For smaller models, we distill them from our largest model,\nthe ViT-g, instead of training them from scratch. Knowledge distillation (Hinton et al., 2015) aims at\nreproducing the output of a large model with a smaller model by minimizing some distance between both\noutputs for a set of given inputs. Since our objective function is a form of distillation from the teacher\nnetwork to the student network, we leverage the same training loop with a few exceptions: we use a larger\nmodel as a frozen teacher, keep a spare EMA of the student that we use as our nal model, remove the\nmasking and stochastic depth, and, apply the iBOT loss on the two global crops. In our ablations, we\n1https://github.com/facebookresearch/dinov2\n2https://github.com/facebookresearch/xformers\n6', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'INet-1k k-NN INet-1k linear\niBOT 72.9 82.3\n+(our reproduction) 74.5 1.6 83.2 0.9\n+LayerScale, Stochastic Depth 75.4 0.9 82.0 1.2\n+128k prototypes 76.6 1.2 81.9 0.1\n+KoLeo 78.9 2.3 82.5 0.6\n+SwiGLU FFN 78.7 0.2 83.1 0.6\n+Patch size 14 78.9 0.2 83.5 0.4\n+Teacher momentum 0.994 79.4 0.5 83.6 0.1\n+Tweak warmup schedules 80.5 1.1 83.8 0.2\n+Batch size 3k 81.7 1.2 84.7 0.9\n+Sinkhorn-Knopp 81.7 = 84.7 =\n+Untying heads = DINOv2 82.0 0.3 84.5 0.2\nTable 1:Ablation study of the training dierences between iBOT and DINOv2. We optimize\nfor k-NN performance, as in our experience, the linear probe performance is lower-bounded by the k-NN\nperformance. Some modications, like LayerScale and a high Stochastic Depth (rate= 0.4), incur a decrease\nin linear probe performance, but have the benets of increasing the stability of training by avoiding NaN loss\nvalues during training. Overall, these modications allowed for the next set of improvements to be added.\nExperiments are run using the ViT-Large architecture on ImageNet-22k.\nobserve that this approach achieves better performance than training from scratch, even for a ViT-L. Our\ndistillation method ends up close to the one described by Duval et al. (2023), except we do not modify the\nloss terms for distillation and evaluate the EMA of the student.\n6 Ablation Studies\nWe present a set of ablations to empirically validate dierent components of our pipeline: the technical\nmodications described in Sec. 4, the pretraining data and the impact of model distillation. We consider\nvarious downstream tasks that are described in Sec. 7.\n6.1 Improved Training Recipe\nOur approach improves over the iBOT method by combining it with several existing components described\nin Sec. 4. To evaluate their importance, we train multiple models where we successively add components to\na baseline iBOT model. We report the Top-1 accuracy on the validation set of ImageNet-1k with a k-NN\nand a linear linear in Table 1. Generally, we observe that each component improves the performance on\neither k-NN or linear probing and even both in most cases. Only LayerScale and Stochastic Depth incur a\nperformance drop in linear probing but signicantly improve the training stability in our experience.\n6.2 Pretraining Data Source\nThe quality of features is directly related to the quality of the pretraining data. In this experiment, we\nprobe the impact of LVD-142M compared to ImageNet-22k, a commonly used pretraining dataset, or using\ndirectly raw and uncurated data. For the uncurated dataset, we randomly sample 142million images from\nthe same data source as LVD-142M. We train a ViT-g/14 on each dataset for the same number of iterations.\nWe also include a variant of ImageNet-22k obtained by removing the synsets of ImageNet-1k (INet-22k \\\nINet-1k) for completeness. We report the comparisons in Table 2.\nThe most salient observation is that training on a curated set of images works better on most benchmarks\nthan training on uncurated data. This conrms the benet of curating data, even in the case of self-\nsupervised pretraining. When compared with models trained on ImageNet-22k, training on LVD-142M is\nalso superior on all the benchmarks but ImageNet-1k. This conrms that training on a more diverse set of\n7', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Training Data INet-1k Im-A ADE-20k Oxford-M\nINet-22k 85.9 73.5 46.6 62.5\nINet-22k\\INet-1k 85.3 70.3 46.2 58.7\nUncurated data 83.3 59.4 48.5 54.3\nLVD-142M 85.8 73.9 47.7 64.6\nTable 2:Ablation of the source of pretraining data. We compare the INet-22k dataset that was used\nin iBOT to our dataset, LVD-142M. Each model is trained for the same number of iterations, that is smaller\nthan in our nal run. Pretraining on LVD-142M maintains the performance over INet-1k while leading to\nmodels that perform better in other domains.\nL H g848586\nImageNet-1k\nINet-22k\nLVD142M\nL H g747678\nImageNet-V2\nL H g5060\nImageNet-Sketch\nL H g9294\nFood101\nL H g8090\nCars\nL H g3540\nAmsterTime\nL H g2040\nOxford-H\nFigure 4: Model scale versus data scale. Evolution of performance as a function of model size for\ntwo dierent pretraining datasets: ImageNet-22k (14M images) and LVD-142M (142M images). The ViT-g\ntrained on LVD-142M surpasses the ViT-g trained on ImageNet-22k on most benchmarks.\nimages improves the quality of the features in domains that are not covered by this dataset. Overall, the\nconclusion of this ablation is that our dataset provides a good balance of dierent types of images that leads\nto the best performance overall.\n6.3 Model Size and Data\nWe quantify the importance of scaling data with the model size in Fig. 4. As the size of models grow, training\non LVD-142M becomes more benecial than training on ImageNet-22k. For instance, a ViT-g trained on\nLVD-142M matches the performance on ImageNet-1k of a model trained on ImageNet-22k while signicantly\noutperforming it on the other benchmarks.\n6.4 Loss Components\nWe validated the proposed technical improvements in Sec. 6.1 by adding them incrementally. This section\nanalyzes the performance hit observed if we ablate specic loss terms, starting from our best-performing\nmodel. We ablate the importance of the KoLeo loss and the impact of the masked image modeling term.\nFor both, we report performance on ImageNet-1k using a linear classier, ADE-20k segmentation using a\nlinear classier, and nearest-neighbor image retrieval on Oxford-M. Table 3a shows the impact of using the\nKoLeo loss. We see that the instance retrieval performance improves by more than 8%, conrming that this\nterm helps spread features in the output space. At the same time, the other metrics do not suer from this\nregularization. In Table 3b, we show the impact of using the masked image modeling term from iBOT. This\nterm is critical for dense prediction tasks, leading to almost 3%performance improvement.\n6.5 Impact of Knowledge Distillation\nFor small architectures, we distill larger models instead of training them from scratch. We use the distillation\nprocedure described in Sec. 5. We evaluate the eectiveness of this approach by comparing a ViT-L/14\ntrained from scratch with one distilled from a ViT-g/14 over 12 benchmarks in Fig. 5. We also report the\nperformance of the ViT-g/14 used for distillation as a topline. The distilled model outperforms the one\ntrained from scratch on 10 out of 12 benchmarks, validating our pretraining approach for small models.\n8', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'KoLeo INet-1k Im-A ADE-20k Oxford-M\n\x15 85.3 70.6 47.2 55.6\n 85.8 72.8 47.1 63.9\n(a) Koleo lossMIM INet-1k Im-A ADE-20k Oxford-M\n\x15 85.3 72.0 44.2 64.3\n 85.8 72.8 47.1 63.9\n(b) MIM objective in iBOT\nTable 3:(a)Eect of the KoLeo loss term. (b)Eect of the iBOT Masked Image Modeling (MIM) loss\nterm. Evaluation performed on ImageNet-{1k,A} (classication with linear probe, accuracy %), ADE-20k\n(segmentation with linear layer, mIoU) and Oxford-M (image retrieval, mAP). Each model is trained on the\nsame number of iterations, that is smaller than our nal run. The KoLeo loss term improves nearest-neighbor\nsearch tasks (e.g. retrieval), and the MIM loss improves patch-level tasks (e.g. segmentation).\nINet-1kFoodCarsiNat18\niNat21\nPlaces 205Oxford-H\nParis-H\nINet-A\nINet-RKitti\nNYUd\nViT-L/14 Scratch\nViT-L/14 Distill\nViT-g/14 Scratch\n84.5 86.3 86.592.894.394.7\n81.890.191.4\n77.880.481.6\n83.185.185.7\n66.067.367.5\n47.7 52.6 52.1\n77.6\n84.482.761.7\n71.3\n75.968.1\n74.1\n78.82.57\n2.5\n2.350.345\n0.333\n0.298\n(a) Comparison on individual metricsArch Method INet-1k Segm. Depth Classif.\nViT-g/14 Scratch 86.5 73.4 1.00 92.1\nViT-L/14 Scratch 84.5 72.2 1.10 90.2\nViT-L/14 Distill 86.3 73.3 1.08 91.2\nArch Method Finegr. Retriev. ARSketch Video\nViT-g/14 Scratch 78.3 75.2 77.0 69.3\nViT-L/14 Scratch 75.8 71.3 69.5 67.3\nViT-L/14 Distill 77.6 76.3 74.5 67.5\n(b) Averaged metrics on 8 vision tasks\nFigure 5: Eectiveness of knowledge distillation. Comparison between a ViT-L trained from scratch\nor distilled from DINOv2 using ViT-g/14. For reference, we also report the performance of the ViT-g/14\nteacher. We show that a ViT-L model distilled from a frozen ViT-g outperforms a the same model trained\nfrom scratch on all benchmarks, sometimes even outperforming the distillation target.\n6.6 Impact of Resolution\nWe measure the impact of changing the resolution during the pretraining on the performance of image and\npatch-level features. We consider models trained from scratch using a xed resolution of either 224224\nor416416, and a model trained from scratch at 224224, then resumed for 10k more iterations at\n416416. High-resolution training is compute-intensive, so we conduct this ablation on a small setup: a\nViT-L/16 trained on ImageNet1k. In Fig. 6, we report the performance of a linear probe on ImageNet-1k\nand ADE-20k, evaluated at various resolutions. The model trained on high-resolution images performs best\nacross resolutions, but this comes at a high cost: training at 416is approximate 3more compute-intensive\nthan training at 224. On the other hand, training at high resolution for only 10k iterations at the end of the\ntraining is almost as good and only requiring a fraction of the compute. As a consequence, we include this\nstep at the end of the training rather than training at a high resolution from scratch.\n7 Results\nIn this section, we present the empirical evaluation of our models on many image understanding tasks. We\nevaluate both global and local image representations, on category and instance-level recognition, semantic\nsegmentation, monocular depth prediction, and action recognition. We detail the list of benchmarks in\nAppendix C. The goal of this evaluation is twofold. First, we show that our self-supervised features outper-\n9', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': '224 336 512 640 768\nresolution78798081828384Accuracy\nImageNet-1k\n224 336 512 640 768\nresolution3941434547mIoU\nADE-20K\n224\n416\n224416\nFigure 6: Role of resolution. Performance of ViT-L/16 trained on ImageNet-1k at xed resolution (224\nand 416) or trained at 224 then 416 for a short duration (224 416). We train linear classiers on top of\nfrozen features at dierent resolutions and report Top-1 accuracy on ImageNet and mIoU on ADE-20k. We\nobserve that performing SSL training at high resolution for a short duration achieve behavior and results\nclose to training at the same high resolution for the full training, at a fraction of the cost.\nform the current state of the art by a very large margin. Second, we show that they match, or surpass the\nperformance of weakly-supervised ones on a substantial number of tasks.\nBaselines. In our comparisons, we use two kinds of models as baselines. We compare to the best performing\nself-supervised models that are openly available. First, we run our evaluations for MAE (He et al., 2021),\nDINO (Caron et al., 2021), SEERv2 (Goyal et al., 2022a), MSN (Assran et al., 2022), EsViT (Li et al.,\n2022a), Mugs (Zhou et al., 2022) and iBOT (Zhou et al., 2021). When several architectural variants were\nproposed for a given method, we report results for the one that leads to best top-1 accuracy on ImageNet-1k.\nSecond, we report performance of open-source weakly-supervised models such as CLIP (Radford et al., 2021),\nOpenCLIP (Ilharco et al., 2021), and SWAG (Singh et al., 2022). When evaluating models on ImageNet-1k,\nwe report the performance for each of the aforementioned methods. For all other evaluations, we report\nthe four best-performing models amongst SSL ones. Also, for reference, we report the best performing\nOpenCLIP-G for weakly-supervised ones.\n7.1 ImageNet Classication\nAs a rst evaluation, we probe the quality of the holistic image representation produced by the model on the\nImageNet-1k classication dataset. We evaluate the quality of features by training a simple classier over a\nfrozen backbone, and do not perform netuning of the backbone weights. Following previous work, we use\na linear model for simplicity, ensuring a reproducible evaluation, despite the fact that classes may not be\nlinearly separable. Because most SSL methods were developped using ImageNet-1k validation performance\nas a debugging signal, we also report the top-1 accuracy on ImageNet-ReaL and ImageNet-V2. In order\nto report this additional validation performance, for all models, we run the evaluation with our code. We\ncompare our frozen features to the best publicly available SSL features in Table 4, regardless of architecture\nor pretraining data. We see the components proposed in this work lead to a very signicant improvement\n(+4.2%) over the previous state of the art (iBOT ViT-L/16 trained on ImageNet-22k) on linear evaluation.\nAt the same time, we also see that the performance increase on the alternative test sets is larger for our\nmethod, indicating stronger generalization. We describe details of our linear evaluation in Appendix B.3.\nHow far are we from weakly-supervised models? We also want to validate that our features are com-\npetitive with state-of-the-art open-source weakly supervised models. To this end, we compare on ImageNet-\n1k, using the linear evaluation, to three o-the-shelf methods with several architectural variants. For all\nmodels, we run the linear evaluation using our code, after making sure that our numbers match those re-\nported in technical reports and papers. We show the result of this evaluation in Table 4. We see that our\nbackbone, surpases the performance of OpenCLIP with a ViT-G/14 architecture ( +0.3%) and EVA-CLIP\nwith a ViT-g/14 ( +0.1%). At the same time, we also observe that our performance on the ImageNet-V2 test\n10', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}]: %s
2023-12-07 11:13:44,538 - INFO - Check the data that is being passed [{'page_content': 'kNN linear\nMethod Arch. Data Text sup. val val ReaL V2\nWeakly supervised\nCLIP ViT-L/14 WIT-400M  79.8 84.3 88.1 75.3\nCLIP ViT-L/14 336WIT-400M  80.5 85.3 88.8 75.8\nSWAG ViT-H/14 IG3.6B  82.6 85.7 88.7 77.6\nOpenCLIP ViT-H/14 LAION  81.7 84.4 88.4 75.5\nOpenCLIP ViT-G/14 LAION  83.2 86.2 89.4 77.2\nEVA-CLIP ViT-g/14 custom 83.5 86.4 89.3 77.4\nSelf-supervised\nMAE ViT-H/14 INet-1k \x15 49.4 76.6 83.3 64.8\nDINO ViT-S/8 INet-1k \x15 78.6 79.2 85.5 68.2\nSEERv2 RG10B IG2B \x15  79.8  \nMSN ViT-L/7 INet-1k \x15 79.2 80.7 86.0 69.7\nEsViT Swin-B/W=14 INet-1k \x15 79.4 81.3 87.0 70.4\nMugs ViT-L/16 INet-1k \x15 80.2 82.1 86.9 70.8\niBOT ViT-L/16 INet-22k \x15 72.9 82.3 87.5 72.4\nDINOv2ViT-S/14 LVD-142M \x15 79.0 81.1 86.6 70.9\nViT-B/14 LVD-142M \x15 82.1 84.5 88.3 75.1\nViT-L/14 LVD-142M \x15 83.5 86.3 89.5 78.0\nViT-g/14 LVD-142M \x15 83.5 86.5 89.6 78.4\nTable4:LinearevaluationonImageNet-1koffrozenpretrainedfeatures. WereportTop-1accuracy\non the validation set for publicly available models trained on public or private data, and with or without\ntext supervision (text sup.). For reference, we also report the kNN performance on the validation set. We\ncompare across any possible architectures (Arch.), at resolution 224224unless stated otherwise. The\ndataset used for training EVA-CLIP is a custom mixture, see paper for details (Fang et al., 2023).\nset is signicantly better ( +1.1%versus EVA-CLIP), indicating better generalization. For the remainder of\nthis section, we report OpenCLIP-G as a reference for weakly-supervised models.\nCan we netune the encoders? We question if the ability of our models to produce high quality frozen\nfeatures impact their performance when netuned with supervision on a specic dataset. While this is not\ncore to this paper, this experiment is indicative of whether we have involuntarily specialized our models\nto the setting of linear evaluations of frozen features. To run this sanity check, we apply the netuning\npipeline from Touvron et al. (2022), without tweaking hyper-parameters. In Table 5, we show that the\nTop-1 accuracy on the validation set of ImageNet-1k improves by more than +2%when the backbone is\nnetuned. This is true both when using models at resolution 224and448. Further gains can be obtained by\ntuning the hyper-parameters of the netuning, but this is beyond the goal of this sanity check. Nonetheless,\nour best netuned performance ( 88.9%) is only a couple of percent below ( 2.2%) the absolute state of the\narts ( 91.1%), obtained by Chen et al. (2023). As DINOv2 leads to features that are strong in both the linear\nand netuning settings, a strong property of our approach is that netuning is optional .\nRobustness analysis. To complement our study, and probe the generalization of our features, we evaluate\nour ImageNet-1k models trained with linear classication heads on domain generalization benchmarks. We\nuse the best performing linear classier as described above and simply run inference on those benchmarks.\nPlease note that most results in the litterature are obtained with models that are netuned end-to-end on\nImageNet-1k. We show the result of this experiment in Table 6. When comparing with state-of-the-art SSL\nmethods, our models shows drastically better robustness ( +29.6%on A, +22.1%on R and +23.0%on Sketch\n11', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Arch. Res. Linear Finetuned \nViT-g/14224 86.5 88.5 +2.0\n448 86.7 88.9 +2.2\nTable 5: Supervised netuning on ImageNet-1k. We use the pipeline of Touvron et al. (2022) to\nnetune our encoders on ImageNet-1k at resolutions 224224or448448. We compare with the accuracy\nobtained with linear probing and observe only modest improvements with ne-tuning: this suggests that\nDINOv2 features already perform well out-of-the-box.\nMethod Arch Data Im-A Im-R Im-C Sketch\nOpenCLIP ViT-G/14 LAION 63.8 87.8 45.366.4\nMAE ViT-H/14 INet-1k 10.2 34.4 61.4 21.9\nDINO ViT-B/8 INet-1k 23.9 37.0 56.6 25.5\niBOT ViT-L/16 INet-22k 41.5 51.0 43.9 38.5\nDINOv2ViT-S/14 LVD-142M 33.5 53.7 54.4 41.2\nViT-B/14 LVD-142M 55.1 63.3 42.7 50.6\nViT-L/14 LVD-142M 71.3 74.4 31.5 59.3\nViT-g/14 LVD-142M 75.978.828.2 62.5\nTable 6:Domain Generalization with a linear probe on top of frozen features at a resolution of 224.\nHigher numbers are better for all benchmarks except Im-C.\ncompared to iBOT). Our model also improves upon the best weakly-supervised model on ImageNet-A while\nlagging behind on R and Sketch.\n7.2 Additional Image and Video classication Benchmarks\nIn this section we study the generalization of our features on downstream classication benchmarks. We\nconsider two sets of evaluations in that context. On one hand, we use large and negrained datasets such\nas iNaturalist and Places205. On the other, we use the 12 image classication tasks originally proposed\nin SimCLR (Chen et al., 2020). For iNaturalist 2018, iNaturalist 2021, and Places205, we train a linear\nclassier with data augmentations as in Sec. 7.1 We report top-1 accuracy for those three datasets in Table 7.\nInterestingly, our model signicantly outperforms OpenCLIP ViT-G/14 on both variants of iNaturalist\n(+8.6%and+9.7%for 2018 and 2021 respectively), and lags slightly behind on Places 205 ( 2.3%).\nIn a second set of evaluations, we measure the performance of our model on video action recognition even\nthough our features were not trained on videos.. We evaluated features on three datasets, namely UCF-\n101 (Soomro et al., 2012), Kinetics-400 (Kay et al., 2017) and Something-Something v2 (Goyal et al., 2017).\nFor this evaluation, we pick 8evenly spaced frames in the video and train a linear classier on the average\nof the features for UCF and K-400. For SSv2, we opt for concatenation to retain more temporal information\nthan with feature averaging. For each dataset, we measure average accuracy and report the results in\nTable 7. We see that amongst self-supervised approaches, our model clearly sets a new state of the art.\nMoreover, our model matches the accuracy of the OpenCLIP features on UCF and Kinetics ( +0.1%and\n+0.5%respectively) and clearly outperforms them on SSv2 ( +2.5%). This is particularly interesting, as\nSSv2 requires a much richer understanding of the video frames.\nFinally, in Table 8, we compare selected frozen features on 12 transfer classication benchmarks initially\nproposed by Chen et al. (2020). This benchmark covers scenes, objects (food, cars, planes), and textures.\nWe replace the Birdsnap dataset with CUB because the former was not publicly available in its entirety. We\nfollow the experimental protocol as outlined by Chen et al. (2020), namely training a logistic regression on\nprecomputed features. Our model signicantly outperforms state-of-the-art SSL models, with most notable\ndierences on Stanford Cars ( +14.8%versus DINO ViT-B/8) and FGVC Aircraft ( +14.8%versus iBOT\n12', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Image classication Video classication\nFeature Arch iNat2018 iNat2021 Places205 K400 UCF-101 SSv2\nOpenCLIP ViT-G/14 73.0 76.0 69.8 78.3 90.7 35.8\nMAE ViT-H/14 31.0 32.3 52.4 54.2 70.6 29.2\nDINO ViT-B/8 59.6 68.3 60.4 64.5 85.0 32.6\niBOT ViT-L/16 66.3 74.6 64.4 72.6 88.6 38.7\nDINOv2ViT-S/14 69 74.2 62.9 67.8 87 33.1\nViT-B/14 76.4 81.1 66.2 73.2 89.1 34.4\nViT-L/14 80.4 85.1 67.3 76.3 90.5 35.6\nViT-g/14 81.6 85.7 67.5 78.4 91.2 38.3\nTable 7:Linear evaluation on other image and video classication. The image benchmarks contain\na large quantity of ne-grained examples about objects or scenes. The video benchmarks cover action\nclassication and human-object interaction. All the features are frozen with a linear probe on top.\nFeature Arch Food C10 C100 SUN Cars Aircr VOC DTD Pets Cal101 Flowers CUB Avg\nOpenCLIP ViT-G/14 94.5 98.7 91.0 84.0 96.1 80.289.3 86.0 95.798.199.5 89.9 91.9\nMAE ViT-H/14 78.4 96.1 83.9 63.9 56.1 63.4 84.3 75.4 89.4 95.9 92.3 57.2 78.0\nDINO ViT-B/8 85.1 97.2 86.9 70.3 76.6 70.6 86.7 79.6 93.2 95.4 97.6 81.7 85.1\niBOT ViT-L/16 91.0 99.0 92.8 75.6 71.8 72.4 89.0 80.7 87.7 97.5 99.6 82.1 86.6\nDINOv2ViT-S/14 89.1 97.7 87.5 74.4 81.6 74.0 87.8 80.6 95.1 97.0 99.6 88.1 87.7\nViT-B/14 92.8 98.7 91.3 77.3 88.2 79.4 88.2 83.3 96.2 96.1 99.6 89.6 90.1\nViT-L/14 94.3 99.3 93.4 78.7 90.1 81.5 88.3 84.0 96.6 97.5 99.7 90.5 91.2\nViT-g/14 94.7 99.5 94.4 78.7 91.4 87.289.0 84.5 96.797.699.7 91.6 92.1\nTable 8:Linear evaluation of frozen features on ne-grained benchmarks. Accuracy on 12 bench-\nmarks covering objects, scenes and textures following the evaluation protocol proposed in Chen et al. (2020).\nViT-L/16). Even though these benchmarks favor text-guided pretraining, our features are still competitive\nwith OpenCLIP on most classication benchmarks, with the exception of a few datasets, especially SUN\n(5.3%) and Cars (4.7%).\n7.3 Instance Recognition\nIn this experiment, we probe our model on the task of instance-level recognition using a non-parametric\napproach. Images from a database are ranked according to their cosine similarity with a query image. We\nevaluated our model and compare to baselines on Paris and Oxford, that are landmark recognition bench-\nmarks. We also evaluated on Met, a dataset of artworks from the Metropolitan museum, and AmsterTime,\ncontaining street view images matched to archival images of Amsterdam. We measure performance by com-\nputing the mean average precision and report our results in Table 9. We see that our features signicantly\noutperform both SSL ( +41%mAP on Oxford-Hard), and weakly-supervised ( +34%mAP on Oxford-Hard)\nones. Itisinterestingtoseethatourfeaturesperformwellacrosstaskgranularities, bothatthecategory-level\nand instance-level. This is a desirable property for strong o-the-shelf computer vision features.\n7.4 Dense Recognition Tasks\nWe probe the quality of patch-level features extracted from our network on several dense downstream tasks.\nWeconsidersemanticimagesegmentationandmonoculardepthestimationinseveralsettingsandweconduct\nevaluations on multiple datasets for each.\n13', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Oxford Paris Met AmsterTime\nFeature Arch M H M H GAP GAP- ACC mAP\nOpenCLIP ViT-G/14 50.7 19.7 79.2 60.2 6.5 23.9 34.4 24.6\nMAE ViT-H/14 11.7 2.2 19.9 4.7 7.5 23.5 30.5 4.2\nDINO ViT-B/8 40.1 13.7 65.3 35.3 17.1 37.7 43.9 24.6\niBOT ViT-L/16 39.0 12.7 70.7 47.0 25.1 54.8 58.2 26.7\nDINOv2ViT-S/14 68.8 43.2 84.6 68.5 29.4 54.3 57.7 43.5\nViT-B/14 72.9 49.5 90.3 78.6 36.7 63.5 66.1 45.6\nViT-L/14 75.1 54.0 92.7 83.5 40.0 68.9 71.6 50.0\nViT-g/14 73.6 52.3 92.1 82.6 36.8 73.6 76.5 46.7\nTable 9:Evaluation of frozen features on instance-level recognition. We consider 4 dierent bench-\nmarks and report their main metrics.\nADE20k CityScapes Pascal VOC\n(62.9) (86.9) (89.0)\nMethod Arch. lin. +ms lin. +ms lin. +ms\nOpenCLIP ViT-G/14 39.3 46.0 60.3 70.3 71.4 79.2\nMAE ViT-H/14 33.3 30.7 58.4 61.0 67.6 63.3\nDINO ViT-B/8 31.8 35.2 56.9 66.2 66.4 75.6\niBOT ViT-L/16 44.6 47.5 64.8 74.5 82.3 84.3\nDINOv2ViT-S/14 44.3 47.2 66.6 77.1 81.1 82.6\nViT-B/14 47.3 51.3 69.4 80.0 82.5 84.9\nViT-L/14 47.7 53.1 70.3 80.9 82.1 86.0\nViT-g/14 49.053.071.3 81.0 83.0 86.2\nTable 10: Semantic segmentation on ADE20K, CityScapes and Pascal VOC with frozen features\nand a linear classier (lin.) and with multiscale (+ms). The absolute state of the art  from Wang et al.\n(2022), Liu et al. (2021) and Chen et al. (2018) respectively  are mentioned at the top of the Table. For\nreference, using the Mask2Former pipeline (Steiner et al., 2021) with a ViT-Adapter (Chen et al., 2022) on\ntop of our frozen ViT-g/14 backbone gives 60.2 mIoU on ADE-20k.\nSemantic segmentation. For our semantic segmentation evaluation, we consider two dierent setups.\nLinear: a linear layer is trained to predict class logits from a patch tokens. It is used to produce a low-\nresolution logit map (eg 32x32 for a model with patch size 16), which is then upsampled to full resolution\n(512x512) to obtain a segmentation map. This procedure is extremely simple but cannot easily produce\nhigh-resolution segmentations. +ms: a boosted version of the linear setup. We concatenate the patch\ntokens of the 4 last layers, use a larger image resolution of 640, and use multiscale test-time augmentations\nto improve the predictions. We report the performance of our model variants as well as the baselines on\nthree datasets under the two setups in Table 10.\nOur models show very good performance on all datasets and for all setups. Interestingly, our evaluation\nusing+msis on par with fully netuning MAE with an Upernet decoder ( 53.0versus 53.6mIoU). This is\nsurprising because we use a signicantly simpler predictor. Also, our best model, when evaluated using the\nboosted recipe, almost matches the state of the art on Pascal VOC ( 86.2versus 89.0mIoU).\nIn a nal experiment, we freeze our backbone, and plug it into a ViT-Adapter Chen et al. (2022) with a\nMask2former head (Cheng et al., 2022). We tune the weights of the adapter and head, but keep the backbone\nfrozen: only a fraction of the weights are tuned, keeping the training procedure lightweight. We reach 60.2\nmIoU on ADE20k, close to the competitive state of the art, standing at 62.9 mIoU (Wang et al., 2022).\n14', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'NYUd KITTI NYUd SUN RGB-D\n(0.330) (2.10) (0.421)\nMethod Arch. lin. 1 lin. 4 DPT lin. 1 lin. 4 DPT lin. 1 lin. 4 DPT\nOpenCLIP ViT-G/14 0.541 0.510 0.414 3.57 3.21 2.56 0.537 0.476 0.408\nMAE ViT-H/14 0.517 0.483 0.415 3.66 3.26 2.59 0.545 0.523 0.506\nDINO ViT-B/8 0.555 0.539 0.492 3.81 3.56 2.74 0.553 0.541 0.520\niBOT ViT-L/16 0.417 0.387 0.358 3.31 3.07 2.55 0.447 0.435 0.426\nDINOv2ViT-S/14 0.449 0.417 0.356 3.10 2.86 2.34 0.477 0.431 0.409\nViT-B/14 0.399 0.362 0.317 2.90 2.59 2.23 0.448 0.400 0.377\nViT-L/14 0.384 0.333 0.293 2.78 2.50 2.14 0.429 0.396 0.360\nViT-g/14 0.344 0.298 0.279 2.62 2.35 2.11 0.402 0.362 0.338\nTable 11: Depth estimation with frozen features . We report performance when training a linear\nclassier on top of one (lin. 1) or four (lin. 4) transformer layers, as well, as the DPT decoder (DPT) of\nRanftl et al. (2021). We report the RMSE metric on the 3 datasets. Lower is better. For reference, we\nreport state-of-the-art results taken from Li et al. (2022b) on each benchmark on top of the Table.\nDepth estimation. In this experiment, we evaluate our patch-level features on three monocular depth\nestimation benchmarks: NYUd, KITTI and zero-shot transfer from NYUd to SUN3d. We follow the evalu-\nation protocol of Li et al. (2022b). We consider three dierent setups for this evaluation. lin. 1: we extract\nthe last layer of the frozen transformer and concatenate the [CLS]token to each patch token. Then we\nbi-linearly upsample the tokens by a factor of 4 to increase the resolution. Finally we train a simple linear\nlayer using a classication loss by dividing the depth prediction range in 256 uniformly distributed bins and\nuse a linear normalization following Bhat et al. (2021). lin. 4: we use the same protocol that we use with\none layer, but concatenate the tokens from layers l={3,6,9,12}for ViT-S/B, l={5,12,18,24}for ViT-L,\nandl={10,20,30,40}for ViT-g. DPT: we use the DPT decoder (Ranftl et al., 2021) on top of our frozen\nmodels and setup a regression task. We scale the size of the head following the dimension of the features for\neach architecture. We show results for all baselines, all datasets and all setups in Table 11.\nFrom this table, we see that our features clearly surpass the best SSL and WSL features available. It\nis interesting to see that iBOT features extracted from a ViT-L outperform the ones of OpenCLIP with\na ViT-G. This observation supports the intuition that caption-based feature learning fails to learn subtle\npatterns like this one. Also, our model, with the DPT decoder and frozen backbone, matches or exceeds\nthe performance of the recent work of Li et al. (2022b). Finally, the out-of-domain generalization result on\nSUN-RGBd shows that our features allow very good transfer between domains. A depth prediction module\ntrained on indoor scenes from NYUd generalizes pretty well to the outdoor examples of SUN-RGBd.\n7.5 Qualitative Results\nIn this nal section of the empirical evaluation of our features, we propose a few qualitative analyses.\nSemantic Segmentation and Depth Estimation. We show some qualitative results for our dense\nprediction evaluations: segmentation on ADE20K in Fig. 7 and depth estimation on NYUd, KITTI and\nSUN RGB-D in Fig. 7. We compare DINOv2 with OpenCLIP with a linear classier on each dataset. While\nnot perfect, the linear segmentation model using our DINOv2 backbone produces good results and behaves\nmuch better than the OpenCLIP one under this evaluation setup. Indeed, the segmentation mask produced\nby OpenCLIP-G shows many artifacts and disconnected components. The qualitative results on depth\nestimation clearly illustrate the quantitative gap between OpenCLIP and DINOv2. These results highlight\nthat our features, as well as the features extracted from OpenCLIP, are able to linearly separate complex\ninformation such as depth, even though neither was trained with this type of information. However, our\nfeatures lead to a much smoother depth estimation, with less artifacts. Some objects such as the chair on\nthe SUN RGB-D image are completely ignored by OpenCLIP and correctly positioned using our features.\n15', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 7: Segmentation and depth estimation with linear classiers. Examples from ADE20K,\nNYUd, SUN RGB-D and KITTI with a linear probe on frozen OpenCLIP-G and DINOv2-g features.\nFigure 8:Examples of out-of-distribution examples with frozen DINOv2-g features and a linear probe.\nOut-of-distribution generalization. We show a few examples of applying the depth prediction and\nsegmentation linear classiers to out-of-distribution examples in Fig. 8. The qualitative results support our\nclaim that our features transfer between domains. The quality of the depth and segmentation predicted for\npictures of animals, or paintings is very good, even though the domains are very dierent.\nPCA of patch features. We show the results of the principal component analysis (PCA) performed on\nthe patch features extracted by our model. We keep only patches with a positive value after we threshold\nthe rst component. This procedure turns out to separate the images main object from the background. We\ncompute a second PCA on the remaining patches across three images depicting the same category. We color\nthe three rst components with three dierent colors and present the results in Fig. 1 and 9. There are two\ninteresting observations: rst, our unsupervised foreground / background detector, based on detecting the\n16', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 9:Morevisualization oftherst PCAcomponents. We compute the PCA between the patches\nfrom all of the images and show their rst 3 components. Each component corresponds to a specic color\nchannel. Same parts are matched between related images depsite changes of pose, style or even objects.\nBackground is removed by removing patches with a negative score of the rst PCA component.\nhighest variance direction, performs very well and is capable of delineating the boundary of the main object\nin the picture. Second, the other components correspond to "parts" of objects and match well for images of\nthe same category. This is an emerging property  our model was not trained to parse parts of objects.\nPatch matching. Finally, we explore what type of information our patch-level features contain by match-\ning them across images. We start by detecting the foreground object using the procedure described above.\nThen, we compute the euclidean distance between patch features extracted from two images and map them\nby solving an assignment problem. In order to reduce the number of matches, we then apply a non-maximum\nsuppression to keep only the salient ones. In Fig. 10, we show some examples of such matchings.\nWe observe that the features seem to capture information about semantic regions that serve similar purpose\nin dierent objects or animals. For instance, the wing of a plane matches the wing of a bird. We also observe\nthat the model is robust to style (image versus drawing), and to large variation of poses (see the elephant).\n8 Fairness and Bias Analysis\nWe conduct two fairness evaluations of our models. We probe for geographical fairness and potential harmful\nlabel associations. For both evaluations, we experiment with our largest ViT-g model.\n8.1 Geographical Fairness\nWe evaluate geographical fairness on the Dollar Street dataset introduced in De Vries et al. (2019) using\nthe evaluation protocol of Goyal et al. (2022b). This benchmark compares performance across countries and\nincome levels. It contains 16,073 images from 289 households across 54 countries. The task is to recognize\n94 concepts that vary visually between households based on income or location. In Table 12, we compare\nour model with SEERv2 (Goyal et al., 2022a), a model trained on a geographically diverse set of images.\nOur model is slightly fairer across regions and incomes than the SEERv2 model and signicantly better than\n17', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 10: Matching across images. We match patch-level features between images from dierent do-\nmains, poses and even objects that share similar semantic information. This exhibits the ability of our model\nto transfer across domains and understand relations between similar parts of dierent objects.\nIncome buckets Regions\nMethod Arch. Data low medium high Africa Asia Americas Europe\nSEERv2 RG-10B IG-1B 59.7 78.5 86.6 65.9 76.3 81.1 85.6\nDINOv2 ViT-g/14 LVD-142M 67.4 83.3 90.5 74.0 81.6 86.2 89.7\nTable 12: Geographical fairness and diversity analysis across income buckets and regions.\nthe supervised baseline reported by Goyal et al. (2022a). However, we still observe a signicant dierence\nbetween regions, particularly in Africa, where our model performance drops by 25.7% compared to Europe.\nThisshowthatourmodelisstillbiasedtowardWesterncountries. Similarly, ourmodelperformssignicantly\nbetter on high-income households than low-income ones, with a dierence of 31.7%. Despite improvements,\nwe observe signicant biases in our models toward wealthy households from Western countries.\n8.2 Gender, Skintones and Age\nIn a second set of evaluations, we question how our model classies images of people of dierent gender, skin\ntone, and age (all self-reported). We follow the protocol of Goyal et al. (2022b), where we train a multiclass\nclassier on a subset of 619 classes of ImageNet-22k. We group the 619 classes into four broader categories:\nHuman, Possibly Human, Non-Human, or Crime. Non-Human and Crime are considered harmful. Using\nthis classier, we run inference on 2955 images from the Casual Conversations dataset (Hazirbas et al., 2021)\nand keep all labels in the top-5 that are assigned a probability of 0.1 or more. Because of that, we can assign\n18', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Gender Skintone Age Groups\nModel Assoc.female\ndarkerfemale\nlightermale\ndarkermale\nlighter18-30 30-45 45-70 70+\nSEER Non-Human 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nRG-10B Crime 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nHuman 94.9 95.8 86.6 79.0 90.5 88.3 91.9 82.3\nPossibly-Human 13.6 6.7 65.0 60.2 32.8 37.2 29.4 6.5\nDINOv2 Non-Human 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nViT-g/14 Crime 0.0 0.0 0.2 0.0 0.0 0.1 0.0 0.0\nHuman 97.3 97.7 86.1 84.0 91.2 90.2 93.2 88.7\nPossibly-Human 15.8 17.2 52.2 48.1 35.3 37.3 23.0 9.7\nTable 13: Label association fairness evaluation across gender, skintones and age groups. We\nfollow the protocol proposed by Goyal et al. (2022b) with a slight modication. Instead of netuning the\nbackbone, we simply learn a linear classier on the subset of 619 classes of ImageNet-22k.\nModel toGPU TypeGPU PowerGPU-hours PUETotal power Carbon emitted\nReproduce consumption consumption (tCO 2eq)\nDINOv2-g A100-40GB 400W 22,016 1.1 9.7 MWh 3.7\nTable 14: Carbon footprint of reproducing DINOv2. We report the potential carbon emission of\nreproducing DINOv2-g when assuming a power consumption for the A100-40GB of 400W, a PUE of 1.1 and\ncarbon intensity factor of 0.385 kg CO 2e per KWh.\nmultiple classes to each image. We make one modication to the original evaluation protocol: we do not\nbackpropagate gradients to the backbone and keep it frozen. We compare our model to SEERv2 in Table 13.\nOur model often classies images of all groups as Human without large deviations across skin tones. Neither\nSEERv2 nor DINOv2 predict harmful labels from the Non-Human or Crime meta-categories (except for two\ninstances where the background contains bars visually similar to prison bars). We see that our model triggers\nthe Possibly-Human classes often. This class is constructed from objects in ImageNet-22k that are often\nrelated to Humans, such as Scarf, Glasses, or Beard. Our model often predicts the Possibly-Human class\nfor men because of the prevalence of the Beard class. No clear pattern indicates a bias against a particular\ngroup in this study. While this is encouraging, we also acknowledge that a more thorough evaluation of\nbiases may reveal aws in our model.\n9 Estimating the Environmental Impact of Training our Models\nTraining foundation models consumes a signicant amount of energy, resulting in carbon dioxide emissions.\nPatterson et al. (2021) propose a methodology to report an estimation of the carbon emitted during the\ntraining of a model based on the specics of the data center and its power grid. This computation informs\nthe design of the data center used for the training of models and the choice of location for data centers.\nThis methodology requires to know the specics of the data center used for training, which can be complex\nwhen multiple data centers are involved over time. Additionally, these specics are most often not in the\ncontrol of the AI practitioner, and hence, this methodology is less helpful when practioners make technical\ndecisions about future trainings. Instead, in this section, we follow an alternative that reports the potential\ncarbon emission of retraining a similar model in an average data center located in the US. This methodology\nwas used in previous work in natural language processing (Strubell et al., 2019; Touvron et al., 2023) to\nestablish an apple-to-apple comparison between pretraining schemes. More precisely, we x the value of all\nexogenous variables, i.e., the Power Usage Eectiveness (PUE) and carbon intensity factor of a power grid\nto the same values as in Touvron et al. (2023), that is, a PUE of 1.1 and the carbon intensity factor to the\n19', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'US average of 0.385 kg CO 2eq/KWh. We use the same formula as in Patterson et al. (2021) to estimate the\npotential energy consumption and the carbon emission. For the power consumption of an A100-80GB, we\ntake the thermal design power for NVLink systems, which is 400W. We report the potential carbon emission\nof retraining a DINOv2 ViT-g in Table 14. For comparison, retraining an OpenCLIP ViT-L or OpenCLIP\nViT-G would require 22.4 MWh and 118.9 MWh, respectively, if run in the same data center. This is 10 \nmore carbon emission. Note that this comparison is not fair to them, since they also train a text encoder in\nparallel, and we thus do not report them in the table. However, it gives a reasonable guidelines for those who\nare interested in training only visual features: in this context, training a self-supervised model is preferable\nin terms of carbon emission. Training a text-guided model still makes sense when planning to reuse the text\nencoder.\nCarbon footprint of the whole project. Additionally, we estimate the footprint of the whole project\nto be between 0.5k and 1k tCO 2eq using the same grid as presented above. This carbon footprint represents\nin the order of 200k GPU-days. The primary sources of emissions are the self-supervised pre-trainings of\nthe models. For example, a single pre-training of a ViT-g model (22k GPU-hours) emits 3.7 tons of CO 2eq,\nwhile a netuning on ImageNet-1k (1k GPU-hours) emits 0.2 tons. This estimate only considers the GPUs\nelectricity consumption and ignores other emissions, such as their manufacturing and disposal.\n10 Future work and Discussion\nIn this work, we present DINOv2, a new series of image encoders pretrained on large curated data with no\nsupervision. This is the rst SSL work on image data that leads to visual features that close the performance\ngap with (weakly) supervised alternatives across a wide range of benchmarks and without the need for\nnetuning. A few properties emerge from these models, such as an understanding of object parts and scene\ngeometry regardless of the image domains. We expect that more of these properties will emerge at larger\nscales of models and data, akin to instruction emergence in large language models, and plan to continue\nscaling along these axes. This paper also demonstrates that these visual features are compatible with\nclassiers as simple as linear layers - meaning the underlying information is readily available . In future work,\nwe plan to leverage this ability to train a a language-enabled AI system that can process visual features as\nif they were word tokens, and extract the required information to ground the system.\nAcknowledgments.\nWe thank Mathilde Caron for initial discussions that led to this work. We thank Olivia Joulin for the horse\ndrawing used in Fig. 10. We also thank the rest of FAIR and Meta AI for feedback on this work through\nthe entire project.\nReferences\nYuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous clustering\nand representation learning. In ICLR, 2020.\nMahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand\nJoulin, Michael Rabbat, and Nicolas Ballas. Masked siamese networks for label-ecient learning. arXiv\npreprint arXiv:2204.07141 , 2022.\nMahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann\nLeCun, and Nicolas Ballas. Self-supervised learning from images with a joint-embedding predictive archi-\ntecture.arXiv preprint arXiv:2301.08243 , 2023.\nAlexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. Data2vec: A general\nframework for self-supervised learning in speech, vision and language. arXiv preprint arXiv:2202.03555 ,\n2022.\nHangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint\narXiv:2106.08254 , 2021.\n20', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}]: %s
2023-12-07 11:13:44,539 - INFO - Check the results [{'page_content': 'kNN linear\nMethod Arch. Data Text sup. val val ReaL V2\nWeakly supervised\nCLIP ViT-L/14 WIT-400M  79.8 84.3 88.1 75.3\nCLIP ViT-L/14 336WIT-400M  80.5 85.3 88.8 75.8\nSWAG ViT-H/14 IG3.6B  82.6 85.7 88.7 77.6\nOpenCLIP ViT-H/14 LAION  81.7 84.4 88.4 75.5\nOpenCLIP ViT-G/14 LAION  83.2 86.2 89.4 77.2\nEVA-CLIP ViT-g/14 custom 83.5 86.4 89.3 77.4\nSelf-supervised\nMAE ViT-H/14 INet-1k \x15 49.4 76.6 83.3 64.8\nDINO ViT-S/8 INet-1k \x15 78.6 79.2 85.5 68.2\nSEERv2 RG10B IG2B \x15  79.8  \nMSN ViT-L/7 INet-1k \x15 79.2 80.7 86.0 69.7\nEsViT Swin-B/W=14 INet-1k \x15 79.4 81.3 87.0 70.4\nMugs ViT-L/16 INet-1k \x15 80.2 82.1 86.9 70.8\niBOT ViT-L/16 INet-22k \x15 72.9 82.3 87.5 72.4\nDINOv2ViT-S/14 LVD-142M \x15 79.0 81.1 86.6 70.9\nViT-B/14 LVD-142M \x15 82.1 84.5 88.3 75.1\nViT-L/14 LVD-142M \x15 83.5 86.3 89.5 78.0\nViT-g/14 LVD-142M \x15 83.5 86.5 89.6 78.4\nTable4:LinearevaluationonImageNet-1koffrozenpretrainedfeatures. WereportTop-1accuracy\non the validation set for publicly available models trained on public or private data, and with or without\ntext supervision (text sup.). For reference, we also report the kNN performance on the validation set. We\ncompare across any possible architectures (Arch.), at resolution 224224unless stated otherwise. The\ndataset used for training EVA-CLIP is a custom mixture, see paper for details (Fang et al., 2023).\nset is signicantly better ( +1.1%versus EVA-CLIP), indicating better generalization. For the remainder of\nthis section, we report OpenCLIP-G as a reference for weakly-supervised models.\nCan we netune the encoders? We question if the ability of our models to produce high quality frozen\nfeatures impact their performance when netuned with supervision on a specic dataset. While this is not\ncore to this paper, this experiment is indicative of whether we have involuntarily specialized our models\nto the setting of linear evaluations of frozen features. To run this sanity check, we apply the netuning\npipeline from Touvron et al. (2022), without tweaking hyper-parameters. In Table 5, we show that the\nTop-1 accuracy on the validation set of ImageNet-1k improves by more than +2%when the backbone is\nnetuned. This is true both when using models at resolution 224and448. Further gains can be obtained by\ntuning the hyper-parameters of the netuning, but this is beyond the goal of this sanity check. Nonetheless,\nour best netuned performance ( 88.9%) is only a couple of percent below ( 2.2%) the absolute state of the\narts ( 91.1%), obtained by Chen et al. (2023). As DINOv2 leads to features that are strong in both the linear\nand netuning settings, a strong property of our approach is that netuning is optional .\nRobustness analysis. To complement our study, and probe the generalization of our features, we evaluate\nour ImageNet-1k models trained with linear classication heads on domain generalization benchmarks. We\nuse the best performing linear classier as described above and simply run inference on those benchmarks.\nPlease note that most results in the litterature are obtained with models that are netuned end-to-end on\nImageNet-1k. We show the result of this experiment in Table 6. When comparing with state-of-the-art SSL\nmethods, our models shows drastically better robustness ( +29.6%on A, +22.1%on R and +23.0%on Sketch\n11', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Arch. Res. Linear Finetuned \nViT-g/14224 86.5 88.5 +2.0\n448 86.7 88.9 +2.2\nTable 5: Supervised netuning on ImageNet-1k. We use the pipeline of Touvron et al. (2022) to\nnetune our encoders on ImageNet-1k at resolutions 224224or448448. We compare with the accuracy\nobtained with linear probing and observe only modest improvements with ne-tuning: this suggests that\nDINOv2 features already perform well out-of-the-box.\nMethod Arch Data Im-A Im-R Im-C Sketch\nOpenCLIP ViT-G/14 LAION 63.8 87.8 45.366.4\nMAE ViT-H/14 INet-1k 10.2 34.4 61.4 21.9\nDINO ViT-B/8 INet-1k 23.9 37.0 56.6 25.5\niBOT ViT-L/16 INet-22k 41.5 51.0 43.9 38.5\nDINOv2ViT-S/14 LVD-142M 33.5 53.7 54.4 41.2\nViT-B/14 LVD-142M 55.1 63.3 42.7 50.6\nViT-L/14 LVD-142M 71.3 74.4 31.5 59.3\nViT-g/14 LVD-142M 75.978.828.2 62.5\nTable 6:Domain Generalization with a linear probe on top of frozen features at a resolution of 224.\nHigher numbers are better for all benchmarks except Im-C.\ncompared to iBOT). Our model also improves upon the best weakly-supervised model on ImageNet-A while\nlagging behind on R and Sketch.\n7.2 Additional Image and Video classication Benchmarks\nIn this section we study the generalization of our features on downstream classication benchmarks. We\nconsider two sets of evaluations in that context. On one hand, we use large and negrained datasets such\nas iNaturalist and Places205. On the other, we use the 12 image classication tasks originally proposed\nin SimCLR (Chen et al., 2020). For iNaturalist 2018, iNaturalist 2021, and Places205, we train a linear\nclassier with data augmentations as in Sec. 7.1 We report top-1 accuracy for those three datasets in Table 7.\nInterestingly, our model signicantly outperforms OpenCLIP ViT-G/14 on both variants of iNaturalist\n(+8.6%and+9.7%for 2018 and 2021 respectively), and lags slightly behind on Places 205 ( 2.3%).\nIn a second set of evaluations, we measure the performance of our model on video action recognition even\nthough our features were not trained on videos.. We evaluated features on three datasets, namely UCF-\n101 (Soomro et al., 2012), Kinetics-400 (Kay et al., 2017) and Something-Something v2 (Goyal et al., 2017).\nFor this evaluation, we pick 8evenly spaced frames in the video and train a linear classier on the average\nof the features for UCF and K-400. For SSv2, we opt for concatenation to retain more temporal information\nthan with feature averaging. For each dataset, we measure average accuracy and report the results in\nTable 7. We see that amongst self-supervised approaches, our model clearly sets a new state of the art.\nMoreover, our model matches the accuracy of the OpenCLIP features on UCF and Kinetics ( +0.1%and\n+0.5%respectively) and clearly outperforms them on SSv2 ( +2.5%). This is particularly interesting, as\nSSv2 requires a much richer understanding of the video frames.\nFinally, in Table 8, we compare selected frozen features on 12 transfer classication benchmarks initially\nproposed by Chen et al. (2020). This benchmark covers scenes, objects (food, cars, planes), and textures.\nWe replace the Birdsnap dataset with CUB because the former was not publicly available in its entirety. We\nfollow the experimental protocol as outlined by Chen et al. (2020), namely training a logistic regression on\nprecomputed features. Our model signicantly outperforms state-of-the-art SSL models, with most notable\ndierences on Stanford Cars ( +14.8%versus DINO ViT-B/8) and FGVC Aircraft ( +14.8%versus iBOT\n12', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Image classication Video classication\nFeature Arch iNat2018 iNat2021 Places205 K400 UCF-101 SSv2\nOpenCLIP ViT-G/14 73.0 76.0 69.8 78.3 90.7 35.8\nMAE ViT-H/14 31.0 32.3 52.4 54.2 70.6 29.2\nDINO ViT-B/8 59.6 68.3 60.4 64.5 85.0 32.6\niBOT ViT-L/16 66.3 74.6 64.4 72.6 88.6 38.7\nDINOv2ViT-S/14 69 74.2 62.9 67.8 87 33.1\nViT-B/14 76.4 81.1 66.2 73.2 89.1 34.4\nViT-L/14 80.4 85.1 67.3 76.3 90.5 35.6\nViT-g/14 81.6 85.7 67.5 78.4 91.2 38.3\nTable 7:Linear evaluation on other image and video classication. The image benchmarks contain\na large quantity of ne-grained examples about objects or scenes. The video benchmarks cover action\nclassication and human-object interaction. All the features are frozen with a linear probe on top.\nFeature Arch Food C10 C100 SUN Cars Aircr VOC DTD Pets Cal101 Flowers CUB Avg\nOpenCLIP ViT-G/14 94.5 98.7 91.0 84.0 96.1 80.289.3 86.0 95.798.199.5 89.9 91.9\nMAE ViT-H/14 78.4 96.1 83.9 63.9 56.1 63.4 84.3 75.4 89.4 95.9 92.3 57.2 78.0\nDINO ViT-B/8 85.1 97.2 86.9 70.3 76.6 70.6 86.7 79.6 93.2 95.4 97.6 81.7 85.1\niBOT ViT-L/16 91.0 99.0 92.8 75.6 71.8 72.4 89.0 80.7 87.7 97.5 99.6 82.1 86.6\nDINOv2ViT-S/14 89.1 97.7 87.5 74.4 81.6 74.0 87.8 80.6 95.1 97.0 99.6 88.1 87.7\nViT-B/14 92.8 98.7 91.3 77.3 88.2 79.4 88.2 83.3 96.2 96.1 99.6 89.6 90.1\nViT-L/14 94.3 99.3 93.4 78.7 90.1 81.5 88.3 84.0 96.6 97.5 99.7 90.5 91.2\nViT-g/14 94.7 99.5 94.4 78.7 91.4 87.289.0 84.5 96.797.699.7 91.6 92.1\nTable 8:Linear evaluation of frozen features on ne-grained benchmarks. Accuracy on 12 bench-\nmarks covering objects, scenes and textures following the evaluation protocol proposed in Chen et al. (2020).\nViT-L/16). Even though these benchmarks favor text-guided pretraining, our features are still competitive\nwith OpenCLIP on most classication benchmarks, with the exception of a few datasets, especially SUN\n(5.3%) and Cars (4.7%).\n7.3 Instance Recognition\nIn this experiment, we probe our model on the task of instance-level recognition using a non-parametric\napproach. Images from a database are ranked according to their cosine similarity with a query image. We\nevaluated our model and compare to baselines on Paris and Oxford, that are landmark recognition bench-\nmarks. We also evaluated on Met, a dataset of artworks from the Metropolitan museum, and AmsterTime,\ncontaining street view images matched to archival images of Amsterdam. We measure performance by com-\nputing the mean average precision and report our results in Table 9. We see that our features signicantly\noutperform both SSL ( +41%mAP on Oxford-Hard), and weakly-supervised ( +34%mAP on Oxford-Hard)\nones. Itisinterestingtoseethatourfeaturesperformwellacrosstaskgranularities, bothatthecategory-level\nand instance-level. This is a desirable property for strong o-the-shelf computer vision features.\n7.4 Dense Recognition Tasks\nWe probe the quality of patch-level features extracted from our network on several dense downstream tasks.\nWeconsidersemanticimagesegmentationandmonoculardepthestimationinseveralsettingsandweconduct\nevaluations on multiple datasets for each.\n13', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Oxford Paris Met AmsterTime\nFeature Arch M H M H GAP GAP- ACC mAP\nOpenCLIP ViT-G/14 50.7 19.7 79.2 60.2 6.5 23.9 34.4 24.6\nMAE ViT-H/14 11.7 2.2 19.9 4.7 7.5 23.5 30.5 4.2\nDINO ViT-B/8 40.1 13.7 65.3 35.3 17.1 37.7 43.9 24.6\niBOT ViT-L/16 39.0 12.7 70.7 47.0 25.1 54.8 58.2 26.7\nDINOv2ViT-S/14 68.8 43.2 84.6 68.5 29.4 54.3 57.7 43.5\nViT-B/14 72.9 49.5 90.3 78.6 36.7 63.5 66.1 45.6\nViT-L/14 75.1 54.0 92.7 83.5 40.0 68.9 71.6 50.0\nViT-g/14 73.6 52.3 92.1 82.6 36.8 73.6 76.5 46.7\nTable 9:Evaluation of frozen features on instance-level recognition. We consider 4 dierent bench-\nmarks and report their main metrics.\nADE20k CityScapes Pascal VOC\n(62.9) (86.9) (89.0)\nMethod Arch. lin. +ms lin. +ms lin. +ms\nOpenCLIP ViT-G/14 39.3 46.0 60.3 70.3 71.4 79.2\nMAE ViT-H/14 33.3 30.7 58.4 61.0 67.6 63.3\nDINO ViT-B/8 31.8 35.2 56.9 66.2 66.4 75.6\niBOT ViT-L/16 44.6 47.5 64.8 74.5 82.3 84.3\nDINOv2ViT-S/14 44.3 47.2 66.6 77.1 81.1 82.6\nViT-B/14 47.3 51.3 69.4 80.0 82.5 84.9\nViT-L/14 47.7 53.1 70.3 80.9 82.1 86.0\nViT-g/14 49.053.071.3 81.0 83.0 86.2\nTable 10: Semantic segmentation on ADE20K, CityScapes and Pascal VOC with frozen features\nand a linear classier (lin.) and with multiscale (+ms). The absolute state of the art  from Wang et al.\n(2022), Liu et al. (2021) and Chen et al. (2018) respectively  are mentioned at the top of the Table. For\nreference, using the Mask2Former pipeline (Steiner et al., 2021) with a ViT-Adapter (Chen et al., 2022) on\ntop of our frozen ViT-g/14 backbone gives 60.2 mIoU on ADE-20k.\nSemantic segmentation. For our semantic segmentation evaluation, we consider two dierent setups.\nLinear: a linear layer is trained to predict class logits from a patch tokens. It is used to produce a low-\nresolution logit map (eg 32x32 for a model with patch size 16), which is then upsampled to full resolution\n(512x512) to obtain a segmentation map. This procedure is extremely simple but cannot easily produce\nhigh-resolution segmentations. +ms: a boosted version of the linear setup. We concatenate the patch\ntokens of the 4 last layers, use a larger image resolution of 640, and use multiscale test-time augmentations\nto improve the predictions. We report the performance of our model variants as well as the baselines on\nthree datasets under the two setups in Table 10.\nOur models show very good performance on all datasets and for all setups. Interestingly, our evaluation\nusing+msis on par with fully netuning MAE with an Upernet decoder ( 53.0versus 53.6mIoU). This is\nsurprising because we use a signicantly simpler predictor. Also, our best model, when evaluated using the\nboosted recipe, almost matches the state of the art on Pascal VOC ( 86.2versus 89.0mIoU).\nIn a nal experiment, we freeze our backbone, and plug it into a ViT-Adapter Chen et al. (2022) with a\nMask2former head (Cheng et al., 2022). We tune the weights of the adapter and head, but keep the backbone\nfrozen: only a fraction of the weights are tuned, keeping the training procedure lightweight. We reach 60.2\nmIoU on ADE20k, close to the competitive state of the art, standing at 62.9 mIoU (Wang et al., 2022).\n14', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'NYUd KITTI NYUd SUN RGB-D\n(0.330) (2.10) (0.421)\nMethod Arch. lin. 1 lin. 4 DPT lin. 1 lin. 4 DPT lin. 1 lin. 4 DPT\nOpenCLIP ViT-G/14 0.541 0.510 0.414 3.57 3.21 2.56 0.537 0.476 0.408\nMAE ViT-H/14 0.517 0.483 0.415 3.66 3.26 2.59 0.545 0.523 0.506\nDINO ViT-B/8 0.555 0.539 0.492 3.81 3.56 2.74 0.553 0.541 0.520\niBOT ViT-L/16 0.417 0.387 0.358 3.31 3.07 2.55 0.447 0.435 0.426\nDINOv2ViT-S/14 0.449 0.417 0.356 3.10 2.86 2.34 0.477 0.431 0.409\nViT-B/14 0.399 0.362 0.317 2.90 2.59 2.23 0.448 0.400 0.377\nViT-L/14 0.384 0.333 0.293 2.78 2.50 2.14 0.429 0.396 0.360\nViT-g/14 0.344 0.298 0.279 2.62 2.35 2.11 0.402 0.362 0.338\nTable 11: Depth estimation with frozen features . We report performance when training a linear\nclassier on top of one (lin. 1) or four (lin. 4) transformer layers, as well, as the DPT decoder (DPT) of\nRanftl et al. (2021). We report the RMSE metric on the 3 datasets. Lower is better. For reference, we\nreport state-of-the-art results taken from Li et al. (2022b) on each benchmark on top of the Table.\nDepth estimation. In this experiment, we evaluate our patch-level features on three monocular depth\nestimation benchmarks: NYUd, KITTI and zero-shot transfer from NYUd to SUN3d. We follow the evalu-\nation protocol of Li et al. (2022b). We consider three dierent setups for this evaluation. lin. 1: we extract\nthe last layer of the frozen transformer and concatenate the [CLS]token to each patch token. Then we\nbi-linearly upsample the tokens by a factor of 4 to increase the resolution. Finally we train a simple linear\nlayer using a classication loss by dividing the depth prediction range in 256 uniformly distributed bins and\nuse a linear normalization following Bhat et al. (2021). lin. 4: we use the same protocol that we use with\none layer, but concatenate the tokens from layers l={3,6,9,12}for ViT-S/B, l={5,12,18,24}for ViT-L,\nandl={10,20,30,40}for ViT-g. DPT: we use the DPT decoder (Ranftl et al., 2021) on top of our frozen\nmodels and setup a regression task. We scale the size of the head following the dimension of the features for\neach architecture. We show results for all baselines, all datasets and all setups in Table 11.\nFrom this table, we see that our features clearly surpass the best SSL and WSL features available. It\nis interesting to see that iBOT features extracted from a ViT-L outperform the ones of OpenCLIP with\na ViT-G. This observation supports the intuition that caption-based feature learning fails to learn subtle\npatterns like this one. Also, our model, with the DPT decoder and frozen backbone, matches or exceeds\nthe performance of the recent work of Li et al. (2022b). Finally, the out-of-domain generalization result on\nSUN-RGBd shows that our features allow very good transfer between domains. A depth prediction module\ntrained on indoor scenes from NYUd generalizes pretty well to the outdoor examples of SUN-RGBd.\n7.5 Qualitative Results\nIn this nal section of the empirical evaluation of our features, we propose a few qualitative analyses.\nSemantic Segmentation and Depth Estimation. We show some qualitative results for our dense\nprediction evaluations: segmentation on ADE20K in Fig. 7 and depth estimation on NYUd, KITTI and\nSUN RGB-D in Fig. 7. We compare DINOv2 with OpenCLIP with a linear classier on each dataset. While\nnot perfect, the linear segmentation model using our DINOv2 backbone produces good results and behaves\nmuch better than the OpenCLIP one under this evaluation setup. Indeed, the segmentation mask produced\nby OpenCLIP-G shows many artifacts and disconnected components. The qualitative results on depth\nestimation clearly illustrate the quantitative gap between OpenCLIP and DINOv2. These results highlight\nthat our features, as well as the features extracted from OpenCLIP, are able to linearly separate complex\ninformation such as depth, even though neither was trained with this type of information. However, our\nfeatures lead to a much smoother depth estimation, with less artifacts. Some objects such as the chair on\nthe SUN RGB-D image are completely ignored by OpenCLIP and correctly positioned using our features.\n15', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 7: Segmentation and depth estimation with linear classiers. Examples from ADE20K,\nNYUd, SUN RGB-D and KITTI with a linear probe on frozen OpenCLIP-G and DINOv2-g features.\nFigure 8:Examples of out-of-distribution examples with frozen DINOv2-g features and a linear probe.\nOut-of-distribution generalization. We show a few examples of applying the depth prediction and\nsegmentation linear classiers to out-of-distribution examples in Fig. 8. The qualitative results support our\nclaim that our features transfer between domains. The quality of the depth and segmentation predicted for\npictures of animals, or paintings is very good, even though the domains are very dierent.\nPCA of patch features. We show the results of the principal component analysis (PCA) performed on\nthe patch features extracted by our model. We keep only patches with a positive value after we threshold\nthe rst component. This procedure turns out to separate the images main object from the background. We\ncompute a second PCA on the remaining patches across three images depicting the same category. We color\nthe three rst components with three dierent colors and present the results in Fig. 1 and 9. There are two\ninteresting observations: rst, our unsupervised foreground / background detector, based on detecting the\n16', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 9:Morevisualization oftherst PCAcomponents. We compute the PCA between the patches\nfrom all of the images and show their rst 3 components. Each component corresponds to a specic color\nchannel. Same parts are matched between related images depsite changes of pose, style or even objects.\nBackground is removed by removing patches with a negative score of the rst PCA component.\nhighest variance direction, performs very well and is capable of delineating the boundary of the main object\nin the picture. Second, the other components correspond to "parts" of objects and match well for images of\nthe same category. This is an emerging property  our model was not trained to parse parts of objects.\nPatch matching. Finally, we explore what type of information our patch-level features contain by match-\ning them across images. We start by detecting the foreground object using the procedure described above.\nThen, we compute the euclidean distance between patch features extracted from two images and map them\nby solving an assignment problem. In order to reduce the number of matches, we then apply a non-maximum\nsuppression to keep only the salient ones. In Fig. 10, we show some examples of such matchings.\nWe observe that the features seem to capture information about semantic regions that serve similar purpose\nin dierent objects or animals. For instance, the wing of a plane matches the wing of a bird. We also observe\nthat the model is robust to style (image versus drawing), and to large variation of poses (see the elephant).\n8 Fairness and Bias Analysis\nWe conduct two fairness evaluations of our models. We probe for geographical fairness and potential harmful\nlabel associations. For both evaluations, we experiment with our largest ViT-g model.\n8.1 Geographical Fairness\nWe evaluate geographical fairness on the Dollar Street dataset introduced in De Vries et al. (2019) using\nthe evaluation protocol of Goyal et al. (2022b). This benchmark compares performance across countries and\nincome levels. It contains 16,073 images from 289 households across 54 countries. The task is to recognize\n94 concepts that vary visually between households based on income or location. In Table 12, we compare\nour model with SEERv2 (Goyal et al., 2022a), a model trained on a geographically diverse set of images.\nOur model is slightly fairer across regions and incomes than the SEERv2 model and signicantly better than\n17', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 10: Matching across images. We match patch-level features between images from dierent do-\nmains, poses and even objects that share similar semantic information. This exhibits the ability of our model\nto transfer across domains and understand relations between similar parts of dierent objects.\nIncome buckets Regions\nMethod Arch. Data low medium high Africa Asia Americas Europe\nSEERv2 RG-10B IG-1B 59.7 78.5 86.6 65.9 76.3 81.1 85.6\nDINOv2 ViT-g/14 LVD-142M 67.4 83.3 90.5 74.0 81.6 86.2 89.7\nTable 12: Geographical fairness and diversity analysis across income buckets and regions.\nthe supervised baseline reported by Goyal et al. (2022a). However, we still observe a signicant dierence\nbetween regions, particularly in Africa, where our model performance drops by 25.7% compared to Europe.\nThisshowthatourmodelisstillbiasedtowardWesterncountries. Similarly, ourmodelperformssignicantly\nbetter on high-income households than low-income ones, with a dierence of 31.7%. Despite improvements,\nwe observe signicant biases in our models toward wealthy households from Western countries.\n8.2 Gender, Skintones and Age\nIn a second set of evaluations, we question how our model classies images of people of dierent gender, skin\ntone, and age (all self-reported). We follow the protocol of Goyal et al. (2022b), where we train a multiclass\nclassier on a subset of 619 classes of ImageNet-22k. We group the 619 classes into four broader categories:\nHuman, Possibly Human, Non-Human, or Crime. Non-Human and Crime are considered harmful. Using\nthis classier, we run inference on 2955 images from the Casual Conversations dataset (Hazirbas et al., 2021)\nand keep all labels in the top-5 that are assigned a probability of 0.1 or more. Because of that, we can assign\n18', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Gender Skintone Age Groups\nModel Assoc.female\ndarkerfemale\nlightermale\ndarkermale\nlighter18-30 30-45 45-70 70+\nSEER Non-Human 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nRG-10B Crime 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nHuman 94.9 95.8 86.6 79.0 90.5 88.3 91.9 82.3\nPossibly-Human 13.6 6.7 65.0 60.2 32.8 37.2 29.4 6.5\nDINOv2 Non-Human 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nViT-g/14 Crime 0.0 0.0 0.2 0.0 0.0 0.1 0.0 0.0\nHuman 97.3 97.7 86.1 84.0 91.2 90.2 93.2 88.7\nPossibly-Human 15.8 17.2 52.2 48.1 35.3 37.3 23.0 9.7\nTable 13: Label association fairness evaluation across gender, skintones and age groups. We\nfollow the protocol proposed by Goyal et al. (2022b) with a slight modication. Instead of netuning the\nbackbone, we simply learn a linear classier on the subset of 619 classes of ImageNet-22k.\nModel toGPU TypeGPU PowerGPU-hours PUETotal power Carbon emitted\nReproduce consumption consumption (tCO 2eq)\nDINOv2-g A100-40GB 400W 22,016 1.1 9.7 MWh 3.7\nTable 14: Carbon footprint of reproducing DINOv2. We report the potential carbon emission of\nreproducing DINOv2-g when assuming a power consumption for the A100-40GB of 400W, a PUE of 1.1 and\ncarbon intensity factor of 0.385 kg CO 2e per KWh.\nmultiple classes to each image. We make one modication to the original evaluation protocol: we do not\nbackpropagate gradients to the backbone and keep it frozen. We compare our model to SEERv2 in Table 13.\nOur model often classies images of all groups as Human without large deviations across skin tones. Neither\nSEERv2 nor DINOv2 predict harmful labels from the Non-Human or Crime meta-categories (except for two\ninstances where the background contains bars visually similar to prison bars). We see that our model triggers\nthe Possibly-Human classes often. This class is constructed from objects in ImageNet-22k that are often\nrelated to Humans, such as Scarf, Glasses, or Beard. Our model often predicts the Possibly-Human class\nfor men because of the prevalence of the Beard class. No clear pattern indicates a bias against a particular\ngroup in this study. While this is encouraging, we also acknowledge that a more thorough evaluation of\nbiases may reveal aws in our model.\n9 Estimating the Environmental Impact of Training our Models\nTraining foundation models consumes a signicant amount of energy, resulting in carbon dioxide emissions.\nPatterson et al. (2021) propose a methodology to report an estimation of the carbon emitted during the\ntraining of a model based on the specics of the data center and its power grid. This computation informs\nthe design of the data center used for the training of models and the choice of location for data centers.\nThis methodology requires to know the specics of the data center used for training, which can be complex\nwhen multiple data centers are involved over time. Additionally, these specics are most often not in the\ncontrol of the AI practitioner, and hence, this methodology is less helpful when practioners make technical\ndecisions about future trainings. Instead, in this section, we follow an alternative that reports the potential\ncarbon emission of retraining a similar model in an average data center located in the US. This methodology\nwas used in previous work in natural language processing (Strubell et al., 2019; Touvron et al., 2023) to\nestablish an apple-to-apple comparison between pretraining schemes. More precisely, we x the value of all\nexogenous variables, i.e., the Power Usage Eectiveness (PUE) and carbon intensity factor of a power grid\nto the same values as in Touvron et al. (2023), that is, a PUE of 1.1 and the carbon intensity factor to the\n19', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'US average of 0.385 kg CO 2eq/KWh. We use the same formula as in Patterson et al. (2021) to estimate the\npotential energy consumption and the carbon emission. For the power consumption of an A100-80GB, we\ntake the thermal design power for NVLink systems, which is 400W. We report the potential carbon emission\nof retraining a DINOv2 ViT-g in Table 14. For comparison, retraining an OpenCLIP ViT-L or OpenCLIP\nViT-G would require 22.4 MWh and 118.9 MWh, respectively, if run in the same data center. This is 10 \nmore carbon emission. Note that this comparison is not fair to them, since they also train a text encoder in\nparallel, and we thus do not report them in the table. However, it gives a reasonable guidelines for those who\nare interested in training only visual features: in this context, training a self-supervised model is preferable\nin terms of carbon emission. Training a text-guided model still makes sense when planning to reuse the text\nencoder.\nCarbon footprint of the whole project. Additionally, we estimate the footprint of the whole project\nto be between 0.5k and 1k tCO 2eq using the same grid as presented above. This carbon footprint represents\nin the order of 200k GPU-days. The primary sources of emissions are the self-supervised pre-trainings of\nthe models. For example, a single pre-training of a ViT-g model (22k GPU-hours) emits 3.7 tons of CO 2eq,\nwhile a netuning on ImageNet-1k (1k GPU-hours) emits 0.2 tons. This estimate only considers the GPUs\nelectricity consumption and ignores other emissions, such as their manufacturing and disposal.\n10 Future work and Discussion\nIn this work, we present DINOv2, a new series of image encoders pretrained on large curated data with no\nsupervision. This is the rst SSL work on image data that leads to visual features that close the performance\ngap with (weakly) supervised alternatives across a wide range of benchmarks and without the need for\nnetuning. A few properties emerge from these models, such as an understanding of object parts and scene\ngeometry regardless of the image domains. We expect that more of these properties will emerge at larger\nscales of models and data, akin to instruction emergence in large language models, and plan to continue\nscaling along these axes. This paper also demonstrates that these visual features are compatible with\nclassiers as simple as linear layers - meaning the underlying information is readily available . In future work,\nwe plan to leverage this ability to train a a language-enabled AI system that can process visual features as\nif they were word tokens, and extract the required information to ground the system.\nAcknowledgments.\nWe thank Mathilde Caron for initial discussions that led to this work. We thank Olivia Joulin for the horse\ndrawing used in Fig. 10. We also thank the rest of FAIR and Meta AI for feedback on this work through\nthe entire project.\nReferences\nYuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous clustering\nand representation learning. In ICLR, 2020.\nMahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand\nJoulin, Michael Rabbat, and Nicolas Ballas. Masked siamese networks for label-ecient learning. arXiv\npreprint arXiv:2204.07141 , 2022.\nMahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann\nLeCun, and Nicolas Ballas. Self-supervised learning from images with a joint-embedding predictive archi-\ntecture.arXiv preprint arXiv:2301.08243 , 2023.\nAlexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. Data2vec: A general\nframework for self-supervised learning in speech, vision and language. arXiv preprint arXiv:2202.03555 ,\n2022.\nHangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint\narXiv:2106.08254 , 2021.\n20', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}]: %s
2023-12-07 11:13:45,844 - INFO - Check the data that is being passed [{'page_content': 'Jan Beirlant, Edward J Dudewicz, Lszl Gyr, Edward C Van der Meulen, et al. Nonparametric entropy\nestimation: An overview. International Journal of Mathematical and Statistical Sciences , 6(1):1739, 1997.\nMaxim Berman, Herv Jgou, Vedaldi Andrea, Iasonas Kokkinos, and Matthijs Douze. MultiGrain: a unied\nimage embedding for classes and instances. arXiv preprint arXiv:1902.05509 , 2019.\nLucas Beyer, Olivier J Hna, Alexander Kolesnikov, Xiaohua Zhai, and Aron van den Oord. Are we done\nwith imagenet? arXiv preprint arXiv:2006.07159 , 2020.\nShariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. AdaBins: Depth estimation using adaptive\nbins. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . IEEE, jun\n2021. doi: 10.1109/cvpr46437.2021.00400. URL https://doi.org/10.1109%2Fcvpr46437.2021.00400 .\nPiotr Bojanowski and Armand Joulin. Unsupervised learning by predicting noise. In ICML, 2017.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of\nfoundation models. arXiv preprint arXiv:2108.07258 , 2021.\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101  mining discriminative components\nwith random forests. In European Conference on Computer Vision , 2014.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\npreprint arXiv:2005.14165 , 2020.\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised\nlearning of visual features. In ECCV, 2018.\nMathilde Caron, Piotr Bojanowski, Julien Mairal, and Armand Joulin. Unsupervised pre-training of image\nfeatures on non-curated data. In ICCV, 2019.\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsuper-\nvised learning of visual features by contrasting cluster assignments. In NeurIPS , 2020.\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv Jgou, Julien Mairal, Piotr Bojanowski, and Armand\nJoulin. Emerging properties in self-supervised vision transformers. arXiv preprint arXiv:2104.14294 , 2021.\nLiang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schro, and Hartwig Adam. Encoder-decoder\nwith atrous separable convolution for semantic image segmentation. In Proceedings of the European con-\nference on computer vision (ECCV) , pp. 801818, 2018.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Georey Hinton. A simple framework for contrastive\nlearning of visual representations. preprint arXiv:2002.05709 , 2020.\nXiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi\nDong, Thang Luong, Cho-Jui Hsieh, et al. Symbolic discovery of optimization algorithms. arXiv preprint\narXiv:2302.06675 , 2023.\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. preprint arXiv:2011.10566 ,\n2020.\nXinleiChen, SainingXie, andKaimingHe. Anempiricalstudyoftrainingself-supervisedvisiontransformers.\narXiv preprint arXiv:2104.02057 , 2021.\nZhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer\nadapter for dense predictions. arXiv preprint arXiv:2205.08534 , 2022.\nBowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention\nmask transformer for universal image segmentation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pp. 12901299, 2022.\n21', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language\nmodeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.\nM. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In\nProceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , 2014.\nMarius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,\nUwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understand-\ning. InProceedings of the IEEE conference on computer vision and pattern recognition , pp. 32133223,\n2016.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R. Flashattention: Fast and memory-\necient exact attention with io-awareness. arXiv preprint arXiv:2205.14135 , 2022.\nTerrance De Vries, Ishan Misra, Changhan Wang, and Laurens Van der Maaten. Does object recognition\nworkforeveryone? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition\nworkshops , pp. 5259, 2019.\nSylvain Delattre and Nicolas Fournier. On the kozachenkoleonenko entropy estimator. Journal of Statistical\nPlanning and Inference , 185:6993, 2017.\nJacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert: Pre-trainingofdeepbidirectional\ntransformers for language understanding. preprint arXiv:1810.04805 , 2018.\nJosip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander Kolesnikov,\nJoan Puigcerver, Matthias Minderer, Alexander DAmour, Dan Moldovan, et al. On robustness and\ntransferabilityofconvolutionalneuralnetworks. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pp. 1645816468, 2021.\nCarl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context\nprediction. In ICCV, 2015.\nAlexey Dosovitskiy, Jost Tobias Springenberg, Martin A. Riedmiller, and Thomas Brox. Discriminative\nunsupervised feature learning with convolutional neural networks. CoRR, abs/1406.6909, 2014. URL\nhttp://arxiv.org/abs/1406.6909 .\nAlexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Dis-\ncriminative unsupervised feature learning with exemplar convolutional neural networks. TPAMI, 2016.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-\nterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. preprint arXiv:2010.11929 , 2020.\nMatthijs Douze, Herv Jgou, Harsimrat Sandhawalia, Laurent Amsaleg, and Cordelia Schmid. Evaluation\nof gist descriptors for web-scale image search. In CIVR, 2009.\nQuentin Duval, Ishan Misra, and Nicolas Ballas. A simple recipe for competitive low-compute self supervised\nvision models. arXiv preprint arXiv:2301.09451 , 2023.\nAlaaeldin El-Nouby, Gautier Izacard, Hugo Touvron, Ivan Laptev, Herv Jegou, and Edouard Grave. Are\nlarge-scale datasets necessary for self-supervised pre-training? arXiv preprint arXiv:2112.10740 , 2021.\nM. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal\nvisual object classes challenge: A retrospective. International Journal of Computer Vision , 111(1):98136,\nJanuary 2015.\nYuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang,\nand Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. 2023.\n22', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples:\nAn incremental bayesian approach tested on 101 object categories. In 2004 conference on computer vision\nand pattern recognition workshop , pp. 178178. IEEE, 2004.\nAndreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti\ndataset. The International Journal of Robotics Research , 32(11):12311237, 2013.\nSpyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting\nimage rotations, 2018.\nRohit Girdhar, Alaaeldin El-Nouby, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Is-\nhan Misra. Omnimae: Single model masked pretraining on images and videos. arXiv preprint\narXiv:2206.08356 , 2022.\nPriya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. Scaling and benchmarking self-supervised\nvisual representation learning. In ICCV, 2019.\nPriya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat Singh,\nVitaliy Liptchinsky, Ishan Misra, Armand Joulin, et al. Self-supervised pretraining of visual features in\nthe wild. preprint arXiv:2103.01988 , 2021.\nPriya Goyal, Quentin Duval, Isaac Seessel, Mathilde Caron, Mannat Singh, Ishan Misra, Levent Sagun,\nArmand Joulin, and Piotr Bojanowski. Vision models are more robust and fair when pretrained on\nuncurated images without supervision. arXiv preprint arXiv:2202.08360 , 2022a.\nPriya Goyal, Adriana Romero Soriano, Caner Hazirbas, Levent Sagun, and Nicolas Usunier. Fairness in-\ndicators for systematic assessments of visual feature extractors. In 2022 ACM Conference on Fairness,\nAccountability, and Transparency , pp. 7088, 2022b.\nRaghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, He-\nuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The" something\nsomething" video database for learning and evaluating visual common sense. In Proceedings of the IEEE\ninternational conference on computer vision , pp. 58425850, 2017.\nJean-Bastien Grill, Florian Strub, Florent Altch, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya,\nCarl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray\nKavukcuoglu, Rmi Munos, and Michal Valko. Bootstrap your own latent: A new approach to self-\nsupervised learning. In NeurIPS , 2020.\nRaia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping.\nInCVPR, 2006.\nCanerHazirbas, JoannaBitton, BrianDolhansky, JacquelinePan, AlbertGordo, andCristianCantonFerrer.\nTowards measuring fairness in ai: the casual conversations dataset. IEEE Transactions on Biometrics,\nBehavior, and Identity Science , 4(3):324332, 2021.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised\nvisual representation learning. In CVPR, 2020.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollr, and Ross Girshick. Masked autoencoders\nare scalable vision learners. arXiv preprint arXiv:2111.06377 , 2021.\nOlivierJHna, AravindSrinivas, JereyDeFauw, AliRazavi, CarlDoersch, SMEslami, andAaronvanden\nOord. Data-ecientimagerecognitionwithcontrastivepredictivecoding. preprint arXiv:1905.09272 ,2019.\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions\nand perturbations. In International Conference on Learning Representations , 2019.\n23', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai,\nTyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-\nof-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pp. 83408349, 2021.\nGeorey Hinton, Oriol Vinyals, and Je Dean. Distilling the knowledge in a neural network. preprint\narXiv:1503.02531 , 2015.\nJordan Homann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal\nlarge language models. arXiv preprint arXiv:2203.15556 , 2022.\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochas-\ntic depth. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands,\nOctober 1114, 2016, Proceedings, Part IV 14 , pp. 646661. Springer, 2016.\nGabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal\nDave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig\nSchmidt. Openclip. 2021.\nHerve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. IEEE\ntransactions on pattern analysis and machine intelligence , 33(1), 2010.\nJe Johnson, Matthijs Douze, and Herv Jgou. Billion-scale similarity search with GPUs. IEEE Transac-\ntions on Big Data , 7(3):535547, 2019.\nArmand Joulin, Laurens Van Der Maaten, Allan Jabri, and Nicolas Vasilache. Learning visual features from\nlarge weakly supervised data. In ECCV, 2016.\nWill Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio\nViola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv\npreprint arXiv:1705.06950 , 2017.\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for ne-grained catego-\nrization. In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13) , Sydney,\nAustralia, 2013.\nAlex Krizhevsky, Georey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\nBenjaminLefaudeux, FranciscoMassa, DianaLiskovich, WenhanXiong, VittorioCaggiano, SeanNaren, Min\nXu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza. xformers: A modular\nand hackable transformer modelling library. https://github.com/facebookresearch/xformers , 2022.\nChunyuan Li, Jianwei Yang, Pengchuan Zhang, Mei Gao, Bin Xiao, Xiyang Dai, Lu Yuan, and Jianfeng\nGao. Ecient self-supervised vision transformers for representation learning. In ICLR, 2022a.\nZhenyu Li, Xuyang Wang, Xianming Liu, and Junjun Jiang. Binsformer: Revisiting adaptive bins for\nmonocular depth estimation. arXiv preprint arXiv:2204.00987 , 2022b.\nHuajun Liu, Fuqiang Liu, Xinyi Fan, and Dong Huang. Polarized self-attention: towards high-quality pixel-\nwise regression. arXiv preprint arXiv:2107.00782 , 2021.\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin\nBharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In ECCV,\n2018.\nS. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classication of aircraft.\nTechnical report, 2013.\n24', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. In\nCVPR, 2020.\nMaria-Elena Nilsback and Andrew Zisserman. Automated ower classication over a large number of classes.\nInIndian Conference on Computer Vision, Graphics and Image Processing , Dec 2008.\nMehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles.\nIn Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (eds.), Computer Vision  ECCV 2016 , pp.\n6984, Cham, 2016. Springer International Publishing. ISBN 978-3-319-46466-4.\nOmkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In IEEE\nConference on Computer Vision and Pattern Recognition , 2012.\nDeepak Pathak, Philipp Krhenbhl, Je Donahue, Trevor Darrell, and Alexei Efros. Context encoders:\nFeature learning by inpainting. In CVPR, 2016.\nDavid Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David\nSo, Maud Texier, and Je Dean. Carbon emissions and large neural network training. arXiv preprint\narXiv:2104.10350 , 2021.\nEd Pizzi, Sreya Dutta Roy, Sugosh Nagavara Ravindra, Priya Goyal, and Matthijs Douze. A self-supervised\ndescriptor for image copy detection. arXiv preprint arXiv:2202.10261 , 2022.\nFilip Radenovi, Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondej Chum. Revisiting oxford and\nparis: Large-scale image retrieval benchmarking. In CVPR, 2018a.\nFilip Radenovi, Giorgos Tolias, and Ondej Chum. Fine-tuning cnn image retrieval with no human anno-\ntation.IEEE transactions on pattern analysis and machine intelligence , 2018b.\nAlec Radford, Jerey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models\nare unsupervised multitask learners.\nAlec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to generate reviews and discovering sentiment.\narXiv preprint arXiv:1704.01444 , 2017.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In International Conference on Machine Learning , pp. 87488763. PMLR,\n2021.\nColin Rael, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unied text-to-text transformer.\nJ. Mach. Learn. Res. , 21(140):167, 2020.\nRen Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Pro-\nceedings of the IEEE/CVF International Conference on Computer Vision , pp. 1217912188, 2021.\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classiers generalize\nto imagenet? In International Conference on Machine Learning , pp. 53895400. PMLR, 2019.\nJerome Revaud, Jon Almazn, Rafael S Rezende, and Cesar Roberto de Souza. Learning with average\nprecision: Training image retrieval with a listwise loss. In ICCV, 2019.\nYangjun Ruan, Saurabh Singh, Warren Morningstar, Alexander A Alemi, Sergey Ioe, Ian Fischer, and\nJoshua V Dillon. Weighted ensemble self-supervised learning. arXiv preprint arXiv:2211.09981 , 2022.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej\nKarpathy, Aditya Khosla, Michael Bernstein, Alexander C Berg, and Li Fei-Fei. Imagenet large scale\nvisual recognition challenge. IJCV, 2015.\n25', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, and Herv Jgou. Spreading vectors for similarity\nsearch.arXiv preprint arXiv:1806.03198 , 2018.\nNoam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202 , 2020.\nNathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support\ninference from rgbd images. In European conference on computer vision , pp. 746760. Springer, 2012.\nMannat Singh, Laura Gustafson, Aaron Adcock, Vinicius de Freitas Reis, Bugra Gedik, Raj Prateek\nKosaraju, Dhruv Mahajan, Ross Girshick, Piotr Dollr, and Laurens van der Maaten. Revisiting Weakly\nSupervised Pre-Training of Visual Perception Models. In CVPR, 2022.\nShuranSong, SamuelPLichtenberg, andJianxiongXiao. Sunrgb-d: Argb-dsceneunderstandingbenchmark\nsuite. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 567576,\n2015.\nKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes\nfrom videos in the wild. arXiv preprint arXiv:1212.0402 , 2012.\nAndreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer.\nHow to train your vit? data, augmentation, and regularization in vision transformers. arXiv preprint\narXiv:2106.10270 , 2021.\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning\nin nlp.arXiv preprint arXiv:1906.02243 , 2019.\nYonglong Tian, Olivier J Hena, and Aron van den Oord. Divide and contrast: Self-supervised learning\nfrom uncurated data. In Proceedings of the IEEE/CVF International Conference on Computer Vision ,\npp. 1006310074, 2021.\nGiorgos Tolias, Ronan Sicre, and Herv Jgou. Particular object retrieval with integral max-pooling of cnn\nactivations. arXiv preprint arXiv:1511.05879 , 2015.\nZhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-ecient\nlearners for self-supervised video pre-training. arXiv preprint arXiv:2203.12602 , 2022.\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv Jgou. Fixing the train-test resolution discrep-\nancy. In NeurIPS , 2019.\nHugo Touvron, Matthieu Cord, and Herv Jgou. Deit iii: Revenge of the vit. arXiv preprint\narXiv:2204.07118 , 2022.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix,\nBaptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. Llama: Open and ecient foundation language models. arXiv preprint\narXiv:2302.13971 , 2023.\nGrant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro\nPerona, and Serge Belongie. The inaturalist species classication and detection dataset. In CVPR, 2018.\nGrant Van Horn, Elijah Cole, Sara Beery, Kimberly Wilber, Serge Belongie, and Oisin Mac Aodha. Bench-\nmarking representation learning for natural world image collections. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition , pp. 1288412893, 2021.\nWenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei\nLu, Hongsheng Li, et al. Internimage: Exploring large-scale vision foundation models with deformable\nconvolutions. arXiv preprint arXiv:2211.05778 , 2022.\nXiaolong Wang, Allan Jabri, and Alexei A Efros. Learning correspondence from the cycle-consistency of\ntime. In CVPR, 2019.\n26', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Philippe Weinzaepfel, Thomas Lucas, Diane Larlus, and Yannis Kalantidis. Learning super-features for\nimage retrieval. In International Conference on Learning Representations , 2021.\nP. Welinder, S. Branson, T. Mita, C. Wah, F. Schro, S. Belongie, and P. Perona. Caltech-UCSD Birds 200.\nTechnical Report CNS-TR-2010-001, California Institute of Technology, 2010.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmn, Armand\nJoulin, and Edouard Grave. Ccnet: Extracting high quality monolingual datasets from web crawl data.\narXiv preprint arXiv:1911.00359 , 2019.\nZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric\ninstance discrimination. In CVPR, 2018.\nJ. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recognition from\nabbey to zoo. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition ,\npp. 34853492, June 2010. doi: 10.1109/CVPR.2010.5539970.\nHuXu,JunchengLi,AlexeiBaevski,MichaelAuli,WojciechGaluba,FlorianMetze,ChristophFeichtenhofer,\net al. Masked autoencoders that listen. arXiv preprint arXiv:2207.06405 , 2022.\nI Zeki Yalniz, Herv Jgou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. Billion-scale semi-supervised\nlearning for image classication. arXiv preprint arXiv:1905.00546 , 2019.\nBurak Yildiz, Seyran Khademi, Ronald Maria Siebes, and Jan van Gemert. Amstertime: A visual place\nrecognition benchmark dataset for severe domain shift. arXiv preprint arXiv:2203.16291 , 2022.\nNikolaos-Antonios Ypsilantis, Noa Garcia, Guangxing Han, Sarah Ibrahimi, Nanne Van Noord, and Giorgos\nTolias. The met dataset: Instance-level recognition for artworks. In Thirty-fth Conference on Neural\nInformation Processing Systems Datasets and Benchmarks Track (Round 2) , 2021.\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 1210412113,\n2022.\nRichard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016.\nBolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep features\nfor scene recognition using places database. In NeurIPS , 2014.\nBolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing\nthroughade20kdataset. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npp. 633641, 2017.\nJinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert\npre-training with online tokenizer. arXiv preprint arXiv:2111.07832 , 2021.\nPan Zhou, Yichen Zhou, Chenyang Si, Weihao Yu, Teck Khim Ng, and Shuicheng Yan. Mugs: A multi-\ngranular self-supervised learning framework. arXiv preprint arXiv:2203.14415 , 2022.\n27', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'A Data Processing\nA.1 Data selection\nOur selection of datasets for building LVD-142M is detailed in Tab. 15. This collection is intended to provide\nimages covering well various downstream vision tasks both for image-level and dense recognition.\nA.2 Image similarity\nWe employ cosine similarity to compare image features (whether ours or feature generated for deduplication)\nwith the following similarity function m:\nm(s, r) =cosine-similarity (f(s), f(r)) =f(s).f(r)\nf(s)2f(r)2\nwhere sandrare a pair of images to compare and fis the model generating features.\nA.3 Deduplication\nSelf-deduplication. To deduplicate our uncurated data source of 1.3B images, we compute and use the\nembeddings generated by Pizzi et al. (2022) and retrieve the k= 64nearest neighbors of each image (using\ncosine similarity). Considering only neighbors with a similarity >0.6, we extract the connected components\nof the associated k-NN graph thanks to a scalable disjoint set data structure implementation. We then only\nkeep one representative for each component of duplicate images. This results in a self-deduplicated data\nsource of 1.1B images.\nRelativededuplication Toreduceredundancyandalsoproperlyevaluatetheperformanceofourfeatures,\nwe discard remaining images of our self-deduplicated data source that are too similar to train and test splits\nof our evaluation datasets. To achieve this, we apply a similar procedure as for self-deduplication, with a\nstricter similarity >0.45, this time identifying the duplicate components (if any) to which each reference\nimage belong and discarding it entirely. This results in a self- and relatively-deduplicated data source of\n744M images.\nA.4 Retrieval\nWe employ two approaches to augment dataset via retrieval: sample-based and cluster-based. The rst one,\nsample-based, applies to datasets larger than 1M images and consists in collecting a xed number kof nearest\nimages for each sample image of the dataset to retrieve, eectively trying to multiply by kthe size of the\ndataset. We use k= 4for Google Landmarks v2 and ImageNet-22k but a larger k= 32to make this specic\nretrieval a core part of our LVD-142M dataset. For smaller datasets, the second approach, cluster-based,\nconsists in rst clustering our uncurated data source into 100,000separate clusters thanks to a distributed\nk-means implementation. Each cluster should capture dierent types of image concept and contents. We\nthen pick 10,000images from each cluster associated with more than 3images of the retrieved dataset. As\nthis can result in a very large number of retrieved images for some dataset, we restrict such retrievals to a\nmaximum of 1M images to maintain the balance between the dierent datasets within LVD-142M.\nB Implementation Details\nB.1 Unsupervised pre-training\nFor unsupervised pre-training we build on the DINO and iBOT codebases. We use hyperparameters shown\nin Table 16, ViT architectures described in Table 17.\nKoLeo regularization. We apply the KoLeo regularizer with a weight of 0.1 between the class tokens of\nthe rst global crop, for all samples within a GPU without cross-communication for this step.\n28', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Task Dataset / Split Images Retrieval Retrieved Final\nclassication ImageNet-22k /  14,197,086 as is  14,197,086\nclassication ImageNet-22k /  14,197,086 sample 56,788,344 56,788,344\nclassication ImageNet-1k / train 1,281,167 sample 40,997,344 40,997,344\nne-grained classif. Caltech 101 / train 3,030 cluster 2,630,000 1,000,000\nne-grained classif. CUB-200-2011 / train 5,994 cluster 1,300,000 1,000,000\nne-grained classif. DTD / train1 1,880 cluster 1,580,000 1,000,000\nne-grained classif. FGVC-Aircraft / train 3,334 cluster 1,170,000 1,000,000\nne-grained classif. Flowers-102 / train 1,020 cluster 1,060,000 1,000,000\nne-grained classif. Food-101 / train 75,750 cluster 21,670,000 1,000,000\nne-grained classif. Oxford-IIIT Pet / trainval 3,680 cluster 2,750,000 1,000,000\nne-grained classif. Stanford Cars / train 8,144 cluster 7,220,000 1,000,000\nne-grained classif. SUN397 / train1 19,850 cluster 18,950,000 1,000,000\nne-grained classif. Pascal VOC 2007 / train 2,501 cluster 1,010,000 1,000,000\nsegmentation ADE20K / train 20,210 cluster 20,720,000 1,000,000\nsegmentation Cityscapes / train 2,975 cluster 1,390,000 1,000,000\nsegmentation Pascal VOC 2012 (seg.) / trainaug 1,464 cluster 10,140,000 1,000,000\ndepth estimation Mapillary SLS / train 1,434,262 as is  1,434,262\ndepth estimation KITTI / train (Eigen) 23,158 cluster 3,700,000 1,000,000\ndepth estimation NYU Depth V2 / train 24,231 cluster 10,850,000 1,000,000\ndepth estimation SUN RGB-D / train 4,829 cluster 4,870,000 1,000,000\nretrieval Google Landmarks v2 / train (clean) 1,580,470 as is  1,580,470\nretrieval Google Landmarks v2 / train (clean) 1,580,470 sample 6,321,880 6,321,880\nretrieval AmsterTime / new 1,231 cluster 960,000 960,000\nretrieval AmsterTime / old 1,231 cluster 830,000 830,000\nretrieval Met / train 397,121 cluster 62,860,000 1,000,000\nretrieval Revisiting Oxford / base 4,993 cluster 3,680,000 1,000,000\nretrieval Revisiting Paris / base 6,322 cluster 3,660,000 1,000,000\n142,109,386\nTable 15: Composition of our LVD-142M dataset. We report the list of datasets and associated splits\nused to build the dataset, how they were included (as is without retrieval or via sample-based or cluster-based\nretrieval). For retrievals, we indicate the actual number of retrieved images and the nal number included\nin the dataset.\n29', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Arch. Drop-rate LR Batch size\nDINOv2-S (distilled) ViT-S/14 0 1e-3 2048\nDINOv2-B (distilled) ViT-B/14 0 1e-3 2048\nDINOv2-L (distilled) ViT-L/14 0 1e-3 2048\nDINOv2-L (from scratch) ViT-L/14 0.4 3.5e-4 3072\nDINOv2-g (from scratch) ViT-g/14 0.4 3.5e-4 3072\nTable 16: Training hyperparameters for DINOv2-S, DINOv2-B, DINOv2-L and DINOv2-g. All\nmodels run for 625k iterations with optimizer AdamW, an initial LayerScale value of 1e-5, a weight decay\ncosine schedule from 0.04 to 0.2, a learning rate warmup of 100k iterations, a teacher momentum cosine\nschedule from 0.994 to 1, and we train in oat16 precision in all cases (except for the DINO heads where we\nreduce the gradients in oat32).\nArch. Embed dim Heads Blocks FFN layer\nViT-S/14 (distilled) 384 6 12 MLP\nViT-B/14 (distilled) 768 12 18 MLP\nViT-L/14 (distilled) 1024 16 24 MLP\nViT-L/14 (from scratch) 1024 16 24 SwiGLU\nViT-g/14 (from scratch) 1536 24 40 SwiGLU\nTable 17: Architecture details of the ViT-S/B/L/g networks used in this work. We use MLP\nfeed-forward networks for distilled models, and SwiGLU (Shazeer, 2020) when training from scratch.\nEMA update for the teacher. The teacher is initialized with the same state as the student, and is an\nexponential moving average of the student network, with a momentum value in [0.994, 1.0] following a cosine\nschedule. It is updated at the end of every training step.\nB.2 High-Resolution adaptation\nWe initialise the model with the pretrained weights then train it for 10k iterations with the same procedure\nas the original pretraining. All the schedules are kept the same as in the original training, but compressed\nto t in 10k iterations. All the hyperparameters are kept the same as in the rst pretraining, except the\nbase learning rate which is reduced.\nB.3 Linear probing evaluation\nFor linear probing we dene 3 evaluation parameters: the learning rate, how many output layers we use,\nwhether we concatenate the average-pooled patch token features with the class token (or use only the\nclass token). We train our linear layer with SGD for 12500 iterations, using random-resized-crop data\naugmentation, and perform the following grid search:\nlearning rate in{0.0001,0.0002,0.0005,0.001,0.002,0.005,0.01,0.02,0.05,0.1,0.2,0.3,0.5}\noutput layers in{1,4}\nconcatenate average-pooled tokens in {yes, no}\nWe then report the highest accuracy value obtained on the validation set as is common practice. Note that\nthis grid search is not expensive, because at each iteration we perform inference on the backbone only once,\nthen feed the output to all linear classiers (each performing a single matrix multiplication).\nC List of benchmarks used for evaluations\nWe show in Table 18 the list of benchmarks and datasets used for evaluation.\n30', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Dataset name Task Citation\nImageNet-1k Image Classication (Russakovsky et al., 2015)\nImageNet-V2 Image Classication (Recht et al., 2019)\nImageNet-ReaL Image Classication (Beyer et al., 2020)\nImageNet-A Image Classication (Djolonga et al., 2021)\nImageNet-C Image Classication (Hendrycks & Dietterich, 2019)\nImageNet-Rendition Image Classication (Hendrycks et al., 2021)\nImageNet-Sketch Image Classication (Wang et al., 2019)\nFood-101 Image Classication (Bossard et al., 2014)\nCIFAR-10 Image Classication (Krizhevsky et al., 2009)\nCIFAR-100 Image Classication (Krizhevsky et al., 2009)\nSUN397 Image Classication (Xiao et al., 2010)\nStanfordCars Image Classication (Krause et al., 2013)\nFGVC-Aircraft Image Classication (Maji et al., 2013)\nPascal VOC 2007 Image Classication (Everingham et al., 2015)\nDescribable Textures Image Classication (Cimpoi et al., 2014)\nOxford Pets Image Classication (Parkhi et al., 2012)\nCaltech101 Image Classication (Fei-Fei et al., 2004)\nOxford Flowers Image Classication (Nilsback & Zisserman, 2008)\nCUB200 Image Classication (Welinder et al., 2010)\niNaturalist 2018 Image Classication (Van Horn et al., 2018)\niNaturalist 2021 Image Classication (Van Horn et al., 2021)\nPlaces-205 Image Classication (Zhou et al., 2014)\nUCF101 Video Classication (Soomro et al., 2012)\nKinetics-400 Video Classication (Kay et al., 2017)\nSomething-Something-V2 Video Classication (Goyal et al., 2017)\nRevisiting-Paris Image Retrieval (Radenovi et al., 2018a)\nRevisiting-Oxford Image Retrieval (Radenovi et al., 2018a)\nMet Image Retrieval (Ypsilantis et al., 2021)\nAmstertime Image Retrieval (Yildiz et al., 2022)\nADE20k Image Segmentation (Zhou et al., 2017)\nCityscapes Image Segmentation (Cordts et al., 2016)\nPascal VOC 2012 Image Segmentation (Everingham et al., 2015)\nNYU-Depth V2 Monocular Depth Estimation (Silberman et al., 2012)\nKITTI Monocular Depth Estimation (Geiger et al., 2013)\nSUN-RGBD Monocular Depth Estimation (Song et al., 2015)\nDollarStreet Fairness Analysis (De Vries et al., 2019)\nCasual Conversations Fairness Analysis (Hazirbas et al., 2021)\nTable 18: List of datasets used for evaluation.\n31', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}]: %s
2023-12-07 11:13:45,845 - INFO - Check the results [{'page_content': 'Jan Beirlant, Edward J Dudewicz, Lszl Gyr, Edward C Van der Meulen, et al. Nonparametric entropy\nestimation: An overview. International Journal of Mathematical and Statistical Sciences , 6(1):1739, 1997.\nMaxim Berman, Herv Jgou, Vedaldi Andrea, Iasonas Kokkinos, and Matthijs Douze. MultiGrain: a unied\nimage embedding for classes and instances. arXiv preprint arXiv:1902.05509 , 2019.\nLucas Beyer, Olivier J Hna, Alexander Kolesnikov, Xiaohua Zhai, and Aron van den Oord. Are we done\nwith imagenet? arXiv preprint arXiv:2006.07159 , 2020.\nShariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. AdaBins: Depth estimation using adaptive\nbins. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . IEEE, jun\n2021. doi: 10.1109/cvpr46437.2021.00400. URL https://doi.org/10.1109%2Fcvpr46437.2021.00400 .\nPiotr Bojanowski and Armand Joulin. Unsupervised learning by predicting noise. In ICML, 2017.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of\nfoundation models. arXiv preprint arXiv:2108.07258 , 2021.\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101  mining discriminative components\nwith random forests. In European Conference on Computer Vision , 2014.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\npreprint arXiv:2005.14165 , 2020.\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised\nlearning of visual features. In ECCV, 2018.\nMathilde Caron, Piotr Bojanowski, Julien Mairal, and Armand Joulin. Unsupervised pre-training of image\nfeatures on non-curated data. In ICCV, 2019.\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsuper-\nvised learning of visual features by contrasting cluster assignments. In NeurIPS , 2020.\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv Jgou, Julien Mairal, Piotr Bojanowski, and Armand\nJoulin. Emerging properties in self-supervised vision transformers. arXiv preprint arXiv:2104.14294 , 2021.\nLiang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schro, and Hartwig Adam. Encoder-decoder\nwith atrous separable convolution for semantic image segmentation. In Proceedings of the European con-\nference on computer vision (ECCV) , pp. 801818, 2018.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Georey Hinton. A simple framework for contrastive\nlearning of visual representations. preprint arXiv:2002.05709 , 2020.\nXiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi\nDong, Thang Luong, Cho-Jui Hsieh, et al. Symbolic discovery of optimization algorithms. arXiv preprint\narXiv:2302.06675 , 2023.\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. preprint arXiv:2011.10566 ,\n2020.\nXinleiChen, SainingXie, andKaimingHe. Anempiricalstudyoftrainingself-supervisedvisiontransformers.\narXiv preprint arXiv:2104.02057 , 2021.\nZhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer\nadapter for dense predictions. arXiv preprint arXiv:2205.08534 , 2022.\nBowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention\nmask transformer for universal image segmentation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pp. 12901299, 2022.\n21', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language\nmodeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.\nM. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In\nProceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , 2014.\nMarius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,\nUwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understand-\ning. InProceedings of the IEEE conference on computer vision and pattern recognition , pp. 32133223,\n2016.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R. Flashattention: Fast and memory-\necient exact attention with io-awareness. arXiv preprint arXiv:2205.14135 , 2022.\nTerrance De Vries, Ishan Misra, Changhan Wang, and Laurens Van der Maaten. Does object recognition\nworkforeveryone? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition\nworkshops , pp. 5259, 2019.\nSylvain Delattre and Nicolas Fournier. On the kozachenkoleonenko entropy estimator. Journal of Statistical\nPlanning and Inference , 185:6993, 2017.\nJacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert: Pre-trainingofdeepbidirectional\ntransformers for language understanding. preprint arXiv:1810.04805 , 2018.\nJosip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander Kolesnikov,\nJoan Puigcerver, Matthias Minderer, Alexander DAmour, Dan Moldovan, et al. On robustness and\ntransferabilityofconvolutionalneuralnetworks. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pp. 1645816468, 2021.\nCarl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context\nprediction. In ICCV, 2015.\nAlexey Dosovitskiy, Jost Tobias Springenberg, Martin A. Riedmiller, and Thomas Brox. Discriminative\nunsupervised feature learning with convolutional neural networks. CoRR, abs/1406.6909, 2014. URL\nhttp://arxiv.org/abs/1406.6909 .\nAlexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Dis-\ncriminative unsupervised feature learning with exemplar convolutional neural networks. TPAMI, 2016.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-\nterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. preprint arXiv:2010.11929 , 2020.\nMatthijs Douze, Herv Jgou, Harsimrat Sandhawalia, Laurent Amsaleg, and Cordelia Schmid. Evaluation\nof gist descriptors for web-scale image search. In CIVR, 2009.\nQuentin Duval, Ishan Misra, and Nicolas Ballas. A simple recipe for competitive low-compute self supervised\nvision models. arXiv preprint arXiv:2301.09451 , 2023.\nAlaaeldin El-Nouby, Gautier Izacard, Hugo Touvron, Ivan Laptev, Herv Jegou, and Edouard Grave. Are\nlarge-scale datasets necessary for self-supervised pre-training? arXiv preprint arXiv:2112.10740 , 2021.\nM. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal\nvisual object classes challenge: A retrospective. International Journal of Computer Vision , 111(1):98136,\nJanuary 2015.\nYuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang,\nand Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. 2023.\n22', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples:\nAn incremental bayesian approach tested on 101 object categories. In 2004 conference on computer vision\nand pattern recognition workshop , pp. 178178. IEEE, 2004.\nAndreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti\ndataset. The International Journal of Robotics Research , 32(11):12311237, 2013.\nSpyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting\nimage rotations, 2018.\nRohit Girdhar, Alaaeldin El-Nouby, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Is-\nhan Misra. Omnimae: Single model masked pretraining on images and videos. arXiv preprint\narXiv:2206.08356 , 2022.\nPriya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. Scaling and benchmarking self-supervised\nvisual representation learning. In ICCV, 2019.\nPriya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat Singh,\nVitaliy Liptchinsky, Ishan Misra, Armand Joulin, et al. Self-supervised pretraining of visual features in\nthe wild. preprint arXiv:2103.01988 , 2021.\nPriya Goyal, Quentin Duval, Isaac Seessel, Mathilde Caron, Mannat Singh, Ishan Misra, Levent Sagun,\nArmand Joulin, and Piotr Bojanowski. Vision models are more robust and fair when pretrained on\nuncurated images without supervision. arXiv preprint arXiv:2202.08360 , 2022a.\nPriya Goyal, Adriana Romero Soriano, Caner Hazirbas, Levent Sagun, and Nicolas Usunier. Fairness in-\ndicators for systematic assessments of visual feature extractors. In 2022 ACM Conference on Fairness,\nAccountability, and Transparency , pp. 7088, 2022b.\nRaghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, He-\nuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The" something\nsomething" video database for learning and evaluating visual common sense. In Proceedings of the IEEE\ninternational conference on computer vision , pp. 58425850, 2017.\nJean-Bastien Grill, Florian Strub, Florent Altch, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya,\nCarl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray\nKavukcuoglu, Rmi Munos, and Michal Valko. Bootstrap your own latent: A new approach to self-\nsupervised learning. In NeurIPS , 2020.\nRaia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping.\nInCVPR, 2006.\nCanerHazirbas, JoannaBitton, BrianDolhansky, JacquelinePan, AlbertGordo, andCristianCantonFerrer.\nTowards measuring fairness in ai: the casual conversations dataset. IEEE Transactions on Biometrics,\nBehavior, and Identity Science , 4(3):324332, 2021.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised\nvisual representation learning. In CVPR, 2020.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollr, and Ross Girshick. Masked autoencoders\nare scalable vision learners. arXiv preprint arXiv:2111.06377 , 2021.\nOlivierJHna, AravindSrinivas, JereyDeFauw, AliRazavi, CarlDoersch, SMEslami, andAaronvanden\nOord. Data-ecientimagerecognitionwithcontrastivepredictivecoding. preprint arXiv:1905.09272 ,2019.\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions\nand perturbations. In International Conference on Learning Representations , 2019.\n23', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai,\nTyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-\nof-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pp. 83408349, 2021.\nGeorey Hinton, Oriol Vinyals, and Je Dean. Distilling the knowledge in a neural network. preprint\narXiv:1503.02531 , 2015.\nJordan Homann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal\nlarge language models. arXiv preprint arXiv:2203.15556 , 2022.\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochas-\ntic depth. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands,\nOctober 1114, 2016, Proceedings, Part IV 14 , pp. 646661. Springer, 2016.\nGabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal\nDave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig\nSchmidt. Openclip. 2021.\nHerve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. IEEE\ntransactions on pattern analysis and machine intelligence , 33(1), 2010.\nJe Johnson, Matthijs Douze, and Herv Jgou. Billion-scale similarity search with GPUs. IEEE Transac-\ntions on Big Data , 7(3):535547, 2019.\nArmand Joulin, Laurens Van Der Maaten, Allan Jabri, and Nicolas Vasilache. Learning visual features from\nlarge weakly supervised data. In ECCV, 2016.\nWill Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio\nViola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv\npreprint arXiv:1705.06950 , 2017.\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for ne-grained catego-\nrization. In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13) , Sydney,\nAustralia, 2013.\nAlex Krizhevsky, Georey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\nBenjaminLefaudeux, FranciscoMassa, DianaLiskovich, WenhanXiong, VittorioCaggiano, SeanNaren, Min\nXu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza. xformers: A modular\nand hackable transformer modelling library. https://github.com/facebookresearch/xformers , 2022.\nChunyuan Li, Jianwei Yang, Pengchuan Zhang, Mei Gao, Bin Xiao, Xiyang Dai, Lu Yuan, and Jianfeng\nGao. Ecient self-supervised vision transformers for representation learning. In ICLR, 2022a.\nZhenyu Li, Xuyang Wang, Xianming Liu, and Junjun Jiang. Binsformer: Revisiting adaptive bins for\nmonocular depth estimation. arXiv preprint arXiv:2204.00987 , 2022b.\nHuajun Liu, Fuqiang Liu, Xinyi Fan, and Dong Huang. Polarized self-attention: towards high-quality pixel-\nwise regression. arXiv preprint arXiv:2107.00782 , 2021.\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin\nBharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In ECCV,\n2018.\nS. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classication of aircraft.\nTechnical report, 2013.\n24', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. In\nCVPR, 2020.\nMaria-Elena Nilsback and Andrew Zisserman. Automated ower classication over a large number of classes.\nInIndian Conference on Computer Vision, Graphics and Image Processing , Dec 2008.\nMehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles.\nIn Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (eds.), Computer Vision  ECCV 2016 , pp.\n6984, Cham, 2016. Springer International Publishing. ISBN 978-3-319-46466-4.\nOmkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In IEEE\nConference on Computer Vision and Pattern Recognition , 2012.\nDeepak Pathak, Philipp Krhenbhl, Je Donahue, Trevor Darrell, and Alexei Efros. Context encoders:\nFeature learning by inpainting. In CVPR, 2016.\nDavid Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David\nSo, Maud Texier, and Je Dean. Carbon emissions and large neural network training. arXiv preprint\narXiv:2104.10350 , 2021.\nEd Pizzi, Sreya Dutta Roy, Sugosh Nagavara Ravindra, Priya Goyal, and Matthijs Douze. A self-supervised\ndescriptor for image copy detection. arXiv preprint arXiv:2202.10261 , 2022.\nFilip Radenovi, Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondej Chum. Revisiting oxford and\nparis: Large-scale image retrieval benchmarking. In CVPR, 2018a.\nFilip Radenovi, Giorgos Tolias, and Ondej Chum. Fine-tuning cnn image retrieval with no human anno-\ntation.IEEE transactions on pattern analysis and machine intelligence , 2018b.\nAlec Radford, Jerey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models\nare unsupervised multitask learners.\nAlec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to generate reviews and discovering sentiment.\narXiv preprint arXiv:1704.01444 , 2017.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In International Conference on Machine Learning , pp. 87488763. PMLR,\n2021.\nColin Rael, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unied text-to-text transformer.\nJ. Mach. Learn. Res. , 21(140):167, 2020.\nRen Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Pro-\nceedings of the IEEE/CVF International Conference on Computer Vision , pp. 1217912188, 2021.\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classiers generalize\nto imagenet? In International Conference on Machine Learning , pp. 53895400. PMLR, 2019.\nJerome Revaud, Jon Almazn, Rafael S Rezende, and Cesar Roberto de Souza. Learning with average\nprecision: Training image retrieval with a listwise loss. In ICCV, 2019.\nYangjun Ruan, Saurabh Singh, Warren Morningstar, Alexander A Alemi, Sergey Ioe, Ian Fischer, and\nJoshua V Dillon. Weighted ensemble self-supervised learning. arXiv preprint arXiv:2211.09981 , 2022.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej\nKarpathy, Aditya Khosla, Michael Bernstein, Alexander C Berg, and Li Fei-Fei. Imagenet large scale\nvisual recognition challenge. IJCV, 2015.\n25', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, and Herv Jgou. Spreading vectors for similarity\nsearch.arXiv preprint arXiv:1806.03198 , 2018.\nNoam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202 , 2020.\nNathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support\ninference from rgbd images. In European conference on computer vision , pp. 746760. Springer, 2012.\nMannat Singh, Laura Gustafson, Aaron Adcock, Vinicius de Freitas Reis, Bugra Gedik, Raj Prateek\nKosaraju, Dhruv Mahajan, Ross Girshick, Piotr Dollr, and Laurens van der Maaten. Revisiting Weakly\nSupervised Pre-Training of Visual Perception Models. In CVPR, 2022.\nShuranSong, SamuelPLichtenberg, andJianxiongXiao. Sunrgb-d: Argb-dsceneunderstandingbenchmark\nsuite. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 567576,\n2015.\nKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes\nfrom videos in the wild. arXiv preprint arXiv:1212.0402 , 2012.\nAndreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer.\nHow to train your vit? data, augmentation, and regularization in vision transformers. arXiv preprint\narXiv:2106.10270 , 2021.\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning\nin nlp.arXiv preprint arXiv:1906.02243 , 2019.\nYonglong Tian, Olivier J Hena, and Aron van den Oord. Divide and contrast: Self-supervised learning\nfrom uncurated data. In Proceedings of the IEEE/CVF International Conference on Computer Vision ,\npp. 1006310074, 2021.\nGiorgos Tolias, Ronan Sicre, and Herv Jgou. Particular object retrieval with integral max-pooling of cnn\nactivations. arXiv preprint arXiv:1511.05879 , 2015.\nZhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-ecient\nlearners for self-supervised video pre-training. arXiv preprint arXiv:2203.12602 , 2022.\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv Jgou. Fixing the train-test resolution discrep-\nancy. In NeurIPS , 2019.\nHugo Touvron, Matthieu Cord, and Herv Jgou. Deit iii: Revenge of the vit. arXiv preprint\narXiv:2204.07118 , 2022.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix,\nBaptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. Llama: Open and ecient foundation language models. arXiv preprint\narXiv:2302.13971 , 2023.\nGrant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro\nPerona, and Serge Belongie. The inaturalist species classication and detection dataset. In CVPR, 2018.\nGrant Van Horn, Elijah Cole, Sara Beery, Kimberly Wilber, Serge Belongie, and Oisin Mac Aodha. Bench-\nmarking representation learning for natural world image collections. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition , pp. 1288412893, 2021.\nWenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei\nLu, Hongsheng Li, et al. Internimage: Exploring large-scale vision foundation models with deformable\nconvolutions. arXiv preprint arXiv:2211.05778 , 2022.\nXiaolong Wang, Allan Jabri, and Alexei A Efros. Learning correspondence from the cycle-consistency of\ntime. In CVPR, 2019.\n26', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Philippe Weinzaepfel, Thomas Lucas, Diane Larlus, and Yannis Kalantidis. Learning super-features for\nimage retrieval. In International Conference on Learning Representations , 2021.\nP. Welinder, S. Branson, T. Mita, C. Wah, F. Schro, S. Belongie, and P. Perona. Caltech-UCSD Birds 200.\nTechnical Report CNS-TR-2010-001, California Institute of Technology, 2010.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmn, Armand\nJoulin, and Edouard Grave. Ccnet: Extracting high quality monolingual datasets from web crawl data.\narXiv preprint arXiv:1911.00359 , 2019.\nZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric\ninstance discrimination. In CVPR, 2018.\nJ. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recognition from\nabbey to zoo. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition ,\npp. 34853492, June 2010. doi: 10.1109/CVPR.2010.5539970.\nHuXu,JunchengLi,AlexeiBaevski,MichaelAuli,WojciechGaluba,FlorianMetze,ChristophFeichtenhofer,\net al. Masked autoencoders that listen. arXiv preprint arXiv:2207.06405 , 2022.\nI Zeki Yalniz, Herv Jgou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. Billion-scale semi-supervised\nlearning for image classication. arXiv preprint arXiv:1905.00546 , 2019.\nBurak Yildiz, Seyran Khademi, Ronald Maria Siebes, and Jan van Gemert. Amstertime: A visual place\nrecognition benchmark dataset for severe domain shift. arXiv preprint arXiv:2203.16291 , 2022.\nNikolaos-Antonios Ypsilantis, Noa Garcia, Guangxing Han, Sarah Ibrahimi, Nanne Van Noord, and Giorgos\nTolias. The met dataset: Instance-level recognition for artworks. In Thirty-fth Conference on Neural\nInformation Processing Systems Datasets and Benchmarks Track (Round 2) , 2021.\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 1210412113,\n2022.\nRichard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016.\nBolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep features\nfor scene recognition using places database. In NeurIPS , 2014.\nBolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing\nthroughade20kdataset. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npp. 633641, 2017.\nJinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert\npre-training with online tokenizer. arXiv preprint arXiv:2111.07832 , 2021.\nPan Zhou, Yichen Zhou, Chenyang Si, Weihao Yu, Teck Khim Ng, and Shuicheng Yan. Mugs: A multi-\ngranular self-supervised learning framework. arXiv preprint arXiv:2203.14415 , 2022.\n27', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'A Data Processing\nA.1 Data selection\nOur selection of datasets for building LVD-142M is detailed in Tab. 15. This collection is intended to provide\nimages covering well various downstream vision tasks both for image-level and dense recognition.\nA.2 Image similarity\nWe employ cosine similarity to compare image features (whether ours or feature generated for deduplication)\nwith the following similarity function m:\nm(s, r) =cosine-similarity (f(s), f(r)) =f(s).f(r)\nf(s)2f(r)2\nwhere sandrare a pair of images to compare and fis the model generating features.\nA.3 Deduplication\nSelf-deduplication. To deduplicate our uncurated data source of 1.3B images, we compute and use the\nembeddings generated by Pizzi et al. (2022) and retrieve the k= 64nearest neighbors of each image (using\ncosine similarity). Considering only neighbors with a similarity >0.6, we extract the connected components\nof the associated k-NN graph thanks to a scalable disjoint set data structure implementation. We then only\nkeep one representative for each component of duplicate images. This results in a self-deduplicated data\nsource of 1.1B images.\nRelativededuplication Toreduceredundancyandalsoproperlyevaluatetheperformanceofourfeatures,\nwe discard remaining images of our self-deduplicated data source that are too similar to train and test splits\nof our evaluation datasets. To achieve this, we apply a similar procedure as for self-deduplication, with a\nstricter similarity >0.45, this time identifying the duplicate components (if any) to which each reference\nimage belong and discarding it entirely. This results in a self- and relatively-deduplicated data source of\n744M images.\nA.4 Retrieval\nWe employ two approaches to augment dataset via retrieval: sample-based and cluster-based. The rst one,\nsample-based, applies to datasets larger than 1M images and consists in collecting a xed number kof nearest\nimages for each sample image of the dataset to retrieve, eectively trying to multiply by kthe size of the\ndataset. We use k= 4for Google Landmarks v2 and ImageNet-22k but a larger k= 32to make this specic\nretrieval a core part of our LVD-142M dataset. For smaller datasets, the second approach, cluster-based,\nconsists in rst clustering our uncurated data source into 100,000separate clusters thanks to a distributed\nk-means implementation. Each cluster should capture dierent types of image concept and contents. We\nthen pick 10,000images from each cluster associated with more than 3images of the retrieved dataset. As\nthis can result in a very large number of retrieved images for some dataset, we restrict such retrievals to a\nmaximum of 1M images to maintain the balance between the dierent datasets within LVD-142M.\nB Implementation Details\nB.1 Unsupervised pre-training\nFor unsupervised pre-training we build on the DINO and iBOT codebases. We use hyperparameters shown\nin Table 16, ViT architectures described in Table 17.\nKoLeo regularization. We apply the KoLeo regularizer with a weight of 0.1 between the class tokens of\nthe rst global crop, for all samples within a GPU without cross-communication for this step.\n28', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Task Dataset / Split Images Retrieval Retrieved Final\nclassication ImageNet-22k /  14,197,086 as is  14,197,086\nclassication ImageNet-22k /  14,197,086 sample 56,788,344 56,788,344\nclassication ImageNet-1k / train 1,281,167 sample 40,997,344 40,997,344\nne-grained classif. Caltech 101 / train 3,030 cluster 2,630,000 1,000,000\nne-grained classif. CUB-200-2011 / train 5,994 cluster 1,300,000 1,000,000\nne-grained classif. DTD / train1 1,880 cluster 1,580,000 1,000,000\nne-grained classif. FGVC-Aircraft / train 3,334 cluster 1,170,000 1,000,000\nne-grained classif. Flowers-102 / train 1,020 cluster 1,060,000 1,000,000\nne-grained classif. Food-101 / train 75,750 cluster 21,670,000 1,000,000\nne-grained classif. Oxford-IIIT Pet / trainval 3,680 cluster 2,750,000 1,000,000\nne-grained classif. Stanford Cars / train 8,144 cluster 7,220,000 1,000,000\nne-grained classif. SUN397 / train1 19,850 cluster 18,950,000 1,000,000\nne-grained classif. Pascal VOC 2007 / train 2,501 cluster 1,010,000 1,000,000\nsegmentation ADE20K / train 20,210 cluster 20,720,000 1,000,000\nsegmentation Cityscapes / train 2,975 cluster 1,390,000 1,000,000\nsegmentation Pascal VOC 2012 (seg.) / trainaug 1,464 cluster 10,140,000 1,000,000\ndepth estimation Mapillary SLS / train 1,434,262 as is  1,434,262\ndepth estimation KITTI / train (Eigen) 23,158 cluster 3,700,000 1,000,000\ndepth estimation NYU Depth V2 / train 24,231 cluster 10,850,000 1,000,000\ndepth estimation SUN RGB-D / train 4,829 cluster 4,870,000 1,000,000\nretrieval Google Landmarks v2 / train (clean) 1,580,470 as is  1,580,470\nretrieval Google Landmarks v2 / train (clean) 1,580,470 sample 6,321,880 6,321,880\nretrieval AmsterTime / new 1,231 cluster 960,000 960,000\nretrieval AmsterTime / old 1,231 cluster 830,000 830,000\nretrieval Met / train 397,121 cluster 62,860,000 1,000,000\nretrieval Revisiting Oxford / base 4,993 cluster 3,680,000 1,000,000\nretrieval Revisiting Paris / base 6,322 cluster 3,660,000 1,000,000\n142,109,386\nTable 15: Composition of our LVD-142M dataset. We report the list of datasets and associated splits\nused to build the dataset, how they were included (as is without retrieval or via sample-based or cluster-based\nretrieval). For retrievals, we indicate the actual number of retrieved images and the nal number included\nin the dataset.\n29', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Arch. Drop-rate LR Batch size\nDINOv2-S (distilled) ViT-S/14 0 1e-3 2048\nDINOv2-B (distilled) ViT-B/14 0 1e-3 2048\nDINOv2-L (distilled) ViT-L/14 0 1e-3 2048\nDINOv2-L (from scratch) ViT-L/14 0.4 3.5e-4 3072\nDINOv2-g (from scratch) ViT-g/14 0.4 3.5e-4 3072\nTable 16: Training hyperparameters for DINOv2-S, DINOv2-B, DINOv2-L and DINOv2-g. All\nmodels run for 625k iterations with optimizer AdamW, an initial LayerScale value of 1e-5, a weight decay\ncosine schedule from 0.04 to 0.2, a learning rate warmup of 100k iterations, a teacher momentum cosine\nschedule from 0.994 to 1, and we train in oat16 precision in all cases (except for the DINO heads where we\nreduce the gradients in oat32).\nArch. Embed dim Heads Blocks FFN layer\nViT-S/14 (distilled) 384 6 12 MLP\nViT-B/14 (distilled) 768 12 18 MLP\nViT-L/14 (distilled) 1024 16 24 MLP\nViT-L/14 (from scratch) 1024 16 24 SwiGLU\nViT-g/14 (from scratch) 1536 24 40 SwiGLU\nTable 17: Architecture details of the ViT-S/B/L/g networks used in this work. We use MLP\nfeed-forward networks for distilled models, and SwiGLU (Shazeer, 2020) when training from scratch.\nEMA update for the teacher. The teacher is initialized with the same state as the student, and is an\nexponential moving average of the student network, with a momentum value in [0.994, 1.0] following a cosine\nschedule. It is updated at the end of every training step.\nB.2 High-Resolution adaptation\nWe initialise the model with the pretrained weights then train it for 10k iterations with the same procedure\nas the original pretraining. All the schedules are kept the same as in the original training, but compressed\nto t in 10k iterations. All the hyperparameters are kept the same as in the rst pretraining, except the\nbase learning rate which is reduced.\nB.3 Linear probing evaluation\nFor linear probing we dene 3 evaluation parameters: the learning rate, how many output layers we use,\nwhether we concatenate the average-pooled patch token features with the class token (or use only the\nclass token). We train our linear layer with SGD for 12500 iterations, using random-resized-crop data\naugmentation, and perform the following grid search:\nlearning rate in{0.0001,0.0002,0.0005,0.001,0.002,0.005,0.01,0.02,0.05,0.1,0.2,0.3,0.5}\noutput layers in{1,4}\nconcatenate average-pooled tokens in {yes, no}\nWe then report the highest accuracy value obtained on the validation set as is common practice. Note that\nthis grid search is not expensive, because at each iteration we perform inference on the backbone only once,\nthen feed the output to all linear classiers (each performing a single matrix multiplication).\nC List of benchmarks used for evaluations\nWe show in Table 18 the list of benchmarks and datasets used for evaluation.\n30', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Dataset name Task Citation\nImageNet-1k Image Classication (Russakovsky et al., 2015)\nImageNet-V2 Image Classication (Recht et al., 2019)\nImageNet-ReaL Image Classication (Beyer et al., 2020)\nImageNet-A Image Classication (Djolonga et al., 2021)\nImageNet-C Image Classication (Hendrycks & Dietterich, 2019)\nImageNet-Rendition Image Classication (Hendrycks et al., 2021)\nImageNet-Sketch Image Classication (Wang et al., 2019)\nFood-101 Image Classication (Bossard et al., 2014)\nCIFAR-10 Image Classication (Krizhevsky et al., 2009)\nCIFAR-100 Image Classication (Krizhevsky et al., 2009)\nSUN397 Image Classication (Xiao et al., 2010)\nStanfordCars Image Classication (Krause et al., 2013)\nFGVC-Aircraft Image Classication (Maji et al., 2013)\nPascal VOC 2007 Image Classication (Everingham et al., 2015)\nDescribable Textures Image Classication (Cimpoi et al., 2014)\nOxford Pets Image Classication (Parkhi et al., 2012)\nCaltech101 Image Classication (Fei-Fei et al., 2004)\nOxford Flowers Image Classication (Nilsback & Zisserman, 2008)\nCUB200 Image Classication (Welinder et al., 2010)\niNaturalist 2018 Image Classication (Van Horn et al., 2018)\niNaturalist 2021 Image Classication (Van Horn et al., 2021)\nPlaces-205 Image Classication (Zhou et al., 2014)\nUCF101 Video Classication (Soomro et al., 2012)\nKinetics-400 Video Classication (Kay et al., 2017)\nSomething-Something-V2 Video Classication (Goyal et al., 2017)\nRevisiting-Paris Image Retrieval (Radenovi et al., 2018a)\nRevisiting-Oxford Image Retrieval (Radenovi et al., 2018a)\nMet Image Retrieval (Ypsilantis et al., 2021)\nAmstertime Image Retrieval (Yildiz et al., 2022)\nADE20k Image Segmentation (Zhou et al., 2017)\nCityscapes Image Segmentation (Cordts et al., 2016)\nPascal VOC 2012 Image Segmentation (Everingham et al., 2015)\nNYU-Depth V2 Monocular Depth Estimation (Silberman et al., 2012)\nKITTI Monocular Depth Estimation (Geiger et al., 2013)\nSUN-RGBD Monocular Depth Estimation (Song et al., 2015)\nDollarStreet Fairness Analysis (De Vries et al., 2019)\nCasual Conversations Fairness Analysis (Hazirbas et al., 2021)\nTable 18: List of datasets used for evaluation.\n31', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}]: %s
2023-12-07 11:22:45,359 - INFO - Received requests to /inference endpoint
2023-12-07 11:22:45,461 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 11:22:45,461 - INFO - Received request: {'username': 'amin', 'prompt': 'Give me a summary of the paper', 'memory': False, 'conversation_number': 0, 'AI_assistance': False, 'collection_name': 'paper', 'llm_model': 'Llama_13b'}
2023-12-07 11:22:51,417 - INFO - Processed the request successfully
2023-12-07 11:23:06,416 - INFO - Received requests to /inference endpoint
2023-12-07 11:23:06,517 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 11:23:06,517 - INFO - Received request: {'username': 'amin', 'prompt': 'Give me a summary of the paper', 'memory': True, 'conversation_number': 1, 'AI_assistance': False, 'collection_name': 'paper', 'llm_model': 'Llama_13b'}
2023-12-07 11:23:12,431 - INFO - Processed the request successfully
2023-12-07 12:19:06,327 - INFO - Created a temporary directory at /tmp/tmp1c0q8e14
2023-12-07 12:19:06,327 - INFO - Writing /tmp/tmp1c0q8e14/_remote_module_non_scriptable.py
2023-12-07 12:19:08,184 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-07 12:19:12,799 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-07 12:20:57,233 - INFO - classes: ['paper']: %s
2023-12-07 12:21:04,759 - INFO - classes: ['paper']: %s
2023-12-07 12:21:04,885 - INFO - classes: ['paper']: %s
2023-12-07 12:21:13,335 - INFO - classes: ['paper']: %s
2023-12-07 12:21:23,093 - INFO - classes: ['paper']: %s
2023-12-07 12:21:23,115 - INFO - checking the request/ username='amin' class_name='docs' mode='create_collection' vectorDB_type='Weaviate' file_path=None: %s
2023-12-07 12:21:23,175 - INFO - checkpoint 1
2023-12-07 12:21:23,175 - INFO - checkpoint 2 amin: %s
2023-12-07 12:21:23,175 - INFO - checkpoint 2 amin_docs: %s
2023-12-07 12:21:23,203 - INFO - class name added successfully to database
2023-12-07 12:21:23,204 - INFO - success: class docs created for user amin
2023-12-07 12:21:23,299 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:21:29,368 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:21:33,695 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:21:35,442 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:21:35,490 - INFO - request received username='amin' class_name='docs' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/a29c93bee1380fec': %s
2023-12-07 12:21:36,447 - INFO - actors creation successful [Actor(WeaviateEmbedder, 53ad824eee264f4556b1947601000000), Actor(WeaviateEmbedder, 82e41e0a29c2431dd66c1c7001000000), Actor(WeaviateEmbedder, 316f98277fc6c910aef05c3501000000)]: %s
2023-12-07 12:21:36,447 - INFO - check 1st step of ray was successful
2023-12-07 12:21:36,448 - INFO - check if ray was successful:
2023-12-07 12:21:36,448 - INFO - check weaviate add data, 
2023-12-07 12:21:36,448 - INFO - response: {'status': 'success', 'message': 'Processed 19 documents in batches for class amin_docs.'}: %s
2023-12-07 12:21:36,448 - INFO - request processed successfully username='amin' class_name='docs' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/a29c93bee1380fec': %s
2023-12-07 12:21:38,096 - INFO - Check the data that is being passed [{'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nTable 4. Main results (%). Size = backbone model size. Question classes: NAT = natural science, SOC = social science, LAN = language\nscience, TXT = text context, IMG = image context, NO = no context, G1-6 = grades 1-6, G7-12 = grades 7-12. Results except ours are\ntaken from Lu et al. (2022a). Segment 1: Human performance; Segment 2: VQA baselines; Segment 3: UniedQA baselines; Segment 4:\nGPT-3.5 baselines; Segment 5: Our Multimodal-CoT results. Results in bold are the best performance.\nModel Size NAT SOC LAN TXT IMG NO G1-6 G7-12 Avg\nHuman -90.23 84.97 87.48 89.60 87.50 88.10 91.59 82.42 88.40\nMCAN (Yu et al., 2019) 95M 56.08 46.23 58.09 59.43 51.17 55.40 51.65 59.72 54.54\nTop-Down (Anderson et al., 2018) 70M 59.50 54.33 61.82 62.90 54.88 59.79 57.27 62.16 59.02\nBAN (Kim et al., 2018) 112M 60.88 46.57 66.64 62.61 52.60 65.51 56.83 63.94 59.37\nDFAF (Gao et al., 2019) 74M 64.03 48.82 63.55 65.88 54.49 64.11 57.12 67.17 60.72\nViLT (Kim et al., 2021) 113M 60.48 63.89 60.27 63.20 61.38 57.00 60.72 61.90 61.14\nPatch-TRM (Lu et al., 2021) 90M 65.19 46.79 65.55 66.96 55.28 64.95 58.04 67.50 61.42\nVisualBERT (Li et al., 2019) 111M 59.33 69.18 61.18 62.71 62.17 58.54 62.96 59.92 61.87\nUniedQA Base (Khashabi et al., 2020) 223M 68.16 69.18 74.91 63.78 61.38 77.84 72.98 65.00 70.12\nUniedQA Base w/ CoT (Lu et al., 2022a) 223M 71.00 76.04 78.91 66.42 66.53 81.81 77.06 68.82 74.11\nGPT-3.5 (Chen et al., 2020) 175B 74.64 69.74 76.00 74.44 67.28 77.42 76.80 68.89 73.97\nGPT-3.5 w/ CoT (Lu et al., 2022a) 175B 75.44 70.87 78.09 74.68 67.43 79.93 78.23 69.68 75.17\nMutimodal-CoT Base 223M 87.52 77.17 85.82 87.88 82.90 86.83 84.65 85.37 84.91\nMutimodal-CoT Large 738M 95.91 82.00 90.82 95.26 88.80 92.89 92.44 90.31 91.68\nTable 5. Ablation results of Multimodal-CoT.\nModel NAT SOC LAN TXT IMG NO G1-6 G7-12 Avg\nMultimodal-CoT 87.52 77.17 85.82 87.88 82.90 86.83 84.65 85.37 84.91\nw/o Two-Stage Framework 80.99 87.40 81.91 80.25 78.83 83.62 82.78 82.20 82.57\nw/o Vision Features 71.09 70.75 69.18 71.16 65.84 71.57 71.00 69.68 70.53\nmaximum input sequence length is 512. The batch sizes for\nthe base and large models are 16 and 8, respectively. Our\nexperiments are run on 4 NVIDIA Tesla V100 32G GPUs.\nBaseline Models Following Lu et al. (2022a), our base-\nlines include (i) Visual question answering (VQA) models\n(Anderson et al., 2018; Kim et al., 2018; Yu et al., 2019;\nGao et al., 2019; Kim et al., 2021; Lu et al., 2021; Li et al.,\n2019); (ii) Text-to-text LM models. (Khashabi et al., 2020);\n(iii) GPT-3.5 models (Chen et al., 2020). More details are\npresented in Appendix B.1.\n5.3. Main Results\nTable 4 shows the main results. Mutimodal-CoT Large out-\nperforms GPT-3.5 by 16.51% (75.17% 91.68%) and sur-\npasses human performance. Specically, among the 8\nquestion classes, Mutimodal-CoT Large achieves a 21.37%\n(67.43%88.80%) performance gain for the questions with\npaired images (IMG). Compared with existing UniedQA\nand GPT-3.5 methods that leverage image captions in the\ncontext to provide vision semantics, the results indicate that\nusing image features is more effective. In addition, our\ntwo-stage framework contributes to the superior results ac-\ncording to our ablation study results in Table 5. Overall,\nthe results verify the effectiveness of multimodality and the\npotential of achieving CoT reasoning with 1B-models via\nour two-stage framework.12345678910405060708090\nEpochAccuracyOne-stage Baseline One-stage Multimodal\nTwo-Stage Baseline Two-Stage Multimodal\nFigure 5. Accuracy curve of the No-CoT baseline and Multimodal-\nCoT variants across epochs.\n6. Analysis\nThe following analysis will investigate how Multimodal-\nCoT works and discuss contribution factors and limitations.\nWe use models under the base size for analysis unless\notherwise stated.\n6.1. Multimodality Boosts Convergence\nFigure 5 shows the evaluation accuracy curve of the baseline\nand Multimodal-CoT in different training epochs. One-\nstage is based on the QCM A input-output format as it', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nachieves the best performance in Table 2 and Two-stage\nis our two-stage framework. We nd that the two-stage\nmethods achieve relatively higher accuracy at the beginning\nthan the one-stage baselines that generate the answer directly\nwithout CoT. However, without the vision features, the two-\nstage baseline could not yield better results as the training\ngoes on due to the low-quality rationales (as observed in\nSection 3). In contrast, using vision features helps generate\nmore effective rationales that contribute to better answer\naccuracy in our two-stage multimodal variant.\n6.2. Using Different Vision Features\nDifferent vision features may affect the model performance.\nWe compare three widely-used types of vision features,\nCLIP (Radford et al., 2021), DETR (Carion et al., 2020),\nand ResNet (He et al., 2016). CLIP and DETR are patch-like\nfeatures where DETR is based on object detection. For the\nResNet features, we repeat the pooled features of ResNet-\n50 to the same length with the text sequence to imitate the\npatch-like features, where each patch is the same as the\npooled image features. More details of the vision features\nare presented in Appendix B.2.\nTable 6. Accuracy (%) of using different vision features.\nMethod One-stage Two-Stage\nw/ CLIP 81.21 84.81\nw/ DETR 82.57 84.91\nw/ ResNet 80.97 84.77\nTable 6 shows the comparative results of vision features. We\nobserve that using vision features generally achieves better\nperformance than the language only baseline. Specically,\nDETR achieves relatively better performance in general.\nTherefore, we use DETR by default in Multimodal-CoT.\n6.3. General Effectiveness Across Backbone Models\nTo test the generality of the benets of our approach to\nother backbone models, we alter the underlying LMs to\nother variants in different sizes or types. As shown in Table\n7, our approach is generally effective for the widely-used\nbackbone models.\nTable 7. Accuracy (%) with different backbone language models.\nMethod Size Language Only Mutimodal-CoT\nUniedQA Base 223M 80.40 84.91\nUniedQA Large 738M 83.60 91.68\nFLAN-T5 Base 248M 83.42 85.85\nFLAN-T5 Large 783M 85.19 93.02\n6.4. Error Analysis\nTo better understand the behavior of Multimodal-CoT and\nfacilitate future studies, we manually investigate randomly\nselected examples generated by our approach. Table 8 sum-marizes the categorization results generated by Multimodal-\nCoT. We randomly picked up 50 samples whose answers\nwere correct and 50 samples whose answers were incor-\nrect. The corresponding examples from each category are\npresented in Appendix C.\nTable 8. Categorization analysis of Multimodal-CoT.\nAnswer CoT Category Percentage (%)\nCorrectCoT is correct 90\nCoT is incorrect 10\nIncorrectCommonsense Mistake 82\nLogical Mistake 12\nCoT is correct 6\nWe nd that the correct samples (i.e., whose answers are cor-\nrect) contain a certain amount of incorrect chain-of-thought\n(10%). The results indicate that CoT may not always benet\nthe answer inference, and the model is robust to some extent\n it can predict the correct answer by ignoring incorrect\nrationales. For incorrect samples (i.e., whose answers are\nincorrect), commonsense mistake in the CoT is the most\nfrequent error type (88%). The model often makes com-\nmonsense mistakes when answering the questions requires\ncommonsense knowledge, e.g., understand maps and count-\ning numbers in the images (Figure 9), and utilizing the\nalphabet (Figure 10). The other type of mistake is a logical\nmistake (12%), with contradictions in the reasoning chains\n(Figure 11). In addition, there are cases with incorrect an-\nswers while their CoT are correct (6%) but might not be\nnecessarily related to answer options (Figure 12).\nThe analysis indicates that there are prospective directions\nfor future studies. It is possible to improve Multimodal-\nCoT by (i) incorporating more informative vision features\nand improving language-vision interaction to be capable of\nunderstanding maps and counting numbers; (ii) injecting\ncommonsense knowledge; (iii) applying a ltering mecha-\nnism, e.g., using only the effective CoT to infer the answer\nand get rid of irrelevant CoT.\n7. Conclusion\nWe formally study the problem of multimodal CoT. We pro-\npose Multimodal-CoT that incorporates language and vision\nmodalities into a two-stage framework that separates ratio-\nnale generation and answer inference, so answer inference\ncan leverage better generated rationales from multimodal in-\nformation. With Multimodal-CoT, we show that our method\nsurpasses GPT-3.5 by 16 percentage points in accuracy on\nthe ScienceQA benchmark. Our error analysis shows that\nit is the potential to leverage more effective vision features,\ninject commonsense knowledge, and apply ltering mecha-\nnisms to improve CoT reasoning in future studies.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nReferences\nAnderson, P., He, X., Buehler, C., Teney, D., Johnson, M.,\nGould, S., and Zhang, L. Bottom-up and top-down atten-\ntion for image captioning and visual question answering.\nIn2018 IEEE Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2018, Salt Lake City, UT, USA,\nJune 18-22, 2018 , pp. 60776086. IEEE Computer Soci-\nety, 2018. doi: 10.1109/CVPR.2018.00636.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,\nHenighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,\nJ., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,\nGray, S., Chess, B., Clark, J., Berner, C., McCandlish,\nS., Radford, A., Sutskever, I., and Amodei, D. Language\nmodels are few-shot learners. In Larochelle, H., Ranzato,\nM., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances\nin Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020.\nCarion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov,\nA., and Zagoruyko, S. End-to-end object detection with\ntransformers. In Computer VisionECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 2328, 2020,\nProceedings, Part I , pp. 213229, 2020.\nChen, T., Kornblith, S., Swersky, K., Norouzi, M., and\nHinton, G. E. Big self-supervised models are strong\nsemi-supervised learners. In Larochelle, H., Ranzato, M.,\nHadsell, R., Balcan, M., and Lin, H. (eds.), Advances\nin Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020.\nChen, W., Ma, X., Wang, X., and Cohen, W. W. Program\nof thoughts prompting: Disentangling computation from\nreasoning for numerical reasoning tasks. ArXiv preprint ,\nabs/2211.12588, 2022.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton,\nC., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,\nS., Maynez, J., Rao, A., Barnes, P., Tay, Y ., Shazeer,\nN., Prabhakaran, V ., Reif, E., Du, N., Hutchinson, B.,\nPope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G.,\nYin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S.,\nMichalewski, H., Garcia, X., Misra, V ., Robinson, K., Fe-\ndus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph,\nB., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal,\nS., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M.,\nLewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee,\nK., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O.,\nCatasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean,J., Petrov, S., and Fiedel, N. Palm: Scaling language mod-\neling with pathways. ArXiv preprint , abs/2204.02311,\n2022.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y .,\nFedus, W., Li, E., Wang, X., Dehghani, M., Brahma,\nS., et al. Scaling instruction-netuned language models.\narXiv preprint arXiv:2210.11416 , 2022.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\nHeigold, G., Gelly, S., et al. An image is worth 16x16\nwords: Transformers for image recognition at scale. In\nThe International Conference on Learning Representa-\ntions (ICLR) , 2021.\nFu, Y ., Peng, H., Sabharwal, A., Clark, P., and Khot, T.\nComplexity-based prompting for multi-step reasoning.\nArXiv preprint , abs/2210.00720, 2022.\nGao, P., Jiang, Z., You, H., Lu, P., Hoi, S. C. H., Wang, X.,\nand Li, H. Dynamic fusion with intra- and inter-modality\nattention ow for visual question answering. In IEEE\nConference on Computer Vision and Pattern Recognition,\nCVPR 2019, Long Beach, CA, USA, June 16-20, 2019 , pp.\n66396648. Computer Vision Foundation / IEEE, 2019.\ndoi: 10.1109/CVPR.2019.00680.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\nlearning for image recognition. In 2016 IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2016,\nLas Vegas, NV, USA, June 27-30, 2016 , pp. 770778.\nIEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.\n90.\nHo, N., Schmid, L., and Yun, S.-Y . Large language models\nare reasoning teachers. arXiv preprint arXiv:2212.10071 ,\n2022.\nJi, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y ., Ishii, E.,\nBang, Y ., Madotto, A., and Fung, P. Survey of halluci-\nnation in natural language generation. ACM Computing\nSurveys , 2022.\nKhashabi, D., Min, S., Khot, T., Sabharwal, A., Tafjord,\nO., Clark, P., and Hajishirzi, H. UNIFIEDQA: Crossing\nformat boundaries with a single QA system. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2020 , pp. 18961907, Online, 2020. Association\nfor Computational Linguistics. doi: 10.18653/v1/2020.\nndings-emnlp.171.\nKhot, T., Trivedi, H., Finlayson, M., Fu, Y ., Richardson, K.,\nClark, P., and Sabharwal, A. Decomposed prompting:\nA modular approach for solving complex tasks. ArXiv\npreprint , abs/2210.02406, 2022.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nKim, J., Jun, J., and Zhang, B. Bilinear attention networks.\nIn Bengio, S., Wallach, H. M., Larochelle, H., Grauman,\nK., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances\nin Neural Information Processing Systems 31: Annual\nConference on Neural Information Processing Systems\n2018, NeurIPS 2018, December 3-8, 2018, Montr eal,\nCanada , pp. 15711581, 2018.\nKim, W., Son, B., and Kim, I. Vilt: Vision-and-language\ntransformer without convolution or region supervision.\nInProceedings of the 38th International Conference on\nMachine Learning (ICML) , pp. 55835594, 2021.\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y ., and Iwasawa,\nY . Large language models are zero-shot reasoners. ArXiv\npreprint , abs/2205.11916, 2022.\nLi, B., Lv, C., Zhou, Z., Zhou, T., Xiao, T., Ma, A., and Zhu,\nJ. On vision features in multimodal machine translation.\nInProceedings of the 60th Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: Long\nPapers) , pp. 63276337, 2022a.\nLi, L. H., Yatskar, M., Yin, D., Hsieh, C.-J., and Chang,\nK.-W. Visualbert: A simple and performant baseline for\nvision and language. ArXiv preprint , abs/1908.03557,\n2019.\nLi, Y ., Lin, Z., Zhang, S., Fu, Q., Chen, B., Lou, J.-G., and\nChen, W. On the advance of making language models\nbetter reasoners. ArXiv preprint , abs/2206.02336, 2022b.\nLu, P., Qiu, L., Chen, J., Xia, T., Zhao, Y ., Zhang, W., Yu,\nZ., Liang, X., and Zhu, S.-C. Iconqa: A new benchmark\nfor abstract diagram understanding and visual language\nreasoning. In The 35th Conference on Neural Information\nProcessing Systems (NeurIPS) Track on Datasets and\nBenchmarks , 2021.\nLu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu,\nS.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to\nexplain: Multimodal reasoning via thought chains for sci-\nence question answering. ArXiv preprint , abs/2209.09513,\n2022a.\nLu, P., Qiu, L., Chang, K.-W., Wu, Y . N., Zhu, S.-C., Ra-\njpurohit, T., Clark, P., and Kalyan, A. Dynamic prompt\nlearning via policy gradient for semi-structured mathemat-\nical reasoning. ArXiv preprint , abs/2209.14610, 2022b.\nMagister, L. C., Mallinson, J., Adamek, J., Malmi, E., and\nSeveryn, A. Teaching small language models to reason.\nArXiv preprint , abs/2212.08410, 2022.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\net al. Learning transferable visual models from natural\nlanguage supervision. In International Conference on\nMachine Learning , pp. 87488763. PMLR, 2021.Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J.,\nSong, F., Aslanides, J., Henderson, S., Ring, R., Young,\nS., Rutherford, E., Hennigan, T., Menick, J., Cassirer, A.,\nPowell, R., Driessche, G. v. d., Hendricks, L. A., Rauh,\nM., Huang, P.-S., Glaese, A., Welbl, J., Dathathri, S.,\nHuang, S., Uesato, J., Mellor, J., Higgins, I., Creswell,\nA., McAleese, N., Wu, A., Elsen, E., Jayakumar, S.,\nBuchatskaya, E., Budden, D., Sutherland, E., Simonyan,\nK., Paganini, M., Sifre, L., Martens, L., Li, X. L., Kun-\ncoro, A., Nematzadeh, A., Gribovskaya, E., Donato, D.,\nLazaridou, A., Mensch, A., Lespiau, J.-B., Tsimpoukelli,\nM., Grigorev, N., Fritz, D., Sottiaux, T., Pajarskas, M.,\nPohlen, T., Gong, Z., Toyama, D., dAutume, C. d. M.,\nLi, Y ., Terzi, T., Mikulik, V ., Babuschkin, I., Clark, A.,\nCasas, D. d. L., Guy, A., Jones, C., Bradbury, J., Johnson,\nM., Hechtman, B., Weidinger, L., Gabriel, I., Isaac, W.,\nLockhart, E., Osindero, S., Rimell, L., Dyer, C., Vinyals,\nO., Ayoub, K., Stanway, J., Bennett, L., Hassabis, D.,\nKavukcuoglu, K., and Irving, G. Scaling language mod-\nels: Methods, analysis & insights from training gopher.\nArXiv preprint , abs/2112.11446, 2021.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring the\nlimits of transfer learning with a unied text-to-text trans-\nformer. Journal of Machine Learning Research (JMLR) ,\n21:167, 2020.\nRubin, O., Herzig, J., and Berant, J. Learning to re-\ntrieve prompts for in-context learning. In Proceedings\nof the 2022 Conference of the North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies , pp. 26552671, 2022. doi:\n10.18653/v1/2022.naacl-main.191.\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kul-\nshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L.,\nDu, Y ., Li, Y ., Lee, H., Zheng, H. S., Ghafouri, A., Mene-\ngali, M., Huang, Y ., Krikun, M., Lepikhin, D., Qin, J.,\nChen, D., Xu, Y ., Chen, Z., Roberts, A., Bosma, M.,\nZhao, V ., Zhou, Y ., Chang, C.-C., Krivokon, I., Rusch,\nW., Pickett, M., Srinivasan, P., Man, L., Meier-Hellstern,\nK., Morris, M. R., Doshi, T., Santos, R. D., Duke, T.,\nSoraker, J., Zevenbergen, B., Prabhakaran, V ., Diaz, M.,\nHutchinson, B., Olson, K., Molina, A., Hoffman-John, E.,\nLee, J., Aroyo, L., Rajakumar, R., Butryna, A., Lamm,\nM., Kuzmina, V ., Fenton, J., Cohen, A., Bernstein, R.,\nKurzweil, R., Aguera-Arcas, B., Cui, C., Croak, M., Chi,\nE., and Le, Q. Lamda: Language models for dialog\napplications. ArXiv preprint , abs/2201.08239, 2022.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need. In Guyon, I., von Luxburg, U., Bengio,\nS., Wallach, H. M., Fergus, R., Vishwanathan, S. V . N.,\nand Garnett, R. (eds.), Advances in Neural Information', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nProcessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA , pp. 59986008, 2017.\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E.,\nand Zhou, D. Self-consistency improves chain of\nthought reasoning in language models. ArXiv preprint ,\nabs/2203.11171, 2022a.\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and\nZhou, D. Rationale-augmented ensembles in language\nmodels. ArXiv preprint , abs/2207.00747, 2022b.\nWei, J., Tay, Y ., Bommasani, R., Raffel, C., Zoph, B.,\nBorgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Met-\nzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang,\nP., Dean, J., and Fedus, W. Emergent abilities of large\nlanguage models. Transactions on Machine Learning\nResearch , 2022a. Survey Certication.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E.,\nLe, Q., and Zhou, D. Chain of thought prompting elic-\nits reasoning in large language models. ArXiv preprint ,\nabs/2201.11903, 2022b.\nWu, Z., Kong, L., Bi, W., Li, X., and Kao, B. Good for\nmisconceived reasons: An empirical revisiting on the\nneed for visual context in multimodal machine transla-\ntion. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers) , pp. 61536166, Online,\n2021. Association for Computational Linguistics. doi:\n10.18653/v1/2021.acl-long.480.\nYu, Z., Yu, J., Cui, Y ., Tao, D., and Tian, Q. Deep modu-\nlar co-attention networks for visual question answering.\nInIEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2019, Long Beach, CA, USA, June\n16-20, 2019 , pp. 62816290. Computer Vision Founda-\ntion / IEEE, 2019. doi: 10.1109/CVPR.2019.00644.\nZhang, Z., Chen, K., Wang, R., Utiyama, M., Sumita, E., Li,\nZ., and Zhao, H. Neural machine translation with univer-\nsal visual representation. In 8th International Conference\non Learning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020 . OpenReview.net, 2020.\nZhang, Z., Zhang, A., Li, M., and Smola, A. Automatic\nchain of thought prompting in large language models.\nArXiv preprint , abs/2210.03493, 2022.\nZhang, Z., Chen, K., Wang, R., Utiyama, M., Sumita, E., Li,\nZ., and Zhao, H. Universal multimodal representation for\nlanguage understanding. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence , pp. 118, 2023. doi:\n10.1109/TPAMI.2023.3234170.Zhou, D., Sch arli, N., Hou, L., Wei, J., Scales, N., Wang,\nX., Schuurmans, D., Bousquet, O., Le, Q., and Chi, E.\nLeast-to-most prompting enables complex reasoning in\nlarge language models. ArXiv preprint , abs/2205.10625,\n2022.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nA. Extended analysis for the challenge of Multimodal-CoT\nA.1. More Examples of Misleading by Hallucinated Rationales\nAccording to our case studies (Section 3.2), we nd that the baseline tends to generate hallucinated rationales. We provide\nfurther examples as shown in Figure 6.\nOptions:(B) lichen(A) mushroomProblem\nBaselineQuestion:Basedonthearrows,whichofthefollowingorganismsisaconsumer?Context:BelowisafoodwebfromatundraecosysteminNunavut,aterritoryinNorthernCanada.Afoodwebmodelshowthemattereatenbyorganismsmovesthroughanecosystem.Thearrowsinafoodwebrepresenthowmattermovesbetweenorganismsinanecosystem.GoldRationale:AfoodwebisamodelAnorganisminafoodwebcanhavemorethanonearrowpointingfromit.Thisshowsthattheorganismiseatenbymorethanoneotherorganisminthefoodweb.Anorganisminafoodwebcanalsohavemorethanonearrowpointingtoit.Thisshowsthattheorganismeatsmorethanoneotherorganisminthefoodweb.Consumerseatotherorganisms.So,therearearrowsinafoodwebthatpointfromotherorganismstoconsumers.Themushroomhasarrowspointingtoitfromthebarren-groundcaribouandthegrizzlybear.So,themushroomisaconsumer.Thelichendoesnothaveanyarrowspointingtoit.So,thelichenisaproducer,notaconsumer.Answer:Theansweris(A).Rationale:AfoodwebisamodelThisshowsthattheorganismiseatenbymorethanoneotherorganisminthefoodweb.Anorganisminafoodwebcanalsohavemorethanonearrowpointingtoit.Thisshowsthattheorganismeatsmorethanoneotherorganisminthefoodweb.Consumerseatproducers.So,inafoodweb,consumershavearrowspointingtothemfromproducers.Themushroomhasanarrowpointingtoitfromthemushroom.Themushroomisnotaconsumer,sothemushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Answer:Theansweris(B).Rationale:AfoodwebisamodelAnorganisminafoodwebcanhavemorethanonearrowpointingfromit.Thisshowsthattheorganismiseatenbymorethanoneotherorganisminthefoodweb.Anorganisminafoodwebcanalsohavemorethanonearrowpointingtoit.Thisshowsthattheorganismeatsmorethanoneotherorganisminthefoodweb.Consumerseatproducersorganisms.So,inarearrowsafoodwebthatpointfromotherorganismstoconsumers.Themushroomhasarrowspointingtoitfromthen-browmushroomibou.thelichenzzlybear.So,themushroomisaconsumer.Thelichenhasnothaveanyarrowspointingtoit.So,themushroomisaconsumer.Lichenisnotaconsumer.Answer:Theansweris(B).Vision\n+ Vision Features\nOptions:(B) sample A(A) neither; the samples have the same temperatureProblem\nBaselineQuestion:Comparetheaveragekineticenergiesoftheparticlesineachsample.Whichsamplehasthehighertemperature?Context:Thediagramsbelowshowtwopuresamplesofgasinidenticalclosed,rigidcontainers.Eachcoloredballrepresentsonegasparticle.Bothsampleshavethesamenumberofparticles.GoldRationale:Thetemperatureofasubstancedependsontheaveragekineticenergyoftheparticlesinthesubstance.Thehighertheaveragekineticenergyoftheparticles,thehigherthetemperatureofthesubstance.Thekineticenergyofaparticleisdeterminedbyitsmassandspeed.Forapuresubstance,thegreaterthemassofeachparticleinthesubstanceandthehighertheaveragespeedoftheparticles,thehighertheiraveragekineticenergy.EachparticleinsampleAhasmoremassthaneachparticleinsampleB.TheparticlesinsampleAalso+ Vision FeaturesVision(C) sample B\nhaveahigheraveragespeedthantheparticlesinsampleB.So,theparticlesinsampleAhaveahigheraveragekineticenergythantheparticlesinsampleB.BecausetheparticlesinsampleAhavethehigheraveragekineticenergy,sampleAmusthavethehighertemperature.Answer:Theansweris(B).Rationale:Thetemperatureofasubstancedependsontheaveragekineticenergyoftheparticlesinthesubstance.Thehighertheaveragekineticenergyoftheparticles,thehigherthetemperatureofthesubstance.Thekineticenergyofaparticleisdeterminedbyitsmassandspeed.Forapuresubstance,thegreaterthemassofeachparticleinthesubstanceandthehighertheaveragespeedoftheparticles,thehighertheiraveragekineticenergy.EachparticleinsampleBhasmoremassthaneachparticleinsampleA.TheparticlesinsampleBalsohaveahigheraveragespeedthantheparticlesinsampleA.So,theparticlesinsampleBhaveahigheraveragekineticenergythantheparticlesinsampleA.BecausetheparticlesinsampleBhavethehigheraveragekineticenergy,sampleBmusthavethehighertemperature.Answer:Theansweris(C).Rationale:Thetemperatureofasubstancedependsontheaveragekineticenergyoftheparticlesinthesubstance.Thehighertheaveragekineticenergyoftheparticles,thehigherthetemperatureofthesubstance.Thekineticenergyofaparticleisdeterminedbyitsmassandspeed.Forapuresubstance,thegreaterthemassofeachparticleinthesubstanceandthehighertheaveragespeedoftheparticles,thehighertheiraveragekineticenergy.EachparticleinsampleAhasmoremassthaneachparticleinsampleB.TheparticlesinsampleAalsohaveahigheraveragespeedthantheparticlesinsampleB.So,theparticlesinsampleAhaveahigheraveragekineticenergythantheparticlesinsampleB.BecausetheparticlesinsampleAhavethehigheraveragekineticenergy,sampleAmusthavethehighertemperature.Answer:Theansweris(B).\nFigure 6. Examples of the two-stage framework without vision features (baseline) and with vision features (ours) for generating rationales\nand predicting answers. The upper part presents the problem details, and the lower part shows the outputs of the baseline and our method.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}]: %s
2023-12-07 12:21:38,096 - INFO - Check the results [{'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nTable 4. Main results (%). Size = backbone model size. Question classes: NAT = natural science, SOC = social science, LAN = language\nscience, TXT = text context, IMG = image context, NO = no context, G1-6 = grades 1-6, G7-12 = grades 7-12. Results except ours are\ntaken from Lu et al. (2022a). Segment 1: Human performance; Segment 2: VQA baselines; Segment 3: UniedQA baselines; Segment 4:\nGPT-3.5 baselines; Segment 5: Our Multimodal-CoT results. Results in bold are the best performance.\nModel Size NAT SOC LAN TXT IMG NO G1-6 G7-12 Avg\nHuman -90.23 84.97 87.48 89.60 87.50 88.10 91.59 82.42 88.40\nMCAN (Yu et al., 2019) 95M 56.08 46.23 58.09 59.43 51.17 55.40 51.65 59.72 54.54\nTop-Down (Anderson et al., 2018) 70M 59.50 54.33 61.82 62.90 54.88 59.79 57.27 62.16 59.02\nBAN (Kim et al., 2018) 112M 60.88 46.57 66.64 62.61 52.60 65.51 56.83 63.94 59.37\nDFAF (Gao et al., 2019) 74M 64.03 48.82 63.55 65.88 54.49 64.11 57.12 67.17 60.72\nViLT (Kim et al., 2021) 113M 60.48 63.89 60.27 63.20 61.38 57.00 60.72 61.90 61.14\nPatch-TRM (Lu et al., 2021) 90M 65.19 46.79 65.55 66.96 55.28 64.95 58.04 67.50 61.42\nVisualBERT (Li et al., 2019) 111M 59.33 69.18 61.18 62.71 62.17 58.54 62.96 59.92 61.87\nUniedQA Base (Khashabi et al., 2020) 223M 68.16 69.18 74.91 63.78 61.38 77.84 72.98 65.00 70.12\nUniedQA Base w/ CoT (Lu et al., 2022a) 223M 71.00 76.04 78.91 66.42 66.53 81.81 77.06 68.82 74.11\nGPT-3.5 (Chen et al., 2020) 175B 74.64 69.74 76.00 74.44 67.28 77.42 76.80 68.89 73.97\nGPT-3.5 w/ CoT (Lu et al., 2022a) 175B 75.44 70.87 78.09 74.68 67.43 79.93 78.23 69.68 75.17\nMutimodal-CoT Base 223M 87.52 77.17 85.82 87.88 82.90 86.83 84.65 85.37 84.91\nMutimodal-CoT Large 738M 95.91 82.00 90.82 95.26 88.80 92.89 92.44 90.31 91.68\nTable 5. Ablation results of Multimodal-CoT.\nModel NAT SOC LAN TXT IMG NO G1-6 G7-12 Avg\nMultimodal-CoT 87.52 77.17 85.82 87.88 82.90 86.83 84.65 85.37 84.91\nw/o Two-Stage Framework 80.99 87.40 81.91 80.25 78.83 83.62 82.78 82.20 82.57\nw/o Vision Features 71.09 70.75 69.18 71.16 65.84 71.57 71.00 69.68 70.53\nmaximum input sequence length is 512. The batch sizes for\nthe base and large models are 16 and 8, respectively. Our\nexperiments are run on 4 NVIDIA Tesla V100 32G GPUs.\nBaseline Models Following Lu et al. (2022a), our base-\nlines include (i) Visual question answering (VQA) models\n(Anderson et al., 2018; Kim et al., 2018; Yu et al., 2019;\nGao et al., 2019; Kim et al., 2021; Lu et al., 2021; Li et al.,\n2019); (ii) Text-to-text LM models. (Khashabi et al., 2020);\n(iii) GPT-3.5 models (Chen et al., 2020). More details are\npresented in Appendix B.1.\n5.3. Main Results\nTable 4 shows the main results. Mutimodal-CoT Large out-\nperforms GPT-3.5 by 16.51% (75.17% 91.68%) and sur-\npasses human performance. Specically, among the 8\nquestion classes, Mutimodal-CoT Large achieves a 21.37%\n(67.43%88.80%) performance gain for the questions with\npaired images (IMG). Compared with existing UniedQA\nand GPT-3.5 methods that leverage image captions in the\ncontext to provide vision semantics, the results indicate that\nusing image features is more effective. In addition, our\ntwo-stage framework contributes to the superior results ac-\ncording to our ablation study results in Table 5. Overall,\nthe results verify the effectiveness of multimodality and the\npotential of achieving CoT reasoning with 1B-models via\nour two-stage framework.12345678910405060708090\nEpochAccuracyOne-stage Baseline One-stage Multimodal\nTwo-Stage Baseline Two-Stage Multimodal\nFigure 5. Accuracy curve of the No-CoT baseline and Multimodal-\nCoT variants across epochs.\n6. Analysis\nThe following analysis will investigate how Multimodal-\nCoT works and discuss contribution factors and limitations.\nWe use models under the base size for analysis unless\notherwise stated.\n6.1. Multimodality Boosts Convergence\nFigure 5 shows the evaluation accuracy curve of the baseline\nand Multimodal-CoT in different training epochs. One-\nstage is based on the QCM A input-output format as it', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nachieves the best performance in Table 2 and Two-stage\nis our two-stage framework. We nd that the two-stage\nmethods achieve relatively higher accuracy at the beginning\nthan the one-stage baselines that generate the answer directly\nwithout CoT. However, without the vision features, the two-\nstage baseline could not yield better results as the training\ngoes on due to the low-quality rationales (as observed in\nSection 3). In contrast, using vision features helps generate\nmore effective rationales that contribute to better answer\naccuracy in our two-stage multimodal variant.\n6.2. Using Different Vision Features\nDifferent vision features may affect the model performance.\nWe compare three widely-used types of vision features,\nCLIP (Radford et al., 2021), DETR (Carion et al., 2020),\nand ResNet (He et al., 2016). CLIP and DETR are patch-like\nfeatures where DETR is based on object detection. For the\nResNet features, we repeat the pooled features of ResNet-\n50 to the same length with the text sequence to imitate the\npatch-like features, where each patch is the same as the\npooled image features. More details of the vision features\nare presented in Appendix B.2.\nTable 6. Accuracy (%) of using different vision features.\nMethod One-stage Two-Stage\nw/ CLIP 81.21 84.81\nw/ DETR 82.57 84.91\nw/ ResNet 80.97 84.77\nTable 6 shows the comparative results of vision features. We\nobserve that using vision features generally achieves better\nperformance than the language only baseline. Specically,\nDETR achieves relatively better performance in general.\nTherefore, we use DETR by default in Multimodal-CoT.\n6.3. General Effectiveness Across Backbone Models\nTo test the generality of the benets of our approach to\nother backbone models, we alter the underlying LMs to\nother variants in different sizes or types. As shown in Table\n7, our approach is generally effective for the widely-used\nbackbone models.\nTable 7. Accuracy (%) with different backbone language models.\nMethod Size Language Only Mutimodal-CoT\nUniedQA Base 223M 80.40 84.91\nUniedQA Large 738M 83.60 91.68\nFLAN-T5 Base 248M 83.42 85.85\nFLAN-T5 Large 783M 85.19 93.02\n6.4. Error Analysis\nTo better understand the behavior of Multimodal-CoT and\nfacilitate future studies, we manually investigate randomly\nselected examples generated by our approach. Table 8 sum-marizes the categorization results generated by Multimodal-\nCoT. We randomly picked up 50 samples whose answers\nwere correct and 50 samples whose answers were incor-\nrect. The corresponding examples from each category are\npresented in Appendix C.\nTable 8. Categorization analysis of Multimodal-CoT.\nAnswer CoT Category Percentage (%)\nCorrectCoT is correct 90\nCoT is incorrect 10\nIncorrectCommonsense Mistake 82\nLogical Mistake 12\nCoT is correct 6\nWe nd that the correct samples (i.e., whose answers are cor-\nrect) contain a certain amount of incorrect chain-of-thought\n(10%). The results indicate that CoT may not always benet\nthe answer inference, and the model is robust to some extent\n it can predict the correct answer by ignoring incorrect\nrationales. For incorrect samples (i.e., whose answers are\nincorrect), commonsense mistake in the CoT is the most\nfrequent error type (88%). The model often makes com-\nmonsense mistakes when answering the questions requires\ncommonsense knowledge, e.g., understand maps and count-\ning numbers in the images (Figure 9), and utilizing the\nalphabet (Figure 10). The other type of mistake is a logical\nmistake (12%), with contradictions in the reasoning chains\n(Figure 11). In addition, there are cases with incorrect an-\nswers while their CoT are correct (6%) but might not be\nnecessarily related to answer options (Figure 12).\nThe analysis indicates that there are prospective directions\nfor future studies. It is possible to improve Multimodal-\nCoT by (i) incorporating more informative vision features\nand improving language-vision interaction to be capable of\nunderstanding maps and counting numbers; (ii) injecting\ncommonsense knowledge; (iii) applying a ltering mecha-\nnism, e.g., using only the effective CoT to infer the answer\nand get rid of irrelevant CoT.\n7. Conclusion\nWe formally study the problem of multimodal CoT. We pro-\npose Multimodal-CoT that incorporates language and vision\nmodalities into a two-stage framework that separates ratio-\nnale generation and answer inference, so answer inference\ncan leverage better generated rationales from multimodal in-\nformation. With Multimodal-CoT, we show that our method\nsurpasses GPT-3.5 by 16 percentage points in accuracy on\nthe ScienceQA benchmark. Our error analysis shows that\nit is the potential to leverage more effective vision features,\ninject commonsense knowledge, and apply ltering mecha-\nnisms to improve CoT reasoning in future studies.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nReferences\nAnderson, P., He, X., Buehler, C., Teney, D., Johnson, M.,\nGould, S., and Zhang, L. Bottom-up and top-down atten-\ntion for image captioning and visual question answering.\nIn2018 IEEE Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2018, Salt Lake City, UT, USA,\nJune 18-22, 2018 , pp. 60776086. IEEE Computer Soci-\nety, 2018. doi: 10.1109/CVPR.2018.00636.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,\nHenighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,\nJ., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,\nGray, S., Chess, B., Clark, J., Berner, C., McCandlish,\nS., Radford, A., Sutskever, I., and Amodei, D. Language\nmodels are few-shot learners. In Larochelle, H., Ranzato,\nM., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances\nin Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020.\nCarion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov,\nA., and Zagoruyko, S. End-to-end object detection with\ntransformers. In Computer VisionECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 2328, 2020,\nProceedings, Part I , pp. 213229, 2020.\nChen, T., Kornblith, S., Swersky, K., Norouzi, M., and\nHinton, G. E. Big self-supervised models are strong\nsemi-supervised learners. In Larochelle, H., Ranzato, M.,\nHadsell, R., Balcan, M., and Lin, H. (eds.), Advances\nin Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020.\nChen, W., Ma, X., Wang, X., and Cohen, W. W. Program\nof thoughts prompting: Disentangling computation from\nreasoning for numerical reasoning tasks. ArXiv preprint ,\nabs/2211.12588, 2022.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton,\nC., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,\nS., Maynez, J., Rao, A., Barnes, P., Tay, Y ., Shazeer,\nN., Prabhakaran, V ., Reif, E., Du, N., Hutchinson, B.,\nPope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G.,\nYin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S.,\nMichalewski, H., Garcia, X., Misra, V ., Robinson, K., Fe-\ndus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph,\nB., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal,\nS., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M.,\nLewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee,\nK., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O.,\nCatasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean,J., Petrov, S., and Fiedel, N. Palm: Scaling language mod-\neling with pathways. ArXiv preprint , abs/2204.02311,\n2022.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y .,\nFedus, W., Li, E., Wang, X., Dehghani, M., Brahma,\nS., et al. Scaling instruction-netuned language models.\narXiv preprint arXiv:2210.11416 , 2022.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\nHeigold, G., Gelly, S., et al. An image is worth 16x16\nwords: Transformers for image recognition at scale. In\nThe International Conference on Learning Representa-\ntions (ICLR) , 2021.\nFu, Y ., Peng, H., Sabharwal, A., Clark, P., and Khot, T.\nComplexity-based prompting for multi-step reasoning.\nArXiv preprint , abs/2210.00720, 2022.\nGao, P., Jiang, Z., You, H., Lu, P., Hoi, S. C. H., Wang, X.,\nand Li, H. Dynamic fusion with intra- and inter-modality\nattention ow for visual question answering. In IEEE\nConference on Computer Vision and Pattern Recognition,\nCVPR 2019, Long Beach, CA, USA, June 16-20, 2019 , pp.\n66396648. Computer Vision Foundation / IEEE, 2019.\ndoi: 10.1109/CVPR.2019.00680.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\nlearning for image recognition. In 2016 IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2016,\nLas Vegas, NV, USA, June 27-30, 2016 , pp. 770778.\nIEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.\n90.\nHo, N., Schmid, L., and Yun, S.-Y . Large language models\nare reasoning teachers. arXiv preprint arXiv:2212.10071 ,\n2022.\nJi, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y ., Ishii, E.,\nBang, Y ., Madotto, A., and Fung, P. Survey of halluci-\nnation in natural language generation. ACM Computing\nSurveys , 2022.\nKhashabi, D., Min, S., Khot, T., Sabharwal, A., Tafjord,\nO., Clark, P., and Hajishirzi, H. UNIFIEDQA: Crossing\nformat boundaries with a single QA system. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2020 , pp. 18961907, Online, 2020. Association\nfor Computational Linguistics. doi: 10.18653/v1/2020.\nndings-emnlp.171.\nKhot, T., Trivedi, H., Finlayson, M., Fu, Y ., Richardson, K.,\nClark, P., and Sabharwal, A. Decomposed prompting:\nA modular approach for solving complex tasks. ArXiv\npreprint , abs/2210.02406, 2022.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nKim, J., Jun, J., and Zhang, B. Bilinear attention networks.\nIn Bengio, S., Wallach, H. M., Larochelle, H., Grauman,\nK., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances\nin Neural Information Processing Systems 31: Annual\nConference on Neural Information Processing Systems\n2018, NeurIPS 2018, December 3-8, 2018, Montr eal,\nCanada , pp. 15711581, 2018.\nKim, W., Son, B., and Kim, I. Vilt: Vision-and-language\ntransformer without convolution or region supervision.\nInProceedings of the 38th International Conference on\nMachine Learning (ICML) , pp. 55835594, 2021.\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y ., and Iwasawa,\nY . Large language models are zero-shot reasoners. ArXiv\npreprint , abs/2205.11916, 2022.\nLi, B., Lv, C., Zhou, Z., Zhou, T., Xiao, T., Ma, A., and Zhu,\nJ. On vision features in multimodal machine translation.\nInProceedings of the 60th Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: Long\nPapers) , pp. 63276337, 2022a.\nLi, L. H., Yatskar, M., Yin, D., Hsieh, C.-J., and Chang,\nK.-W. Visualbert: A simple and performant baseline for\nvision and language. ArXiv preprint , abs/1908.03557,\n2019.\nLi, Y ., Lin, Z., Zhang, S., Fu, Q., Chen, B., Lou, J.-G., and\nChen, W. On the advance of making language models\nbetter reasoners. ArXiv preprint , abs/2206.02336, 2022b.\nLu, P., Qiu, L., Chen, J., Xia, T., Zhao, Y ., Zhang, W., Yu,\nZ., Liang, X., and Zhu, S.-C. Iconqa: A new benchmark\nfor abstract diagram understanding and visual language\nreasoning. In The 35th Conference on Neural Information\nProcessing Systems (NeurIPS) Track on Datasets and\nBenchmarks , 2021.\nLu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu,\nS.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to\nexplain: Multimodal reasoning via thought chains for sci-\nence question answering. ArXiv preprint , abs/2209.09513,\n2022a.\nLu, P., Qiu, L., Chang, K.-W., Wu, Y . N., Zhu, S.-C., Ra-\njpurohit, T., Clark, P., and Kalyan, A. Dynamic prompt\nlearning via policy gradient for semi-structured mathemat-\nical reasoning. ArXiv preprint , abs/2209.14610, 2022b.\nMagister, L. C., Mallinson, J., Adamek, J., Malmi, E., and\nSeveryn, A. Teaching small language models to reason.\nArXiv preprint , abs/2212.08410, 2022.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\net al. Learning transferable visual models from natural\nlanguage supervision. In International Conference on\nMachine Learning , pp. 87488763. PMLR, 2021.Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J.,\nSong, F., Aslanides, J., Henderson, S., Ring, R., Young,\nS., Rutherford, E., Hennigan, T., Menick, J., Cassirer, A.,\nPowell, R., Driessche, G. v. d., Hendricks, L. A., Rauh,\nM., Huang, P.-S., Glaese, A., Welbl, J., Dathathri, S.,\nHuang, S., Uesato, J., Mellor, J., Higgins, I., Creswell,\nA., McAleese, N., Wu, A., Elsen, E., Jayakumar, S.,\nBuchatskaya, E., Budden, D., Sutherland, E., Simonyan,\nK., Paganini, M., Sifre, L., Martens, L., Li, X. L., Kun-\ncoro, A., Nematzadeh, A., Gribovskaya, E., Donato, D.,\nLazaridou, A., Mensch, A., Lespiau, J.-B., Tsimpoukelli,\nM., Grigorev, N., Fritz, D., Sottiaux, T., Pajarskas, M.,\nPohlen, T., Gong, Z., Toyama, D., dAutume, C. d. M.,\nLi, Y ., Terzi, T., Mikulik, V ., Babuschkin, I., Clark, A.,\nCasas, D. d. L., Guy, A., Jones, C., Bradbury, J., Johnson,\nM., Hechtman, B., Weidinger, L., Gabriel, I., Isaac, W.,\nLockhart, E., Osindero, S., Rimell, L., Dyer, C., Vinyals,\nO., Ayoub, K., Stanway, J., Bennett, L., Hassabis, D.,\nKavukcuoglu, K., and Irving, G. Scaling language mod-\nels: Methods, analysis & insights from training gopher.\nArXiv preprint , abs/2112.11446, 2021.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring the\nlimits of transfer learning with a unied text-to-text trans-\nformer. Journal of Machine Learning Research (JMLR) ,\n21:167, 2020.\nRubin, O., Herzig, J., and Berant, J. Learning to re-\ntrieve prompts for in-context learning. In Proceedings\nof the 2022 Conference of the North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies , pp. 26552671, 2022. doi:\n10.18653/v1/2022.naacl-main.191.\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kul-\nshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L.,\nDu, Y ., Li, Y ., Lee, H., Zheng, H. S., Ghafouri, A., Mene-\ngali, M., Huang, Y ., Krikun, M., Lepikhin, D., Qin, J.,\nChen, D., Xu, Y ., Chen, Z., Roberts, A., Bosma, M.,\nZhao, V ., Zhou, Y ., Chang, C.-C., Krivokon, I., Rusch,\nW., Pickett, M., Srinivasan, P., Man, L., Meier-Hellstern,\nK., Morris, M. R., Doshi, T., Santos, R. D., Duke, T.,\nSoraker, J., Zevenbergen, B., Prabhakaran, V ., Diaz, M.,\nHutchinson, B., Olson, K., Molina, A., Hoffman-John, E.,\nLee, J., Aroyo, L., Rajakumar, R., Butryna, A., Lamm,\nM., Kuzmina, V ., Fenton, J., Cohen, A., Bernstein, R.,\nKurzweil, R., Aguera-Arcas, B., Cui, C., Croak, M., Chi,\nE., and Le, Q. Lamda: Language models for dialog\napplications. ArXiv preprint , abs/2201.08239, 2022.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need. In Guyon, I., von Luxburg, U., Bengio,\nS., Wallach, H. M., Fergus, R., Vishwanathan, S. V . N.,\nand Garnett, R. (eds.), Advances in Neural Information', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nProcessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA , pp. 59986008, 2017.\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E.,\nand Zhou, D. Self-consistency improves chain of\nthought reasoning in language models. ArXiv preprint ,\nabs/2203.11171, 2022a.\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and\nZhou, D. Rationale-augmented ensembles in language\nmodels. ArXiv preprint , abs/2207.00747, 2022b.\nWei, J., Tay, Y ., Bommasani, R., Raffel, C., Zoph, B.,\nBorgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Met-\nzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang,\nP., Dean, J., and Fedus, W. Emergent abilities of large\nlanguage models. Transactions on Machine Learning\nResearch , 2022a. Survey Certication.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E.,\nLe, Q., and Zhou, D. Chain of thought prompting elic-\nits reasoning in large language models. ArXiv preprint ,\nabs/2201.11903, 2022b.\nWu, Z., Kong, L., Bi, W., Li, X., and Kao, B. Good for\nmisconceived reasons: An empirical revisiting on the\nneed for visual context in multimodal machine transla-\ntion. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers) , pp. 61536166, Online,\n2021. Association for Computational Linguistics. doi:\n10.18653/v1/2021.acl-long.480.\nYu, Z., Yu, J., Cui, Y ., Tao, D., and Tian, Q. Deep modu-\nlar co-attention networks for visual question answering.\nInIEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2019, Long Beach, CA, USA, June\n16-20, 2019 , pp. 62816290. Computer Vision Founda-\ntion / IEEE, 2019. doi: 10.1109/CVPR.2019.00644.\nZhang, Z., Chen, K., Wang, R., Utiyama, M., Sumita, E., Li,\nZ., and Zhao, H. Neural machine translation with univer-\nsal visual representation. In 8th International Conference\non Learning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020 . OpenReview.net, 2020.\nZhang, Z., Zhang, A., Li, M., and Smola, A. Automatic\nchain of thought prompting in large language models.\nArXiv preprint , abs/2210.03493, 2022.\nZhang, Z., Chen, K., Wang, R., Utiyama, M., Sumita, E., Li,\nZ., and Zhao, H. Universal multimodal representation for\nlanguage understanding. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence , pp. 118, 2023. doi:\n10.1109/TPAMI.2023.3234170.Zhou, D., Sch arli, N., Hou, L., Wei, J., Scales, N., Wang,\nX., Schuurmans, D., Bousquet, O., Le, Q., and Chi, E.\nLeast-to-most prompting enables complex reasoning in\nlarge language models. ArXiv preprint , abs/2205.10625,\n2022.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nA. Extended analysis for the challenge of Multimodal-CoT\nA.1. More Examples of Misleading by Hallucinated Rationales\nAccording to our case studies (Section 3.2), we nd that the baseline tends to generate hallucinated rationales. We provide\nfurther examples as shown in Figure 6.\nOptions:(B) lichen(A) mushroomProblem\nBaselineQuestion:Basedonthearrows,whichofthefollowingorganismsisaconsumer?Context:BelowisafoodwebfromatundraecosysteminNunavut,aterritoryinNorthernCanada.Afoodwebmodelshowthemattereatenbyorganismsmovesthroughanecosystem.Thearrowsinafoodwebrepresenthowmattermovesbetweenorganismsinanecosystem.GoldRationale:AfoodwebisamodelAnorganisminafoodwebcanhavemorethanonearrowpointingfromit.Thisshowsthattheorganismiseatenbymorethanoneotherorganisminthefoodweb.Anorganisminafoodwebcanalsohavemorethanonearrowpointingtoit.Thisshowsthattheorganismeatsmorethanoneotherorganisminthefoodweb.Consumerseatotherorganisms.So,therearearrowsinafoodwebthatpointfromotherorganismstoconsumers.Themushroomhasarrowspointingtoitfromthebarren-groundcaribouandthegrizzlybear.So,themushroomisaconsumer.Thelichendoesnothaveanyarrowspointingtoit.So,thelichenisaproducer,notaconsumer.Answer:Theansweris(A).Rationale:AfoodwebisamodelThisshowsthattheorganismiseatenbymorethanoneotherorganisminthefoodweb.Anorganisminafoodwebcanalsohavemorethanonearrowpointingtoit.Thisshowsthattheorganismeatsmorethanoneotherorganisminthefoodweb.Consumerseatproducers.So,inafoodweb,consumershavearrowspointingtothemfromproducers.Themushroomhasanarrowpointingtoitfromthemushroom.Themushroomisnotaconsumer,sothemushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Answer:Theansweris(B).Rationale:AfoodwebisamodelAnorganisminafoodwebcanhavemorethanonearrowpointingfromit.Thisshowsthattheorganismiseatenbymorethanoneotherorganisminthefoodweb.Anorganisminafoodwebcanalsohavemorethanonearrowpointingtoit.Thisshowsthattheorganismeatsmorethanoneotherorganisminthefoodweb.Consumerseatproducersorganisms.So,inarearrowsafoodwebthatpointfromotherorganismstoconsumers.Themushroomhasarrowspointingtoitfromthen-browmushroomibou.thelichenzzlybear.So,themushroomisaconsumer.Thelichenhasnothaveanyarrowspointingtoit.So,themushroomisaconsumer.Lichenisnotaconsumer.Answer:Theansweris(B).Vision\n+ Vision Features\nOptions:(B) sample A(A) neither; the samples have the same temperatureProblem\nBaselineQuestion:Comparetheaveragekineticenergiesoftheparticlesineachsample.Whichsamplehasthehighertemperature?Context:Thediagramsbelowshowtwopuresamplesofgasinidenticalclosed,rigidcontainers.Eachcoloredballrepresentsonegasparticle.Bothsampleshavethesamenumberofparticles.GoldRationale:Thetemperatureofasubstancedependsontheaveragekineticenergyoftheparticlesinthesubstance.Thehighertheaveragekineticenergyoftheparticles,thehigherthetemperatureofthesubstance.Thekineticenergyofaparticleisdeterminedbyitsmassandspeed.Forapuresubstance,thegreaterthemassofeachparticleinthesubstanceandthehighertheaveragespeedoftheparticles,thehighertheiraveragekineticenergy.EachparticleinsampleAhasmoremassthaneachparticleinsampleB.TheparticlesinsampleAalso+ Vision FeaturesVision(C) sample B\nhaveahigheraveragespeedthantheparticlesinsampleB.So,theparticlesinsampleAhaveahigheraveragekineticenergythantheparticlesinsampleB.BecausetheparticlesinsampleAhavethehigheraveragekineticenergy,sampleAmusthavethehighertemperature.Answer:Theansweris(B).Rationale:Thetemperatureofasubstancedependsontheaveragekineticenergyoftheparticlesinthesubstance.Thehighertheaveragekineticenergyoftheparticles,thehigherthetemperatureofthesubstance.Thekineticenergyofaparticleisdeterminedbyitsmassandspeed.Forapuresubstance,thegreaterthemassofeachparticleinthesubstanceandthehighertheaveragespeedoftheparticles,thehighertheiraveragekineticenergy.EachparticleinsampleBhasmoremassthaneachparticleinsampleA.TheparticlesinsampleBalsohaveahigheraveragespeedthantheparticlesinsampleA.So,theparticlesinsampleBhaveahigheraveragekineticenergythantheparticlesinsampleA.BecausetheparticlesinsampleBhavethehigheraveragekineticenergy,sampleBmusthavethehighertemperature.Answer:Theansweris(C).Rationale:Thetemperatureofasubstancedependsontheaveragekineticenergyoftheparticlesinthesubstance.Thehighertheaveragekineticenergyoftheparticles,thehigherthetemperatureofthesubstance.Thekineticenergyofaparticleisdeterminedbyitsmassandspeed.Forapuresubstance,thegreaterthemassofeachparticleinthesubstanceandthehighertheaveragespeedoftheparticles,thehighertheiraveragekineticenergy.EachparticleinsampleAhasmoremassthaneachparticleinsampleB.TheparticlesinsampleAalsohaveahigheraveragespeedthantheparticlesinsampleB.So,theparticlesinsampleAhaveahigheraveragekineticenergythantheparticlesinsampleB.BecausetheparticlesinsampleAhavethehigheraveragekineticenergy,sampleAmusthavethehighertemperature.Answer:Theansweris(B).\nFigure 6. Examples of the two-stage framework without vision features (baseline) and with vision features (ours) for generating rationales\nand predicting answers. The upper part presents the problem details, and the lower part shows the outputs of the baseline and our method.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}]: %s
2023-12-07 12:21:39,728 - INFO - Check the data that is being passed [{'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nZhuosheng Zhang1Aston Zhang2Mu Li2Hai Zhao1George Karypis2Alex Smola2\nAbstract\nLarge language models (LLMs) have shown im-\npressive performance on complex reasoning by\nleveraging chain-of-thought (CoT) prompting to\ngenerate intermediate reasoning chains as the ra-\ntionale to infer the answer. However, existing\nCoT studies have focused on the language modal-\nity. We propose Multimodal-CoT that incorpo-\nrates language (text) and vision (images) modal-\nities into a two-stage framework that separates\nrationale generation and answer inference. In this\nway, answer inference can leverage better gen-\nerated rationales that are based on multimodal\ninformation. With Multimodal-CoT, our model\nunder 1 billion parameters outperforms the previ-\nous state-of-the-art LLM (GPT-3.5) by 16 percent-\nage points (75.17% 91.68% accuracy) and even\nsurpasses human performance on the ScienceQA\nbenchmark. Code is publicly available.1\n1. Introduction\nImagine reading a textbook with no gures or tables. Our\nability to knowledge acquisition is greatly strengthened by\njointly modeling diverse data modalities, such as vision, lan-\nguage, and audio. Recently, large language models (LLMs)\n(Brown et al., 2020; Thoppilan et al., 2022; Rae et al., 2021;\nChowdhery et al., 2022) have shown impressive perfor-\nmance in complex reasoning by generating intermediate\nreasoning steps before inferring the answer. The intriguing\ntechnique is called chain-of-thought (CoT) reasoning (Wei\net al., 2022b; Kojima et al., 2022; Zhang et al., 2022).\nHowever, existing studies related to CoT reasoning are\nlargely isolated in the language modality (Wang et al.,\n2022b; Zhou et al., 2022; Lu et al., 2022b; Fu et al., 2022),\nwith little consideration of multimodal scenarios. To elicit\nCoT reasoning in multimodality, we advocate a Multimodal-\n1Shanghai Jiao Tong University2Amazon Web Services.\nCorrespondence to: Zhuosheng Zhang (work done at Ama-\nzon Web Services) <zhangzs@sjtu.edu.cn >, Aston Zhang\n<az@astonzhang.com >.\n1https://github.com/amazon-science/mm-cot\nOptions:(B) salty(A) softOutputQuestion:Whichpropertydothesetwoobjectshaveincommon?Context: Select the better answer.\nRationale:Lookateachobject.Foreachobject,decideifithasthatproperty.Potatochipshaveasaltytaste.Bothobjectsaresalty.Asoftobjectchangesshapewhenyousqueezeit.Thefriesaresoft,butthecrackerisnot.Thepropertythatbothobjectshaveincommonissalty.Answer:Theansweris(B).VisionLanguageInputFigure 1. Example of the multimodal CoT task.\nCoT paradigm. Given the inputs in different modalities,\nMultimodal-CoT decomposes multi-step problems into in-\ntermediate reasoning steps (rationale) and then infers the\nanswer. Since vision and language are the most popular\nmodalities, we focus on those two modalities in this work.\nAn example is shown in Figure 1. In general, there are two\nways to elicit Multimodal-CoT reasoning as follows: (i)\nprompting LLMs and (ii) ne-tuning small models.2\nThe most immediate way to perform Multimodal-CoT is to\ntransform the input of different modalities into one modality\nand prompt LLMs to perform CoT. For example, it is possi-\nble to extract the caption of an image by a captioning model\nand then concatenate the caption with the original language\ninput to be fed into LLMs (Lu et al., 2022a). However, there\nis severe information loss in the captioning process; thus,\nusing the captions (as opposed to vision features) may suffer\nfrom a lack of mutual synergy in the representation space\nof different modalities.\nTo facilitate the interaction between modalities, another\npotential solution is to ne-tune smaller language models\n(LMs) by fusing multimodal features (Zhang et al., 2023).\nAs this approach allows the exibility of adjusting model\narchitectures to incorporate multimodal features, we study\nne-tuning models in this work instead of prompting LLMs.\nThe key challenge is that language models under 100 billion\nparameters tend to generate hallucinated rationales that mis-\nlead the answer inference (Ho et al., 2022; Magister et al.,\n2In this work, we refer to small models as models with less\nthan 1 billion parameters (hereinafter dubbed as 1B-models).arXiv:2302.00923v4  [cs.CL]  17 Feb 2023', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nTable 1. Typical CoT techniques (FT: ne-tuning; KD: knowledge distillation). Segment 1: in-context learning techniques; Segment 2:\nne-tuning techniques. To the best of our knowledge, our work is the rst to study CoT reasoning in different modalities. Besides, we\nfocus on 1B-models, without relying on the outputs of LLMs.\nModels Mutimodal w/o LLM Model / Engine Training CoT Role CoT Source\nZero-Shot-CoT (Kojima et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Template\nFew-Shot-CoT (Wei et al., 2022b) \x17 \x17 PaLM (540B) ICL Reasoning Hand-crafted\nSelf-Consistency-CoT (Wang et al., 2022a) \x17 \x17 Codex (175B) ICL Reasoning Hand-crafted\nLeast-to-Most Prompting (Zhou et al., 2022) \x17 \x17 Codex (175B) ICL Reasoning Hand-crafted\nRetrieval-CoT (Zhang et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Auto-generated\nPromptPG-CoT (Lu et al., 2022b) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Hand-crafted\nAuto-CoT (Zhang et al., 2022) \x17 \x17 Codex (175B) ICL Reasoning Auto-generated\nComplexity-CoT (Fu et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Hand-crafted\nFew-Shot-PoT (Chen et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Hand-crafted\nUniedQA (Lu et al., 2022a) \x17  T5 (770M) FT Explanation Crawled\nFine-Tuned T5 XXL (Magister et al., 2022) \x17 \x17 T5 (11B) KD Reasoning LLM-generated\nFine-Tune-CoT (Ho et al., 2022) \x17 \x17 GPT-3 (6.7B) KD Reasoning LLM-generated\nMultimodal-CoT (our work)   T5 (770M) FT Reasoning Crawled\n2022; Ji et al., 2022).\nTo mitigate the challenge of hallucination, we propose\nMultimodal-CoT that incorporates language (text) and vi-\nsion (images) modalities into a two-stage framework that\nseparates rationale generation and answer inference. In\nthis way, answer inference can leverage better generated\nrationales that are based on multimodal information. Our\nexperiments are conducted on the ScienceQA benchmark\n(Lu et al., 2022a), which is the latest multimodal reasoning\nbenchmark with annotated reasoning chains. Experimental\nresults show that our method surpasses the previous state-of-\nthe-art GPT-3.5 model by +16% (75.17% 91.68%) on the\nbenchmark. Our contributions are summarized as follows:\n(i) To the best of our knowledge, this work is the rst to\nstudy CoT reasoning in different modalities.\n(ii) We propose a two-stage framework by ne-tuning lan-\nguage models to fuse vision and language representations\nto perform Multimodal-CoT. The model is able to generate\ninformative rationales to facilitate inferring nal answers.\n(iii) Our method achieves new state-of-the-art performance\non the ScienceQA benchmark, outperforming accuracy of\nGPT-3.5 by 16% and even surpassing human performance.\n2. Background\nThis section reviews recent progress of eliciting CoT rea-\nsoning by prompting and ne-tuning language models.\n2.1. CoT Reasoning with LLMs\nRecently, CoT has been widely used to elicit the multi-step\nreasoning abilities of LLMs (Wei et al., 2022b). Concretely,\nCoT techniques encourage the LLM to generate intermedi-\nate reasoning chains for solving a problem. Studies have\nshown that LLMs can perform CoT reasoning with two ma-\njor paradigms of techniques: Zero-Shot-CoT (Kojima et al.,2022) and Few-Shot-CoT (Wei et al., 2022b; Zhang et al.,\n2022). For Zero-Shot-CoT, Kojima et al. (2022) showed that\nLLMs are decent zero-shot reasoners by adding a prompt\nlike Lets think step by step after the test question to in-\nvoke CoT reasoning. For Few-Shot-CoT, a few step-by-step\nreasoning demonstrations are used as conditions for infer-\nence. Each demonstration has a question and a reasoning\nchain that leads to the nal answer. The demonstrations are\ncommonly obtained by hand-crafting or automatic gener-\nation. The corresponding techniques are thus referred to\nas Manual-CoT (Wei et al., 2022b) and Auto-CoT (Zhang\net al., 2022).\nWith effective demonstrations, Few-Shot-CoT often\nachieves stronger performance than Zero-Shot-CoT and has\nattracted more research interest. Therefore, most recent\nstudies focused on how to improve Few-Shot-CoT. Those\nstudies are categorized into two major research lines: (i)\noptimizing the demonstrations; (ii) optimizing the reasoning\nchains. Table 1 compares typical CoT techniques.\nOptimizing Demonstrations The performance of Few-\nShot-CoT relies on the quality of demonstrations. As re-\nported in Wei et al. (2022b), using demonstrations written\nby different annotators results in dramatic accuracy dispar-\nity in a symbolic reasoning task. Beyond hand-crafting the\ndemonstrations, recent studies have investigated ways to op-\ntimize the demonstration selection process. Notably, Rubin\net al. (2022) retrieved the semantically similar demonstra-\ntions with the test instance. However, this approach shows\na degraded performance when there are mistakes in the rea-\nsoning chains (Zhang et al., 2022). To address the limitation,\nZhang et al. (2022) found that the key is the diversity of\ndemonstration questions and proposed Auto-CoT: (i) par-\ntition questions of a given dataset into a few clusters; (ii)\nsample a representative question from each cluster and gen-\nerate its reasoning chain using Zero-Shot-CoT with simple\nheuristics. In addition, reinforcement learning (RL) and', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\ncomplexity-based selection strategies were also proposed\nto obtain effective demonstrations. Fu et al. (2022) chose\nexamples with complex reasoning chains (i.e., with more\nreasoning steps) as the demonstrations. Lu et al. (2022b)\ntrained an agent to nd optimal in-context examples from\na candidate pool and maximize the prediction rewards on\ngiven training examples when interacting with GPT-3.5.\nOptimizing Reasoning Chains A notable way to opti-\nmize reasoning chains is problem decomposition. Zhou\net al. (2022) proposed least-to-most prompting to decom-\npose complex problems into sub-problems and then solve\nthese sub-problems sequentially. As a result, solving a\ngiven sub-problem is facilitated by the answers to previ-\nously solved sub-problems. Similarly, Khot et al. (2022)\nused diverse decomposition structures and designed differ-\nent prompts to answer each sub-question. In addition to\nprompting the reasoning chains as natural language texts,\nChen et al. (2022) proposed program-of-thoughts (PoT),\nwhich modeled the reasoning process as a program and\nprompted LLMs to derive the answer by executing the gen-\nerated programs. Another trend is to vote over multiple\nreasoning paths for a test question. Wang et al. (2022a)\nintroduced a self-consistency decoding strategy to sample\nmultiple outputs of LLMs and then took a majority over\nthe nal answers. Wang et al. (2022b) and Li et al. (2022b)\nintroduced randomness in the input space to produce more\ndiverse outputs for voting.\n2.2. Eliciting CoT Reasoning by Fine-Tuning Models\nA recent interest is eliciting CoT reasoning by ne-tuning\nlanguage models. Lu et al. (2022a) ne-tuned the encoder-\ndecoder T5 model on a large-scale dataset with CoT annota-\ntions. However, a dramatic performance decline is observed\nwhen using CoT to infer the answer, i.e., generating the rea-\nsoning chain before the answer (reasoning). Instead, CoT\nis only used as an explanation after the answer. Magister\net al. (2022) and Ho et al. (2022) employed knowledge\ndistillation by ne-tuning a student model on the chain-of-\nthought outputs generated by a larger teacher model. The\nproposed methods showed performance gains in arithmetic,\ncommonsense, and symbolic reasoning tasks.\nThere is a key challenge in training 1B-models to be CoT\nreasoners. As observed by Wei et al. (2022b), models un-\nder 100 billion parameters tend to produce illogical CoT\nthat leads to wrong answers. In other words, it might be\nharder for 1B-models to generate effective CoT than directly\ngenerating the answer. It becomes even more challenging\nin a multimodal setting where answering the question also\nrequires understanding the multimodal inputs. In the follow-\ning part, we will explore the challenge of Multimodal-CoT\nand investigate how to perform effective multi-step reason-\ning.3. Challenge of Multimodal-CoT\nExisting studies have suggested that the CoT reasoning abil-\nity may emerge in language models at a certain scale, e.g.,\nover 100 billion parameters (Wei et al., 2022a). However,\nit remains an unresolved challenge to elicit such reasoning\nabilities in 1B-models, let alone in the multimodal scenario.\nThis work focuses on 1B-models as they can be ne-tuned\nand deployed with consumer-grade GPUs (e.g., 32G mem-\nory). In this section, we will investigate why 1B-models\nfail at CoT reasoning and study how to design an effective\napproach to overcome the challenge.\n3.1. Towards the Role of CoT\nTo begin with, we ne-tune a text-only baseline for CoT rea-\nsoning on the ScienceQA benchmark (Lu et al., 2022a).\nFollowing Lu et al. (2022a), we adopt UniedQA Base\n(Khashabi et al., 2020) as the backbone language model.3\nOur task is modeled as a text generation problem, where the\nmodel takes the textual information as the input and gener-\nates the output sequence that consists of the rationale and\nthe answer. As an example shown in Figure 1, the model\ntakes the concatenation of tokens of the question text (Q),\nthe context text (C), and multiple options (M) as the input.\nTo study the effect of CoT, we compare the performance\nwith three variants: (i) No-CoT which predicts the answer\ndirectly (QCMA); (ii) Reasoning where answer inference\nis conditioned to the rationale (QCM RA); (iii) Explana-\ntion where the rationale is used for explaining the answer\ninference (QCMAR).\nTable 2. Effects of CoT in the one-stage setting.\nMethod Format Accuracy\nNo-CoT QCM A 80.40\nReasoning QCM RA 67.86\nExplanation QCM AR 69.77\nSurprisingly, we observe a 12.54% accuracy decrease\n(80.40%67.86%) if the model predicts rationales before\nanswers (QCMRA). The results imply that the rationales\nmight not necessarily contribute to predicting the right an-\nswer. A similar phenomenon was observed in Lu et al.\n(2022a), where the plausible reason might be that the model\nexceeds the maximum token limits before obtaining the\nrequired answer or stops generating the prediction early.\nHowever, we nd that the maximum length of the gener-\nated outputs (RA) is always less than 400 tokens, which\nis below the length limit of language models (i.e., 512 in\nUniedQA Base). Therefore, it deserves a more in-depth\ninvestigation into why the rationales harm answer inference.\n3UniedQA (Khashabi et al., 2020) is adopted as it is the best\nne-tuning model in Lu et al. (2022a). Model information and\nimplementation details are presented in Appendix B.1.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nGeneratedRationale:Magnetscanpullorpushoneachotherwithouttouching.Whenmagnetsattract,theypulltogether.Whenmagnetsrepel,theypushapart.Whetheramagnetattractsorrepelsothermagnetsdependsonthepositionsofitspoles,orends.Everymagnethastwopoles,callednorthandsouth.Herearesomeexamplesofmagnets.ThenorthpoleofeachmagnetismarkedN,andthesouthpoleismarkedS.Ifdifferentpolesareclosesttoeachother,themagnetsattract.Themagnetsinthepairbelowattract.Ifthesamepolesareclosesttoeachother,themagnetsrepel.Themagnetsinbothpairsbelowrepel.Willthesemagnetsattractorrepel?Tofindout,lookatwhichpolesareclosesttoeachother.Thesouthpoleofonemagnetisclosesttothesouthpoleoftheothermagnet.Polesthatarethesamerepel.So,thesemagnetswillrepeleachother.Answer:Theansweris(B).Options:(B) repel(A) attractProblem\nBaselineQuestion:Willthesemagnetsattractorrepeleachother?Context:Twomagnetsareplacedasshown.Hint:Magnetsthatattractpulltogether.Magnetsthatrepelpushapart.GoldRationale:Magnetscanpullorpushoneachotherwithouttouching.Whenmagnetsattract,theypulltogether.Whenmagnetsrepel,theypushapart.Whetheramagnetattractsorrepelsothermagnetsdependsonthepositionsofitspoles,orends.Everymagnethastwopoles,callednorthandsouth.Herearesomeexamplesofmagnets.ThenorthpoleofeachmagnetismarkedN,andthesouthpoleismarkedS.Ifdifferentpolesareclosesttoeachother,themagnetsattract.Themagnetsinthepairbelowattract.Ifthesamepolesareclosesttoeachother,themagnetsrepel.Themagnetsinbothpairsbelowrepel.Willthesemagnetsattractorrepel?Tofindout,lookatwhichpolesareclosesttoeachother.Thenorthpoleofonemagnetisclosesttothesouthpoleoftheothermagnet.Polesthataredifferentattract.So,thesemagnetswillattracteachother.Answer:Theansweris(A).GeneratedRationale:Magnetscanpullorpushoneachotherwithouttouching.Whenmagnetsattract,theypulltogether.Whenmagnetsrepel,theypushapart.Whetheramagnetattractsorrepelsothermagnetsdependsonthepositionsofitspoles,orends.Everymagnethastwopoles,callednorthandsouth.Herearesomeexamplesofmagnets.ThenorthpoleofeachmagnetismarkedN,andthesouthpoleismarkedS.Ifdifferentpolesareclosesttoeachother,themagnetsattract.Themagnetsinthepairbelowattract.Ifthesamepolesareclosesttoeachother,themagnetsrepel.Themagnetsinbothpairsbelowrepel.Willthesemagnetsattractorrepel?Tofindout,lookatwhichpolesareclosesttoeachother.Thenorthpoleofonemagnetisclosesttothesouthpoleoftheothermagnet.Polesthataredifferentattract.So,thesemagnetswillattracteachother.Answer:Theansweris(A).+ Vision Features\nVision\nFigure 2. Example of the two-stage framework without vision features (baseline) and with vision features (ours) for generating rationales\nand predicting answers. The upper part presents the problem details with a gold rationale, and the lower part shows the outputs of the\nbaseline and our method incorporated with vision features. We observe that the baseline fails to predict the right answer due to the\nmisleading by hallucinated rationales. More examples are shown in Appendix A.1.\n3.2. Misleading by Hallucinated Rationales\nTo dive into how the rationales affect the answer prediction,\nwe separate the CoT problem into two stages, rationale\ngeneration andanswer inference . We report the RougeL\nscore and accuracy for the rationale generation and answer\ninference, respectively. Table 3 shows the results based\non the two-stage framework. Although the two-stage base-\nline model achieves a 91.76 RougeL score of the rationale\ngeneration, the answer inference accuracy is only 70.53%.\nCompared with the QCM A variant (80.40%) in Table 2,\nthe result shows that the generated rationale in the two-stage\nframework does not improve answer accuracy.\nTable 3. Two-stage setting of (i) rationale generation (RougeL) and\n(ii) answer inference (Accuracy).\nMethod (i) QCM R (ii) QCMRA\nTwo-Stage Framework 91.76 70.53\nw/ Captions 91.85 71.12\nw/ Vision Features 96.97 84.91\nThen, we randomly sample 50 error cases and nd that the\nmodel tends to generate hallucinated rationales that mislead\nthe answer inference. As an example shown in Figure 2, the\nmodel (left part) hallucinates that,  The south pole of one\nmagnet is closest to the south pole of the other magnet , due\nto the lack of reference to the vision content. We nd that\nsuch mistakes occur at a ratio of 64% among the error cases\nOthers(36%)Resolved (62.5%)Unresolved(37.5%)Hallucination(64%)(a) ratio of hallucination mistakes(b) correction rate w/ vision features  Figure 3. The ratio of hallucination mistakes (a) and correction\nrate w/ vision features (b).\n(Figure 3(a)).\n3.3. Multimodality Contributes to Effective Rationales\nWe speculate that such a phenomenon of hallucination is\ndue to a lack of necessary vision contexts for performing\neffective Multimodal-CoT. To inject vision information, a\nsimple way is to transform the paired image into a caption\n(Lu et al., 2022a) and then append the caption in the input of\nboth stages. However, as shown in Table 3, using captions\nonly yields marginal performance gains ( 0.59%). Then,\nwe explore an advanced technique by incorporating vision\nfeatures into the language model. Concretely, we feed the\npaired image to the DETR model (Carion et al., 2020) to\nextract vision features. Then we fuse the vision features', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nVisionLanguageRationale GenerationQuestion:Whichpropertydothesetwoobjectshaveincommon?Context: Select the better answer.Lookateachobject.Foreachobject,decideifithasthatproperty.Potatochipshaveasaltytaste.Bothobjectsaresalty.Asoftobjectchangesshapewhenyousqueezeit.Thefriesaresoft,butthecrackerisnot.Thepropertythatbothobjectshaveincommonissalty.\nOptions:(B) salty(A) softRationaleAnswer InferenceTheansweris(B).Answer\nFigure 4. Overview of our Multimodal-CoT framework. Multimodal-CoT consists of two stages: (i) rationale generation and (ii) answer\ninference. Both stages share the same model architecture but differ in the input and output. In the rst stage, we feed the model with\nlanguage and vision inputs to generate rationales. In the second stage, we append the original language input with the rationale generated\nfrom the rst stage. Then, we feed the updated language input with the original vision input to the model to infer the answer.\nwith the encoded language representations before feeding\nto the decoder (more details will be presented in Section\n4). Interestingly, with vision features, the RougeL score of\nthe rationale generation has boosted to 96.97% (QCM R),\nwhich correspondingly contributes to better answer accuracy\nof 84.91% (QCMR A). With those effective rationales,\nthe phenomenon of hallucination is mitigated  62.5%\nhallucination mistakes in Section 3.2 have been corrected\n(Figure 3(b)), as an example shown in Figure 2 (right part).4\nThe analysis so far compellingly shows that vision features\nare indeed benecial for generating effective rationales and\ncontributing to accurate answer inference. As the two-stage\nmethod (QCMRA) in Table 3 achieves better performance\nthan all the one-stage method in Table 2, we choose the two-\nstage method in our Multimodal-CoT framework.\n4. Multimodal-CoT\nBased on the observations and discussions in Section 3, we\npropose Multimodal-CoT to incorporate language (text) and\nvision (images) modalities into a two-stage framework. In\nthis section, we will rst overview the procedure of the\nframework and then elaborate on the technical design of the\nmodel architecture.\n4.1. Framework Overview\nMultimodal-CoT consists of two training stages: (i) ratio-\nnale generation and (ii) answer inference. Both stages share\nthe same model architecture but differ in the input Xand\noutputY. The overall procedure is illustrated in Figure 4.\nWe will take vision-language as an example to show how\nMultimodal-CoT works.\n4The left mistakes are mainly about map understanding, which\nrequires more advanced vision features. We will discuss them in\nSection 6.4.In the rationale generation stage, we feed the model with\nX={X1\nlanguage,Xvision}whereX1\nlanguage represents the lan-\nguage input in the rst stage and Xvision represents the vision\ninput, i.e., the image. For example, Xcan be instantiated as\na concatenation of question, context, and options of a multi-\nple choice reasoning problem (Lu et al., 2022a) as shown in\nFigure 4. The goal is to learn a rationale generation model\nR=F(X)whereRis the rationale.\nIn the answer inference stage, the rationale Ris appended\nto the original language input X1\nlanguage to construct the lan-\nguage input in the second stage, X2\nlanguage =X1\nlanguageR\nwheredenotes concatenation. Then, we feed the updated\ninputX={X2\nlanguage,Xvision}to the answer inference\nmodel to infer the nal answer A=F(X).\nIn both stages, we train two models with the same archi-\ntecture independently. They take the annotated elements\n(e.g.,XR,XRA, respectively) from the training\nset for supervised learning. During inference, given X, the\nrationales for the test sets are generated using the model\ntrained in the rst stage; they are used in the second stage\nfor answer inference.\n4.2. Model Architecture\nGiven the language input Xlanguage{X1\nlanguage,X2\nlanguage}\nand the vision input Xvision, we compute the probability of\ngenerating target text Y(either the rationale or the answer\nin Figure 4) of length Nby\np(Y|Xlanguage,Xvision ) =N\ni=1p(Yi|Xlanguage,Xvision,Y<i),\n(1)\nwherep(Yi|Xlanguage,Xvision,Y<i)is implemented with\na Transformer-based network (Vaswani et al., 2017). The\nnetwork has three major procedures: encoding, interaction,', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nAlgorithm 1 Multimodal-CoT\nInput: Language input X1\nlanguage , vision input Xvision\nOutput: Generated rationale R, inferred answer A\n1: Construct the input X={Xlanguage,X vision}\n2: Generate rationale R=F(X)using the model F()\n3:Append the rationale Rto the original language input\nX2\nlanguage =X1\nlanguageR.\n4: Construct new input X={X2\nlanguage,X vision}\n5:Infer the answer Aby conditioning on the new input, A=\nF(X).\n6:procedure F(X)\n7: Encode the language and vision inputs Hlanguage andHvision,\nrespectively\n8: Build the interaction between language and vision features\nby attention Hattn\nvision\n9: FuseHlanguage andHattn\nvision by a gated fusion mechanism to\nhaveHfuse\n10: FeedHfuseto the decoder to obtain the target prediction Y\n11: return Y\n12:end procedure\nand decoding. Specically, we feed the language text into\na Transformer encoder to obtain a textual representation,\nwhich is then interacted and fused with the vision represen-\ntation before being fed into the Transformer decoder.\nEncoding The modelF(X)takes both the language and\nvision inputs and obtains the text representation Hlanguage\nand the image feature Hvision by the following functions:\nHlanguage =LanguageEncoder (Xlanguage ), (2)\nHvision =WhVisionExtractor (Xvision ),(3)\nwhere LanguageEncoder( ) is implemented as a Trans-\nformer model. We use the hidden states of the last layer\nin the Transformer encoder as the language representation\nHlanguageRndwherendenotes the length of the lan-\nguage input, and dis the hidden dimension. Meanwhile,\nVisionExtractor() is used to vectorize the input image into\nvision features. Inspired by the recent success of Vision\nTransformers (Dosovitskiy et al., 2021), we fetch the patch-\nlevel features by off-the-shelf vision extraction models,5\nsuch as DETR (Carion et al., 2020). After obtaining the\npatch-level vision features, we apply a learnable projection\nmatrixWhto convert the shape of VisionExtractor (Xvision )\ninto that ofHlanguage ; thus we have HvisionRmdwhere\nmis the number of patches.\nInteraction After obtaining language and vision represen-\ntations, we use a single-head attention network to correlate\ntext tokens with image patches, where the query ( Q), key\n(K) and value (V) areHlanguage ,Hvision andHvision, respec-\n5The parameters of the vision extraction are frozen.tively. The attention output Hattn\nvisionRndis dened as:\nHattn\nvision =Softmax (QK\ndk)V, (4)\nwheredkis the same as the dimension of Hlanguage because\na single head is used.\nThen, we apply the gated fusion mechanism (Zhang et al.,\n2020; Wu et al., 2021; Li et al., 2022a) to fuse Hlanguage and\nHvision. The fused output HfuseRndis obtained by:\n=Sigmoid (WlHlanguage +WvHattn\nvision ),(5)\nHfuse = (1)Hlanguage +Hattn\nvision, (6)\nwhereWlandWvare learnable parameters.\nDecoding Finally, the fused output Hfuseis fed into the\nTransformer decoder to predict the target Y. The complete\nprocedure of Multimodal-CoT is shown in Algorithm 1.\n5. Experiments\nThis section will present the benchmark dataset, the imple-\nmentation of our technique, and the baselines for compar-\nisons. Then, we will report our main results and ndings.\n5.1. Dataset\nOur method is evaluated on the ScienceQA benchmark (Lu\net al., 2022a). ScienceQA is the rst large-scale multimodal\nscience question dataset that annotates the answers with de-\ntailed lectures and explanations. It contains 21k multimodal\nmultiple choice questions with rich domain diversity across\n3 subjects, 26 topics, 127 categories, and 379 skills. The\nbenchmark dataset is split into training, validation, and test\nsplits with 12726, 4241, and 4241 examples, respectively.\n5.2. Implementation\nThe following part presents the experimental settings of\nMultimodal-CoT and the baseline methods.\nExperimental Settings As the Multimodal-CoT task re-\nquires generating the reasoning chains and leveraging the\nvision features, we use the T5 encoder-decoder architec-\nture (Raffel et al., 2020). Specically, we adopt UniedQA\n(Khashabi et al., 2020) to initialize our models in the two\nstages because it achieves the best ne-tuning results in\nLu et al. (2022a). To verify the generality of our approach\nacross different LMs, we also employ FLAN-T5 (Chung\net al., 2022) as the backbone in Section 6.3. As using im-\nage captions does not yield signicant performance gains in\nSection 3.3, we did not use the captions. We ne-tune the\nmodels up to 20 epochs, with a learning rate of 5e-5. The', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}]: %s
2023-12-07 12:21:39,728 - INFO - Check the results [{'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nZhuosheng Zhang1Aston Zhang2Mu Li2Hai Zhao1George Karypis2Alex Smola2\nAbstract\nLarge language models (LLMs) have shown im-\npressive performance on complex reasoning by\nleveraging chain-of-thought (CoT) prompting to\ngenerate intermediate reasoning chains as the ra-\ntionale to infer the answer. However, existing\nCoT studies have focused on the language modal-\nity. We propose Multimodal-CoT that incorpo-\nrates language (text) and vision (images) modal-\nities into a two-stage framework that separates\nrationale generation and answer inference. In this\nway, answer inference can leverage better gen-\nerated rationales that are based on multimodal\ninformation. With Multimodal-CoT, our model\nunder 1 billion parameters outperforms the previ-\nous state-of-the-art LLM (GPT-3.5) by 16 percent-\nage points (75.17% 91.68% accuracy) and even\nsurpasses human performance on the ScienceQA\nbenchmark. Code is publicly available.1\n1. Introduction\nImagine reading a textbook with no gures or tables. Our\nability to knowledge acquisition is greatly strengthened by\njointly modeling diverse data modalities, such as vision, lan-\nguage, and audio. Recently, large language models (LLMs)\n(Brown et al., 2020; Thoppilan et al., 2022; Rae et al., 2021;\nChowdhery et al., 2022) have shown impressive perfor-\nmance in complex reasoning by generating intermediate\nreasoning steps before inferring the answer. The intriguing\ntechnique is called chain-of-thought (CoT) reasoning (Wei\net al., 2022b; Kojima et al., 2022; Zhang et al., 2022).\nHowever, existing studies related to CoT reasoning are\nlargely isolated in the language modality (Wang et al.,\n2022b; Zhou et al., 2022; Lu et al., 2022b; Fu et al., 2022),\nwith little consideration of multimodal scenarios. To elicit\nCoT reasoning in multimodality, we advocate a Multimodal-\n1Shanghai Jiao Tong University2Amazon Web Services.\nCorrespondence to: Zhuosheng Zhang (work done at Ama-\nzon Web Services) <zhangzs@sjtu.edu.cn >, Aston Zhang\n<az@astonzhang.com >.\n1https://github.com/amazon-science/mm-cot\nOptions:(B) salty(A) softOutputQuestion:Whichpropertydothesetwoobjectshaveincommon?Context: Select the better answer.\nRationale:Lookateachobject.Foreachobject,decideifithasthatproperty.Potatochipshaveasaltytaste.Bothobjectsaresalty.Asoftobjectchangesshapewhenyousqueezeit.Thefriesaresoft,butthecrackerisnot.Thepropertythatbothobjectshaveincommonissalty.Answer:Theansweris(B).VisionLanguageInputFigure 1. Example of the multimodal CoT task.\nCoT paradigm. Given the inputs in different modalities,\nMultimodal-CoT decomposes multi-step problems into in-\ntermediate reasoning steps (rationale) and then infers the\nanswer. Since vision and language are the most popular\nmodalities, we focus on those two modalities in this work.\nAn example is shown in Figure 1. In general, there are two\nways to elicit Multimodal-CoT reasoning as follows: (i)\nprompting LLMs and (ii) ne-tuning small models.2\nThe most immediate way to perform Multimodal-CoT is to\ntransform the input of different modalities into one modality\nand prompt LLMs to perform CoT. For example, it is possi-\nble to extract the caption of an image by a captioning model\nand then concatenate the caption with the original language\ninput to be fed into LLMs (Lu et al., 2022a). However, there\nis severe information loss in the captioning process; thus,\nusing the captions (as opposed to vision features) may suffer\nfrom a lack of mutual synergy in the representation space\nof different modalities.\nTo facilitate the interaction between modalities, another\npotential solution is to ne-tune smaller language models\n(LMs) by fusing multimodal features (Zhang et al., 2023).\nAs this approach allows the exibility of adjusting model\narchitectures to incorporate multimodal features, we study\nne-tuning models in this work instead of prompting LLMs.\nThe key challenge is that language models under 100 billion\nparameters tend to generate hallucinated rationales that mis-\nlead the answer inference (Ho et al., 2022; Magister et al.,\n2In this work, we refer to small models as models with less\nthan 1 billion parameters (hereinafter dubbed as 1B-models).arXiv:2302.00923v4  [cs.CL]  17 Feb 2023', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nTable 1. Typical CoT techniques (FT: ne-tuning; KD: knowledge distillation). Segment 1: in-context learning techniques; Segment 2:\nne-tuning techniques. To the best of our knowledge, our work is the rst to study CoT reasoning in different modalities. Besides, we\nfocus on 1B-models, without relying on the outputs of LLMs.\nModels Mutimodal w/o LLM Model / Engine Training CoT Role CoT Source\nZero-Shot-CoT (Kojima et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Template\nFew-Shot-CoT (Wei et al., 2022b) \x17 \x17 PaLM (540B) ICL Reasoning Hand-crafted\nSelf-Consistency-CoT (Wang et al., 2022a) \x17 \x17 Codex (175B) ICL Reasoning Hand-crafted\nLeast-to-Most Prompting (Zhou et al., 2022) \x17 \x17 Codex (175B) ICL Reasoning Hand-crafted\nRetrieval-CoT (Zhang et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Auto-generated\nPromptPG-CoT (Lu et al., 2022b) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Hand-crafted\nAuto-CoT (Zhang et al., 2022) \x17 \x17 Codex (175B) ICL Reasoning Auto-generated\nComplexity-CoT (Fu et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Hand-crafted\nFew-Shot-PoT (Chen et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Hand-crafted\nUniedQA (Lu et al., 2022a) \x17  T5 (770M) FT Explanation Crawled\nFine-Tuned T5 XXL (Magister et al., 2022) \x17 \x17 T5 (11B) KD Reasoning LLM-generated\nFine-Tune-CoT (Ho et al., 2022) \x17 \x17 GPT-3 (6.7B) KD Reasoning LLM-generated\nMultimodal-CoT (our work)   T5 (770M) FT Reasoning Crawled\n2022; Ji et al., 2022).\nTo mitigate the challenge of hallucination, we propose\nMultimodal-CoT that incorporates language (text) and vi-\nsion (images) modalities into a two-stage framework that\nseparates rationale generation and answer inference. In\nthis way, answer inference can leverage better generated\nrationales that are based on multimodal information. Our\nexperiments are conducted on the ScienceQA benchmark\n(Lu et al., 2022a), which is the latest multimodal reasoning\nbenchmark with annotated reasoning chains. Experimental\nresults show that our method surpasses the previous state-of-\nthe-art GPT-3.5 model by +16% (75.17% 91.68%) on the\nbenchmark. Our contributions are summarized as follows:\n(i) To the best of our knowledge, this work is the rst to\nstudy CoT reasoning in different modalities.\n(ii) We propose a two-stage framework by ne-tuning lan-\nguage models to fuse vision and language representations\nto perform Multimodal-CoT. The model is able to generate\ninformative rationales to facilitate inferring nal answers.\n(iii) Our method achieves new state-of-the-art performance\non the ScienceQA benchmark, outperforming accuracy of\nGPT-3.5 by 16% and even surpassing human performance.\n2. Background\nThis section reviews recent progress of eliciting CoT rea-\nsoning by prompting and ne-tuning language models.\n2.1. CoT Reasoning with LLMs\nRecently, CoT has been widely used to elicit the multi-step\nreasoning abilities of LLMs (Wei et al., 2022b). Concretely,\nCoT techniques encourage the LLM to generate intermedi-\nate reasoning chains for solving a problem. Studies have\nshown that LLMs can perform CoT reasoning with two ma-\njor paradigms of techniques: Zero-Shot-CoT (Kojima et al.,2022) and Few-Shot-CoT (Wei et al., 2022b; Zhang et al.,\n2022). For Zero-Shot-CoT, Kojima et al. (2022) showed that\nLLMs are decent zero-shot reasoners by adding a prompt\nlike Lets think step by step after the test question to in-\nvoke CoT reasoning. For Few-Shot-CoT, a few step-by-step\nreasoning demonstrations are used as conditions for infer-\nence. Each demonstration has a question and a reasoning\nchain that leads to the nal answer. The demonstrations are\ncommonly obtained by hand-crafting or automatic gener-\nation. The corresponding techniques are thus referred to\nas Manual-CoT (Wei et al., 2022b) and Auto-CoT (Zhang\net al., 2022).\nWith effective demonstrations, Few-Shot-CoT often\nachieves stronger performance than Zero-Shot-CoT and has\nattracted more research interest. Therefore, most recent\nstudies focused on how to improve Few-Shot-CoT. Those\nstudies are categorized into two major research lines: (i)\noptimizing the demonstrations; (ii) optimizing the reasoning\nchains. Table 1 compares typical CoT techniques.\nOptimizing Demonstrations The performance of Few-\nShot-CoT relies on the quality of demonstrations. As re-\nported in Wei et al. (2022b), using demonstrations written\nby different annotators results in dramatic accuracy dispar-\nity in a symbolic reasoning task. Beyond hand-crafting the\ndemonstrations, recent studies have investigated ways to op-\ntimize the demonstration selection process. Notably, Rubin\net al. (2022) retrieved the semantically similar demonstra-\ntions with the test instance. However, this approach shows\na degraded performance when there are mistakes in the rea-\nsoning chains (Zhang et al., 2022). To address the limitation,\nZhang et al. (2022) found that the key is the diversity of\ndemonstration questions and proposed Auto-CoT: (i) par-\ntition questions of a given dataset into a few clusters; (ii)\nsample a representative question from each cluster and gen-\nerate its reasoning chain using Zero-Shot-CoT with simple\nheuristics. In addition, reinforcement learning (RL) and', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\ncomplexity-based selection strategies were also proposed\nto obtain effective demonstrations. Fu et al. (2022) chose\nexamples with complex reasoning chains (i.e., with more\nreasoning steps) as the demonstrations. Lu et al. (2022b)\ntrained an agent to nd optimal in-context examples from\na candidate pool and maximize the prediction rewards on\ngiven training examples when interacting with GPT-3.5.\nOptimizing Reasoning Chains A notable way to opti-\nmize reasoning chains is problem decomposition. Zhou\net al. (2022) proposed least-to-most prompting to decom-\npose complex problems into sub-problems and then solve\nthese sub-problems sequentially. As a result, solving a\ngiven sub-problem is facilitated by the answers to previ-\nously solved sub-problems. Similarly, Khot et al. (2022)\nused diverse decomposition structures and designed differ-\nent prompts to answer each sub-question. In addition to\nprompting the reasoning chains as natural language texts,\nChen et al. (2022) proposed program-of-thoughts (PoT),\nwhich modeled the reasoning process as a program and\nprompted LLMs to derive the answer by executing the gen-\nerated programs. Another trend is to vote over multiple\nreasoning paths for a test question. Wang et al. (2022a)\nintroduced a self-consistency decoding strategy to sample\nmultiple outputs of LLMs and then took a majority over\nthe nal answers. Wang et al. (2022b) and Li et al. (2022b)\nintroduced randomness in the input space to produce more\ndiverse outputs for voting.\n2.2. Eliciting CoT Reasoning by Fine-Tuning Models\nA recent interest is eliciting CoT reasoning by ne-tuning\nlanguage models. Lu et al. (2022a) ne-tuned the encoder-\ndecoder T5 model on a large-scale dataset with CoT annota-\ntions. However, a dramatic performance decline is observed\nwhen using CoT to infer the answer, i.e., generating the rea-\nsoning chain before the answer (reasoning). Instead, CoT\nis only used as an explanation after the answer. Magister\net al. (2022) and Ho et al. (2022) employed knowledge\ndistillation by ne-tuning a student model on the chain-of-\nthought outputs generated by a larger teacher model. The\nproposed methods showed performance gains in arithmetic,\ncommonsense, and symbolic reasoning tasks.\nThere is a key challenge in training 1B-models to be CoT\nreasoners. As observed by Wei et al. (2022b), models un-\nder 100 billion parameters tend to produce illogical CoT\nthat leads to wrong answers. In other words, it might be\nharder for 1B-models to generate effective CoT than directly\ngenerating the answer. It becomes even more challenging\nin a multimodal setting where answering the question also\nrequires understanding the multimodal inputs. In the follow-\ning part, we will explore the challenge of Multimodal-CoT\nand investigate how to perform effective multi-step reason-\ning.3. Challenge of Multimodal-CoT\nExisting studies have suggested that the CoT reasoning abil-\nity may emerge in language models at a certain scale, e.g.,\nover 100 billion parameters (Wei et al., 2022a). However,\nit remains an unresolved challenge to elicit such reasoning\nabilities in 1B-models, let alone in the multimodal scenario.\nThis work focuses on 1B-models as they can be ne-tuned\nand deployed with consumer-grade GPUs (e.g., 32G mem-\nory). In this section, we will investigate why 1B-models\nfail at CoT reasoning and study how to design an effective\napproach to overcome the challenge.\n3.1. Towards the Role of CoT\nTo begin with, we ne-tune a text-only baseline for CoT rea-\nsoning on the ScienceQA benchmark (Lu et al., 2022a).\nFollowing Lu et al. (2022a), we adopt UniedQA Base\n(Khashabi et al., 2020) as the backbone language model.3\nOur task is modeled as a text generation problem, where the\nmodel takes the textual information as the input and gener-\nates the output sequence that consists of the rationale and\nthe answer. As an example shown in Figure 1, the model\ntakes the concatenation of tokens of the question text (Q),\nthe context text (C), and multiple options (M) as the input.\nTo study the effect of CoT, we compare the performance\nwith three variants: (i) No-CoT which predicts the answer\ndirectly (QCMA); (ii) Reasoning where answer inference\nis conditioned to the rationale (QCM RA); (iii) Explana-\ntion where the rationale is used for explaining the answer\ninference (QCMAR).\nTable 2. Effects of CoT in the one-stage setting.\nMethod Format Accuracy\nNo-CoT QCM A 80.40\nReasoning QCM RA 67.86\nExplanation QCM AR 69.77\nSurprisingly, we observe a 12.54% accuracy decrease\n(80.40%67.86%) if the model predicts rationales before\nanswers (QCMRA). The results imply that the rationales\nmight not necessarily contribute to predicting the right an-\nswer. A similar phenomenon was observed in Lu et al.\n(2022a), where the plausible reason might be that the model\nexceeds the maximum token limits before obtaining the\nrequired answer or stops generating the prediction early.\nHowever, we nd that the maximum length of the gener-\nated outputs (RA) is always less than 400 tokens, which\nis below the length limit of language models (i.e., 512 in\nUniedQA Base). Therefore, it deserves a more in-depth\ninvestigation into why the rationales harm answer inference.\n3UniedQA (Khashabi et al., 2020) is adopted as it is the best\nne-tuning model in Lu et al. (2022a). Model information and\nimplementation details are presented in Appendix B.1.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nGeneratedRationale:Magnetscanpullorpushoneachotherwithouttouching.Whenmagnetsattract,theypulltogether.Whenmagnetsrepel,theypushapart.Whetheramagnetattractsorrepelsothermagnetsdependsonthepositionsofitspoles,orends.Everymagnethastwopoles,callednorthandsouth.Herearesomeexamplesofmagnets.ThenorthpoleofeachmagnetismarkedN,andthesouthpoleismarkedS.Ifdifferentpolesareclosesttoeachother,themagnetsattract.Themagnetsinthepairbelowattract.Ifthesamepolesareclosesttoeachother,themagnetsrepel.Themagnetsinbothpairsbelowrepel.Willthesemagnetsattractorrepel?Tofindout,lookatwhichpolesareclosesttoeachother.Thesouthpoleofonemagnetisclosesttothesouthpoleoftheothermagnet.Polesthatarethesamerepel.So,thesemagnetswillrepeleachother.Answer:Theansweris(B).Options:(B) repel(A) attractProblem\nBaselineQuestion:Willthesemagnetsattractorrepeleachother?Context:Twomagnetsareplacedasshown.Hint:Magnetsthatattractpulltogether.Magnetsthatrepelpushapart.GoldRationale:Magnetscanpullorpushoneachotherwithouttouching.Whenmagnetsattract,theypulltogether.Whenmagnetsrepel,theypushapart.Whetheramagnetattractsorrepelsothermagnetsdependsonthepositionsofitspoles,orends.Everymagnethastwopoles,callednorthandsouth.Herearesomeexamplesofmagnets.ThenorthpoleofeachmagnetismarkedN,andthesouthpoleismarkedS.Ifdifferentpolesareclosesttoeachother,themagnetsattract.Themagnetsinthepairbelowattract.Ifthesamepolesareclosesttoeachother,themagnetsrepel.Themagnetsinbothpairsbelowrepel.Willthesemagnetsattractorrepel?Tofindout,lookatwhichpolesareclosesttoeachother.Thenorthpoleofonemagnetisclosesttothesouthpoleoftheothermagnet.Polesthataredifferentattract.So,thesemagnetswillattracteachother.Answer:Theansweris(A).GeneratedRationale:Magnetscanpullorpushoneachotherwithouttouching.Whenmagnetsattract,theypulltogether.Whenmagnetsrepel,theypushapart.Whetheramagnetattractsorrepelsothermagnetsdependsonthepositionsofitspoles,orends.Everymagnethastwopoles,callednorthandsouth.Herearesomeexamplesofmagnets.ThenorthpoleofeachmagnetismarkedN,andthesouthpoleismarkedS.Ifdifferentpolesareclosesttoeachother,themagnetsattract.Themagnetsinthepairbelowattract.Ifthesamepolesareclosesttoeachother,themagnetsrepel.Themagnetsinbothpairsbelowrepel.Willthesemagnetsattractorrepel?Tofindout,lookatwhichpolesareclosesttoeachother.Thenorthpoleofonemagnetisclosesttothesouthpoleoftheothermagnet.Polesthataredifferentattract.So,thesemagnetswillattracteachother.Answer:Theansweris(A).+ Vision Features\nVision\nFigure 2. Example of the two-stage framework without vision features (baseline) and with vision features (ours) for generating rationales\nand predicting answers. The upper part presents the problem details with a gold rationale, and the lower part shows the outputs of the\nbaseline and our method incorporated with vision features. We observe that the baseline fails to predict the right answer due to the\nmisleading by hallucinated rationales. More examples are shown in Appendix A.1.\n3.2. Misleading by Hallucinated Rationales\nTo dive into how the rationales affect the answer prediction,\nwe separate the CoT problem into two stages, rationale\ngeneration andanswer inference . We report the RougeL\nscore and accuracy for the rationale generation and answer\ninference, respectively. Table 3 shows the results based\non the two-stage framework. Although the two-stage base-\nline model achieves a 91.76 RougeL score of the rationale\ngeneration, the answer inference accuracy is only 70.53%.\nCompared with the QCM A variant (80.40%) in Table 2,\nthe result shows that the generated rationale in the two-stage\nframework does not improve answer accuracy.\nTable 3. Two-stage setting of (i) rationale generation (RougeL) and\n(ii) answer inference (Accuracy).\nMethod (i) QCM R (ii) QCMRA\nTwo-Stage Framework 91.76 70.53\nw/ Captions 91.85 71.12\nw/ Vision Features 96.97 84.91\nThen, we randomly sample 50 error cases and nd that the\nmodel tends to generate hallucinated rationales that mislead\nthe answer inference. As an example shown in Figure 2, the\nmodel (left part) hallucinates that,  The south pole of one\nmagnet is closest to the south pole of the other magnet , due\nto the lack of reference to the vision content. We nd that\nsuch mistakes occur at a ratio of 64% among the error cases\nOthers(36%)Resolved (62.5%)Unresolved(37.5%)Hallucination(64%)(a) ratio of hallucination mistakes(b) correction rate w/ vision features  Figure 3. The ratio of hallucination mistakes (a) and correction\nrate w/ vision features (b).\n(Figure 3(a)).\n3.3. Multimodality Contributes to Effective Rationales\nWe speculate that such a phenomenon of hallucination is\ndue to a lack of necessary vision contexts for performing\neffective Multimodal-CoT. To inject vision information, a\nsimple way is to transform the paired image into a caption\n(Lu et al., 2022a) and then append the caption in the input of\nboth stages. However, as shown in Table 3, using captions\nonly yields marginal performance gains ( 0.59%). Then,\nwe explore an advanced technique by incorporating vision\nfeatures into the language model. Concretely, we feed the\npaired image to the DETR model (Carion et al., 2020) to\nextract vision features. Then we fuse the vision features', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nVisionLanguageRationale GenerationQuestion:Whichpropertydothesetwoobjectshaveincommon?Context: Select the better answer.Lookateachobject.Foreachobject,decideifithasthatproperty.Potatochipshaveasaltytaste.Bothobjectsaresalty.Asoftobjectchangesshapewhenyousqueezeit.Thefriesaresoft,butthecrackerisnot.Thepropertythatbothobjectshaveincommonissalty.\nOptions:(B) salty(A) softRationaleAnswer InferenceTheansweris(B).Answer\nFigure 4. Overview of our Multimodal-CoT framework. Multimodal-CoT consists of two stages: (i) rationale generation and (ii) answer\ninference. Both stages share the same model architecture but differ in the input and output. In the rst stage, we feed the model with\nlanguage and vision inputs to generate rationales. In the second stage, we append the original language input with the rationale generated\nfrom the rst stage. Then, we feed the updated language input with the original vision input to the model to infer the answer.\nwith the encoded language representations before feeding\nto the decoder (more details will be presented in Section\n4). Interestingly, with vision features, the RougeL score of\nthe rationale generation has boosted to 96.97% (QCM R),\nwhich correspondingly contributes to better answer accuracy\nof 84.91% (QCMR A). With those effective rationales,\nthe phenomenon of hallucination is mitigated  62.5%\nhallucination mistakes in Section 3.2 have been corrected\n(Figure 3(b)), as an example shown in Figure 2 (right part).4\nThe analysis so far compellingly shows that vision features\nare indeed benecial for generating effective rationales and\ncontributing to accurate answer inference. As the two-stage\nmethod (QCMRA) in Table 3 achieves better performance\nthan all the one-stage method in Table 2, we choose the two-\nstage method in our Multimodal-CoT framework.\n4. Multimodal-CoT\nBased on the observations and discussions in Section 3, we\npropose Multimodal-CoT to incorporate language (text) and\nvision (images) modalities into a two-stage framework. In\nthis section, we will rst overview the procedure of the\nframework and then elaborate on the technical design of the\nmodel architecture.\n4.1. Framework Overview\nMultimodal-CoT consists of two training stages: (i) ratio-\nnale generation and (ii) answer inference. Both stages share\nthe same model architecture but differ in the input Xand\noutputY. The overall procedure is illustrated in Figure 4.\nWe will take vision-language as an example to show how\nMultimodal-CoT works.\n4The left mistakes are mainly about map understanding, which\nrequires more advanced vision features. We will discuss them in\nSection 6.4.In the rationale generation stage, we feed the model with\nX={X1\nlanguage,Xvision}whereX1\nlanguage represents the lan-\nguage input in the rst stage and Xvision represents the vision\ninput, i.e., the image. For example, Xcan be instantiated as\na concatenation of question, context, and options of a multi-\nple choice reasoning problem (Lu et al., 2022a) as shown in\nFigure 4. The goal is to learn a rationale generation model\nR=F(X)whereRis the rationale.\nIn the answer inference stage, the rationale Ris appended\nto the original language input X1\nlanguage to construct the lan-\nguage input in the second stage, X2\nlanguage =X1\nlanguageR\nwheredenotes concatenation. Then, we feed the updated\ninputX={X2\nlanguage,Xvision}to the answer inference\nmodel to infer the nal answer A=F(X).\nIn both stages, we train two models with the same archi-\ntecture independently. They take the annotated elements\n(e.g.,XR,XRA, respectively) from the training\nset for supervised learning. During inference, given X, the\nrationales for the test sets are generated using the model\ntrained in the rst stage; they are used in the second stage\nfor answer inference.\n4.2. Model Architecture\nGiven the language input Xlanguage{X1\nlanguage,X2\nlanguage}\nand the vision input Xvision, we compute the probability of\ngenerating target text Y(either the rationale or the answer\nin Figure 4) of length Nby\np(Y|Xlanguage,Xvision ) =N\ni=1p(Yi|Xlanguage,Xvision,Y<i),\n(1)\nwherep(Yi|Xlanguage,Xvision,Y<i)is implemented with\na Transformer-based network (Vaswani et al., 2017). The\nnetwork has three major procedures: encoding, interaction,', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nAlgorithm 1 Multimodal-CoT\nInput: Language input X1\nlanguage , vision input Xvision\nOutput: Generated rationale R, inferred answer A\n1: Construct the input X={Xlanguage,X vision}\n2: Generate rationale R=F(X)using the model F()\n3:Append the rationale Rto the original language input\nX2\nlanguage =X1\nlanguageR.\n4: Construct new input X={X2\nlanguage,X vision}\n5:Infer the answer Aby conditioning on the new input, A=\nF(X).\n6:procedure F(X)\n7: Encode the language and vision inputs Hlanguage andHvision,\nrespectively\n8: Build the interaction between language and vision features\nby attention Hattn\nvision\n9: FuseHlanguage andHattn\nvision by a gated fusion mechanism to\nhaveHfuse\n10: FeedHfuseto the decoder to obtain the target prediction Y\n11: return Y\n12:end procedure\nand decoding. Specically, we feed the language text into\na Transformer encoder to obtain a textual representation,\nwhich is then interacted and fused with the vision represen-\ntation before being fed into the Transformer decoder.\nEncoding The modelF(X)takes both the language and\nvision inputs and obtains the text representation Hlanguage\nand the image feature Hvision by the following functions:\nHlanguage =LanguageEncoder (Xlanguage ), (2)\nHvision =WhVisionExtractor (Xvision ),(3)\nwhere LanguageEncoder( ) is implemented as a Trans-\nformer model. We use the hidden states of the last layer\nin the Transformer encoder as the language representation\nHlanguageRndwherendenotes the length of the lan-\nguage input, and dis the hidden dimension. Meanwhile,\nVisionExtractor() is used to vectorize the input image into\nvision features. Inspired by the recent success of Vision\nTransformers (Dosovitskiy et al., 2021), we fetch the patch-\nlevel features by off-the-shelf vision extraction models,5\nsuch as DETR (Carion et al., 2020). After obtaining the\npatch-level vision features, we apply a learnable projection\nmatrixWhto convert the shape of VisionExtractor (Xvision )\ninto that ofHlanguage ; thus we have HvisionRmdwhere\nmis the number of patches.\nInteraction After obtaining language and vision represen-\ntations, we use a single-head attention network to correlate\ntext tokens with image patches, where the query ( Q), key\n(K) and value (V) areHlanguage ,Hvision andHvision, respec-\n5The parameters of the vision extraction are frozen.tively. The attention output Hattn\nvisionRndis dened as:\nHattn\nvision =Softmax (QK\ndk)V, (4)\nwheredkis the same as the dimension of Hlanguage because\na single head is used.\nThen, we apply the gated fusion mechanism (Zhang et al.,\n2020; Wu et al., 2021; Li et al., 2022a) to fuse Hlanguage and\nHvision. The fused output HfuseRndis obtained by:\n=Sigmoid (WlHlanguage +WvHattn\nvision ),(5)\nHfuse = (1)Hlanguage +Hattn\nvision, (6)\nwhereWlandWvare learnable parameters.\nDecoding Finally, the fused output Hfuseis fed into the\nTransformer decoder to predict the target Y. The complete\nprocedure of Multimodal-CoT is shown in Algorithm 1.\n5. Experiments\nThis section will present the benchmark dataset, the imple-\nmentation of our technique, and the baselines for compar-\nisons. Then, we will report our main results and ndings.\n5.1. Dataset\nOur method is evaluated on the ScienceQA benchmark (Lu\net al., 2022a). ScienceQA is the rst large-scale multimodal\nscience question dataset that annotates the answers with de-\ntailed lectures and explanations. It contains 21k multimodal\nmultiple choice questions with rich domain diversity across\n3 subjects, 26 topics, 127 categories, and 379 skills. The\nbenchmark dataset is split into training, validation, and test\nsplits with 12726, 4241, and 4241 examples, respectively.\n5.2. Implementation\nThe following part presents the experimental settings of\nMultimodal-CoT and the baseline methods.\nExperimental Settings As the Multimodal-CoT task re-\nquires generating the reasoning chains and leveraging the\nvision features, we use the T5 encoder-decoder architec-\nture (Raffel et al., 2020). Specically, we adopt UniedQA\n(Khashabi et al., 2020) to initialize our models in the two\nstages because it achieves the best ne-tuning results in\nLu et al. (2022a). To verify the generality of our approach\nacross different LMs, we also employ FLAN-T5 (Chung\net al., 2022) as the backbone in Section 6.3. As using im-\nage captions does not yield signicant performance gains in\nSection 3.3, we did not use the captions. We ne-tune the\nmodels up to 20 epochs, with a learning rate of 5e-5. The', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}]: %s
2023-12-07 12:21:41,145 - INFO - Check the data that is being passed [{'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nA.2. Two-Stage Training Performance with Different Sizes of LMs.\nIn Section 3, we obverse that incorporating vision features helps generate more effective rationales, thus leading to improved\nanswer accuracy. Besides incorporating vision features, it is possible to scale the LM size to mitigate the issue of incorrect\nrationales. Figure 7 shows the answer accuracy with UniedQA Base and UniedQA Large . When using a larger LM, the\naccuracy of the baseline (w/o vision features) is boosted. The result indicates that scaling the LM is possible to mitigate the\nissue of incorrect rationales. However, the performance is still much inferior to using vision features. The result further\nveries the effectiveness of our Multimodal-CoT with different sizes of LMs.\nbase large6580100\n70.5384.9182.9791.68Accuracy (%)w/o Vision Modality w/ Vision Modality\nFigure 7. Answer accuracy with different sizes of LMs.\nB. Experimental Details\nB.1. Baseline Methods\nFollowing Lu et al. (2022a), our baselines include three types of methods:\n(i) Visual question answering (VQA) models (Yu et al., 2019; Anderson et al., 2018; Kim et al., 2018; Gao et al., 2019; Lu\net al., 2021; Li et al., 2019). The VQA baselines take the question, the context, and choices as the textual input, take the\nimage as the vision input, and predict the score distribution over choice candidates via a linear classier.\n(ii) Text-to-text LM models. UniedQA (Khashabi et al., 2020) is adopted as it is the best ne-tuning model in Lu et al.\n(2022a). UniedQA takes the textual information as the input and outputs the answer option. The image is converted into a\ncaption extracted by an image captioning model based on ViT and GPT-2.6UniedQA treats our task as a text generation\nproblem. In Lu et al. (2022a), it is trained to generate a target answer text, i.e., one of the candidate options. Then, the most\nsimilar option is selected as the nal prediction to evaluate the question answering accuracy.\n(iii) GPT-3.5 models (Chen et al., 2020) based on the text-davinci-002 engine. The inference is based on the few-shot\nprompting, where two in-context examples from the training set are concatenated before the test instance.\nFor UniedQA and GPT-3.5, CoT is applied after the answer (Lu et al., 2022a). Besides the above baselines, we develop a\nstronger baseline with a slight modication of the output format of UniedQA. Instead of predicting the answer texts, our\nbaseline directly predicts the choice, e.g., the answer is B . This setting helps our baseline achieve better results than the\nexisting UniedQA. Therefore, we use the stronger method as the language only baseline for analysis.\nB.2. Details of Vision Features\nIn Section 6.2, we compared four types of vision features, CLIP (Radford et al., 2021), DETR (Carion et al., 2020), and\nResNet (He et al., 2016). The specic models are: (i) CLIP: RN101;7(ii) DETR: detr resnet101 dc5;8(iii) ResNet: we use\n6https://huggingface.co/nlpconnect/vit-gpt2-image-captioning .\n7https://github.com/jianjieluo/OpenAI-CLIP-Feature .\n8https://github.com/facebookresearch/detr .', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nthe averaged pooled features of a pre-trained ResNet50 CNN. Table 9 presents the dimension of the vision features (after the\nfunction VisionExtractor( ) in Eq. 3). For ResNet-50, we repeat the pooled features of ResNet-50 to the same length as the\ntext sequence to imitate the patch-like features, where each patch is the same as the pooled image features.\nTable 9. Dimension of vision features\nMethod Dimension\nCLIP (49, 2048)\nDETR (100, 256)\nResNet (512, 2048)\nC. Examples of Case Studies\nTo better understand the behavior of Multimodal-CoT, we manually investigate randomly selected examples generated by\nour approach. Table 8 summarizes the categorization results generated by Multimodal-CoT. We randomly picked up 50\nsamples whose prediction results were correct and 50 samples whose prediction results were incorrect.\nWe nd that the correct samples contain a certain amount of incorrect chain-of-thought. As shown in Figure 8(b), the model\ngenerates the incorrect rationale,  Animals cannot their food by digesting other organisms  but the predicted answer is\ncorrect. The result indicates that CoT may not always benet the answer inference, and the model is robust to some extent \nit can predict the correct answer by ignoring incorrect rationales.\nFor incorrect samples, commonsense mistake is the most frequent error type. The model also makes commonsense mistakes\nwhen answering the questions requires commonsense knowledge, e.g., understand maps and counting numbers in the\nimages (Figure 9), and utilizing the alphabet (Figure 10). The other type of mistake is the logical mistake, where there are\ncontradictions in the reasoning chains (Figure 11). In addition, there are cases that the CoT is correct but might not be\nnecessarily related to answer options; thus the model chooses the incorrect answer.\nThe analysis indicates that there are prospective directions for future studies. On the one hand, it is possible to improve the\nquality of CoT by (i) using more ne-grained interaction of language and vision features; and (ii) injecting commonsense\nknowledge. On the other hand, applying a ltering mechanism to using only the effective CoT to infer the answer and\neliminate irrelevant CoT.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Birds,mammals,fish,reptiles,andamphibiansaregroupsofanimals.Theanimalsineachgrouphavetraitsincommon.Scientistssortanimalsintogroupsbasedontraitstheyhaveincommon.Thisprocessiscalledclassification.Atigersharkisafish.Itlivesunderwater.Ithasfins,notlimbs.Tigersharkscannocturnal.Theymeansthattheyareactivemostlyatnight.Ared-tailedhawkisabird.Ithasfeathers,twowings,andabeak.Red-tailedhawkslivetheirbeteethakstopushforpre.birds,and,repamptilesAnswer:Theansweris(B).Options:(A) red-tailed hawkProblem\nPredictionQuestion:Selectthebirdbelow.Context:Birdshavefeathers,twowings,andabeak.Aperegrinefalconisanexampleofabird.Rationale:Birds,mammals,fish,reptiles,andamphibiansaregroupsofanimals.Theanimalsineachgrouphavetraitsincommon.Scientistssortanimalsintogroupsbasedontraitstheyhaveincommon.Thisprocessiscalledclassification.Atigersharkisafish.Itlivesunderwater.Ithasfins,notlimbs.Tigersharkscannocturnal.Theymeansthattheyareactivemostlyatnight.Ared-tailedhawkisabird.Ithasfeathers,twowings,andabeak.Red-tailedhawkslivetheirbeteethakstopushforpre.birds,and,repamptilesAnswer:Theansweris(A).Vision(B) tiger shark\n(a)CoTiscorrect\nRationale:Leopardthepast,scientistsclassifiedlivingorganismsintotwogroups:plantsandanimals.Overthepast300years,scientistshavediscoveredmanymoretypesoforganisms.Today,manyscientistsclassifyorganismsintosixbroadgroups,calledkingdoms.Organismsineachkingdomhavespecifictraits.Thetablebelowshowssometraitsusedtodescribeeachkingdom.|Bacteria|Archaea|Protists|Fungi|Animals|Plants.Howmanycellsdotheyhave?|one|one|oneormany|oneormany|many|many.Dotheircellshaveanucleus?|no|no|yes|yes|yes|yesnCantheircellsmakefood?|somespeciescan|somespeciescan|somespeciescan|no|no|yesLeoparduswiediiisananimal.Animalcellscannotmaketheirownfood.Animalscannottheirfoodbydigestingotherorganisms.Answer:Theansweris(B).Options:(A) noProblem\nPredictionQuestion:CanLeoparduswiediicellsmaketheirownfood?Context:ThisorganismisLeoparduswiedii.Itisamemberoftheanimalkingdom.Leoparduswiediiiscommonlycalledamargay.MargaysliveintheforestinCentralandSouthAmerica.Margaysarenocturnal.Anocturnalanimalsleepsmostofthedayandisawakeatnight.Rationale:Inthepast,scientistsclassifiedlivingorganismsintotwogroups:plantsandanimals.Overthepast300years,scientistshavediscoveredmanymoretypesoforganisms.Today,manyscientistsclassifyorganismsintosixbroadgroups,calledkingdoms.Organismsineachkingdomhavespecifictraits.Thetablebelowshowssometraitsusedtodescribeeachkingdom.|Bacteria|Archaea|Protists|Fungi|Animals|Plants.Howmanycellsdotheyhave?|one|one|oneormany|oneormany|many|manyDotheircellshaveanucleus?|no|no|yes|yes|yes|yesCantheircellsmakefood?|somespeciescan|somespeciescan|somespeciescan|no|no|yesLeoparduswiediiisananimal.Animalcellscannotmaketheirownfood.Animalsgettheirfoodbydigestingotherorganisms.Answer:Theansweris(A).Vision(B) yes\n(b)CoTisincorrect\nFigure 8. Examples of answers are correct while the CoT is correct (a) or incorrect (b).', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Oceansarehugebodiesofsaltwater.Theworldhasfiveoceans.Alloftheoceansareconnected,makingoneworldocean.ThisistheIndianOcean.Answer:Theansweris(B).Options:(A) the Atlantic OceanProblem\nPredictionQuestion:Whichoceanishighlighted?Context:N/ARationale:Oceansarehugebodiesofsaltwater.Theworldhasfiveoceans.Alloftheoceansareconnected,makingoneworldocean.ThisisthePacificOcean.Answer:Theansweris(C).Vision(B) the Indian Ocean\n(a)Mistakesaboutmaps.\nRationale:Inasolution,soluteparticlesmoveandspreadthroughoutthesolvent.Thediagrambelowshowshowasolutioncanchangeovertime.SoluteparticlesmovefromtheareawheretheyareatahigherconcentrationtotheareawheretheyareatalowerconcentrationLookatthediagramagain.Itshowsyouhowthesolutionchangedduringtheprocessofdiffusion.Beforethesoluteparticlesreachedequilibrium,therewere5soluteparticlesontheleftsideofthemembraneand4soluteparticlesontherightsideofthemembrane.Whenthesoluteparticlesreachedequilibrium,therewere5soluteparticlesoneachsideofthemembrane.Therewas1moresoluteparticleontherightsideofthemembranethanbefore.So,forthesoluteparticlestoreachequilibrium,moresoluteparticlesmusthavemovedacrossthemembranetotherightthantotheright.Answer:Theansweris(B).Options:(A) to the left than to the rightProblem\nPredictionQuestion:Completethetexttodescribethediagram.Soluteparticlesmovedinbothdirectionsacrossthepermeablemembrane.Butmoresoluteparticlesmovedacrossthemembrane().Whentherewasanequalconcentrationonbothsides,theparticlesreachedequilibrium.Context:Thediagrambelowshowsasolutionwithonesolute.Eachsoluteparticleisrepresentedbyagreenball.Thesolutionfillsaclosedcontainerthatisdividedinhalfbyamembrane.Themembrane,representedbyadottedline,ispermeabletothesoluteparticles.Thediagramshowshowthesolutioncanchangeovertimeduringtheprocessofdiffusion.Rationale:Inasolution,soluteparticlesmoveandspreadthroughoutthesolvent.Thediagrambelowshowshowasolutioncanchangeovertime.SoluteparticlesmovefromtheareawheretheyareatahigherconcentrationtotheareawheretheyareatalowerconcentrationLookatthediagramagain.Itshowsyouhowthesolutionchangedduringtheprocessofdiffusion.Beforethesoluteparticlesreachedequilibrium,therewere3soluteparticlesontheleftsideofthemembraneand5soluteparticlesontherightsideofthemembrane.Whenthesoluteparticlesreachedequilibrium,therewere4soluteparticlesoneachsideofthemembrane.Therewas1moresoluteparticleontheleftsideofthemembranethanbefore.So,forthesoluteparticlestoreachequilibrium,moresoluteparticlesmusthavemovedacrossthemembranetotheleftthantotheright.Answer:Theansweris(A).Vision\n(b)Mistakesaboutcountingnumbersintheimage.(C) the Pacific Ocean(D) the Southern Ocean\n(B) to the right than to the left\nFigure 9. Examples of commonsense mistakes about understanding maps and counting numbers.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Guidewordsappearoneachpageofadictionary.Theytellyouthefirstwordandlastwordonthepage.Theotherwordsonthepagecomebetweentheguidewordsinalphabeticalorder.Toputwordsinalphabeticalorder,puttheminorderbytheirfirstletters.Ifthefirstlettersarethesame,lookatthesecondletters.Ifthesecondlettersarethesame,lookatthethirdletters,andsoon.Ifonewordisshorter,andtherearenomoreletterstocompare,thentheshorterwordcomesfirstinalphabeticalorder.Forexample,becomesbeforebed.Putthewordsinalphabeticalorder.Sincehardshipisbetweentheguidewordshelping-hunter,itwouldbefoundonthatpage.Answer:Theansweris(B).Options:(A) hospitalProblem\nPredictionQuestion:Whichwordwouldyoufindonadictionarypagewiththefollowingguidewords?helping-hunterContext:Thediagrambelowshowsasolutionwithonesolute.Eachsoluteparticleisrepresentedbyagreenball.Thesolutionfillsaclosedcontainerthatisdividedinhalfbyamembrane.Themembrane,representedbyadottedline,ispermeabletothesoluteparticles.Thediagramshowshowthesolutioncanchangeovertimeduringtheprocessofdiffusion.Rationale:Guidewordsappearoneachpageofadictionary.Theytellyouthefirstwordandlastwordonthepage.Theotherwordsonthepagecomebetweentheguidewordsinalphabeticalorder.Toputwordsinalphabeticalorder,puttheminorderbytheirfirstletters.Ifthefirstlettersarethesame,lookatthesecondletters.Ifthesecondlettersarethesame,lookatthethirdletters,andsoon.Ifonewordisshorter,andtherearenomoreletterstocompare,thentheshorterwordcomesfirstinalphabeticalorder.Forexample,becomesbeforebed.Putthewordsinalphabeticalorder.Sincehospitalisbetweentheguidewordshelping-hunter,itwouldbefoundonthatpage.Answer:Theansweris(A).(B) hardshipVisionN/A\nFigure 10. Examples of commonsense mistakes about utilizing alphabet.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': "Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Measurementsarewrittenwithbothanumberandaunit.Theunitcomesafterthenumber.Theunitshowswhatthenumbermeans.TemperaturecanbewrittenwithunitsofdegreesFahrenheit(F)orCelsius(C).Usethelistbelowtocomparethetwounits.212F|Waterboils|100C98.6F|Bodytemperature|37C68F|Roomtemperature|20C32F|Waterfreezes|0C.Thebetterestimateforthetemperatureoftheaironahotdayis36F.36Fistoohot.Answer:Theansweris(A).Options:(A) 36FProblem\nPredictionQuestion:Whatisthetemperatureoftheaironahotday?Context:Selectthebetterestimate.Rationale:Measurementsarewrittenwithbothanumberandaunit.Theunitcomesafterthenumber.Theunitshowswhatthenumbermeans.TemperaturecanbewrittenwithunitsofdegreesFahrenheit(F)orCelsius(C).Usethelistbelowtocomparethetwounits.212F|Waterboils|100C98.6F|Bodytemperature|37C68F|Roomtemperature|20C32F|Waterfreezes|0C.Thebetterestimateforthetemperatureoftheaironahotdayis36C.36Fistoocold.Answer:Theansweris(B).(B) 36CVisionN/A\nRationale::Asentenceisagroupofwordsthatexpressesacompletethought.ThebandI'minhasbeenrehearsingdailybecausewehaveaconcertintwoweeks.Arun-onsentenceisformedwhentwosentencesareruntogether,joinedbyjustacommaorbynopunctuationatall.Ifonlyacommaisused,therun-oniscalledacommaspliceThebandI'minhasbeenrehearsingdailysincewehaveaconcertintwoweeks.Thisisnotarun-onsentence.Itisformedacommaspliceformedfromtwosentencesruntogether,joinedwithoutjustacomma.Right-handednessiscontrolledbygenetics,interestinglyenough,thesamegenesarealsoresponsibleforthebrainbecomingmorespecializedatcertaintasks.Answer:Theansweris(B).Options:(A) yesProblem\nPredictionQuestion:Isthisarun-onsentence?Right-handednessiscontrolledbygenetics,interestinglyenough,thesamegenesarealsoresponsibleforthebrainbecomingmorespecializedatcertaintasks.Context:N/ARationale:Asentenceisagroupofwordsthatexpressesacompletethought.ThebandI'minhasbeenrehearsingdailybecausewehaveaconcertintwoweeks.Arun-onsentenceisformedwhentwosentencesareruntogether,joinedbyjustacommaorbynopunctuationatall.Ifonlyacommaisused,therun-oniscalledacommaspliceThebandI'minhasbeenrehearsingdailysincewehaveaconcertintwoweeks.Thisisarun-onsentence.Itisacommaspliceformedfromtwosentencesruntogether,joinedbyjustacomma.Right-handednessiscontrolledbygenetics,interestinglyenough,thesamegenesarealsoresponsibleforthebrainbecomingmorespecializedatcertaintasks.Answer:Theansweris(A).(B) noVisionN/A(a)Logicalmistakewherethemodelfailsatcomparisons.\n(b)Logicalmistakewherethethereisacontradictionintherationale.\nFigure 11. Examples of logical mistakes.", 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': "Multimodal Chain-of-Thought Reasoning in Language Models\nOptions:(A) black stripes on its skinProblem\nPredictionRationale:Thewayanorganismlooksoractsiscalledatrait.Scientistsusefossilstolearnmoreaboutthetraitsofancientorganisms.Fossilscanpreservetheremainsofbodypartsandactivities.Afossilofabodypart,suchasatailorawing,cantellyouwhatanorganismlookedlike.Afossilofanorganism'sactivities,suchasaburroworafootprint,cantellyouabouttheorganism'sbehavior.Herearethreeexamplesoffossilsandthetraitsthatyoucanobservefromthem:Thisisafossilofananimal.Thisfossiltellsyouthattheanimalhadaspiral-shapedshell.Thisisafossilofaplant.Thisfossiltellsyouthattheplanthadsmallleavesarrangedinabranchedpattern.Thisisafossilofananimal'sfootprint.Thisfossiltellsyouthattheanimalcouldwalkonland.Anorganism'sfossilmaynotshowalloftheorganism'straits.Thisisbecausemostbodypartsaredestroyedduringfossilformation.Whenanorganism'sbodyturnsintoafossil,onlyafewbodypartsareusuallypreserved.Answer:Theansweris(C).(B) large fins on its bodyVision(C) a long, thin bodyQuestion:WhichtraitdidPalaeopythonhave?Selectthetraityoucanobserveonthefossil.Context:ThispictureshowsafossilofanancientanimalcalledPalaeopython.Palaeopythonlivedintreesandcouldgrowmorethansixfeetlong.\nRationale:Thewayanorganismlooksoractsiscalledatrait.Scientistsusefossilstolearnmoreaboutthetraitsofancientorganisms.Fossilscanpreservetheremainsofbodypartsandactivities.Afossilofabodypart,suchasatailorawing,cantellyouwhatanorganismlookedlike.Afossilofanorganism'sactivities,suchasaburroworafootprint,cantellyouabouttheorganism'sbehavior.Herearethreeexamplesoffossilsandthetraitsthatyoucanobservefromthem:Thisisafossilofananimal.Thisfossiltellsyouthattheanimalhadaspiral-shapedshell.Thisisafossilofaplant.Thisfossiltellsyouthattheplanthadsmallleavesarrangedinabranchedpattern.Thisisafossilofananimal'sfootprint.Thisfossiltellsyouthattheanimalcouldwalkonland.Anorganism'sfossilmaynotshowalloftheorganism'straits.Thisisbecausemostbodypartsaredestroyedduringfossilformation.Whenanorganism'sbodyturnsintoafossil,onlyafewbodypartsareusuallypreserved.Answer:Theansweris(B).\nFigure 12. Examples of answers are incorrect while the CoT is correct.", 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}]: %s
2023-12-07 12:21:41,145 - INFO - Check the results [{'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nA.2. Two-Stage Training Performance with Different Sizes of LMs.\nIn Section 3, we obverse that incorporating vision features helps generate more effective rationales, thus leading to improved\nanswer accuracy. Besides incorporating vision features, it is possible to scale the LM size to mitigate the issue of incorrect\nrationales. Figure 7 shows the answer accuracy with UniedQA Base and UniedQA Large . When using a larger LM, the\naccuracy of the baseline (w/o vision features) is boosted. The result indicates that scaling the LM is possible to mitigate the\nissue of incorrect rationales. However, the performance is still much inferior to using vision features. The result further\nveries the effectiveness of our Multimodal-CoT with different sizes of LMs.\nbase large6580100\n70.5384.9182.9791.68Accuracy (%)w/o Vision Modality w/ Vision Modality\nFigure 7. Answer accuracy with different sizes of LMs.\nB. Experimental Details\nB.1. Baseline Methods\nFollowing Lu et al. (2022a), our baselines include three types of methods:\n(i) Visual question answering (VQA) models (Yu et al., 2019; Anderson et al., 2018; Kim et al., 2018; Gao et al., 2019; Lu\net al., 2021; Li et al., 2019). The VQA baselines take the question, the context, and choices as the textual input, take the\nimage as the vision input, and predict the score distribution over choice candidates via a linear classier.\n(ii) Text-to-text LM models. UniedQA (Khashabi et al., 2020) is adopted as it is the best ne-tuning model in Lu et al.\n(2022a). UniedQA takes the textual information as the input and outputs the answer option. The image is converted into a\ncaption extracted by an image captioning model based on ViT and GPT-2.6UniedQA treats our task as a text generation\nproblem. In Lu et al. (2022a), it is trained to generate a target answer text, i.e., one of the candidate options. Then, the most\nsimilar option is selected as the nal prediction to evaluate the question answering accuracy.\n(iii) GPT-3.5 models (Chen et al., 2020) based on the text-davinci-002 engine. The inference is based on the few-shot\nprompting, where two in-context examples from the training set are concatenated before the test instance.\nFor UniedQA and GPT-3.5, CoT is applied after the answer (Lu et al., 2022a). Besides the above baselines, we develop a\nstronger baseline with a slight modication of the output format of UniedQA. Instead of predicting the answer texts, our\nbaseline directly predicts the choice, e.g., the answer is B . This setting helps our baseline achieve better results than the\nexisting UniedQA. Therefore, we use the stronger method as the language only baseline for analysis.\nB.2. Details of Vision Features\nIn Section 6.2, we compared four types of vision features, CLIP (Radford et al., 2021), DETR (Carion et al., 2020), and\nResNet (He et al., 2016). The specic models are: (i) CLIP: RN101;7(ii) DETR: detr resnet101 dc5;8(iii) ResNet: we use\n6https://huggingface.co/nlpconnect/vit-gpt2-image-captioning .\n7https://github.com/jianjieluo/OpenAI-CLIP-Feature .\n8https://github.com/facebookresearch/detr .', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nthe averaged pooled features of a pre-trained ResNet50 CNN. Table 9 presents the dimension of the vision features (after the\nfunction VisionExtractor( ) in Eq. 3). For ResNet-50, we repeat the pooled features of ResNet-50 to the same length as the\ntext sequence to imitate the patch-like features, where each patch is the same as the pooled image features.\nTable 9. Dimension of vision features\nMethod Dimension\nCLIP (49, 2048)\nDETR (100, 256)\nResNet (512, 2048)\nC. Examples of Case Studies\nTo better understand the behavior of Multimodal-CoT, we manually investigate randomly selected examples generated by\nour approach. Table 8 summarizes the categorization results generated by Multimodal-CoT. We randomly picked up 50\nsamples whose prediction results were correct and 50 samples whose prediction results were incorrect.\nWe nd that the correct samples contain a certain amount of incorrect chain-of-thought. As shown in Figure 8(b), the model\ngenerates the incorrect rationale,  Animals cannot their food by digesting other organisms  but the predicted answer is\ncorrect. The result indicates that CoT may not always benet the answer inference, and the model is robust to some extent \nit can predict the correct answer by ignoring incorrect rationales.\nFor incorrect samples, commonsense mistake is the most frequent error type. The model also makes commonsense mistakes\nwhen answering the questions requires commonsense knowledge, e.g., understand maps and counting numbers in the\nimages (Figure 9), and utilizing the alphabet (Figure 10). The other type of mistake is the logical mistake, where there are\ncontradictions in the reasoning chains (Figure 11). In addition, there are cases that the CoT is correct but might not be\nnecessarily related to answer options; thus the model chooses the incorrect answer.\nThe analysis indicates that there are prospective directions for future studies. On the one hand, it is possible to improve the\nquality of CoT by (i) using more ne-grained interaction of language and vision features; and (ii) injecting commonsense\nknowledge. On the other hand, applying a ltering mechanism to using only the effective CoT to infer the answer and\neliminate irrelevant CoT.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Birds,mammals,fish,reptiles,andamphibiansaregroupsofanimals.Theanimalsineachgrouphavetraitsincommon.Scientistssortanimalsintogroupsbasedontraitstheyhaveincommon.Thisprocessiscalledclassification.Atigersharkisafish.Itlivesunderwater.Ithasfins,notlimbs.Tigersharkscannocturnal.Theymeansthattheyareactivemostlyatnight.Ared-tailedhawkisabird.Ithasfeathers,twowings,andabeak.Red-tailedhawkslivetheirbeteethakstopushforpre.birds,and,repamptilesAnswer:Theansweris(B).Options:(A) red-tailed hawkProblem\nPredictionQuestion:Selectthebirdbelow.Context:Birdshavefeathers,twowings,andabeak.Aperegrinefalconisanexampleofabird.Rationale:Birds,mammals,fish,reptiles,andamphibiansaregroupsofanimals.Theanimalsineachgrouphavetraitsincommon.Scientistssortanimalsintogroupsbasedontraitstheyhaveincommon.Thisprocessiscalledclassification.Atigersharkisafish.Itlivesunderwater.Ithasfins,notlimbs.Tigersharkscannocturnal.Theymeansthattheyareactivemostlyatnight.Ared-tailedhawkisabird.Ithasfeathers,twowings,andabeak.Red-tailedhawkslivetheirbeteethakstopushforpre.birds,and,repamptilesAnswer:Theansweris(A).Vision(B) tiger shark\n(a)CoTiscorrect\nRationale:Leopardthepast,scientistsclassifiedlivingorganismsintotwogroups:plantsandanimals.Overthepast300years,scientistshavediscoveredmanymoretypesoforganisms.Today,manyscientistsclassifyorganismsintosixbroadgroups,calledkingdoms.Organismsineachkingdomhavespecifictraits.Thetablebelowshowssometraitsusedtodescribeeachkingdom.|Bacteria|Archaea|Protists|Fungi|Animals|Plants.Howmanycellsdotheyhave?|one|one|oneormany|oneormany|many|many.Dotheircellshaveanucleus?|no|no|yes|yes|yes|yesnCantheircellsmakefood?|somespeciescan|somespeciescan|somespeciescan|no|no|yesLeoparduswiediiisananimal.Animalcellscannotmaketheirownfood.Animalscannottheirfoodbydigestingotherorganisms.Answer:Theansweris(B).Options:(A) noProblem\nPredictionQuestion:CanLeoparduswiediicellsmaketheirownfood?Context:ThisorganismisLeoparduswiedii.Itisamemberoftheanimalkingdom.Leoparduswiediiiscommonlycalledamargay.MargaysliveintheforestinCentralandSouthAmerica.Margaysarenocturnal.Anocturnalanimalsleepsmostofthedayandisawakeatnight.Rationale:Inthepast,scientistsclassifiedlivingorganismsintotwogroups:plantsandanimals.Overthepast300years,scientistshavediscoveredmanymoretypesoforganisms.Today,manyscientistsclassifyorganismsintosixbroadgroups,calledkingdoms.Organismsineachkingdomhavespecifictraits.Thetablebelowshowssometraitsusedtodescribeeachkingdom.|Bacteria|Archaea|Protists|Fungi|Animals|Plants.Howmanycellsdotheyhave?|one|one|oneormany|oneormany|many|manyDotheircellshaveanucleus?|no|no|yes|yes|yes|yesCantheircellsmakefood?|somespeciescan|somespeciescan|somespeciescan|no|no|yesLeoparduswiediiisananimal.Animalcellscannotmaketheirownfood.Animalsgettheirfoodbydigestingotherorganisms.Answer:Theansweris(A).Vision(B) yes\n(b)CoTisincorrect\nFigure 8. Examples of answers are correct while the CoT is correct (a) or incorrect (b).', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Oceansarehugebodiesofsaltwater.Theworldhasfiveoceans.Alloftheoceansareconnected,makingoneworldocean.ThisistheIndianOcean.Answer:Theansweris(B).Options:(A) the Atlantic OceanProblem\nPredictionQuestion:Whichoceanishighlighted?Context:N/ARationale:Oceansarehugebodiesofsaltwater.Theworldhasfiveoceans.Alloftheoceansareconnected,makingoneworldocean.ThisisthePacificOcean.Answer:Theansweris(C).Vision(B) the Indian Ocean\n(a)Mistakesaboutmaps.\nRationale:Inasolution,soluteparticlesmoveandspreadthroughoutthesolvent.Thediagrambelowshowshowasolutioncanchangeovertime.SoluteparticlesmovefromtheareawheretheyareatahigherconcentrationtotheareawheretheyareatalowerconcentrationLookatthediagramagain.Itshowsyouhowthesolutionchangedduringtheprocessofdiffusion.Beforethesoluteparticlesreachedequilibrium,therewere5soluteparticlesontheleftsideofthemembraneand4soluteparticlesontherightsideofthemembrane.Whenthesoluteparticlesreachedequilibrium,therewere5soluteparticlesoneachsideofthemembrane.Therewas1moresoluteparticleontherightsideofthemembranethanbefore.So,forthesoluteparticlestoreachequilibrium,moresoluteparticlesmusthavemovedacrossthemembranetotherightthantotheright.Answer:Theansweris(B).Options:(A) to the left than to the rightProblem\nPredictionQuestion:Completethetexttodescribethediagram.Soluteparticlesmovedinbothdirectionsacrossthepermeablemembrane.Butmoresoluteparticlesmovedacrossthemembrane().Whentherewasanequalconcentrationonbothsides,theparticlesreachedequilibrium.Context:Thediagrambelowshowsasolutionwithonesolute.Eachsoluteparticleisrepresentedbyagreenball.Thesolutionfillsaclosedcontainerthatisdividedinhalfbyamembrane.Themembrane,representedbyadottedline,ispermeabletothesoluteparticles.Thediagramshowshowthesolutioncanchangeovertimeduringtheprocessofdiffusion.Rationale:Inasolution,soluteparticlesmoveandspreadthroughoutthesolvent.Thediagrambelowshowshowasolutioncanchangeovertime.SoluteparticlesmovefromtheareawheretheyareatahigherconcentrationtotheareawheretheyareatalowerconcentrationLookatthediagramagain.Itshowsyouhowthesolutionchangedduringtheprocessofdiffusion.Beforethesoluteparticlesreachedequilibrium,therewere3soluteparticlesontheleftsideofthemembraneand5soluteparticlesontherightsideofthemembrane.Whenthesoluteparticlesreachedequilibrium,therewere4soluteparticlesoneachsideofthemembrane.Therewas1moresoluteparticleontheleftsideofthemembranethanbefore.So,forthesoluteparticlestoreachequilibrium,moresoluteparticlesmusthavemovedacrossthemembranetotheleftthantotheright.Answer:Theansweris(A).Vision\n(b)Mistakesaboutcountingnumbersintheimage.(C) the Pacific Ocean(D) the Southern Ocean\n(B) to the right than to the left\nFigure 9. Examples of commonsense mistakes about understanding maps and counting numbers.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Guidewordsappearoneachpageofadictionary.Theytellyouthefirstwordandlastwordonthepage.Theotherwordsonthepagecomebetweentheguidewordsinalphabeticalorder.Toputwordsinalphabeticalorder,puttheminorderbytheirfirstletters.Ifthefirstlettersarethesame,lookatthesecondletters.Ifthesecondlettersarethesame,lookatthethirdletters,andsoon.Ifonewordisshorter,andtherearenomoreletterstocompare,thentheshorterwordcomesfirstinalphabeticalorder.Forexample,becomesbeforebed.Putthewordsinalphabeticalorder.Sincehardshipisbetweentheguidewordshelping-hunter,itwouldbefoundonthatpage.Answer:Theansweris(B).Options:(A) hospitalProblem\nPredictionQuestion:Whichwordwouldyoufindonadictionarypagewiththefollowingguidewords?helping-hunterContext:Thediagrambelowshowsasolutionwithonesolute.Eachsoluteparticleisrepresentedbyagreenball.Thesolutionfillsaclosedcontainerthatisdividedinhalfbyamembrane.Themembrane,representedbyadottedline,ispermeabletothesoluteparticles.Thediagramshowshowthesolutioncanchangeovertimeduringtheprocessofdiffusion.Rationale:Guidewordsappearoneachpageofadictionary.Theytellyouthefirstwordandlastwordonthepage.Theotherwordsonthepagecomebetweentheguidewordsinalphabeticalorder.Toputwordsinalphabeticalorder,puttheminorderbytheirfirstletters.Ifthefirstlettersarethesame,lookatthesecondletters.Ifthesecondlettersarethesame,lookatthethirdletters,andsoon.Ifonewordisshorter,andtherearenomoreletterstocompare,thentheshorterwordcomesfirstinalphabeticalorder.Forexample,becomesbeforebed.Putthewordsinalphabeticalorder.Sincehospitalisbetweentheguidewordshelping-hunter,itwouldbefoundonthatpage.Answer:Theansweris(A).(B) hardshipVisionN/A\nFigure 10. Examples of commonsense mistakes about utilizing alphabet.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': "Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Measurementsarewrittenwithbothanumberandaunit.Theunitcomesafterthenumber.Theunitshowswhatthenumbermeans.TemperaturecanbewrittenwithunitsofdegreesFahrenheit(F)orCelsius(C).Usethelistbelowtocomparethetwounits.212F|Waterboils|100C98.6F|Bodytemperature|37C68F|Roomtemperature|20C32F|Waterfreezes|0C.Thebetterestimateforthetemperatureoftheaironahotdayis36F.36Fistoohot.Answer:Theansweris(A).Options:(A) 36FProblem\nPredictionQuestion:Whatisthetemperatureoftheaironahotday?Context:Selectthebetterestimate.Rationale:Measurementsarewrittenwithbothanumberandaunit.Theunitcomesafterthenumber.Theunitshowswhatthenumbermeans.TemperaturecanbewrittenwithunitsofdegreesFahrenheit(F)orCelsius(C).Usethelistbelowtocomparethetwounits.212F|Waterboils|100C98.6F|Bodytemperature|37C68F|Roomtemperature|20C32F|Waterfreezes|0C.Thebetterestimateforthetemperatureoftheaironahotdayis36C.36Fistoocold.Answer:Theansweris(B).(B) 36CVisionN/A\nRationale::Asentenceisagroupofwordsthatexpressesacompletethought.ThebandI'minhasbeenrehearsingdailybecausewehaveaconcertintwoweeks.Arun-onsentenceisformedwhentwosentencesareruntogether,joinedbyjustacommaorbynopunctuationatall.Ifonlyacommaisused,therun-oniscalledacommaspliceThebandI'minhasbeenrehearsingdailysincewehaveaconcertintwoweeks.Thisisnotarun-onsentence.Itisformedacommaspliceformedfromtwosentencesruntogether,joinedwithoutjustacomma.Right-handednessiscontrolledbygenetics,interestinglyenough,thesamegenesarealsoresponsibleforthebrainbecomingmorespecializedatcertaintasks.Answer:Theansweris(B).Options:(A) yesProblem\nPredictionQuestion:Isthisarun-onsentence?Right-handednessiscontrolledbygenetics,interestinglyenough,thesamegenesarealsoresponsibleforthebrainbecomingmorespecializedatcertaintasks.Context:N/ARationale:Asentenceisagroupofwordsthatexpressesacompletethought.ThebandI'minhasbeenrehearsingdailybecausewehaveaconcertintwoweeks.Arun-onsentenceisformedwhentwosentencesareruntogether,joinedbyjustacommaorbynopunctuationatall.Ifonlyacommaisused,therun-oniscalledacommaspliceThebandI'minhasbeenrehearsingdailysincewehaveaconcertintwoweeks.Thisisarun-onsentence.Itisacommaspliceformedfromtwosentencesruntogether,joinedbyjustacomma.Right-handednessiscontrolledbygenetics,interestinglyenough,thesamegenesarealsoresponsibleforthebrainbecomingmorespecializedatcertaintasks.Answer:Theansweris(A).(B) noVisionN/A(a)Logicalmistakewherethemodelfailsatcomparisons.\n(b)Logicalmistakewherethethereisacontradictionintherationale.\nFigure 11. Examples of logical mistakes.", 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': "Multimodal Chain-of-Thought Reasoning in Language Models\nOptions:(A) black stripes on its skinProblem\nPredictionRationale:Thewayanorganismlooksoractsiscalledatrait.Scientistsusefossilstolearnmoreaboutthetraitsofancientorganisms.Fossilscanpreservetheremainsofbodypartsandactivities.Afossilofabodypart,suchasatailorawing,cantellyouwhatanorganismlookedlike.Afossilofanorganism'sactivities,suchasaburroworafootprint,cantellyouabouttheorganism'sbehavior.Herearethreeexamplesoffossilsandthetraitsthatyoucanobservefromthem:Thisisafossilofananimal.Thisfossiltellsyouthattheanimalhadaspiral-shapedshell.Thisisafossilofaplant.Thisfossiltellsyouthattheplanthadsmallleavesarrangedinabranchedpattern.Thisisafossilofananimal'sfootprint.Thisfossiltellsyouthattheanimalcouldwalkonland.Anorganism'sfossilmaynotshowalloftheorganism'straits.Thisisbecausemostbodypartsaredestroyedduringfossilformation.Whenanorganism'sbodyturnsintoafossil,onlyafewbodypartsareusuallypreserved.Answer:Theansweris(C).(B) large fins on its bodyVision(C) a long, thin bodyQuestion:WhichtraitdidPalaeopythonhave?Selectthetraityoucanobserveonthefossil.Context:ThispictureshowsafossilofanancientanimalcalledPalaeopython.Palaeopythonlivedintreesandcouldgrowmorethansixfeetlong.\nRationale:Thewayanorganismlooksoractsiscalledatrait.Scientistsusefossilstolearnmoreaboutthetraitsofancientorganisms.Fossilscanpreservetheremainsofbodypartsandactivities.Afossilofabodypart,suchasatailorawing,cantellyouwhatanorganismlookedlike.Afossilofanorganism'sactivities,suchasaburroworafootprint,cantellyouabouttheorganism'sbehavior.Herearethreeexamplesoffossilsandthetraitsthatyoucanobservefromthem:Thisisafossilofananimal.Thisfossiltellsyouthattheanimalhadaspiral-shapedshell.Thisisafossilofaplant.Thisfossiltellsyouthattheplanthadsmallleavesarrangedinabranchedpattern.Thisisafossilofananimal'sfootprint.Thisfossiltellsyouthattheanimalcouldwalkonland.Anorganism'sfossilmaynotshowalloftheorganism'straits.Thisisbecausemostbodypartsaredestroyedduringfossilformation.Whenanorganism'sbodyturnsintoafossil,onlyafewbodypartsareusuallypreserved.Answer:Theansweris(B).\nFigure 12. Examples of answers are incorrect while the CoT is correct.", 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}]: %s
2023-12-07 12:23:43,332 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:23:45,731 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:23:54,039 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:23:54,141 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:23:54,217 - INFO - query success: 1 documents found
2023-12-07 12:23:59,543 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:24:02,303 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:24:02,443 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:24:05,162 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:24:05,313 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:24:08,344 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:24:08,468 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:24:24,259 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:24:24,398 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:24:24,453 - INFO - Received requests to /inference endpoint
2023-12-07 12:24:24,554 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 12:24:24,555 - INFO - Received request: {'username': 'amin', 'prompt': 'provide me with a summary of multimodal paper.', 'memory': True, 'conversation_number': 2, 'AI_assistance': False, 'collection_name': 'docs', 'llm_model': 'Llama_13b'}
2023-12-07 12:25:07,137 - INFO - Processed the request successfully
2023-12-07 12:26:17,461 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:26:17,619 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:26:17,646 - INFO - Received requests to /inference endpoint
2023-12-07 12:26:17,747 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 12:26:17,748 - INFO - Received request: {'username': 'amin', 'prompt': 'provide me with a summary of multimodal paper.', 'memory': True, 'conversation_number': 2, 'AI_assistance': False, 'collection_name': 'docs', 'llm_model': 'Llama_13b'}
2023-12-07 12:26:56,719 - INFO - Processed the request successfully
2023-12-07 12:27:08,893 - INFO - Received requests to /inference endpoint
2023-12-07 12:27:08,994 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 12:27:08,995 - INFO - Received request: {'username': 'amin', 'prompt': 'Give me a summary of the paper', 'memory': False, 'conversation_number': 1, 'AI_assistance': False, 'collection_name': 'docs', 'llm_model': 'Llama_13b'}
2023-12-07 12:27:15,031 - INFO - Processed the request successfully
2023-12-07 12:33:45,181 - INFO - Created a temporary directory at /tmp/tmpqpn275fr
2023-12-07 12:33:45,181 - INFO - Writing /tmp/tmpqpn275fr/_remote_module_non_scriptable.py
2023-12-07 12:33:46,997 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-07 12:33:51,307 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-07 12:34:03,515 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:34:24,108 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:34:24,230 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:34:26,064 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:34:26,175 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:34:28,947 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:34:29,055 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:34:36,639 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:34:36,733 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:34:36,773 - INFO - Received requests to /inference endpoint
2023-12-07 12:34:36,874 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 12:34:36,874 - INFO - Received request: {'username': 'amin', 'prompt': 'provide me with a summary of multimodal paper.', 'memory': True, 'conversation_number': 2, 'AI_assistance': False, 'collection_name': 'docs', 'llm_model': 'Llama_13b'}
2023-12-07 12:34:36,888 - INFO - Collection name: Amin_docs
2023-12-07 12:35:19,401 - INFO - Processed the request successfully
2023-12-07 12:35:38,104 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:35:40,740 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:35:40,816 - INFO - Received requests to /inference endpoint
2023-12-07 12:35:40,917 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 12:35:40,917 - INFO - Received request: {'username': 'amin', 'prompt': 'hi', 'memory': True, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-12-07 12:35:51,777 - INFO - Processed the request successfully
2023-12-07 12:36:34,921 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:36:34,995 - INFO - Received requests to /inference endpoint
2023-12-07 12:36:35,096 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 12:36:35,097 - INFO - Received request: {'username': 'amin', 'prompt': 'what is the title of the paper?', 'memory': True, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-12-07 12:36:37,889 - INFO - Processed the request successfully
2023-12-07 12:43:32,375 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:43:32,538 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:43:51,590 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:43:51,691 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:43:51,731 - INFO - Received requests to /inference endpoint
2023-12-07 12:43:51,832 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 12:43:51,832 - INFO - Received request: {'username': 'amin', 'prompt': 'what is the novility of the approach in the paper?', 'memory': True, 'conversation_number': 2, 'AI_assistance': False, 'collection_name': 'paper', 'llm_model': 'Llama_13b'}
2023-12-07 12:43:51,837 - INFO - Collection name: Amin_paper
2023-12-07 12:44:31,344 - INFO - Processed the request successfully
2023-12-07 12:44:39,443 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:44:49,068 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:44:49,146 - INFO - Received requests to /inference endpoint
2023-12-07 12:44:49,247 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 12:44:49,247 - INFO - Received request: {'username': 'amin', 'prompt': 'what is the novility of the approach in the paper?', 'memory': True, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-12-07 12:44:51,154 - INFO - Processed the request successfully
2023-12-07 12:46:31,362 - INFO - Created a temporary directory at /tmp/tmp1i5w0ujd
2023-12-07 12:46:31,362 - INFO - Writing /tmp/tmp1i5w0ujd/_remote_module_non_scriptable.py
2023-12-07 12:46:33,205 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-07 12:46:37,572 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-07 12:46:45,418 - INFO - classes: ['pdf', 'ssd', 'videos', 'General_collection']: %s
2023-12-07 12:46:48,614 - INFO - classes: ['pdf', 'ssd', 'videos', 'General_collection']: %s
2023-12-07 12:46:51,737 - INFO - classes: ['pdf', 'ssd', 'videos', 'General_collection']: %s
2023-12-07 12:46:51,938 - INFO - classes: ['pdf', 'ssd', 'videos', 'General_collection']: %s
2023-12-07 12:46:53,068 - INFO - classes: ['pdf', 'ssd', 'videos', 'General_collection']: %s
2023-12-07 12:46:53,246 - INFO - classes: ['pdf', 'ssd', 'videos', 'General_collection']: %s
2023-12-07 12:46:55,235 - INFO - classes: ['pdf', 'ssd', 'videos', 'General_collection']: %s
2023-12-07 12:46:55,623 - INFO - classes: ['pdf', 'ssd', 'videos', 'General_collection']: %s
2023-12-07 12:46:58,855 - INFO - classes: ['pdf', 'ssd', 'videos', 'General_collection']: %s
2023-12-07 12:47:11,507 - INFO - classes: ['class_2']: %s
2023-12-07 12:47:14,250 - INFO - classes: ['class_2']: %s
2023-12-07 12:47:34,007 - INFO - classes: ['class_2']: %s
2023-12-07 12:47:35,895 - INFO - checking the request/ username='test' class_name='docs' mode='create_collection' vectorDB_type='Weaviate' file_path=None: %s
2023-12-07 12:47:35,957 - INFO - checkpoint 1
2023-12-07 12:47:35,958 - INFO - checkpoint 2 test: %s
2023-12-07 12:47:35,958 - INFO - checkpoint 2 test_docs: %s
2023-12-07 12:47:35,985 - INFO - class name added successfully to database
2023-12-07 12:47:35,985 - INFO - success: class docs created for user test
2023-12-07 12:47:36,065 - INFO - classes: ['class_2', 'docs']: %s
2023-12-07 12:47:41,781 - INFO - classes: ['class_2', 'docs']: %s
2023-12-07 12:47:41,879 - INFO - classes: ['class_2', 'docs']: %s
2023-12-07 12:48:09,754 - INFO - classes: ['class_2', 'docs']: %s
2023-12-07 12:48:21,367 - INFO - classes: ['class_2', 'docs']: %s
2023-12-07 12:48:21,495 - INFO - query success: 0 documents found
2023-12-07 12:48:26,748 - INFO - classes: ['class_2', 'docs']: %s
2023-12-07 12:48:27,715 - INFO - actors creation successful [Actor(WeaviateEmbedder, 9fa996144965410645206b7c01000000), Actor(WeaviateEmbedder, 6a8fffdd74902b24f7ce6db301000000), Actor(WeaviateEmbedder, b2cfbca8a090957dbab2a1e601000000)]: %s
2023-12-07 12:48:27,715 - INFO - check 1st step of ray was successful
2023-12-07 12:48:27,716 - INFO - check if ray was successful:
2023-12-07 12:48:27,716 - INFO - response: {'status': 'success', 'message': 'Processed 19 documents in batches for class test_docs.'}: %s
2023-12-07 12:48:27,716 - INFO - request processed successfully username='test' class_name='docs' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/8178800152aa96b3': %s
2023-12-07 12:48:29,767 - INFO - classes: ['class_2', 'docs']: %s
2023-12-07 12:48:29,867 - INFO - query success: 1 documents found
2023-12-07 12:48:38,141 - INFO - classes: ['class_2', 'docs']: %s
2023-12-07 12:48:40,835 - INFO - classes: ['class_2', 'docs']: %s
2023-12-07 12:48:40,948 - INFO - classes: ['class_2', 'docs']: %s
2023-12-07 12:48:43,251 - INFO - classes: ['class_2', 'docs']: %s
2023-12-07 12:48:43,400 - INFO - classes: ['class_2', 'docs']: %s
2023-12-07 12:48:45,353 - INFO - classes: ['class_2', 'docs']: %s
2023-12-07 12:48:45,480 - INFO - classes: ['class_2', 'docs']: %s
2023-12-07 12:49:01,807 - INFO - classes: ['class_2', 'docs']: %s
2023-12-07 12:49:01,936 - INFO - classes: ['class_2', 'docs']: %s
2023-12-07 12:49:01,975 - INFO - Received requests to /inference endpoint
2023-12-07 12:49:02,076 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 12:49:02,077 - INFO - Received request: {'username': 'test', 'prompt': 'provide me with a summary of multimodal paper. ', 'memory': True, 'conversation_number': 1, 'AI_assistance': False, 'collection_name': 'docs', 'llm_model': 'Llama_13b'}
2023-12-07 12:49:02,090 - INFO - Collection name: Test_docs
2023-12-07 12:49:49,682 - INFO - Processed the request successfully
2023-12-07 12:49:57,744 - INFO - classes: ['class_2', 'docs']: %s
2023-12-07 12:49:58,756 - INFO - classes: ['class_2', 'docs']: %s
2023-12-07 12:49:58,913 - INFO - classes: ['class_2', 'docs']: %s
2023-12-07 12:51:11,859 - INFO - classes: ['class_2', 'docs']: %s
2023-12-07 12:51:11,994 - INFO - classes: ['class_2', 'docs']: %s
2023-12-07 12:51:13,435 - INFO - classes: ['class_2', 'docs']: %s
2023-12-07 12:51:13,583 - INFO - classes: ['class_2', 'docs']: %s
2023-12-07 12:51:16,671 - INFO - classes: ['class_2', 'docs']: %s
2023-12-07 12:51:16,811 - INFO - classes: ['class_2', 'docs']: %s
2023-12-07 12:51:16,846 - INFO - Received requests to /inference endpoint
2023-12-07 12:51:16,947 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 12:51:16,948 - INFO - Received request: {'username': 'test', 'prompt': 'hot to Optimize Reasoning Chains?', 'memory': True, 'conversation_number': 1, 'AI_assistance': False, 'collection_name': 'docs', 'llm_model': 'Llama_13b'}
2023-12-07 12:51:16,952 - INFO - Collection name: Test_docs
2023-12-07 12:51:45,917 - INFO - Processed the request successfully
2023-12-07 13:24:38,458 - INFO - Created a temporary directory at /tmp/tmph3q4h1ma
2023-12-07 13:24:38,459 - INFO - Writing /tmp/tmph3q4h1ma/_remote_module_non_scriptable.py
2023-12-07 13:24:40,267 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-07 13:24:44,638 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-07 13:28:54,950 - ERROR - Error processing the request: 'Config' object has no attribute 'dict'
2023-12-07 13:29:38,786 - INFO - Created a temporary directory at /tmp/tmphgx3j2_8
2023-12-07 13:29:38,786 - INFO - Writing /tmp/tmphgx3j2_8/_remote_module_non_scriptable.py
2023-12-07 13:29:40,746 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-07 13:29:45,133 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-07 13:29:53,747 - ERROR - Error processing the request: 'Config' object has no attribute 'dict'
2023-12-07 13:29:56,165 - ERROR - Error processing the request: 'Config' object has no attribute 'dict'
2023-12-07 13:31:31,105 - INFO - Created a temporary directory at /tmp/tmpcnqn5ye3
2023-12-07 13:31:31,105 - INFO - Writing /tmp/tmpcnqn5ye3/_remote_module_non_scriptable.py
2023-12-07 13:31:33,015 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-07 13:31:37,280 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-07 13:31:49,479 - ERROR - Error processing the request: 'dict' object has no attribute 'dict'
2023-12-07 13:31:51,620 - ERROR - Error processing the request: 'dict' object has no attribute 'dict'
2023-12-07 13:33:14,362 - INFO - Created a temporary directory at /tmp/tmpmmugu71g
2023-12-07 13:33:14,362 - INFO - Writing /tmp/tmpmmugu71g/_remote_module_non_scriptable.py
2023-12-07 13:33:16,261 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-07 13:33:20,535 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-07 13:33:31,840 - ERROR - Error processing the request: 'dict' object has no attribute 'dict'
2023-12-07 13:34:44,730 - INFO - Created a temporary directory at /tmp/tmp8je6pnan
2023-12-07 13:34:44,731 - INFO - Writing /tmp/tmp8je6pnan/_remote_module_non_scriptable.py
2023-12-07 13:34:46,657 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-07 13:34:50,912 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-07 13:35:00,977 - INFO - Collection name: Amin_docs
2023-12-07 13:35:06,460 - INFO - Processed the request successfully
2023-12-07 13:35:54,379 - INFO - Processed the request successfully
2023-12-07 13:39:38,427 - INFO - Collection name: Amin_docs
2023-12-07 13:39:44,436 - INFO - Processed the request successfully
2023-12-07 13:54:26,132 - INFO - Created a temporary directory at /tmp/tmpfxmlfgsz
2023-12-07 13:54:26,132 - INFO - Writing /tmp/tmpfxmlfgsz/_remote_module_non_scriptable.py
2023-12-07 13:54:27,939 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-07 13:54:32,195 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-07 13:57:06,432 - INFO - classes: []: %s
2023-12-07 13:57:10,725 - INFO - classes: []: %s
2023-12-07 13:57:13,269 - INFO - classes: []: %s
2023-12-07 13:57:17,784 - INFO - classes: []: %s
2023-12-07 13:57:26,300 - INFO - classes: []: %s
2023-12-07 13:57:57,425 - INFO - classes: []: %s
2023-12-07 13:58:48,599 - INFO - Created a temporary directory at /tmp/tmprdocufrq
2023-12-07 13:58:48,600 - INFO - Writing /tmp/tmprdocufrq/_remote_module_non_scriptable.py
2023-12-07 13:58:50,428 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-07 13:58:54,948 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-07 14:09:53,489 - INFO - classes: []: %s
2023-12-07 14:10:47,960 - INFO - Received requests to /inference endpoint
2023-12-07 14:10:48,061 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 14:10:48,062 - INFO - Received request: {'username': 'amin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'Llama_13b'}
2023-12-07 14:10:52,734 - INFO - Processed the request successfully
2023-12-07 14:11:21,166 - INFO - classes: []: %s
2023-12-07 14:11:29,631 - INFO - classes: []: %s
2023-12-07 14:11:30,225 - INFO - checking the request/ username='amin' class_name='docs' mode='create_collection' vectorDB_type='Weaviate' file_path=None: %s
2023-12-07 14:11:30,323 - INFO - checkpoint 1
2023-12-07 14:11:30,323 - INFO - checkpoint 2 amin: %s
2023-12-07 14:11:30,323 - INFO - checkpoint 2 amin_docs: %s
2023-12-07 14:11:30,368 - INFO - class name added successfully to database
2023-12-07 14:11:30,368 - INFO - success: class docs created for user amin
2023-12-07 14:11:30,466 - INFO - classes: ['docs']: %s
2023-12-07 14:11:35,085 - INFO - classes: ['docs']: %s
2023-12-07 14:11:55,380 - INFO - classes: ['docs']: %s
2023-12-07 14:11:56,720 - INFO - classes: ['docs']: %s
2023-12-07 14:11:58,638 - INFO - response: {'status': 'error', 'message': '\x1b[36mray::WeaviateEmbedder.adding_weaviate_document()\x1b[39m (pid=1506892, ip=192.168.0.224, actor_id=090a5989c427425fce24af9501000000, repr=<backend_vectordatabase.WeaviateEmbedder object at 0x7fbc12d11e10>)\n  File "/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/backend_vectordatabase.py", line 87, in adding_weaviate_document\n    ray.get(results)\nTypeError: Attempting to call `get` on the value {\'page_content\': " \\nData Scientist  \\nAbout the job  \\nAbout this role:  \\n \\nMcKenzie Intelligence Services (MIS)  \\n \\nWere a fast -growing SaaS business focusing on natural catastrophe inte lligence to enable fast \\nresponse to human distress situations. We are able to assess the devastating impact of catastrophic \\noccurrences almost as soon as they happen.  \\n \\nIn the three years since we launched our GEO platform, we have helped our clients proce ss claims in \\nrecord time and get funds quickly to those who need it in difficult times.  \\n \\nWe\'re currently seeking a skilled  Data Scientist  to join our team. If you have a passion for \\ndeveloping Machine Learning and Computer Vision Models, we want to hear fr om you.  \\n \\nOur Mission:  \\n \\nWe are on a mission to revolutionise the Insurance industrys natural catastrophe claims processing \\ntime to deliver funds to those in difficulty via the use of an exciting mix of satellites, drones, machine \\nlearning, and human -led an alysis.  \\n \\nOur values are:  \\n \\n Innovative  innovation is at the heart of all we do. We push the boundaries of possibility to \\ncreate meaning and value  \\n Trusted  we provide truth during confusion and can be counted on to deliver when it is \\nneeded the most for o ur clients, our partners and each other  \\n Collaborative  we work together, we cooperate, we share knowledge. We support each \\nother, our clients and our partners  \\n \\nThis is what you should expect if you hit \'apply\':  \\n Phone call with our Talent team  \\n Take -home  test \\n 1st interview with our Head of Technology  \\n 2nd interview with a senior manager  \\n Final interview with the wider technical team  \\n \\n", \'document_title\': \'MIS - Data Scientist.pdf\'}, which is not an ray.ObjectRef.'}: %s
2023-12-07 14:11:58,638 - INFO - request processed successfully username='amin' class_name='docs' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/8699331488cf21f5': %s
2023-12-07 14:12:34,331 - INFO - Received requests to /inference endpoint
2023-12-07 14:12:34,432 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 14:12:34,432 - INFO - Received request: {'username': 'amin', 'prompt': 'tell me about the job description', 'memory': False, 'conversation_number': 0, 'AI_assistance': False, 'collection_name': 'string', 'llm_model': 'Llama_13b'}
2023-12-07 14:12:44,164 - INFO - Received requests to /inference endpoint
2023-12-07 14:12:44,265 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 14:12:44,265 - INFO - Received request: {'username': 'amin', 'prompt': 'tell me about the job description', 'memory': False, 'conversation_number': 0, 'AI_assistance': False, 'collection_name': 'docs', 'llm_model': 'Llama_13b'}
2023-12-07 14:12:55,998 - INFO - Processed the request successfully
2023-12-07 14:13:31,940 - INFO - classes: ['docs']: %s
2023-12-07 14:13:38,700 - INFO - classes: ['docs']: %s
2023-12-07 14:13:38,828 - INFO - classes: ['docs']: %s
2023-12-07 14:13:52,938 - INFO - classes: ['docs']: %s
2023-12-07 14:13:53,086 - INFO - classes: ['docs']: %s
2023-12-07 14:17:07,776 - INFO - Received requests to /inference endpoint
2023-12-07 14:17:07,877 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 14:17:07,877 - INFO - Received request: {'username': 'amin', 'prompt': 'what are the main responsibilities?', 'memory': True, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-12-07 14:17:21,813 - INFO - Processed the request successfully
2023-12-07 14:46:53,205 - INFO - classes: ['docs']: %s
2023-12-07 14:47:10,867 - INFO - classes: ['docs']: %s
2023-12-07 14:47:17,127 - INFO - classes: ['docs']: %s
2023-12-07 14:47:19,308 - INFO - classes: ['docs']: %s
2023-12-07 14:47:20,178 - INFO - actors creation successful [Actor(WeaviateEmbedder, e18827b72332c2fe06edaaf001000000), Actor(WeaviateEmbedder, 349d249a83551bbad4e48b5d01000000), Actor(WeaviateEmbedder, 31a17c9f28aacab82e8283d101000000)]: %s
2023-12-07 14:47:20,179 - INFO - check 1st step of ray was successful
2023-12-07 14:47:20,179 - INFO - check if ray was successful:
2023-12-07 14:47:20,179 - INFO - response: {'status': 'success', 'message': 'Processed 17 documents in batches for class amin_docs .'}: %s
2023-12-07 14:47:20,179 - INFO - request processed successfully username='amin' class_name='docs ' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/248cb21401ac4dfe': %s
2023-12-07 14:53:45,882 - INFO - classes: ['docs']: %s
2023-12-07 14:53:48,341 - INFO - classes: ['docs']: %s
2023-12-07 14:53:48,487 - INFO - classes: ['docs']: %s
2023-12-07 14:53:50,652 - INFO - classes: ['docs']: %s
2023-12-07 14:53:50,805 - INFO - classes: ['docs']: %s
2023-12-07 14:54:02,574 - INFO - classes: ['docs']: %s
2023-12-07 14:54:02,792 - INFO - classes: ['docs']: %s
2023-12-07 14:54:20,619 - INFO - classes: ['docs']: %s
2023-12-07 14:54:20,757 - INFO - classes: ['docs']: %s
2023-12-07 14:54:27,172 - INFO - classes: ['docs']: %s
2023-12-07 14:54:27,326 - INFO - classes: ['docs']: %s
2023-12-07 14:54:45,810 - INFO - classes: ['docs']: %s
2023-12-07 14:54:45,938 - INFO - classes: ['docs']: %s
2023-12-07 14:54:45,974 - INFO - Received requests to /inference endpoint
2023-12-07 14:54:46,075 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 14:54:46,075 - INFO - Received request: {'username': 'amin', 'prompt': 'provide me with a summary of Flava paper', 'memory': True, 'conversation_number': 3, 'AI_assistance': False, 'collection_name': 'docs', 'llm_model': 'Llama_13b'}
2023-12-07 14:54:53,723 - INFO - Processed the request successfully
2023-12-07 14:55:34,172 - INFO - classes: ['docs']: %s
2023-12-07 14:55:40,512 - INFO - classes: ['docs']: %s
2023-12-07 14:55:56,553 - INFO - classes: ['docs']: %s
2023-12-07 14:55:58,484 - INFO - classes: ['docs']: %s
2023-12-07 14:56:04,289 - INFO - classes: ['docs']: %s
2023-12-07 14:56:06,772 - INFO - actors creation successful [Actor(WeaviateEmbedder, 699d5dd7293f2b884d088ae301000000), Actor(WeaviateEmbedder, 4044c890a918dbcc47f7b96201000000), Actor(WeaviateEmbedder, 5440dd55ec23c86f8f16d5aa01000000)]: %s
2023-12-07 14:56:06,774 - INFO - check 1st step of ray was successful
2023-12-07 14:56:06,774 - INFO - check if ray was successful:
2023-12-07 14:56:06,774 - INFO - response: {'status': 'success', 'message': 'Processed 99 documents in batches for class amin_docs.'}: %s
2023-12-07 14:56:06,774 - INFO - request processed successfully username='amin' class_name='docs' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/9ae0ea2960417621': %s
2023-12-07 14:56:11,899 - INFO - classes: ['docs']: %s
2023-12-07 14:56:11,995 - INFO - query success: 2 documents found
2023-12-07 14:56:39,563 - INFO - classes: ['docs']: %s
2023-12-07 14:56:43,387 - INFO - classes: ['docs']: %s
2023-12-07 14:56:45,864 - INFO - actors creation successful [Actor(WeaviateEmbedder, aa986d88ff51477b2017205101000000), Actor(WeaviateEmbedder, 1a6fd99e29a9aeaa30fa478401000000), Actor(WeaviateEmbedder, eb8228062f3dea93b355321701000000)]: %s
2023-12-07 14:56:45,866 - INFO - check 1st step of ray was successful
2023-12-07 14:56:45,866 - INFO - check if ray was successful:
2023-12-07 14:56:45,866 - INFO - response: {'status': 'success', 'message': 'Processed 99 documents in batches for class amin_docs.'}: %s
2023-12-07 14:56:45,866 - INFO - request processed successfully username='amin' class_name='docs' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/430b1d8eb90f892d': %s
2023-12-07 14:56:46,732 - INFO - actors creation successful [Actor(WeaviateEmbedder, cbd5693814dcf5e1a4e6b77a01000000), Actor(WeaviateEmbedder, a0cffe9a7988448b61fde05a01000000), Actor(WeaviateEmbedder, 3f18eaa9fa56538af057773401000000)]: %s
2023-12-07 14:56:46,733 - INFO - check 1st step of ray was successful
2023-12-07 14:56:46,733 - INFO - check if ray was successful:
2023-12-07 14:56:46,733 - INFO - response: {'status': 'success', 'message': 'Processed 17 documents in batches for class amin_docs.'}: %s
2023-12-07 14:56:46,733 - INFO - request processed successfully username='amin' class_name='docs' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/d9d526ebde0d23e0': %s
2023-12-07 14:56:48,746 - INFO - classes: ['docs']: %s
2023-12-07 14:56:48,846 - INFO - query success: 2 documents found
2023-12-07 14:57:11,691 - INFO - classes: ['docs']: %s
2023-12-07 14:57:30,732 - INFO - classes: ['docs']: %s
2023-12-07 14:57:33,192 - INFO - actors creation successful [Actor(WeaviateEmbedder, 407c6575b71fdba8deaa5a8401000000), Actor(WeaviateEmbedder, ddf1dbf940c97a9bd7e4dde101000000), Actor(WeaviateEmbedder, 132065c3c286588c46f117a801000000)]: %s
2023-12-07 14:57:33,193 - INFO - check 1st step of ray was successful
2023-12-07 14:57:33,193 - INFO - check if ray was successful:
2023-12-07 14:57:33,194 - INFO - response: {'status': 'success', 'message': 'Processed 99 documents in batches for class amin_docs.'}: %s
2023-12-07 14:57:33,194 - INFO - request processed successfully username='amin' class_name='docs' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/626be5d9a3bed4b1': %s
2023-12-07 14:57:34,027 - INFO - actors creation successful [Actor(WeaviateEmbedder, 0cff402660873150b9dfbc7201000000), Actor(WeaviateEmbedder, 029685d513dd0a46905233bf01000000), Actor(WeaviateEmbedder, 95191ec5f725fca7ee2997c901000000)]: %s
2023-12-07 14:57:34,028 - INFO - check 1st step of ray was successful
2023-12-07 14:57:34,028 - INFO - check if ray was successful:
2023-12-07 14:57:34,028 - INFO - response: {'status': 'success', 'message': 'Processed 17 documents in batches for class amin_docs.'}: %s
2023-12-07 14:57:34,028 - INFO - request processed successfully username='amin' class_name='docs' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/4ac06b36813d3408': %s
2023-12-07 14:57:35,281 - INFO - actors creation successful [Actor(WeaviateEmbedder, f698e6e897b1d3b59a0896af01000000), Actor(WeaviateEmbedder, cf1b841923127c8cf4f3a7c601000000), Actor(WeaviateEmbedder, a8227260d01ee3578e00098401000000)]: %s
2023-12-07 14:57:35,282 - INFO - check 1st step of ray was successful
2023-12-07 14:57:35,282 - INFO - check if ray was successful:
2023-12-07 14:57:35,282 - INFO - response: {'status': 'success', 'message': 'Processed 11 documents in batches for class amin_docs.'}: %s
2023-12-07 14:57:35,282 - INFO - request processed successfully username='amin' class_name='docs' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/71e2e2888dc61f06': %s
2023-12-07 14:57:37,886 - INFO - classes: ['docs']: %s
2023-12-07 14:57:38,021 - INFO - query success: 3 documents found
2023-12-07 14:58:09,547 - INFO - classes: ['docs']: %s
2023-12-07 14:58:14,734 - INFO - classes: ['docs']: %s
2023-12-07 14:58:14,824 - INFO - classes: ['docs']: %s
2023-12-07 14:58:14,905 - INFO - query success: 4 documents found
2023-12-07 14:58:21,770 - INFO - classes: ['docs']: %s
2023-12-07 14:58:26,184 - INFO - classes: ['docs']: %s
2023-12-07 14:59:15,871 - INFO - classes: ['docs']: %s
2023-12-07 14:59:15,935 - INFO - Received requests to /inference endpoint
2023-12-07 14:59:16,036 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 14:59:16,036 - INFO - Received request: {'username': 'amin', 'prompt': 'what is the Computational bottleneck of  reasoning chains?', 'memory': True, 'conversation_number': 3, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-12-07 14:59:25,707 - INFO - Processed the request successfully
2023-12-07 14:59:29,911 - INFO - classes: ['docs']: %s
2023-12-07 14:59:30,051 - INFO - classes: ['docs']: %s
2023-12-07 14:59:39,400 - INFO - classes: ['docs']: %s
2023-12-07 14:59:39,540 - INFO - classes: ['docs']: %s
2023-12-07 14:59:47,321 - INFO - classes: ['docs']: %s
2023-12-07 14:59:47,447 - INFO - classes: ['docs']: %s
2023-12-07 14:59:47,479 - INFO - Received requests to /inference endpoint
2023-12-07 14:59:47,580 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 14:59:47,580 - INFO - Received request: {'username': 'amin', 'prompt': 'what is the Computational bottleneck of reasoning chains?', 'memory': True, 'conversation_number': 3, 'AI_assistance': False, 'collection_name': 'docs', 'llm_model': 'Llama_13b'}
2023-12-07 15:00:00,443 - INFO - Processed the request successfully
2023-12-07 15:07:41,083 - INFO - classes: ['docs']: %s
2023-12-07 15:07:41,235 - INFO - classes: ['docs']: %s
2023-12-07 15:07:41,276 - INFO - Received requests to /inference endpoint
2023-12-07 15:07:41,377 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 15:07:41,377 - INFO - Received request: {'username': 'amin', 'prompt': 'what are the models compared in the table 1 of flava paper? ', 'memory': True, 'conversation_number': 3, 'AI_assistance': False, 'collection_name': 'docs', 'llm_model': 'Llama_13b'}
2023-12-07 15:07:55,351 - INFO - Processed the request successfully
2023-12-07 15:08:47,776 - INFO - classes: ['docs']: %s
2023-12-07 15:08:47,917 - INFO - classes: ['docs']: %s
2023-12-07 15:08:47,953 - INFO - Received requests to /inference endpoint
2023-12-07 15:08:48,054 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 15:08:48,054 - INFO - Received request: {'username': 'amin', 'prompt': 'give me  An overview of our FLAVA model', 'memory': True, 'conversation_number': 3, 'AI_assistance': False, 'collection_name': 'docs', 'llm_model': 'Llama_13b'}
2023-12-07 15:09:32,538 - INFO - Processed the request successfully
2023-12-07 15:09:43,627 - INFO - classes: ['docs']: %s
2023-12-07 15:09:43,756 - INFO - classes: ['docs']: %s
2023-12-07 15:09:43,793 - INFO - Received requests to /inference endpoint
2023-12-07 15:09:43,894 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 15:09:43,894 - INFO - Received request: {'username': 'amin', 'prompt': 'what are Public Multimodal Datasets mentioned in table 2 of Flava paper?', 'memory': True, 'conversation_number': 3, 'AI_assistance': False, 'collection_name': 'docs', 'llm_model': 'Llama_13b'}
2023-12-07 15:09:52,412 - INFO - Processed the request successfully
2023-12-07 15:10:18,262 - INFO - classes: ['docs']: %s
2023-12-07 15:10:18,443 - INFO - classes: ['docs']: %s
2023-12-07 15:10:30,371 - INFO - classes: ['docs']: %s
2023-12-07 15:10:30,489 - INFO - classes: ['docs']: %s
2023-12-07 15:10:30,523 - INFO - Received requests to /inference endpoint
2023-12-07 15:10:30,624 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 15:10:30,624 - INFO - Received request: {'username': 'amin', 'prompt': 'give me An overview of our FLAVA model', 'memory': True, 'conversation_number': 4, 'AI_assistance': False, 'collection_name': 'docs', 'llm_model': 'Llama_13b'}
2023-12-07 15:11:09,792 - INFO - Processed the request successfully
2023-12-07 15:11:16,378 - INFO - classes: ['docs']: %s
2023-12-07 15:11:16,508 - INFO - classes: ['docs']: %s
2023-12-07 15:11:25,575 - INFO - classes: ['docs']: %s
2023-12-07 15:11:25,716 - INFO - classes: ['docs']: %s
2023-12-07 15:11:27,293 - INFO - classes: ['docs']: %s
2023-12-07 15:11:27,410 - INFO - classes: ['docs']: %s
2023-12-07 15:11:32,198 - INFO - classes: ['docs']: %s
2023-12-07 15:11:32,301 - INFO - classes: ['docs']: %s
2023-12-07 15:11:32,337 - INFO - Received requests to /inference endpoint
2023-12-07 15:11:32,438 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 15:11:32,438 - INFO - Received request: {'username': 'amin', 'prompt': 'what is the Computational bottleneck of reasoning chains?', 'memory': True, 'conversation_number': 1, 'AI_assistance': False, 'collection_name': 'docs', 'llm_model': 'Llama_13b'}
2023-12-07 15:11:45,363 - INFO - Processed the request successfully
2023-12-07 15:11:52,129 - INFO - classes: ['docs']: %s
2023-12-07 15:11:52,255 - INFO - classes: ['docs']: %s
2023-12-07 15:11:54,109 - INFO - classes: ['docs']: %s
2023-12-07 15:11:54,257 - INFO - classes: ['docs']: %s
2023-12-07 15:12:08,816 - INFO - classes: ['docs']: %s
2023-12-07 15:12:08,926 - INFO - classes: ['docs']: %s
2023-12-07 15:12:10,067 - INFO - classes: ['docs']: %s
2023-12-07 15:12:10,153 - INFO - classes: ['docs']: %s
2023-12-07 15:12:10,690 - INFO - classes: ['docs']: %s
2023-12-07 15:12:10,827 - INFO - classes: ['docs']: %s
2023-12-07 15:12:14,625 - INFO - classes: ['docs']: %s
2023-12-07 15:12:14,750 - INFO - classes: ['docs']: %s
2023-12-07 15:12:14,785 - INFO - Received requests to /inference endpoint
2023-12-07 15:12:14,885 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 15:12:14,885 - INFO - Received request: {'username': 'amin', 'prompt': 'provide me with a summary of Flava paper', 'memory': True, 'conversation_number': 3, 'AI_assistance': False, 'collection_name': 'docs', 'llm_model': 'Llama_13b'}
2023-12-07 15:12:52,682 - INFO - Processed the request successfully
2023-12-07 15:13:06,467 - INFO - classes: ['docs']: %s
2023-12-07 15:13:06,645 - INFO - classes: ['docs']: %s
2023-12-07 15:13:10,917 - INFO - classes: ['docs']: %s
2023-12-07 15:13:11,045 - INFO - classes: ['docs']: %s
2023-12-07 15:13:11,076 - INFO - Received requests to /inference endpoint
2023-12-07 15:13:11,177 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 15:13:11,177 - INFO - Received request: {'username': 'amin', 'prompt': 'provide me with a summary of Flava paper', 'memory': True, 'conversation_number': 5, 'AI_assistance': False, 'collection_name': 'docs', 'llm_model': 'Llama_13b'}
2023-12-07 15:13:49,861 - INFO - Processed the request successfully
2023-12-07 15:14:08,343 - INFO - classes: ['docs']: %s
2023-12-07 15:14:08,457 - INFO - classes: ['docs']: %s
2023-12-07 15:14:08,491 - INFO - Received requests to /inference endpoint
2023-12-07 15:14:08,592 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 15:14:08,592 - INFO - Received request: {'username': 'amin', 'prompt': 'according to Flava paper, what is Joint unimodal and multimodal training?', 'memory': True, 'conversation_number': 5, 'AI_assistance': False, 'collection_name': 'docs', 'llm_model': 'Llama_13b'}
2023-12-07 15:14:47,256 - INFO - Processed the request successfully
2023-12-07 15:15:10,900 - INFO - classes: ['docs']: %s
2023-12-07 15:15:11,037 - INFO - classes: ['docs']: %s
2023-12-07 15:33:03,559 - INFO - Created a temporary directory at /tmp/tmpx4atls99
2023-12-07 15:33:03,559 - INFO - Writing /tmp/tmpx4atls99/_remote_module_non_scriptable.py
2023-12-07 15:33:05,948 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-07 15:33:32,921 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-07 15:33:47,822 - INFO - Created a temporary directory at /tmp/tmprx6u0_iu
2023-12-07 15:33:47,823 - INFO - Writing /tmp/tmprx6u0_iu/_remote_module_non_scriptable.py
2023-12-07 15:33:49,723 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-07 15:33:54,082 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-07 15:34:21,133 - INFO - classes: ['docs']: %s
2023-12-07 15:34:21,290 - INFO - classes: ['docs']: %s
2023-12-07 15:34:22,861 - INFO - classes: ['docs']: %s
2023-12-07 15:34:23,054 - INFO - classes: ['docs']: %s
2023-12-07 15:34:42,285 - INFO - classes: ['docs']: %s
2023-12-07 15:34:42,414 - INFO - classes: ['docs']: %s
2023-12-07 15:35:25,479 - INFO - classes: ['docs']: %s
2023-12-07 15:35:30,447 - INFO - classes: ['docs']: %s
2023-12-07 15:37:03,198 - INFO - Received requests to /inference endpoint
2023-12-07 15:37:03,300 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 15:37:03,300 - INFO - Received request: {'username': 'amin', 'prompt': 'hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'Llama_70b'}
2023-12-07 15:37:09,744 - INFO - Processed the request successfully
2023-12-07 15:37:21,464 - INFO - classes: ['docs']: %s
2023-12-07 15:37:23,660 - INFO - classes: ['docs']: %s
2023-12-07 15:37:23,753 - INFO - classes: ['docs']: %s
2023-12-07 15:37:39,075 - INFO - classes: ['docs']: %s
2023-12-07 15:37:39,211 - INFO - classes: ['docs']: %s
2023-12-07 15:37:39,237 - INFO - Received requests to /inference endpoint
2023-12-07 15:37:39,337 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 15:37:39,338 - INFO - Received request: {'username': 'amin', 'prompt': 'provide me with a summary of Flava paper', 'memory': True, 'conversation_number': 6, 'AI_assistance': False, 'collection_name': 'docs', 'llm_model': 'Llama_70b'}
2023-12-07 15:39:04,179 - INFO - Processed the request successfully
2023-12-07 15:39:13,631 - INFO - classes: ['docs']: %s
2023-12-07 15:39:19,434 - INFO - classes: ['docs']: %s
2023-12-07 15:39:20,938 - INFO - classes: ['docs']: %s
2023-12-07 15:39:21,040 - INFO - query success: 4 documents found
2023-12-07 15:39:31,871 - INFO - classes: ['docs']: %s
2023-12-07 15:39:32,106 - INFO - collection delete: {'success': 'Class amin_docs has been removed'}: %s
2023-12-07 15:39:32,217 - INFO - classes: []: %s
2023-12-07 15:39:36,642 - INFO - classes: []: %s
2023-12-07 15:39:36,751 - INFO - checking the request/ username='amin' class_name='paper' mode='create_collection' vectorDB_type='Weaviate' file_path=None: %s
2023-12-07 15:39:36,826 - INFO - checkpoint 1
2023-12-07 15:39:36,826 - INFO - checkpoint 2 amin: %s
2023-12-07 15:39:36,826 - INFO - checkpoint 2 amin_paper: %s
2023-12-07 15:39:36,835 - INFO - class name added successfully to database
2023-12-07 15:39:36,835 - INFO - success: class paper created for user amin
2023-12-07 15:39:36,917 - INFO - classes: ['paper']: %s
2023-12-07 15:39:42,355 - INFO - classes: ['paper']: %s
2023-12-07 15:39:55,689 - INFO - classes: ['paper']: %s
2023-12-07 15:40:11,472 - INFO - classes: ['paper']: %s
2023-12-07 15:40:14,299 - INFO - classes: ['paper']: %s
2023-12-07 15:40:16,344 - INFO - classes: ['paper']: %s
2023-12-07 15:40:16,481 - INFO - classes: ['paper']: %s
2023-12-07 15:40:29,629 - INFO - classes: ['paper']: %s
2023-12-07 15:40:29,762 - INFO - classes: ['paper']: %s
2023-12-07 15:40:29,789 - INFO - Received requests to /inference endpoint
2023-12-07 15:40:29,890 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 15:40:29,890 - INFO - Received request: {'username': 'amin', 'prompt': 'what is  PFMs ?', 'memory': True, 'conversation_number': 7, 'AI_assistance': False, 'collection_name': 'paper', 'llm_model': 'Llama_70b'}
2023-12-07 15:41:41,622 - INFO - Processed the request successfully
2023-12-07 15:42:47,117 - INFO - classes: ['paper']: %s
2023-12-07 15:42:47,315 - INFO - classes: ['paper']: %s
2023-12-07 15:43:14,266 - INFO - classes: ['paper']: %s
2023-12-07 15:43:14,413 - INFO - classes: ['paper']: %s
2023-12-07 15:43:14,456 - INFO - Received requests to /inference endpoint
2023-12-07 15:43:14,557 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 15:43:14,557 - INFO - Received request: {'username': 'amin', 'prompt': 'base on A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT paper what is Pretrained Foundation Models?', 'memory': True, 'conversation_number': 8, 'AI_assistance': False, 'collection_name': 'paper', 'llm_model': 'Llama_70b'}
2023-12-07 15:43:29,093 - INFO - Processed the request successfully
2023-12-07 15:44:41,138 - INFO - classes: ['paper']: %s
2023-12-07 15:44:51,210 - INFO - classes: ['paper']: %s
2023-12-07 15:44:59,013 - INFO - classes: ['paper']: %s
2023-12-07 15:45:00,021 - INFO - classes: ['paper']: %s
2023-12-07 15:45:00,858 - INFO - actors creation successful [Actor(WeaviateEmbedder, feb06cad6aac7588063c58f701000000), Actor(WeaviateEmbedder, 4b83bebbf54a2d0a0734bcc401000000), Actor(WeaviateEmbedder, 208882d20bcb53a8ff8324eb01000000)]: %s
2023-12-07 15:45:00,859 - INFO - check 1st step of ray was successful
2023-12-07 15:45:00,859 - INFO - check if ray was successful:
2023-12-07 15:45:00,859 - INFO - response: {'status': 'success', 'message': 'Processed 17 documents in batches for class amin_paper.'}: %s
2023-12-07 15:45:00,859 - INFO - request processed successfully username='amin' class_name='paper' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/120ee2cffff8ef61': %s
2023-12-07 15:45:08,909 - INFO - classes: ['paper']: %s
2023-12-07 15:45:09,784 - INFO - classes: ['paper']: %s
2023-12-07 15:45:10,645 - INFO - actors creation successful [Actor(WeaviateEmbedder, 0913743e7a97811f9e1871d001000000), Actor(WeaviateEmbedder, d377a7eeed23b1e406b4c2fe01000000), Actor(WeaviateEmbedder, e9a58908e4f88626c192341b01000000)]: %s
2023-12-07 15:45:10,646 - INFO - check 1st step of ray was successful
2023-12-07 15:45:10,646 - INFO - check if ray was successful:
2023-12-07 15:45:10,646 - INFO - response: {'status': 'success', 'message': 'Processed 17 documents in batches for class amin_paper.'}: %s
2023-12-07 15:45:10,646 - INFO - request processed successfully username='amin' class_name='paper' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/805da4fcd224409b': %s
2023-12-07 15:45:11,060 - INFO - actors creation successful [Actor(WeaviateEmbedder, 0ee484c0983dfcf68d7cb2bf01000000), Actor(WeaviateEmbedder, 3e60facc85400c58ff80177a01000000), Actor(WeaviateEmbedder, f574b0caa0927df5c98649db01000000)]: %s
2023-12-07 15:45:11,061 - INFO - check 1st step of ray was successful
2023-12-07 15:45:11,061 - INFO - check if ray was successful:
2023-12-07 15:45:11,061 - INFO - response: {'status': 'success', 'message': 'Processed 15 documents in batches for class amin_paper.'}: %s
2023-12-07 15:45:11,061 - INFO - request processed successfully username='amin' class_name='paper' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/3322b71a13aa4df2': %s
2023-12-07 15:45:21,267 - INFO - classes: ['paper']: %s
2023-12-07 15:45:23,281 - INFO - classes: ['paper']: %s
2023-12-07 15:45:24,148 - INFO - actors creation successful [Actor(WeaviateEmbedder, 8b9e42d416a942d4d58a562001000000), Actor(WeaviateEmbedder, f9d61ff396506f04058a6f4601000000), Actor(WeaviateEmbedder, a3fc7d342b695187c7903b3f01000000)]: %s
2023-12-07 15:45:24,149 - INFO - check 1st step of ray was successful
2023-12-07 15:45:24,149 - INFO - check if ray was successful:
2023-12-07 15:45:24,149 - INFO - response: {'status': 'success', 'message': 'Processed 17 documents in batches for class amin_paper.'}: %s
2023-12-07 15:45:24,149 - INFO - request processed successfully username='amin' class_name='paper' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/0e5e44c832f7ac9d': %s
2023-12-07 15:45:24,630 - INFO - actors creation successful [Actor(WeaviateEmbedder, f0b90c1e855b0c0626bfcb0401000000), Actor(WeaviateEmbedder, 43279947fa7e5934080f64f901000000), Actor(WeaviateEmbedder, de668213e75999608168d41301000000)]: %s
2023-12-07 15:45:24,631 - INFO - check 1st step of ray was successful
2023-12-07 15:45:24,631 - INFO - check if ray was successful:
2023-12-07 15:45:24,631 - INFO - response: {'status': 'success', 'message': 'Processed 15 documents in batches for class amin_paper.'}: %s
2023-12-07 15:45:24,631 - INFO - request processed successfully username='amin' class_name='paper' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/4ec1ace8392a994b': %s
2023-12-07 15:45:25,764 - INFO - actors creation successful [Actor(WeaviateEmbedder, 705ad3bfb8fbb17e131ccd4101000000), Actor(WeaviateEmbedder, 26563d91534db715abb84ea401000000), Actor(WeaviateEmbedder, 7dcb7df0065e12bf0187f63501000000)]: %s
2023-12-07 15:45:25,765 - INFO - check 1st step of ray was successful
2023-12-07 15:45:25,765 - INFO - check if ray was successful:
2023-12-07 15:45:25,765 - INFO - response: {'status': 'success', 'message': 'Processed 10 documents in batches for class amin_paper.'}: %s
2023-12-07 15:45:25,765 - INFO - request processed successfully username='amin' class_name='paper' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/1bab18961b0ee0b3': %s
2023-12-07 15:45:27,282 - INFO - classes: ['paper']: %s
2023-12-07 15:45:27,359 - INFO - query success: 2 documents found
2023-12-07 15:45:53,000 - INFO - classes: ['paper']: %s
2023-12-07 15:45:58,507 - INFO - classes: ['paper']: %s
2023-12-07 15:45:58,606 - INFO - classes: ['paper']: %s
2023-12-07 15:45:58,704 - INFO - query success: 3 documents found
2023-12-07 15:46:11,058 - INFO - classes: ['paper']: %s
2023-12-07 15:46:11,181 - INFO - query success: 3 documents found
2023-12-07 15:46:11,796 - INFO - classes: ['paper']: %s
2023-12-07 15:46:11,877 - INFO - query success: 3 documents found
2023-12-07 15:46:26,961 - INFO - classes: ['paper']: %s
2023-12-07 15:46:27,586 - INFO - classes: ['paper']: %s
2023-12-07 15:46:38,474 - INFO - classes: ['paper']: %s
2023-12-07 15:46:38,573 - INFO - query success: 3 documents found
2023-12-07 15:46:49,630 - INFO - classes: ['paper']: %s
2023-12-07 15:46:54,763 - INFO - classes: ['paper']: %s
2023-12-07 15:46:54,980 - INFO - classes: ['paper']: %s
2023-12-07 15:46:55,088 - INFO - query success: 3 documents found
2023-12-07 15:46:55,560 - INFO - classes: ['paper']: %s
2023-12-07 15:46:55,664 - INFO - query success: 3 documents found
2023-12-07 15:47:01,904 - INFO - classes: ['paper']: %s
2023-12-07 15:47:42,636 - INFO - classes: ['paper']: %s
2023-12-07 15:47:42,685 - INFO - Received requests to /inference endpoint
2023-12-07 15:47:42,786 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 15:47:42,786 - INFO - Received request: {'username': 'amin', 'prompt': 'what  are the novilities of A Foundational Language And Vision Alignment Model paper?', 'memory': True, 'conversation_number': 8, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-12-07 15:48:42,514 - INFO - Processed the request successfully
2023-12-07 15:49:23,914 - INFO - classes: ['paper']: %s
2023-12-07 15:49:23,967 - INFO - Received requests to /inference endpoint
2023-12-07 15:49:24,068 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 15:49:24,068 - INFO - Received request: {'username': 'amin', 'prompt': 'base on A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT paper what is Weakly-Supervised Learning?', 'memory': True, 'conversation_number': 8, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-12-07 15:50:36,837 - INFO - Processed the request successfully
2023-12-07 15:51:10,518 - INFO - classes: ['paper']: %s
2023-12-07 15:51:10,641 - INFO - classes: ['paper']: %s
2023-12-07 15:51:15,263 - INFO - classes: ['paper']: %s
2023-12-07 15:51:15,458 - INFO - classes: ['paper']: %s
2023-12-07 15:51:19,453 - INFO - classes: ['paper']: %s
2023-12-07 15:51:19,570 - INFO - classes: ['paper']: %s
2023-12-07 15:51:19,603 - INFO - Received requests to /inference endpoint
2023-12-07 15:51:19,704 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 15:51:19,704 - INFO - Received request: {'username': 'amin', 'prompt': 'base on A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT paper what is Weakly-Supervised Learning?', 'memory': True, 'conversation_number': 9, 'AI_assistance': False, 'collection_name': 'paper', 'llm_model': 'Llama_70b'}
2023-12-07 15:52:58,070 - INFO - Processed the request successfully
2023-12-07 15:57:45,655 - INFO - Created a temporary directory at /tmp/tmpphem_zgq
2023-12-07 15:57:45,655 - INFO - Writing /tmp/tmpphem_zgq/_remote_module_non_scriptable.py
2023-12-07 15:57:47,483 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-07 15:57:51,716 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-07 15:57:59,128 - INFO - classes: ['paper']: %s
2023-12-07 15:57:59,244 - INFO - classes: ['paper']: %s
2023-12-07 15:58:09,175 - INFO - classes: ['paper']: %s
2023-12-07 15:58:09,328 - INFO - classes: ['paper']: %s
2023-12-07 15:58:09,373 - INFO - Received requests to /inference endpoint
2023-12-07 15:58:09,474 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 15:58:09,474 - INFO - Received request: {'username': 'amin', 'prompt': 'base on A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT paper what is Weakly-Supervised Learning?', 'memory': True, 'conversation_number': 9, 'AI_assistance': False, 'collection_name': 'paper', 'llm_model': 'Llama_13b'}
2023-12-07 15:58:09,516 - INFO - Retrieved docs: [Document(page_content='Table 1. Average classication accuracy of DIY-FSCIL framework using our self-designed baselines and adopted SOTA FSCIL [17] (not\nspecically designed for cross-modal). For every experiment, we create 600episodes each with 5random classes from both novel and base\ncategories separately. Each episode contains a total 155(15 samples from each of the 5 classes) and 155query photos from from\nboth novel and base categories respectively. B5is an upper bound.\nMethods5-Shot Learning 1-Shot Learning\nAcc@both Acc@base Acc@novel Acc@both Acc@base Acc@novel\nBaselinesB1 36.29 % 73.94% 38.92% 31.52% 73.98% 34.68%\nB2 25.86% 32.85% 70.58% 28.81% 40.91% 50.24%\nB3 58.92% 73.81% 72.34% 53.35% 73.75% 59.93%\nB4 54.5% 71.68% 71.81% 51.41% 71.68% 51.44%\nB571.52% 75.72% 85.46% 63.47% 75.83% 73.90%\nSOTA FSCIL[17] 50.45% 74.35% 65.81% 44.71% 73.98% 64.21%\n[50] 45.25% 74.10% 63.46% 41.97% 74.60% 61.85%\n[54] 51.54% 73.21% 66.82% 45.81% 73.58% 63.95%\nOurs DIY-FSCIL 60.54% 74.38% 75.84% 54.97 % 74.06% 64.10%\nmance. This metric helps to determine how the base classes\nknowledge affects novel classes and vice-versa.\n4.2. Competitors\nAs there exists no prior work dealing with sketch-based\nFSCIL, we implement the following set of baselines and\ntheir adaptions in order to assess the contribution of our\nproposed framework. B1: We use a combination of old-\nbase and new-novel classes to retrain the complete model.\nBesides requiring a lot of computational power, this suf-\nfers from a severe class imbalance problem between suf-\nciently available base classes and few exemplars from novel\nclasses. Nevertheless, this can not be realised in a real sce-\nnario.B2: We only ne-tune the model using the novel\nclasses. It acts as a naive baseline, and is limited due to the\nissue of catastrophic forgetting. B3:We freeze backbone\nfeature extractor F, and use the class-wise average feature\nof sketch exemplars as the representative weight-vectors of\nnovel classes along with the pre-trained base-classier. In\nother words, we remove the GAT module from our proposed\nframework.B4: We further examine the performance of\nour framework by training both the Falong with theG.\nThis is used to analyse the importance of freezing the fea-\nture extractorF.B5:During testing, we utilise real im-\nages as the support set. As images are more detailed than\nsketches, this model serves as our upper boundary. How-\never, it fails to address our main concern of violating the\ndata privacy norm. For a fair comparison, we utilise the\nsame settings for all the models as our framework. Though\nexisting FSCIL methods [17, 54, 50] are not specically\ndesigned to deal with cross-modal sketch exemplars, we\nnaively adopt those under our sketch-based FSCIL setup.\n4.3. Performance Analysis\nIn Table 1, we report the comparative results using the\nstandard one-shot and ve-shot sketch-based FSCIL setting\non Sketchy dataset. We make the following observations:\n(i) Despite using abundant memory and computational re-\nsources B1performs poorly on the novel classes, due to\nthe absence of any mechanism to handle few shot classes\n(i.e., severe class imbalance). This suggests that few shotparadigm is essential to perform reasonably well on novel\nclasses. (ii) B2adapts ne-tuning on the novel classes with-\nout heavy computational overhead. However, doing so de-\nclines the models performance on the base classes due to\ncatastrophic interference. (iii) While B3outperforms base-\nlines B1andB2, it fails to model mutual agreement between\nbase and novel classes for learning discriminative decision\nboundaries under incremental setup, revealing the impor-\ntance of our weight rening strategy through GAT module.\n(iv) Low performance of B4signies the necessity of freez-\ning the weights of Fduring the second stage of training\nin order to reduce the catastrophic forgetting problem, and\nalso to generalise notably better on unseen categories. (v)\nB5(upper bound) achieves the best numbers, as the support\nset comes directly from photos, and this is unlike ours where\nwe have a critical challenge due to the domain gap between\nsketch exemplars and query photos. (vi) Moreover, the per-\nformance of SOTA FSCIL methods is limited by a margin\nof9.09% under DIY-FSCIL setup.\nTo summarise, our framework helps in solving the chal-\nlenging DIY-FSCIL problem by both alleviating the catas-\ntrophic forgetting of the old classes and enhancing the learn-\ning of the new classes under a cross-modal sketch-based few\nshot setting. Moreover, the proposed framework effectively\nenables the users to build their own novel classes with the\nsupport of their imaginative drawings.\n4.4. Further Analysis and Insights\nAblation Study: We further dive deeper to gure out the\ncontribution of individual design components in Table 2.\n(i)GAT: To access the importance of weight renement,\nwe remove the GAT module and adapt the framework ac-\ncordingly. Consequently Acc@novel signicantly drops to\n62.34% with a decrease of 3.5%for5-shot case, and is more\npronounced for 1-shot context, where we perceive larger a\ndrop of 4.17%. This observation further strengthens our ini-\ntial assumption that GAT models an effective mutual agree-\nment strategy for learning discriminative decision bound-\naries across all the Kb+Knclasses. (ii) Gradient Con-\nsensus (GC): The use of GC improves the models perfor-\n2299\n'), Document(page_content='ting. This paper is mainly concerned with CIL setup, which\nis the most challenging task among its variants.\nFew-Shot Class-Incremental Learning (FSCIL): Few\nshot learning (FSL) aims at adapting a trained model to\nlearn patterns from novel classes (unseen during training)\nusing only a few labelled samples [61]. Recently, it has\nexperienced rapid proliferation [40, 50, 58] in the research\ncommunity. There are three major swim lanes of the FSL\nproblem: (a) recurrent-based [40, 48] (b) optimisation-\nbased [43, 59], and (c) metric-based frameworks [17, 24].\nOur work falls under metric-based methods in which sim-\nilarity is drawn between the query sample and the novel\nsupport classes. Conventional CIL presumes that the in-\ncrementally provided novel classes have access to a sub-\nstantial amount of labelled data. Although in the FSCIL\nparadigm [54], the initial dataset contains sufcient training\ndata (base classes), the subsequently provided novel classes\ncontain only a few labelled samples. Very few methods are\npresent to tackle the FSCIL problem like, pseudo incremen-\ntal learning [70], knowledge distillation [9, 13], neural-gas\nnetwork [54]. While existing works intend to build a model\nto incrementally learn novel classes, we aim at building a\nmodel for a much harder and practically applicable sketch-\nbased FSCIL setting that addresses users privacy concerns.\nMinimising Domain Discrepancy: Minimising sketch-\nphoto domain discrepancy [12] is the key in our problem\nsetup. In this context, the two most relevant branch of lit-\nerature involves Domain Adaptation (DA) [15] and Domain\nGeneralisation (DG) [27, 26]. While DA intends to adapt a\nmodel trained on a source domain to perform well on a new\ntarget domain using only unlabelled images, the aim of DG\nis to generalise a model from a set of seen domain samples\ntounseen domain samples without accessing the unseen do-\nmain instances. Our objective is more aligned with DG as\nwe do not update the model parameters during inference.\nIn this work, we take inspiration from the recent develop-\nments [69, 32] in DG to learn a domain-agnostic network,\nminimising the domain gap between sketch and photo.\n3. Sketch for Incremental Learning\n3.1. Problem Denition\nDataset: In few-shot class-incremental learning, we are\ngiven withKbbase classes and Knnovel classes respec-\ntively. From the set of base classes, we have sufcient ac-\ncess to labelled samples from photoDP\nbase={(pi,yp\ni)}Ns\nb\ni=1\nandsketchDS\nbase ={(si,ys\ni)}Ns\nb\ni=1domains, where yi\nCbase={Cb\n1,Cb\n2,,Cb\nKb}. On the other side, for novel\nclasses, we have minimal access to labelled samples from\nonly sketch domainDS\nnovel ={(sj,yj)}Ns\nn\nj=1where num-\nber of samples for each novel category is limited, and\nyjCnovel ={Cn\n1,Cn\n2,,Cn\nKn}. Here, base and novel\nclasses are completely disjoint, so that CbaseCnovel = .Model: We have a neural network classier, compris-\ning of a feature extractor Ffollowed by linear classier\nRw, such thaty=Rw(F(x)).Fis employed using a\nconvolutional neural network followed by global-average\npooling, and given an input image xRhw3, we get\na feature representation as fd=F(x)Rd. Follow-\ning [17], for better generalisation Rwis devised as a co-\nsine similarity function (unlike dot product based typical\nlinear classier), consisting a learnable Wmatrix whose\nsize is of R|C|d, where|C|is the number of classes. Thus,\nRw:RdR|C|outputs a probability distribution over\nclasses asp(y) =softmax (Wfd\nfd2).Wis obtained\nbyl2normalising every ddimensional row-vector wkW\nthat depicts weight-vector for kthclass, i.e.wk=wk\nwk2.\nLearning Objective: The neural network classier\n{F,Rw}is trained from the abundant labelled samples of\nKbbase classes, and let the initial base classier be Rbase\nw:\nRdRKbwhose weight matrix is WbaseRKbd. Dur-\ning inference under FSCIL [54], we do not have any access\nto labelled data of base classes, and given only k(small\nnumber) sketch samples for each of Cnnovel categories,\nwe intend to update the classier Rbase\nw toRnew\nw which\ncan recognise photos from bothCbaseCnovel classes. To\ndo so, we need to compute a new weight matrix Wnew\nR(Kb+Kn)dwith respect toRnew\nw:RdR(Kb+Kn)that\ncan perform (Kb+Kn)-way class classication.\nTherefore, our objective is to gure out a new Wnew\nmatrix for classier Rnew\nwusing the previous base classes\nknowledgeWbase and a few hand-drawn sketch exemplars\nfrom novel classes such that (i) the knowledge of base\nclasses is not forgotten (preserved), as well as (ii) it quickly\nadapts to novel classes using few samples, (iii) thus, en-\nabling it to perform well on real photos minimising the do-\nmain gap [26] with sketch samples from novel classes as\nsupport. Overall, our framework consists of three mod-\nules (i) a backbone feature extractorF, (ii) a classier\nRw(iii) a weight generator Gthat will take previous base\nclassier weights Wbaseand sketch exemplars (support set)\nfrom novel classes as input, to generate a new weight ma-\ntrixWnewfor updated classier Rbase\nwRnew\nwin order to\nclassify real photos from both base and novel classes.\n3.2. Cross Modal Pre-Training for Base Classes\nOur framework follows a two-stage training. In the rst\nstage, we train the model for base classes using standard\ncross-entropy loss, while in the second stage, we learn the\nweight generator via few-shot pseudo-incremental learning.\nOnce trained, we freeze the weights of Fin the next stage\nto (i) avoid over-tting during the few-shot update and (ii)\nto alleviate catastrophic forgetting [23] of the base classes.\nUnlike existing few-shot incremental learning, we need\nto handle the domain gap [27] between photos and sketches,\nso that the knowledge of incremental classes acquired\n2295\n'), Document(page_content='the class based initialization performs best. At\nsmaller model sizes, there are large gaps between\nthe different initializations, but once the model is\nscaled to XXL size, those differences disappear.\nWith class label initialization, we observe that\nthe class labels typically persist in the learned\nprompts, such that the nearest token embeddings\n(in cosine distance) match the tokens used for ini-\ntialization. Beyond this, we did not nd our learned\nprompts to be interpretable, similar to those of Shin\net al. (2020). See Section 7 for details.\nPre-training Objective In Figures 3(c) and 3(d),\nwe see pre-training objective has a clear effect on\nprompt tuning quality. As hypothesized in Sec-\ntion 2.2, T5s default span corruption objective\nis not well-suited for training frozen models to be\nlater conditioned by prompts. Intuitively, models\npre-trained to read and write sentinel tokens are\nhard to apply directly to tasks of reading and writ-\ning text without sentinels. As seen in Figure 3(c),\neven the workaround of adding a sentinel to the\ndownstream targets has little benet. While LM\nadaptation adds value across all model sizes, we\nnote our largest XXL model is the most forgiving\nand gives strong results even with span corruption.\nGiven the benet of LM adaptation, we also\nexplore how long of an adaptation is helpful. Fig-\nure 3(d) shows that longer adaptation provides ad-\nditional gains, up to 100K steps. This suggests\nthat the transition from span corruption to a lan-\nguage modeling objective is not a trivial change,\nand making an effective switch takes an investment\nof training resources ( 10% of the steps of the orig-\ninal T5 pre-training). At the same time, as in our\nother ablations, we observe that the XXL model\nis robust to even non-ideal congurations. At this\nsize, the gains from adaptation are quite modest.\nIn the non-optimal span corruption setting, we\nobserve instability across model sizes, with the\nSmall model outperforming the larger Base, Large,\nand XL models. On inspection, we nd that for\nmany tasks, these mid-sized models never learn to\noutput a legal class label and thus score 0%. The\ntwo most common error modes are copying sub-\nspans from the input and predicting an empty string.\nFurthermore, this poor performance is not due to\nrandom variance in prompt tuning, as we observe\nlow variance across 3runs for each size. These\nresults indicate that using models pre-trained with\nthe span corruption objective can be unreliable,\nwith only 2out of 5models working well, whereas\n1081091010\nModel Parameters103105107109Task Parameters\nModel Tuning\nPrefix Tuning (Train)\nPrefix Tuning (Infer)WARP\nPrompt Tuning\nPrompt Design\n0.001%0.01%0.1%1%10%100%Task Parameters (%)Figure 4: Parameter usage of various adaptation tech-\nniques, xing architecture to T5.1.1 and prompt/prex\nlength to 1100tokens (bands show mean and stddev).\nModel Tuning : All parameters are task-specic. Pre-\nx Tuning : Activations are tuned in the prex of each\nlayer, requiring 0.11% task-specic parameters for in-\nference, but more are used for training. WARP : Task\nparameters are reduced to under 0.1% by only tuning\ninput and output layers. Prompt Tuning : Only prompt\nembeddings are tuned, reaching under 0.01% for most\nmodel sizes. Prompt Design : Only a sequence of\nprompt IDs ( 5002000 tokens) is required.\nthe LM adapated versions work reliably across all\nmodel sizes.\nWe have released T5 1.1 checkpoints adapted\nusing the LM objective for 100K steps for all model\nsizes.8\n4 Comparison to Similar Approaches\nIn this section, we review recent work on learn-\ning continuous prompts, and draw comparisons\nwith our method. One important axis of compari-\nson is the number of task-specic parameters each\nmethod requires, as shown in Figure 4. Among\nmethods with learnable parameters, prompt tuning\nis the most parameter efcient, requiring less than\n0.01% task-specic parameters for models over a\nbillion parameters.9\nLi and Liang (2021) propose prex tuning:\nlearning a sequence of prexes that are prepended\n8https://github.com/google-research/\ntext-to-text-transfer-transformer/\nblob/main/released_checkpoints.md#\nlm-adapted-t511lm100k\n9To compare with prompt design, we count each token\nID in the prompt as a parameter, and assume a prompt of\nbetween 5002000 tokens to match the GPT-3 setting. While\nthis technique is by far the most parameter efcient, it comes\nat the cost of task quality.'), Document(page_content='the class based initialization performs best. At\nsmaller model sizes, there are large gaps between\nthe different initializations, but once the model is\nscaled to XXL size, those differences disappear.\nWith class label initialization, we observe that\nthe class labels typically persist in the learned\nprompts, such that the nearest token embeddings\n(in cosine distance) match the tokens used for ini-\ntialization. Beyond this, we did not nd our learned\nprompts to be interpretable, similar to those of Shin\net al. (2020). See Section 7 for details.\nPre-training Objective In Figures 3(c) and 3(d),\nwe see pre-training objective has a clear effect on\nprompt tuning quality. As hypothesized in Sec-\ntion 2.2, T5s default span corruption objective\nis not well-suited for training frozen models to be\nlater conditioned by prompts. Intuitively, models\npre-trained to read and write sentinel tokens are\nhard to apply directly to tasks of reading and writ-\ning text without sentinels. As seen in Figure 3(c),\neven the workaround of adding a sentinel to the\ndownstream targets has little benet. While LM\nadaptation adds value across all model sizes, we\nnote our largest XXL model is the most forgiving\nand gives strong results even with span corruption.\nGiven the benet of LM adaptation, we also\nexplore how long of an adaptation is helpful. Fig-\nure 3(d) shows that longer adaptation provides ad-\nditional gains, up to 100K steps. This suggests\nthat the transition from span corruption to a lan-\nguage modeling objective is not a trivial change,\nand making an effective switch takes an investment\nof training resources ( 10% of the steps of the orig-\ninal T5 pre-training). At the same time, as in our\nother ablations, we observe that the XXL model\nis robust to even non-ideal congurations. At this\nsize, the gains from adaptation are quite modest.\nIn the non-optimal span corruption setting, we\nobserve instability across model sizes, with the\nSmall model outperforming the larger Base, Large,\nand XL models. On inspection, we nd that for\nmany tasks, these mid-sized models never learn to\noutput a legal class label and thus score 0%. The\ntwo most common error modes are copying sub-\nspans from the input and predicting an empty string.\nFurthermore, this poor performance is not due to\nrandom variance in prompt tuning, as we observe\nlow variance across 3runs for each size. These\nresults indicate that using models pre-trained with\nthe span corruption objective can be unreliable,\nwith only 2out of 5models working well, whereas\n1081091010\nModel Parameters103105107109Task Parameters\nModel Tuning\nPrefix Tuning (Train)\nPrefix Tuning (Infer)WARP\nPrompt Tuning\nPrompt Design\n0.001%0.01%0.1%1%10%100%Task Parameters (%)Figure 4: Parameter usage of various adaptation tech-\nniques, xing architecture to T5.1.1 and prompt/prex\nlength to 1100tokens (bands show mean and stddev).\nModel Tuning : All parameters are task-specic. Pre-\nx Tuning : Activations are tuned in the prex of each\nlayer, requiring 0.11% task-specic parameters for in-\nference, but more are used for training. WARP : Task\nparameters are reduced to under 0.1% by only tuning\ninput and output layers. Prompt Tuning : Only prompt\nembeddings are tuned, reaching under 0.01% for most\nmodel sizes. Prompt Design : Only a sequence of\nprompt IDs ( 5002000 tokens) is required.\nthe LM adapated versions work reliably across all\nmodel sizes.\nWe have released T5 1.1 checkpoints adapted\nusing the LM objective for 100K steps for all model\nsizes.8\n4 Comparison to Similar Approaches\nIn this section, we review recent work on learn-\ning continuous prompts, and draw comparisons\nwith our method. One important axis of compari-\nson is the number of task-specic parameters each\nmethod requires, as shown in Figure 4. Among\nmethods with learnable parameters, prompt tuning\nis the most parameter efcient, requiring less than\n0.01% task-specic parameters for models over a\nbillion parameters.9\nLi and Liang (2021) propose prex tuning:\nlearning a sequence of prexes that are prepended\n8https://github.com/google-research/\ntext-to-text-transfer-transformer/\nblob/main/released_checkpoints.md#\nlm-adapted-t511lm100k\n9To compare with prompt design, we count each token\nID in the prompt as a parameter, and assume a prompt of\nbetween 5002000 tokens to match the GPT-3 setting. While\nthis technique is by far the most parameter efcient, it comes\nat the cost of task quality.'), Document(page_content='Doodle It Yourself: Class Incremental Learning by Drawing a Few Sketches\nAyan Kumar Bhunia1Viswanatha Reddy Gajjala*Subhadeep Koley1,2Rohit Kundu*\nAneeshan Sain1,2Tao Xiang1,2Yi-Zhe Song1,2\n1SketchX, CVSSP, University of Surrey, United Kingdom.\n2iFlyTek-Surrey Joint Research Centre on Articial Intelligence.\n{a.bhunia, s.koley, a.sain, t.xiang, y.song }@surrey.ac.uk; viswanathareddy998@gmail.com\nAbstract\nThe human visual system is remarkable in learning new\nvisual concepts from just a few examples. This is precisely\nthe goal behind few-shot class incremental learning (FS-\nCIL), where the emphasis is additionally placed on ensuring\nthe model does not suffer from forgetting. In this paper,\nwe push the boundary further for FSCIL by addressing two\nkey questions that bottleneck its ubiquitous application (i)\ncan the model learn from diverse modalities other than just\nphoto (as humans do), and (ii) what if photos are not read-\nily accessible (due to ethical and privacy constraints). Our\nkey innovation lies in advocating the use of sketches as a\nnew modality for class support. The product is a Doodle\nIt Yourself (DIY) FSCIL framework where the users can\nfreely sketch a few examples of a novel class for the model to\nlearn to recognise photos of that class. For that, we present\na framework that infuses (i) gradient consensus for domain\ninvariant learning, (ii) knowledge distillation for preserving\nold class information, and (iii) graph attention networks for\nmessage passing between old and novel classes. We exper-\nimentally show that sketches are better class support than\ntext in the context of FSCIL, echoing ndings elsewhere in\nthe sketching literature.\n1. Introduction\nFully supervised learning has served us great with per-\nformances on ImageNet already surpassing human-level\n[18]. In reality, however, such progress is primarily lim-\nited to a small number of object classes where labels were\nexplicitly curated (1000 in ImageNet vs. possibly millions\nout there). Class Incremental Learning [29, 21, 23] is one of\nthe popular fronts that attempt to extend model perception\nto novel classes while not forgetting about classes learned\nalready. Amongst its many variants, the recent Few-Shot\nClass Incremental Learning (FSCIL) [54] is the most realis-\ntic where it also dictates the model to learn new classes with\nvery few examples, the same as humans do.\n*Interned with SketchX\n(10+3) Class Classifier\nFew-Shot  \nIncremental Learning\n10 Class Classifier\nDoodle It Y ouself\nBase Class Photos Base+Novel Class Photos\n1 2 10 1 2 10 11 12 13Figure 1. Illustration of our DIY-FSCIL framework. For instance,\ngiven sketch exemplars (1-shot here) from 3novel classes as\nsupport-set, a 10-class classier gets updated to (10 + 3) -class\nclassier that can classify photos from both base and novel classes.\nAs easy as providing a few samples might sound, ques-\ntions start to emerge in practice as to (i) what data modality\nshould the samples take? and (ii) how could these samples\nbe obtained in practice. These questions, we argue, are key\nto the potentially ubiquitous application of FSCIL as (i) hu-\nmans also learn from a broad range of data modalities that\nare not limited to just photo, and (ii) there are scenarios\nwhere photos are not necessarily always readily available\ndue to privacy and ethical constraints (e.g., copyright).\nIn this paper, we set out to study the role of human\nsketches as a support modality for FSCIL. This results in\na exible FSCIL system that learns new classes just by ob-\nserving a few sketches doodled by users themselves. Fig. 1\nillustrates schematically our Doodle It Yourself (DIY)\nFSCIL scenario  DIY-FSCIL. This importantly ad-\ndresses the aforementioned problems in that (i) learning\nis no longer xed to just photos but exibly cross-modal\nwith other data forms (just as humans do), and (ii) it works\nwithout asking the users to source photos which might have\npractical constraints attached (e.g., copyright, hazardous en-\nvironments). There is of course also the added benet of\ninjecting creativity to the classier by sketching something\noff the users imagination [16], e.g., a ying cow?\nThe advocate of sketches is largely motivated by the\nline of work examining human-centric characteristics of\nsketches in many parallel applications  notably image re-\n2293\n'), Document(page_content='The Power of Scale for Parameter-Efcient Prompt Tuning\nBrian LesterRami Al-Rfou Noah Constant\nGoogle Research\n{brianlester,rmyeid,nconstant}@google.com\nAbstract\nIn this work, we explore prompt tuning,\na simple yet effective mechanism for learn-\ning soft prompts to condition frozen lan-\nguage models to perform specic downstream\ntasks. Unlike the discrete text prompts used by\nGPT-3, soft prompts are learned through back-\npropagation and can be tuned to incorporate\nsignals from any number of labeled examples.\nOur end-to-end learned approach outperforms\nGPT-3s few-shot learning by a large margin.\nMore remarkably, through ablations on model\nsize using T5, we show that prompt tuning be-\ncomes more competitive with scale: as mod-\nels exceed billions of parameters, our method\ncloses the gap and matches the strong per-\nformance of model tuning (where all model\nweights are tuned). This nding is especially\nrelevant because large models are costly to\nshare and serve and the ability to reuse one\nfrozen model for multiple downstream tasks\ncan ease this burden. Our method can be seen\nas a simplication of the recently proposed\nprex tuning of Li and Liang (2021) and we\nprovide a comparison to this and other similar\napproaches. Finally, we show that condition-\ning a frozen model with soft prompts confers\nbenets in robustness to domain transfer and\nenables efcient prompt ensembling.\n1 Introduction\nWith the wide success of pre-trained large lan-\nguage models, a range of techniques has arisen to\nadapt these general-purpose models to downstream\ntasks. ELMo (Peters et al., 2018) proposed freezing\nthe pre-trained model and learning a task-specic\nweighting of its per-layer representations. How-\never, since GPT (Radford et al., 2018) and BERT\n(Devlin et al., 2019), the dominant adaptation tech-\nnique has been model tuning (or ne-tuning),\nwhere all model parameters are tuned during adap-\ntation, as proposed by Howard and Ruder (2018).\nWork done as a Google AI Resident.\n10810910101011\nModel Parameters5060708090100SuperGLUE Score\nModel Tuning\nModel Tuning (Multi-task)Prompt Design\nPrompt TuningFigure 1: Standard model tuning of T5 achieves strong\nperformance, but requires storing separate copies of the\nmodel for each end task. Our prompt tuning of T5\nmatches the quality of model tuning as size increases,\nwhile enabling the reuse of a single frozen model for\nall tasks. Our approach signicantly outperforms few-\nshot prompt design using GPT-3. We show mean and\nstandard deviation across 3runs for tuning methods.\nMore recently, Brown et al. (2020) showed that\nprompt design (or priming) is surprisingly effec-\ntive at modulating a frozen GPT-3 models behavior\nthrough text prompts. Prompts are typically com-\nposed of a task description and/or several canonical\nexamples. This return to freezing pre-trained\nmodels is appealing, especially as model size con-\ntinues to increase. Rather than requiring a separate\ncopy of the model for each downstream task, a\nsingle generalist model can simultaneously serve\nmany different tasks.\nUnfortunately, prompt-based adaptation has sev-\neral key drawbacks. Task description is error-prone\nand requires human involvement, and the effective-\nness of a prompt is limited by how much condition-\ning text can t into the models input. As a result,\ndownstream task quality still lags far behind that\nof tuned models. For instance, GPT-3 175B few-\nshot performance on SuperGLUE is 17.5points be-arXiv:2104.08691v2  [cs.CL]  2 Sep 2021')]
2023-12-07 15:58:54,994 - INFO - Processed the request successfully
2023-12-07 15:59:40,439 - INFO - classes: ['paper']: %s
2023-12-07 15:59:40,553 - INFO - classes: ['paper']: %s
2023-12-07 15:59:52,123 - INFO - classes: ['paper']: %s
2023-12-07 15:59:52,270 - INFO - classes: ['paper']: %s
2023-12-07 15:59:52,301 - INFO - Received requests to /inference endpoint
2023-12-07 15:59:52,402 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 15:59:52,403 - INFO - Received request: {'username': 'amin', 'prompt': 'what are the novilities of A Foundational Language And Vision Alignment Model paper?', 'memory': True, 'conversation_number': 8, 'AI_assistance': False, 'collection_name': 'paper', 'llm_model': 'Llama_13b'}
2023-12-07 15:59:52,439 - INFO - Retrieved docs: [Document(page_content='FLA V A: A Foundational Language And Vision Alignment Model\nAmanpreet Singh*Ronghang Hu*Vedanuj Goswami*\nGuillaume Couairon Wojciech Galuba Marcus Rohrbach Douwe Kiela\nFacebook AI Research (FAIR)\nAbstract\nState-of-the-art vision and vision-and-language models\nrely on large-scale visio-linguistic pretraining for obtaining\ngood performance on a variety of downstream tasks. Gener-\nally, such models are often either cross-modal (contrastive)\nor multi-modal (with earlier fusion) but not both; and they\noften only target specic modalities or tasks. A promising\ndirection would be to use a single holistic universal model,\nas a foundation, that targets all modalities at oncea\ntrue vision and language foundation model should be good\nat vision tasks, language tasks, and cross- and multi-modal\nvision and language tasks. We introduce FLAVA as such a\nmodel and demonstrate impressive performance on a wide\nrange of 35 tasks spanning these target modalities.\n1. Introduction\nLarge-scale pre-training of vision and language trans-\nformers has led to impressive performance gains in a wide\nvariety of downstream tasks. In particular, contrastive meth-\nods such as CLIP [83] and ALIGN [50] have shown that\nnatural language supervision can lead to very high quality\nvisual models for transfer learning.\nPurely contrastive methods, however, also have impor-\ntant shortcomings. Their cross-modal nature does not make\nthem easily usable on multimodal problems that require\ndealing with both modalities at the same time. They re-\nquire large corpora, which for both CLIP and ALIGN have\nnot been made accessible to the research community and\nthe details of which remain shrouded in mystery, notwith-\nstanding well-known issues with the construction of such\ndatasets [9].\nIn contrast, the recent literature is rich with transformer\nmodels that explicitly target the multimodal vision-and-\nlanguage domain by having earlier fusion and shared self-\nattention across modalities. For those cases, however, the\nunimodal vision-only or language-only performance of the\nmodel is often either glossed over or ignored completely.\nIf the future of our eld lies in generalized foundation\nmodels [10] or universal transformers [72] with many\n*Equal contribution.\nmultimodalandunimodalpretrainingdata\nunpairedtext\nunpairedimages\nimage-textpairs\nFLAVAfor multi-domain joint pretrainingglobal contrastive, MMM,MIM,MLM,)\nvisualrecognition(e.g.ImageNet)\nmultimodalreasoning(e.g.VQA)\nlanguageunderstanding(e.g.GLUE)Figure 1. We present FLA V A, a language and vision alignment\nmodel that learns strong representations from multimodal (image-\ntext pairs) and unimodal data (unpaired images and text) and can\nbe applied to target a broad scope of tasks from three domains\n(visual recognition, language understanding, and multimodal rea-\nsoning) under a common transformer model architecture.\ndifferent capabilities, then the following limitation should\nbe overcome: a true foundation model in the vision and lan-\nguage space should not only be good at vision, or language,\nor vision-and-language problemsit should be good at all\nthree, at the same time.\nCombining information from different modalities into\none universal architecture holds promise not only because it\nis similar to how humans make sense of the world, but also\nbecause it may lead to better sample efciency and much\nricher representations.\nIn this work, we introduce FLA V A, a foundational lan-\nguage and vision alignment model that explicitly targets\nvision, language, and their multimodal combination all at\nonce. FLA V A learns strong representations through joint\npretraining on both unimodal and multimodal data while en-\ncompassing cross-modal alignment objectives and multi-\nmodal fusion objectives. We validate FLA V A by applying\nit to 35 tasks across vision, NLP, and multimodal domains\nand show impressive performance. An important advantage\nof our approach is that it was trained on a corpus of openly\navailable datasets that is an order of magnitude smaller than\ndatasets used in comparable models. Our models and code\nare available in https://flava-model.github.io/ .arXiv:2112.04482v3  [cs.CV]  29 Mar 2022'), Document(page_content='FLA V A: A Foundational Language And Vision Alignment Model\nAmanpreet Singh*Ronghang Hu*Vedanuj Goswami*\nGuillaume Couairon Wojciech Galuba Marcus Rohrbach Douwe Kiela\nFacebook AI Research (FAIR)\nAbstract\nState-of-the-art vision and vision-and-language models\nrely on large-scale visio-linguistic pretraining for obtaining\ngood performance on a variety of downstream tasks. Gener-\nally, such models are often either cross-modal (contrastive)\nor multi-modal (with earlier fusion) but not both; and they\noften only target specic modalities or tasks. A promising\ndirection would be to use a single holistic universal model,\nas a foundation, that targets all modalities at oncea\ntrue vision and language foundation model should be good\nat vision tasks, language tasks, and cross- and multi-modal\nvision and language tasks. We introduce FLAVA as such a\nmodel and demonstrate impressive performance on a wide\nrange of 35 tasks spanning these target modalities.\n1. Introduction\nLarge-scale pre-training of vision and language trans-\nformers has led to impressive performance gains in a wide\nvariety of downstream tasks. In particular, contrastive meth-\nods such as CLIP [83] and ALIGN [50] have shown that\nnatural language supervision can lead to very high quality\nvisual models for transfer learning.\nPurely contrastive methods, however, also have impor-\ntant shortcomings. Their cross-modal nature does not make\nthem easily usable on multimodal problems that require\ndealing with both modalities at the same time. They re-\nquire large corpora, which for both CLIP and ALIGN have\nnot been made accessible to the research community and\nthe details of which remain shrouded in mystery, notwith-\nstanding well-known issues with the construction of such\ndatasets [9].\nIn contrast, the recent literature is rich with transformer\nmodels that explicitly target the multimodal vision-and-\nlanguage domain by having earlier fusion and shared self-\nattention across modalities. For those cases, however, the\nunimodal vision-only or language-only performance of the\nmodel is often either glossed over or ignored completely.\nIf the future of our eld lies in generalized foundation\nmodels [10] or universal transformers [72] with many\n*Equal contribution.\nmultimodalandunimodalpretrainingdata\nunpairedtext\nunpairedimages\nimage-textpairs\nFLAVAfor multi-domain joint pretrainingglobal contrastive, MMM,MIM,MLM,)\nvisualrecognition(e.g.ImageNet)\nmultimodalreasoning(e.g.VQA)\nlanguageunderstanding(e.g.GLUE)Figure 1. We present FLA V A, a language and vision alignment\nmodel that learns strong representations from multimodal (image-\ntext pairs) and unimodal data (unpaired images and text) and can\nbe applied to target a broad scope of tasks from three domains\n(visual recognition, language understanding, and multimodal rea-\nsoning) under a common transformer model architecture.\ndifferent capabilities, then the following limitation should\nbe overcome: a true foundation model in the vision and lan-\nguage space should not only be good at vision, or language,\nor vision-and-language problemsit should be good at all\nthree, at the same time.\nCombining information from different modalities into\none universal architecture holds promise not only because it\nis similar to how humans make sense of the world, but also\nbecause it may lead to better sample efciency and much\nricher representations.\nIn this work, we introduce FLA V A, a foundational lan-\nguage and vision alignment model that explicitly targets\nvision, language, and their multimodal combination all at\nonce. FLA V A learns strong representations through joint\npretraining on both unimodal and multimodal data while en-\ncompassing cross-modal alignment objectives and multi-\nmodal fusion objectives. We validate FLA V A by applying\nit to 35 tasks across vision, NLP, and multimodal domains\nand show impressive performance. An important advantage\nof our approach is that it was trained on a corpus of openly\navailable datasets that is an order of magnitude smaller than\ndatasets used in comparable models. Our models and code\nare available in https://flava-model.github.io/ .arXiv:2112.04482v3  [cs.CV]  29 Mar 2022'), Document(page_content='FLA V A: A Foundational Language And Vision Alignment Model\nAmanpreet Singh*Ronghang Hu*Vedanuj Goswami*\nGuillaume Couairon Wojciech Galuba Marcus Rohrbach Douwe Kiela\nFacebook AI Research (FAIR)\nAbstract\nState-of-the-art vision and vision-and-language models\nrely on large-scale visio-linguistic pretraining for obtaining\ngood performance on a variety of downstream tasks. Gener-\nally, such models are often either cross-modal (contrastive)\nor multi-modal (with earlier fusion) but not both; and they\noften only target specic modalities or tasks. A promising\ndirection would be to use a single holistic universal model,\nas a foundation, that targets all modalities at oncea\ntrue vision and language foundation model should be good\nat vision tasks, language tasks, and cross- and multi-modal\nvision and language tasks. We introduce FLAVA as such a\nmodel and demonstrate impressive performance on a wide\nrange of 35 tasks spanning these target modalities.\n1. Introduction\nLarge-scale pre-training of vision and language trans-\nformers has led to impressive performance gains in a wide\nvariety of downstream tasks. In particular, contrastive meth-\nods such as CLIP [83] and ALIGN [50] have shown that\nnatural language supervision can lead to very high quality\nvisual models for transfer learning.\nPurely contrastive methods, however, also have impor-\ntant shortcomings. Their cross-modal nature does not make\nthem easily usable on multimodal problems that require\ndealing with both modalities at the same time. They re-\nquire large corpora, which for both CLIP and ALIGN have\nnot been made accessible to the research community and\nthe details of which remain shrouded in mystery, notwith-\nstanding well-known issues with the construction of such\ndatasets [9].\nIn contrast, the recent literature is rich with transformer\nmodels that explicitly target the multimodal vision-and-\nlanguage domain by having earlier fusion and shared self-\nattention across modalities. For those cases, however, the\nunimodal vision-only or language-only performance of the\nmodel is often either glossed over or ignored completely.\nIf the future of our eld lies in generalized foundation\nmodels [10] or universal transformers [72] with many\n*Equal contribution.\nmultimodalandunimodalpretrainingdata\nunpairedtext\nunpairedimages\nimage-textpairs\nFLAVAfor multi-domain joint pretrainingglobal contrastive, MMM,MIM,MLM,)\nvisualrecognition(e.g.ImageNet)\nmultimodalreasoning(e.g.VQA)\nlanguageunderstanding(e.g.GLUE)Figure 1. We present FLA V A, a language and vision alignment\nmodel that learns strong representations from multimodal (image-\ntext pairs) and unimodal data (unpaired images and text) and can\nbe applied to target a broad scope of tasks from three domains\n(visual recognition, language understanding, and multimodal rea-\nsoning) under a common transformer model architecture.\ndifferent capabilities, then the following limitation should\nbe overcome: a true foundation model in the vision and lan-\nguage space should not only be good at vision, or language,\nor vision-and-language problemsit should be good at all\nthree, at the same time.\nCombining information from different modalities into\none universal architecture holds promise not only because it\nis similar to how humans make sense of the world, but also\nbecause it may lead to better sample efciency and much\nricher representations.\nIn this work, we introduce FLA V A, a foundational lan-\nguage and vision alignment model that explicitly targets\nvision, language, and their multimodal combination all at\nonce. FLA V A learns strong representations through joint\npretraining on both unimodal and multimodal data while en-\ncompassing cross-modal alignment objectives and multi-\nmodal fusion objectives. We validate FLA V A by applying\nit to 35 tasks across vision, NLP, and multimodal domains\nand show impressive performance. An important advantage\nof our approach is that it was trained on a corpus of openly\navailable datasets that is an order of magnitude smaller than\ndatasets used in comparable models. Our models and code\nare available in https://flava-model.github.io/ .arXiv:2112.04482v3  [cs.CV]  29 Mar 2022'), Document(page_content='MethodMultimodal Pretraining data Pretraining Objectives Target Modalities\npublic dataset(s) size Contr. ITM Masking Unimodal V CV&L MV&L L\nCLIP [83] \x17WebImageText 400M \x13    \x13 \x13  \nALIGN [50] \x17 JFT 1.8B \x13    \x13 \x13  \nSimVLM [109] \x17 JFT 1.8B   PrexLM CLM * \x13 \x13 \x13\nUniT [43]  None      *  \x13 \x13\nVinVL [118] \x13 Combination 9M \x13  MLM   \x13 \x13 \nViLT [54] \x13 Combination 10M  \x13 MLM   \x13 \x13 \nALBEF [62] \x13 Combination 5M \x13 \x13 MLM   \x13 \x13 \nFLA V A (ours) \x13 PMD (Tbl. 2) 70M \x13 \x13 MMM MLM+MIM \x13 \x13 \x13 \x13\nTable 1. Comparison of recent models in different modalities. CV&L and MV&L stands for cross-modal and multi-modal vision-and-\nlanguage. * means the modality is partially targeted (SimVLM [109] and UniT [43] include ImageNet and object detection, respectively).\n2. Background\nThe self-supervised pretraining paradigm has signi-\ncantly advanced the state of the art across various domains,\nfrom natural language processing [6, 1719, 23, 24, 28, 30,\n61, 68, 73, 8385], to computer vision [2, 5, 8, 12, 31, 33, 37,\n59, 75, 104, 117], to speech recognition [4, 22, 42, 67, 119]\nand multimodal domains such as vision and language un-\nderstanding [12,16,34,4345,50,6265,70,71,95,101,102,\n109,116,118,120]. Even though this progress is based on a\nshared recipe of self-supervised learning on top of trans-\nformers, we are still missing major progress in building\nfoundational models [10] that work well across all of these\ndifferent domains and modalities at once.\nTable 1 shows an extensive comparison of popular and\nrecent models w.r.t. our FLA V A on multiple axes. Recent\nwork either (i) focuses on a single target domain [54, 118];\n(ii) targets a specic unimodal domain along with the joint\nvision-and-language domain [50, 83]; or (iii) targets all do-\nmains but only a specic set of tasks in a particular domain.\nSimVLM [109], ALIGN [50], and CLIP [83] have\ndemonstrated impressive gains by training transformer-\nbased models on giant private paired image-and-text cor-\npora, as opposed to the previous vision-and-language state-\nof-the-art such as VinVL [118] and ViLT [54], which were\ntrained on smaller public paired datasets [15,57,66,77,92].\nGenerally, models in the vision-and-language space can\nbe divided into two categories: (i) dual encoders where the\nimage and text are encoded separately followed by a shal-\nlow interaction layer for downstream tasks [50, 83]; and\n(ii) fusion encoder(s) with self-attention spanning across\nthe modalities [16, 34, 44, 45, 6265, 70, 71, 101, 102, 109,\n118, 120]. The dual encoder approach works well for uni-\nmodal [107, 108] and cross-modal retrieval tasks [66, 81]\nbut their lack of fusion usually causes them to underperform\non tasks that involve visual reasoning and question answer-\ning [39, 53, 93, 96] which is where models based on fusion\nencoder(s) shine.\nWithin the fusion encoder category, a further distinction\ncan be made as to whether the model uses a single trans-\nformer for early and unconstrained fusion between modal-\nities ( e.g., VisualBERT, UNITER, VLBERT, OSCAR [16,63, 65, 101, 120]) or allows cross-attention only in specic\nco-attention transformer layers while having some modal-\nity specic layers ( e.g., LXMERT, ViLBERT, ERNIE-ViL\n[70, 71, 102, 116]. Another distinguishing factor between\ndifferent models lies in the image features that are used,\nranging from region features [63, 70, 118], to patch embed-\ndings [54, 62, 109], to convolution or grid features [46, 51].\nDual encoder models use contrastive pretraining to pre-\ndict the correct N paired combinations among N2possi-\nbilities. On the other hand, with fusion encoders, inspired\nby unimodal pretraining schemes such as masked language\nmodeling [28, 68], masked image modeling [5], and causal\nlanguage modeling [84], numerous pretraining tasks have\nbeen explored: (i) Masked Language Modeling (MLM) for\nV&L where masked words in the caption are predicted with\nhelp of the paired image [63, 70, 102]; (ii) prexLM, where\nwith the help of an image, the model tries to complete a cap-\ntion [26, 109]; (iii) image-text matching, where the model\npredicts whether given pair of image and text match or\nnot; and (iv) masked region modeling, where the model re-\ngresses onto the image features or predicts its object class.\nCompared to previous work, our model FLA V A works\non a wide range of tasks in each of the vision, language, and\nvision-and-language domains. FLA V A uses a shared trunk\nwhich was pretrained on only openly available public paired\ndata. FLA V A combines dual and fusion encoder approaches\ninto one holistic model that can be pretrained with our novel\nFLA V A pretraining scheme that leverages pretraining objec-\ntives from both categories. FLA V A is designed to be able to\ntake advantage of unpaired unimodal data along with multi-\nmodal paired data, resulting in a model that can handle uni-\nmodal and retrieval tasks as well as cross-modal and multi-\nmodal vision-and-language tasks.\n3. FLA V A: A Foundational Language And Vi-\nsion Alignment Model\nThe goal of this work is to learn a foundational lan-\nguage and vision representation that enables unimodal vi-\nsion and language understanding as well as multimodal rea-\nsoning, all within a single pre-trained model. We show how\nthis can be achieved with a simple and elegant architecture'), Document(page_content='MethodMultimodal Pretraining data Pretraining Objectives Target Modalities\npublic dataset(s) size Contr. ITM Masking Unimodal V CV&L MV&L L\nCLIP [83] \x17WebImageText 400M \x13    \x13 \x13  \nALIGN [50] \x17 JFT 1.8B \x13    \x13 \x13  \nSimVLM [109] \x17 JFT 1.8B   PrexLM CLM * \x13 \x13 \x13\nUniT [43]  None      *  \x13 \x13\nVinVL [118] \x13 Combination 9M \x13  MLM   \x13 \x13 \nViLT [54] \x13 Combination 10M  \x13 MLM   \x13 \x13 \nALBEF [62] \x13 Combination 5M \x13 \x13 MLM   \x13 \x13 \nFLA V A (ours) \x13 PMD (Tbl. 2) 70M \x13 \x13 MMM MLM+MIM \x13 \x13 \x13 \x13\nTable 1. Comparison of recent models in different modalities. CV&L and MV&L stands for cross-modal and multi-modal vision-and-\nlanguage. * means the modality is partially targeted (SimVLM [109] and UniT [43] include ImageNet and object detection, respectively).\n2. Background\nThe self-supervised pretraining paradigm has signi-\ncantly advanced the state of the art across various domains,\nfrom natural language processing [6, 1719, 23, 24, 28, 30,\n61, 68, 73, 8385], to computer vision [2, 5, 8, 12, 31, 33, 37,\n59, 75, 104, 117], to speech recognition [4, 22, 42, 67, 119]\nand multimodal domains such as vision and language un-\nderstanding [12,16,34,4345,50,6265,70,71,95,101,102,\n109,116,118,120]. Even though this progress is based on a\nshared recipe of self-supervised learning on top of trans-\nformers, we are still missing major progress in building\nfoundational models [10] that work well across all of these\ndifferent domains and modalities at once.\nTable 1 shows an extensive comparison of popular and\nrecent models w.r.t. our FLA V A on multiple axes. Recent\nwork either (i) focuses on a single target domain [54, 118];\n(ii) targets a specic unimodal domain along with the joint\nvision-and-language domain [50, 83]; or (iii) targets all do-\nmains but only a specic set of tasks in a particular domain.\nSimVLM [109], ALIGN [50], and CLIP [83] have\ndemonstrated impressive gains by training transformer-\nbased models on giant private paired image-and-text cor-\npora, as opposed to the previous vision-and-language state-\nof-the-art such as VinVL [118] and ViLT [54], which were\ntrained on smaller public paired datasets [15,57,66,77,92].\nGenerally, models in the vision-and-language space can\nbe divided into two categories: (i) dual encoders where the\nimage and text are encoded separately followed by a shal-\nlow interaction layer for downstream tasks [50, 83]; and\n(ii) fusion encoder(s) with self-attention spanning across\nthe modalities [16, 34, 44, 45, 6265, 70, 71, 101, 102, 109,\n118, 120]. The dual encoder approach works well for uni-\nmodal [107, 108] and cross-modal retrieval tasks [66, 81]\nbut their lack of fusion usually causes them to underperform\non tasks that involve visual reasoning and question answer-\ning [39, 53, 93, 96] which is where models based on fusion\nencoder(s) shine.\nWithin the fusion encoder category, a further distinction\ncan be made as to whether the model uses a single trans-\nformer for early and unconstrained fusion between modal-\nities ( e.g., VisualBERT, UNITER, VLBERT, OSCAR [16,63, 65, 101, 120]) or allows cross-attention only in specic\nco-attention transformer layers while having some modal-\nity specic layers ( e.g., LXMERT, ViLBERT, ERNIE-ViL\n[70, 71, 102, 116]. Another distinguishing factor between\ndifferent models lies in the image features that are used,\nranging from region features [63, 70, 118], to patch embed-\ndings [54, 62, 109], to convolution or grid features [46, 51].\nDual encoder models use contrastive pretraining to pre-\ndict the correct N paired combinations among N2possi-\nbilities. On the other hand, with fusion encoders, inspired\nby unimodal pretraining schemes such as masked language\nmodeling [28, 68], masked image modeling [5], and causal\nlanguage modeling [84], numerous pretraining tasks have\nbeen explored: (i) Masked Language Modeling (MLM) for\nV&L where masked words in the caption are predicted with\nhelp of the paired image [63, 70, 102]; (ii) prexLM, where\nwith the help of an image, the model tries to complete a cap-\ntion [26, 109]; (iii) image-text matching, where the model\npredicts whether given pair of image and text match or\nnot; and (iv) masked region modeling, where the model re-\ngresses onto the image features or predicts its object class.\nCompared to previous work, our model FLA V A works\non a wide range of tasks in each of the vision, language, and\nvision-and-language domains. FLA V A uses a shared trunk\nwhich was pretrained on only openly available public paired\ndata. FLA V A combines dual and fusion encoder approaches\ninto one holistic model that can be pretrained with our novel\nFLA V A pretraining scheme that leverages pretraining objec-\ntives from both categories. FLA V A is designed to be able to\ntake advantage of unpaired unimodal data along with multi-\nmodal paired data, resulting in a model that can handle uni-\nmodal and retrieval tasks as well as cross-modal and multi-\nmodal vision-and-language tasks.\n3. FLA V A: A Foundational Language And Vi-\nsion Alignment Model\nThe goal of this work is to learn a foundational lan-\nguage and vision representation that enables unimodal vi-\nsion and language understanding as well as multimodal rea-\nsoning, all within a single pre-trained model. We show how\nthis can be achieved with a simple and elegant architecture'), Document(page_content='MethodMultimodal Pretraining data Pretraining Objectives Target Modalities\npublic dataset(s) size Contr. ITM Masking Unimodal V CV&L MV&L L\nCLIP [83] \x17WebImageText 400M \x13    \x13 \x13  \nALIGN [50] \x17 JFT 1.8B \x13    \x13 \x13  \nSimVLM [109] \x17 JFT 1.8B   PrexLM CLM * \x13 \x13 \x13\nUniT [43]  None      *  \x13 \x13\nVinVL [118] \x13 Combination 9M \x13  MLM   \x13 \x13 \nViLT [54] \x13 Combination 10M  \x13 MLM   \x13 \x13 \nALBEF [62] \x13 Combination 5M \x13 \x13 MLM   \x13 \x13 \nFLA V A (ours) \x13 PMD (Tbl. 2) 70M \x13 \x13 MMM MLM+MIM \x13 \x13 \x13 \x13\nTable 1. Comparison of recent models in different modalities. CV&L and MV&L stands for cross-modal and multi-modal vision-and-\nlanguage. * means the modality is partially targeted (SimVLM [109] and UniT [43] include ImageNet and object detection, respectively).\n2. Background\nThe self-supervised pretraining paradigm has signi-\ncantly advanced the state of the art across various domains,\nfrom natural language processing [6, 1719, 23, 24, 28, 30,\n61, 68, 73, 8385], to computer vision [2, 5, 8, 12, 31, 33, 37,\n59, 75, 104, 117], to speech recognition [4, 22, 42, 67, 119]\nand multimodal domains such as vision and language un-\nderstanding [12,16,34,4345,50,6265,70,71,95,101,102,\n109,116,118,120]. Even though this progress is based on a\nshared recipe of self-supervised learning on top of trans-\nformers, we are still missing major progress in building\nfoundational models [10] that work well across all of these\ndifferent domains and modalities at once.\nTable 1 shows an extensive comparison of popular and\nrecent models w.r.t. our FLA V A on multiple axes. Recent\nwork either (i) focuses on a single target domain [54, 118];\n(ii) targets a specic unimodal domain along with the joint\nvision-and-language domain [50, 83]; or (iii) targets all do-\nmains but only a specic set of tasks in a particular domain.\nSimVLM [109], ALIGN [50], and CLIP [83] have\ndemonstrated impressive gains by training transformer-\nbased models on giant private paired image-and-text cor-\npora, as opposed to the previous vision-and-language state-\nof-the-art such as VinVL [118] and ViLT [54], which were\ntrained on smaller public paired datasets [15,57,66,77,92].\nGenerally, models in the vision-and-language space can\nbe divided into two categories: (i) dual encoders where the\nimage and text are encoded separately followed by a shal-\nlow interaction layer for downstream tasks [50, 83]; and\n(ii) fusion encoder(s) with self-attention spanning across\nthe modalities [16, 34, 44, 45, 6265, 70, 71, 101, 102, 109,\n118, 120]. The dual encoder approach works well for uni-\nmodal [107, 108] and cross-modal retrieval tasks [66, 81]\nbut their lack of fusion usually causes them to underperform\non tasks that involve visual reasoning and question answer-\ning [39, 53, 93, 96] which is where models based on fusion\nencoder(s) shine.\nWithin the fusion encoder category, a further distinction\ncan be made as to whether the model uses a single trans-\nformer for early and unconstrained fusion between modal-\nities ( e.g., VisualBERT, UNITER, VLBERT, OSCAR [16,63, 65, 101, 120]) or allows cross-attention only in specic\nco-attention transformer layers while having some modal-\nity specic layers ( e.g., LXMERT, ViLBERT, ERNIE-ViL\n[70, 71, 102, 116]. Another distinguishing factor between\ndifferent models lies in the image features that are used,\nranging from region features [63, 70, 118], to patch embed-\ndings [54, 62, 109], to convolution or grid features [46, 51].\nDual encoder models use contrastive pretraining to pre-\ndict the correct N paired combinations among N2possi-\nbilities. On the other hand, with fusion encoders, inspired\nby unimodal pretraining schemes such as masked language\nmodeling [28, 68], masked image modeling [5], and causal\nlanguage modeling [84], numerous pretraining tasks have\nbeen explored: (i) Masked Language Modeling (MLM) for\nV&L where masked words in the caption are predicted with\nhelp of the paired image [63, 70, 102]; (ii) prexLM, where\nwith the help of an image, the model tries to complete a cap-\ntion [26, 109]; (iii) image-text matching, where the model\npredicts whether given pair of image and text match or\nnot; and (iv) masked region modeling, where the model re-\ngresses onto the image features or predicts its object class.\nCompared to previous work, our model FLA V A works\non a wide range of tasks in each of the vision, language, and\nvision-and-language domains. FLA V A uses a shared trunk\nwhich was pretrained on only openly available public paired\ndata. FLA V A combines dual and fusion encoder approaches\ninto one holistic model that can be pretrained with our novel\nFLA V A pretraining scheme that leverages pretraining objec-\ntives from both categories. FLA V A is designed to be able to\ntake advantage of unpaired unimodal data along with multi-\nmodal paired data, resulting in a model that can handle uni-\nmodal and retrieval tasks as well as cross-modal and multi-\nmodal vision-and-language tasks.\n3. FLA V A: A Foundational Language And Vi-\nsion Alignment Model\nThe goal of this work is to learn a foundational lan-\nguage and vision representation that enables unimodal vi-\nsion and language understanding as well as multimodal rea-\nsoning, all within a single pre-trained model. We show how\nthis can be achieved with a simple and elegant architecture')]
2023-12-07 16:00:29,850 - INFO - Processed the request successfully
2023-12-07 16:02:46,194 - INFO - classes: ['paper']: %s
2023-12-07 16:02:55,738 - INFO - classes: ['paper']: %s
2023-12-07 16:03:09,226 - INFO - classes: ['paper']: %s
2023-12-07 16:03:14,783 - INFO - classes: ['paper']: %s
2023-12-07 16:03:15,775 - INFO - actors creation successful [Actor(WeaviateEmbedder, 5e78d01027155c8a3cff9d9001000000), Actor(WeaviateEmbedder, 2230282082a6482c35fde69401000000), Actor(WeaviateEmbedder, 711fb1f5e0473c5f0cf8ff3501000000)]: %s
2023-12-07 16:03:15,775 - INFO - check 1st step of ray was successful
2023-12-07 16:03:15,775 - INFO - check if ray was successful:
2023-12-07 16:03:15,776 - INFO - response: {'status': 'success', 'message': 'Processed 19 documents in batches for class amin_paper.'}: %s
2023-12-07 16:03:15,776 - INFO - request processed successfully username='amin' class_name='paper' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/387f923480846231': %s
2023-12-07 16:03:35,144 - INFO - classes: ['paper']: %s
2023-12-07 16:03:35,256 - INFO - query success: 4 documents found
2023-12-07 16:04:07,055 - INFO - classes: ['paper']: %s
2023-12-07 16:04:47,334 - INFO - classes: ['paper']: %s
2023-12-07 16:04:47,457 - INFO - classes: ['paper']: %s
2023-12-07 16:04:50,865 - INFO - classes: ['paper']: %s
2023-12-07 16:04:51,013 - INFO - classes: ['paper']: %s
2023-12-07 16:04:54,481 - INFO - classes: ['paper']: %s
2023-12-07 16:04:54,609 - INFO - classes: ['paper']: %s
2023-12-07 16:04:54,637 - INFO - Received requests to /inference endpoint
2023-12-07 16:04:54,738 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 16:04:54,738 - INFO - Received request: {'username': 'amin', 'prompt': 'based on Multimodal Chain-of-Thought Reasoning in Language Models paper what is Challenge of Multimodal-CoT ?', 'memory': True, 'conversation_number': 9, 'AI_assistance': False, 'collection_name': 'paper', 'llm_model': 'Llama_13b'}
2023-12-07 16:04:54,771 - INFO - Retrieved docs: [Document(page_content='Multimodal Chain-of-Thought Reasoning in Language Models\nZhuosheng Zhang1Aston Zhang2Mu Li2Hai Zhao1George Karypis2Alex Smola2\nAbstract\nLarge language models (LLMs) have shown im-\npressive performance on complex reasoning by\nleveraging chain-of-thought (CoT) prompting to\ngenerate intermediate reasoning chains as the ra-\ntionale to infer the answer. However, existing\nCoT studies have focused on the language modal-\nity. We propose Multimodal-CoT that incorpo-\nrates language (text) and vision (images) modal-\nities into a two-stage framework that separates\nrationale generation and answer inference. In this\nway, answer inference can leverage better gen-\nerated rationales that are based on multimodal\ninformation. With Multimodal-CoT, our model\nunder 1 billion parameters outperforms the previ-\nous state-of-the-art LLM (GPT-3.5) by 16 percent-\nage points (75.17% 91.68% accuracy) and even\nsurpasses human performance on the ScienceQA\nbenchmark. Code is publicly available.1\n1. Introduction\nImagine reading a textbook with no gures or tables. Our\nability to knowledge acquisition is greatly strengthened by\njointly modeling diverse data modalities, such as vision, lan-\nguage, and audio. Recently, large language models (LLMs)\n(Brown et al., 2020; Thoppilan et al., 2022; Rae et al., 2021;\nChowdhery et al., 2022) have shown impressive perfor-\nmance in complex reasoning by generating intermediate\nreasoning steps before inferring the answer. The intriguing\ntechnique is called chain-of-thought (CoT) reasoning (Wei\net al., 2022b; Kojima et al., 2022; Zhang et al., 2022).\nHowever, existing studies related to CoT reasoning are\nlargely isolated in the language modality (Wang et al.,\n2022b; Zhou et al., 2022; Lu et al., 2022b; Fu et al., 2022),\nwith little consideration of multimodal scenarios. To elicit\nCoT reasoning in multimodality, we advocate a Multimodal-\n1Shanghai Jiao Tong University2Amazon Web Services.\nCorrespondence to: Zhuosheng Zhang (work done at Ama-\nzon Web Services) <zhangzs@sjtu.edu.cn >, Aston Zhang\n<az@astonzhang.com >.\n1https://github.com/amazon-science/mm-cot\nOptions:(B) salty(A) softOutputQuestion:Whichpropertydothesetwoobjectshaveincommon?Context: Select the better answer.\nRationale:Lookateachobject.Foreachobject,decideifithasthatproperty.Potatochipshaveasaltytaste.Bothobjectsaresalty.Asoftobjectchangesshapewhenyousqueezeit.Thefriesaresoft,butthecrackerisnot.Thepropertythatbothobjectshaveincommonissalty.Answer:Theansweris(B).VisionLanguageInputFigure 1. Example of the multimodal CoT task.\nCoT paradigm. Given the inputs in different modalities,\nMultimodal-CoT decomposes multi-step problems into in-\ntermediate reasoning steps (rationale) and then infers the\nanswer. Since vision and language are the most popular\nmodalities, we focus on those two modalities in this work.\nAn example is shown in Figure 1. In general, there are two\nways to elicit Multimodal-CoT reasoning as follows: (i)\nprompting LLMs and (ii) ne-tuning small models.2\nThe most immediate way to perform Multimodal-CoT is to\ntransform the input of different modalities into one modality\nand prompt LLMs to perform CoT. For example, it is possi-\nble to extract the caption of an image by a captioning model\nand then concatenate the caption with the original language\ninput to be fed into LLMs (Lu et al., 2022a). However, there\nis severe information loss in the captioning process; thus,\nusing the captions (as opposed to vision features) may suffer\nfrom a lack of mutual synergy in the representation space\nof different modalities.\nTo facilitate the interaction between modalities, another\npotential solution is to ne-tune smaller language models\n(LMs) by fusing multimodal features (Zhang et al., 2023).\nAs this approach allows the exibility of adjusting model\narchitectures to incorporate multimodal features, we study\nne-tuning models in this work instead of prompting LLMs.\nThe key challenge is that language models under 100 billion\nparameters tend to generate hallucinated rationales that mis-\nlead the answer inference (Ho et al., 2022; Magister et al.,\n2In this work, we refer to small models as models with less\nthan 1 billion parameters (hereinafter dubbed as 1B-models).arXiv:2302.00923v4  [cs.CL]  17 Feb 2023'), Document(page_content='Multimodal Chain-of-Thought Reasoning in Language Models\nVisionLanguageRationale GenerationQuestion:Whichpropertydothesetwoobjectshaveincommon?Context: Select the better answer.Lookateachobject.Foreachobject,decideifithasthatproperty.Potatochipshaveasaltytaste.Bothobjectsaresalty.Asoftobjectchangesshapewhenyousqueezeit.Thefriesaresoft,butthecrackerisnot.Thepropertythatbothobjectshaveincommonissalty.\nOptions:(B) salty(A) softRationaleAnswer InferenceTheansweris(B).Answer\nFigure 4. Overview of our Multimodal-CoT framework. Multimodal-CoT consists of two stages: (i) rationale generation and (ii) answer\ninference. Both stages share the same model architecture but differ in the input and output. In the rst stage, we feed the model with\nlanguage and vision inputs to generate rationales. In the second stage, we append the original language input with the rationale generated\nfrom the rst stage. Then, we feed the updated language input with the original vision input to the model to infer the answer.\nwith the encoded language representations before feeding\nto the decoder (more details will be presented in Section\n4). Interestingly, with vision features, the RougeL score of\nthe rationale generation has boosted to 96.97% (QCM R),\nwhich correspondingly contributes to better answer accuracy\nof 84.91% (QCMR A). With those effective rationales,\nthe phenomenon of hallucination is mitigated  62.5%\nhallucination mistakes in Section 3.2 have been corrected\n(Figure 3(b)), as an example shown in Figure 2 (right part).4\nThe analysis so far compellingly shows that vision features\nare indeed benecial for generating effective rationales and\ncontributing to accurate answer inference. As the two-stage\nmethod (QCMRA) in Table 3 achieves better performance\nthan all the one-stage method in Table 2, we choose the two-\nstage method in our Multimodal-CoT framework.\n4. Multimodal-CoT\nBased on the observations and discussions in Section 3, we\npropose Multimodal-CoT to incorporate language (text) and\nvision (images) modalities into a two-stage framework. In\nthis section, we will rst overview the procedure of the\nframework and then elaborate on the technical design of the\nmodel architecture.\n4.1. Framework Overview\nMultimodal-CoT consists of two training stages: (i) ratio-\nnale generation and (ii) answer inference. Both stages share\nthe same model architecture but differ in the input Xand\noutputY. The overall procedure is illustrated in Figure 4.\nWe will take vision-language as an example to show how\nMultimodal-CoT works.\n4The left mistakes are mainly about map understanding, which\nrequires more advanced vision features. We will discuss them in\nSection 6.4.In the rationale generation stage, we feed the model with\nX={X1\nlanguage,Xvision}whereX1\nlanguage represents the lan-\nguage input in the rst stage and Xvision represents the vision\ninput, i.e., the image. For example, Xcan be instantiated as\na concatenation of question, context, and options of a multi-\nple choice reasoning problem (Lu et al., 2022a) as shown in\nFigure 4. The goal is to learn a rationale generation model\nR=F(X)whereRis the rationale.\nIn the answer inference stage, the rationale Ris appended\nto the original language input X1\nlanguage to construct the lan-\nguage input in the second stage, X2\nlanguage =X1\nlanguageR\nwheredenotes concatenation. Then, we feed the updated\ninputX={X2\nlanguage,Xvision}to the answer inference\nmodel to infer the nal answer A=F(X).\nIn both stages, we train two models with the same archi-\ntecture independently. They take the annotated elements\n(e.g.,XR,XRA, respectively) from the training\nset for supervised learning. During inference, given X, the\nrationales for the test sets are generated using the model\ntrained in the rst stage; they are used in the second stage\nfor answer inference.\n4.2. Model Architecture\nGiven the language input Xlanguage{X1\nlanguage,X2\nlanguage}\nand the vision input Xvision, we compute the probability of\ngenerating target text Y(either the rationale or the answer\nin Figure 4) of length Nby\np(Y|Xlanguage,Xvision ) =N\ni=1p(Yi|Xlanguage,Xvision,Y<i),\n(1)\nwherep(Yi|Xlanguage,Xvision,Y<i)is implemented with\na Transformer-based network (Vaswani et al., 2017). The\nnetwork has three major procedures: encoding, interaction,'), Document(page_content='Multimodal Chain-of-Thought Reasoning in Language Models\ncomplexity-based selection strategies were also proposed\nto obtain effective demonstrations. Fu et al. (2022) chose\nexamples with complex reasoning chains (i.e., with more\nreasoning steps) as the demonstrations. Lu et al. (2022b)\ntrained an agent to nd optimal in-context examples from\na candidate pool and maximize the prediction rewards on\ngiven training examples when interacting with GPT-3.5.\nOptimizing Reasoning Chains A notable way to opti-\nmize reasoning chains is problem decomposition. Zhou\net al. (2022) proposed least-to-most prompting to decom-\npose complex problems into sub-problems and then solve\nthese sub-problems sequentially. As a result, solving a\ngiven sub-problem is facilitated by the answers to previ-\nously solved sub-problems. Similarly, Khot et al. (2022)\nused diverse decomposition structures and designed differ-\nent prompts to answer each sub-question. In addition to\nprompting the reasoning chains as natural language texts,\nChen et al. (2022) proposed program-of-thoughts (PoT),\nwhich modeled the reasoning process as a program and\nprompted LLMs to derive the answer by executing the gen-\nerated programs. Another trend is to vote over multiple\nreasoning paths for a test question. Wang et al. (2022a)\nintroduced a self-consistency decoding strategy to sample\nmultiple outputs of LLMs and then took a majority over\nthe nal answers. Wang et al. (2022b) and Li et al. (2022b)\nintroduced randomness in the input space to produce more\ndiverse outputs for voting.\n2.2. Eliciting CoT Reasoning by Fine-Tuning Models\nA recent interest is eliciting CoT reasoning by ne-tuning\nlanguage models. Lu et al. (2022a) ne-tuned the encoder-\ndecoder T5 model on a large-scale dataset with CoT annota-\ntions. However, a dramatic performance decline is observed\nwhen using CoT to infer the answer, i.e., generating the rea-\nsoning chain before the answer (reasoning). Instead, CoT\nis only used as an explanation after the answer. Magister\net al. (2022) and Ho et al. (2022) employed knowledge\ndistillation by ne-tuning a student model on the chain-of-\nthought outputs generated by a larger teacher model. The\nproposed methods showed performance gains in arithmetic,\ncommonsense, and symbolic reasoning tasks.\nThere is a key challenge in training 1B-models to be CoT\nreasoners. As observed by Wei et al. (2022b), models un-\nder 100 billion parameters tend to produce illogical CoT\nthat leads to wrong answers. In other words, it might be\nharder for 1B-models to generate effective CoT than directly\ngenerating the answer. It becomes even more challenging\nin a multimodal setting where answering the question also\nrequires understanding the multimodal inputs. In the follow-\ning part, we will explore the challenge of Multimodal-CoT\nand investigate how to perform effective multi-step reason-\ning.3. Challenge of Multimodal-CoT\nExisting studies have suggested that the CoT reasoning abil-\nity may emerge in language models at a certain scale, e.g.,\nover 100 billion parameters (Wei et al., 2022a). However,\nit remains an unresolved challenge to elicit such reasoning\nabilities in 1B-models, let alone in the multimodal scenario.\nThis work focuses on 1B-models as they can be ne-tuned\nand deployed with consumer-grade GPUs (e.g., 32G mem-\nory). In this section, we will investigate why 1B-models\nfail at CoT reasoning and study how to design an effective\napproach to overcome the challenge.\n3.1. Towards the Role of CoT\nTo begin with, we ne-tune a text-only baseline for CoT rea-\nsoning on the ScienceQA benchmark (Lu et al., 2022a).\nFollowing Lu et al. (2022a), we adopt UniedQA Base\n(Khashabi et al., 2020) as the backbone language model.3\nOur task is modeled as a text generation problem, where the\nmodel takes the textual information as the input and gener-\nates the output sequence that consists of the rationale and\nthe answer. As an example shown in Figure 1, the model\ntakes the concatenation of tokens of the question text (Q),\nthe context text (C), and multiple options (M) as the input.\nTo study the effect of CoT, we compare the performance\nwith three variants: (i) No-CoT which predicts the answer\ndirectly (QCMA); (ii) Reasoning where answer inference\nis conditioned to the rationale (QCM RA); (iii) Explana-\ntion where the rationale is used for explaining the answer\ninference (QCMAR).\nTable 2. Effects of CoT in the one-stage setting.\nMethod Format Accuracy\nNo-CoT QCM A 80.40\nReasoning QCM RA 67.86\nExplanation QCM AR 69.77\nSurprisingly, we observe a 12.54% accuracy decrease\n(80.40%67.86%) if the model predicts rationales before\nanswers (QCMRA). The results imply that the rationales\nmight not necessarily contribute to predicting the right an-\nswer. A similar phenomenon was observed in Lu et al.\n(2022a), where the plausible reason might be that the model\nexceeds the maximum token limits before obtaining the\nrequired answer or stops generating the prediction early.\nHowever, we nd that the maximum length of the gener-\nated outputs (RA) is always less than 400 tokens, which\nis below the length limit of language models (i.e., 512 in\nUniedQA Base). Therefore, it deserves a more in-depth\ninvestigation into why the rationales harm answer inference.\n3UniedQA (Khashabi et al., 2020) is adopted as it is the best\nne-tuning model in Lu et al. (2022a). Model information and\nimplementation details are presented in Appendix B.1.'), Document(page_content='Multimodal Chain-of-Thought Reasoning in Language Models\nTable 1. Typical CoT techniques (FT: ne-tuning; KD: knowledge distillation). Segment 1: in-context learning techniques; Segment 2:\nne-tuning techniques. To the best of our knowledge, our work is the rst to study CoT reasoning in different modalities. Besides, we\nfocus on 1B-models, without relying on the outputs of LLMs.\nModels Mutimodal w/o LLM Model / Engine Training CoT Role CoT Source\nZero-Shot-CoT (Kojima et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Template\nFew-Shot-CoT (Wei et al., 2022b) \x17 \x17 PaLM (540B) ICL Reasoning Hand-crafted\nSelf-Consistency-CoT (Wang et al., 2022a) \x17 \x17 Codex (175B) ICL Reasoning Hand-crafted\nLeast-to-Most Prompting (Zhou et al., 2022) \x17 \x17 Codex (175B) ICL Reasoning Hand-crafted\nRetrieval-CoT (Zhang et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Auto-generated\nPromptPG-CoT (Lu et al., 2022b) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Hand-crafted\nAuto-CoT (Zhang et al., 2022) \x17 \x17 Codex (175B) ICL Reasoning Auto-generated\nComplexity-CoT (Fu et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Hand-crafted\nFew-Shot-PoT (Chen et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Hand-crafted\nUniedQA (Lu et al., 2022a) \x17  T5 (770M) FT Explanation Crawled\nFine-Tuned T5 XXL (Magister et al., 2022) \x17 \x17 T5 (11B) KD Reasoning LLM-generated\nFine-Tune-CoT (Ho et al., 2022) \x17 \x17 GPT-3 (6.7B) KD Reasoning LLM-generated\nMultimodal-CoT (our work)   T5 (770M) FT Reasoning Crawled\n2022; Ji et al., 2022).\nTo mitigate the challenge of hallucination, we propose\nMultimodal-CoT that incorporates language (text) and vi-\nsion (images) modalities into a two-stage framework that\nseparates rationale generation and answer inference. In\nthis way, answer inference can leverage better generated\nrationales that are based on multimodal information. Our\nexperiments are conducted on the ScienceQA benchmark\n(Lu et al., 2022a), which is the latest multimodal reasoning\nbenchmark with annotated reasoning chains. Experimental\nresults show that our method surpasses the previous state-of-\nthe-art GPT-3.5 model by +16% (75.17% 91.68%) on the\nbenchmark. Our contributions are summarized as follows:\n(i) To the best of our knowledge, this work is the rst to\nstudy CoT reasoning in different modalities.\n(ii) We propose a two-stage framework by ne-tuning lan-\nguage models to fuse vision and language representations\nto perform Multimodal-CoT. The model is able to generate\ninformative rationales to facilitate inferring nal answers.\n(iii) Our method achieves new state-of-the-art performance\non the ScienceQA benchmark, outperforming accuracy of\nGPT-3.5 by 16% and even surpassing human performance.\n2. Background\nThis section reviews recent progress of eliciting CoT rea-\nsoning by prompting and ne-tuning language models.\n2.1. CoT Reasoning with LLMs\nRecently, CoT has been widely used to elicit the multi-step\nreasoning abilities of LLMs (Wei et al., 2022b). Concretely,\nCoT techniques encourage the LLM to generate intermedi-\nate reasoning chains for solving a problem. Studies have\nshown that LLMs can perform CoT reasoning with two ma-\njor paradigms of techniques: Zero-Shot-CoT (Kojima et al.,2022) and Few-Shot-CoT (Wei et al., 2022b; Zhang et al.,\n2022). For Zero-Shot-CoT, Kojima et al. (2022) showed that\nLLMs are decent zero-shot reasoners by adding a prompt\nlike Lets think step by step after the test question to in-\nvoke CoT reasoning. For Few-Shot-CoT, a few step-by-step\nreasoning demonstrations are used as conditions for infer-\nence. Each demonstration has a question and a reasoning\nchain that leads to the nal answer. The demonstrations are\ncommonly obtained by hand-crafting or automatic gener-\nation. The corresponding techniques are thus referred to\nas Manual-CoT (Wei et al., 2022b) and Auto-CoT (Zhang\net al., 2022).\nWith effective demonstrations, Few-Shot-CoT often\nachieves stronger performance than Zero-Shot-CoT and has\nattracted more research interest. Therefore, most recent\nstudies focused on how to improve Few-Shot-CoT. Those\nstudies are categorized into two major research lines: (i)\noptimizing the demonstrations; (ii) optimizing the reasoning\nchains. Table 1 compares typical CoT techniques.\nOptimizing Demonstrations The performance of Few-\nShot-CoT relies on the quality of demonstrations. As re-\nported in Wei et al. (2022b), using demonstrations written\nby different annotators results in dramatic accuracy dispar-\nity in a symbolic reasoning task. Beyond hand-crafting the\ndemonstrations, recent studies have investigated ways to op-\ntimize the demonstration selection process. Notably, Rubin\net al. (2022) retrieved the semantically similar demonstra-\ntions with the test instance. However, this approach shows\na degraded performance when there are mistakes in the rea-\nsoning chains (Zhang et al., 2022). To address the limitation,\nZhang et al. (2022) found that the key is the diversity of\ndemonstration questions and proposed Auto-CoT: (i) par-\ntition questions of a given dataset into a few clusters; (ii)\nsample a representative question from each cluster and gen-\nerate its reasoning chain using Zero-Shot-CoT with simple\nheuristics. In addition, reinforcement learning (RL) and'), Document(page_content='James Bradbury, Roy Frostig, Peter Hawkins,\nMatthew James Johnson, Chris Leary, Dougal\nMaclaurin, George Necula, Adam Paszke, Jake\nVanderPlas, Skye Wanderman-Milne, and Qiao\nZhang. 2018. JAX: composable transformations of\nPython+NumPy programs.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems ,\nvolume 33, pages 18771901. Curran Associates,\nInc.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. BoolQ: Exploring the surprising\ndifculty of natural yes/no questions. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) , pages 29242936, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The PASCAL recognising textual entailment\nchallenge. In Machine Learning Challenges Work-\nshop , pages 177190. Springer.\nMarie-Catherine De Marneff, Mandy Simons, and Ju-\ndith Tonhauser. 2019. The CommitmentBank: In-\nvestigating projection in naturally occurring dis-\ncourse. Proceedings of Sinn und Bedeutung 23 .\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 41714186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nInProceedings of the Third International Workshop\non Paraphrasing (IWP2005) .\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel\nStanovsky, Sameer Singh, and Matt Gardner. 2019.\nDROP: A reading comprehension benchmark requir-\ning discrete reasoning over paragraphs. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1(Long and Short Papers) , pages 23682378, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nAdam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eu-\nnsol Choi, and Danqi Chen. 2019. MRQA 2019\nshared task: Evaluating generalization in reading\ncomprehension. In Proceedings of 2nd Machine\nReading for Reading Comprehension (MRQA) Work-\nshop at EMNLP .\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\nand Bill Dolan. 2007. The third PASCAL recog-\nnizing textual entailment challenge. In Proceedings\nof the ACL-PASCAL workshop on textual entailment\nand paraphrasing , pages 19. Association for Com-\nputational Linguistics.\nKaren Hambardzumyan, Hrant Khachatrian, and\nJonathan May. 2021. WARP: Word-level Adversar-\nial ReProgramming. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers) , pages 49214933, Online. Associa-\ntion for Computational Linguistics.\nL. K. Hansen and P. Salamon. 1990. Neural network\nensembles. IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 12(10):9931001.\nJonathan Heek, Anselm Levskaya, Avital Oliver, Mar-\nvin Ritter, Bertrand Rondepierre, Andreas Steiner,\nand Marc van Zee. 2020. Flax: A neural network\nlibrary and ecosystem for JAX.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efcient transfer learning for NLP.\nInProceedings of the 36th International Conference\non Machine Learning , volume 97 of Proceedings\nof Machine Learning Research , pages 27902799.\nPMLR.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ne-tuning for text classication. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers) , pages 328339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nShankar Iyer, Nikhil Dandekar, and Kornel Csernai.\n2017. First Quora dataset release: Question pairs.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics , 8:423438.\nA. Kembhavi, M. Seo, D. Schwenk, J. Choi, A. Farhadi,\nand H. Hajishirzi. 2017. Are you smarter than a\nsixth grader? textbook question answering for multi-\nmodal machine comprehension. In 2017 IEEE Con-\nference on Computer Vision and Pattern Recognition\n(CVPR) , pages 53765384.'), Document(page_content='James Bradbury, Roy Frostig, Peter Hawkins,\nMatthew James Johnson, Chris Leary, Dougal\nMaclaurin, George Necula, Adam Paszke, Jake\nVanderPlas, Skye Wanderman-Milne, and Qiao\nZhang. 2018. JAX: composable transformations of\nPython+NumPy programs.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems ,\nvolume 33, pages 18771901. Curran Associates,\nInc.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. BoolQ: Exploring the surprising\ndifculty of natural yes/no questions. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) , pages 29242936, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The PASCAL recognising textual entailment\nchallenge. In Machine Learning Challenges Work-\nshop , pages 177190. Springer.\nMarie-Catherine De Marneff, Mandy Simons, and Ju-\ndith Tonhauser. 2019. The CommitmentBank: In-\nvestigating projection in naturally occurring dis-\ncourse. Proceedings of Sinn und Bedeutung 23 .\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 41714186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nInProceedings of the Third International Workshop\non Paraphrasing (IWP2005) .\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel\nStanovsky, Sameer Singh, and Matt Gardner. 2019.\nDROP: A reading comprehension benchmark requir-\ning discrete reasoning over paragraphs. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1(Long and Short Papers) , pages 23682378, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nAdam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eu-\nnsol Choi, and Danqi Chen. 2019. MRQA 2019\nshared task: Evaluating generalization in reading\ncomprehension. In Proceedings of 2nd Machine\nReading for Reading Comprehension (MRQA) Work-\nshop at EMNLP .\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\nand Bill Dolan. 2007. The third PASCAL recog-\nnizing textual entailment challenge. In Proceedings\nof the ACL-PASCAL workshop on textual entailment\nand paraphrasing , pages 19. Association for Com-\nputational Linguistics.\nKaren Hambardzumyan, Hrant Khachatrian, and\nJonathan May. 2021. WARP: Word-level Adversar-\nial ReProgramming. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers) , pages 49214933, Online. Associa-\ntion for Computational Linguistics.\nL. K. Hansen and P. Salamon. 1990. Neural network\nensembles. IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 12(10):9931001.\nJonathan Heek, Anselm Levskaya, Avital Oliver, Mar-\nvin Ritter, Bertrand Rondepierre, Andreas Steiner,\nand Marc van Zee. 2020. Flax: A neural network\nlibrary and ecosystem for JAX.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efcient transfer learning for NLP.\nInProceedings of the 36th International Conference\non Machine Learning , volume 97 of Proceedings\nof Machine Learning Research , pages 27902799.\nPMLR.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ne-tuning for text classication. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers) , pages 328339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nShankar Iyer, Nikhil Dandekar, and Kornel Csernai.\n2017. First Quora dataset release: Question pairs.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics , 8:423438.\nA. Kembhavi, M. Seo, D. Schwenk, J. Choi, A. Farhadi,\nand H. Hajishirzi. 2017. Are you smarter than a\nsixth grader? textbook question answering for multi-\nmodal machine comprehension. In 2017 IEEE Con-\nference on Computer Vision and Pattern Recognition\n(CVPR) , pages 53765384.')]
2023-12-07 16:05:35,749 - INFO - Processed the request successfully
