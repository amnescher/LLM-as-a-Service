2023-11-16 10:28:43,078 - INFO - Created a temporary directory at /tmp/tmp40rexoeq
2023-11-16 10:28:43,079 - INFO - Writing /tmp/tmp40rexoeq/_remote_module_non_scriptable.py
2023-11-16 10:28:47,464 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-16 10:29:05,462 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-16 10:29:14,176 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-16 10:43:39,420 - INFO - Created a temporary directory at /tmp/tmpgf3b0dpl
2023-11-16 10:43:39,420 - INFO - Writing /tmp/tmpgf3b0dpl/_remote_module_non_scriptable.py
2023-11-16 10:43:41,697 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-16 10:44:18,671 - INFO - Created a temporary directory at /tmp/tmp_pph2hxc
2023-11-16 10:44:18,672 - INFO - Writing /tmp/tmp_pph2hxc/_remote_module_non_scriptable.py
2023-11-16 10:44:20,979 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-16 10:44:38,533 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-16 10:44:47,288 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-16 11:08:25,502 - INFO - Created a temporary directory at /tmp/tmpewtfe5jt
2023-11-16 11:08:25,502 - INFO - Writing /tmp/tmpewtfe5jt/_remote_module_non_scriptable.py
2023-11-16 11:08:27,842 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-16 11:08:45,263 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-16 11:08:54,039 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-16 11:12:37,695 - INFO - Created a temporary directory at /tmp/tmpadnpm7or
2023-11-16 11:12:37,695 - INFO - Writing /tmp/tmpadnpm7or/_remote_module_non_scriptable.py
2023-11-16 11:12:40,036 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-16 11:17:30,520 - INFO - Created a temporary directory at /tmp/tmpozgmf3cd
2023-11-16 11:17:30,520 - INFO - Writing /tmp/tmpozgmf3cd/_remote_module_non_scriptable.py
2023-11-16 11:17:32,842 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-16 11:17:50,594 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-16 11:17:59,362 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-16 13:56:50,799 - INFO - Created a temporary directory at /tmp/tmp6xltg6vk
2023-11-16 13:56:50,799 - INFO - Writing /tmp/tmp6xltg6vk/_remote_module_non_scriptable.py
2023-11-16 13:56:53,130 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-16 13:57:10,786 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-16 13:57:19,527 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-16 16:38:08,187 - INFO - Created a temporary directory at /tmp/tmp2x1h8gqn
2023-11-16 16:38:08,187 - INFO - Writing /tmp/tmp2x1h8gqn/_remote_module_non_scriptable.py
2023-11-16 16:38:10,460 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-16 16:38:28,054 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-16 16:38:36,787 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 13:17:26,890 - INFO - Created a temporary directory at /tmp/tmpic_bcnim
2023-11-17 13:17:26,890 - INFO - Writing /tmp/tmpic_bcnim/_remote_module_non_scriptable.py
2023-11-17 13:17:29,177 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-17 13:17:46,706 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-17 13:17:55,422 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 13:20:00,432 - INFO - Created a temporary directory at /tmp/tmpjh79yy92
2023-11-17 13:20:00,432 - INFO - Writing /tmp/tmpjh79yy92/_remote_module_non_scriptable.py
2023-11-17 13:20:02,751 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-17 13:20:20,300 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-17 13:20:29,076 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 13:32:14,753 - INFO - Created a temporary directory at /tmp/tmpwzm8z2rt
2023-11-17 13:32:14,754 - INFO - Writing /tmp/tmpwzm8z2rt/_remote_module_non_scriptable.py
2023-11-17 13:32:17,060 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-17 13:32:34,612 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-17 13:32:43,423 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 13:43:52,096 - INFO - Created a temporary directory at /tmp/tmpnpblkxu1
2023-11-17 13:43:52,096 - INFO - Writing /tmp/tmpnpblkxu1/_remote_module_non_scriptable.py
2023-11-17 13:43:54,422 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-17 13:44:11,946 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-17 13:44:20,682 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 13:48:19,994 - INFO - Created a temporary directory at /tmp/tmpfbq3u_0v
2023-11-17 13:48:19,995 - INFO - Writing /tmp/tmpfbq3u_0v/_remote_module_non_scriptable.py
2023-11-17 13:48:22,333 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-17 13:48:39,786 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-17 13:48:48,510 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 13:52:55,427 - INFO - Created a temporary directory at /tmp/tmpex8gn7x6
2023-11-17 13:52:55,427 - INFO - Writing /tmp/tmpex8gn7x6/_remote_module_non_scriptable.py
2023-11-17 13:52:57,737 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-17 13:53:15,238 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-17 13:53:23,986 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 14:53:17,942 - INFO - Created a temporary directory at /tmp/tmpswd7owy1
2023-11-17 14:53:17,942 - INFO - Writing /tmp/tmpswd7owy1/_remote_module_non_scriptable.py
2023-11-17 14:53:20,222 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-17 14:53:37,626 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-17 14:53:46,323 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 15:14:50,176 - INFO - Created a temporary directory at /tmp/tmpivtdxefc
2023-11-17 15:14:50,176 - INFO - Writing /tmp/tmpivtdxefc/_remote_module_non_scriptable.py
2023-11-17 15:14:52,494 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-17 15:15:10,137 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-17 15:15:18,880 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 15:17:43,855 - INFO - Created a temporary directory at /tmp/tmp5xd6gl_f
2023-11-17 15:17:43,855 - INFO - Writing /tmp/tmp5xd6gl_f/_remote_module_non_scriptable.py
2023-11-17 15:17:46,210 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-17 15:18:03,816 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-17 15:18:12,547 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 15:20:13,075 - INFO - Created a temporary directory at /tmp/tmpsckwbjlh
2023-11-17 15:20:13,075 - INFO - Writing /tmp/tmpsckwbjlh/_remote_module_non_scriptable.py
2023-11-17 15:20:15,416 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-17 15:20:32,864 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-17 15:20:41,613 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-17 15:29:03,756 - INFO - Received requests to /inference endpoint
2023-11-17 15:29:03,857 - INFO - Received a batch of request with batch size of: 1 
2023-11-17 15:29:03,858 - INFO - Received request: {'username': 'amin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-17 15:29:03,860 - ERROR - Error processing the request: name 'time' is not defined
2023-11-17 15:29:15,146 - INFO - Received requests to /inference endpoint
2023-11-17 15:29:15,247 - INFO - Received a batch of request with batch size of: 1 
2023-11-17 15:29:15,247 - INFO - Received request: {'username': 'amin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-17 15:29:15,248 - ERROR - Error processing the request: name 'time' is not defined
2023-11-20 11:14:02,694 - INFO - Created a temporary directory at /tmp/tmpn0s4u1ju
2023-11-20 11:14:02,695 - INFO - Writing /tmp/tmpn0s4u1ju/_remote_module_non_scriptable.py
2023-11-20 11:14:09,191 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-20 11:24:47,681 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-20 11:25:10,405 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-20 11:28:22,467 - INFO - Created a temporary directory at /tmp/tmp47as7c_x
2023-11-20 11:28:22,467 - INFO - Writing /tmp/tmp47as7c_x/_remote_module_non_scriptable.py
2023-11-20 11:28:24,761 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-20 11:28:45,695 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-20 11:28:54,618 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-20 12:04:30,088 - INFO - Received requests to /inference endpoint
2023-11-20 12:04:30,189 - INFO - Received a batch of request with batch size of: 1 
2023-11-20 12:04:30,190 - INFO - Received request: {'username': 'amin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-20 12:04:30,192 - ERROR - Error processing the request: name 'time' is not defined
2023-11-20 12:11:51,021 - INFO - Created a temporary directory at /tmp/tmppj91djaj
2023-11-20 12:11:51,021 - INFO - Writing /tmp/tmppj91djaj/_remote_module_non_scriptable.py
2023-11-20 12:11:53,325 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-20 12:12:10,995 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-20 12:12:19,779 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-20 12:14:28,694 - INFO - Received requests to /inference endpoint
2023-11-20 12:14:28,795 - INFO - Received a batch of request with batch size of: 1 
2023-11-20 12:14:28,795 - INFO - Received request: {'username': 'amin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-20 12:14:28,798 - ERROR - Error processing the request: name 'time' is not defined
2023-11-20 12:15:30,904 - INFO - Received requests to /inference endpoint
2023-11-20 12:15:31,005 - INFO - Received a batch of request with batch size of: 1 
2023-11-20 12:15:31,005 - INFO - Received request: {'username': 'amin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-20 12:15:31,005 - ERROR - Error processing the request: name 'time' is not defined
2023-11-20 12:17:30,660 - INFO - Created a temporary directory at /tmp/tmpjb1zh064
2023-11-20 12:17:30,660 - INFO - Writing /tmp/tmpjb1zh064/_remote_module_non_scriptable.py
2023-11-20 12:17:32,965 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-20 12:17:50,498 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-20 12:17:59,256 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-20 12:20:08,787 - INFO - Received requests to /inference endpoint
2023-11-20 12:20:08,888 - INFO - Received a batch of request with batch size of: 1 
2023-11-20 12:20:08,888 - INFO - Received request: {'username': 'amin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-20 12:20:17,930 - INFO - Processed the request successfully
2023-11-20 12:25:38,633 - INFO - Created a temporary directory at /tmp/tmp6j752gcz
2023-11-20 12:25:38,633 - INFO - Writing /tmp/tmp6j752gcz/_remote_module_non_scriptable.py
2023-11-20 12:25:40,899 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-20 12:25:58,258 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-20 12:26:06,966 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-20 12:26:36,396 - INFO - Received requests to /inference endpoint
2023-11-20 12:26:36,497 - INFO - Received a batch of request with batch size of: 1 
2023-11-20 12:26:36,497 - INFO - Received request: {'username': 'amin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-20 12:26:43,258 - INFO - Processed the request successfully
2023-11-20 12:30:25,377 - INFO - Received requests to /inference endpoint
2023-11-20 12:30:25,478 - INFO - Received a batch of request with batch size of: 1 
2023-11-20 12:30:25,478 - INFO - Received request: {'username': 'amin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-20 12:30:31,658 - INFO - Processed the request successfully
2023-11-20 12:32:34,636 - INFO - Received requests to /inference endpoint
2023-11-20 12:32:34,738 - INFO - Received a batch of request with batch size of: 1 
2023-11-20 12:32:34,738 - INFO - Received request: {'username': 'admin', 'prompt': 'HI', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-20 12:32:40,799 - INFO - Processed the request successfully
2023-11-20 12:53:57,962 - INFO - Created a temporary directory at /tmp/tmpmccgt757
2023-11-20 12:53:57,962 - INFO - Writing /tmp/tmpmccgt757/_remote_module_non_scriptable.py
2023-11-20 12:54:00,281 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-20 12:54:17,694 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-20 12:54:26,424 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-20 12:54:36,397 - ERROR - An error occurred: 'VDBaseInput' object has no attribute 'mode'
2023-11-20 12:56:12,810 - INFO - Created a temporary directory at /tmp/tmp_3e5c5ut
2023-11-20 12:56:12,810 - INFO - Writing /tmp/tmp_3e5c5ut/_remote_module_non_scriptable.py
2023-11-20 12:56:15,107 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-20 12:56:32,403 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-20 12:56:41,107 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-20 12:59:54,117 - INFO - Created a temporary directory at /tmp/tmptro6pmkp
2023-11-20 12:59:54,117 - INFO - Writing /tmp/tmptro6pmkp/_remote_module_non_scriptable.py
2023-11-20 12:59:56,401 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-20 13:00:13,970 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-20 13:00:22,694 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-20 13:02:02,511 - INFO - Collection created for user admin: {'collection_name': 'admin_xc'}
2023-11-20 13:02:11,864 - INFO - Collection created for user amin: {'collection_name': 'amin_xc'}
2023-11-20 13:05:18,411 - INFO - Received requests to /inference endpoint
2023-11-20 13:05:18,512 - INFO - Received a batch of request with batch size of: 1 
2023-11-20 13:05:18,513 - INFO - Received request: {'username': 'admin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-20 13:05:25,325 - INFO - Processed the request successfully
2023-11-20 13:11:41,928 - INFO - Collection created for user amin: {'collection_name': 'amin_xc'}
2023-11-20 13:12:33,821 - INFO - Collection created for user admin: {'collection_name': 'admin_vc'}
2023-11-20 14:16:56,429 - INFO - Created a temporary directory at /tmp/tmp4wtg9gbw
2023-11-20 14:16:56,429 - INFO - Writing /tmp/tmp4wtg9gbw/_remote_module_non_scriptable.py
2023-11-20 14:16:58,723 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-20 14:17:16,193 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-20 14:17:24,931 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-20 14:17:28,736 - INFO - Collection created for user admin: {'collection_name': 'admin_ff'}
2023-11-20 14:36:18,105 - INFO - Collection created for user admin: {'collection_name': 'admin_None'}
2023-11-20 14:36:28,260 - INFO - Collection created for user admin: {'collection_name': 'admin_zz'}
2023-11-21 10:08:48,545 - INFO - Collection created for user admin: {'collection_name': 'admin_test1'}
2023-11-21 10:09:28,307 - INFO - Collection created for user admin: {'collection_name': 'admin_test2'}
2023-11-21 10:26:38,196 - INFO -  file Car rental doc.pdf received for user admin: 
2023-11-21 10:26:38,218 - INFO - Collection created for user admin: {'collection_name': 'admin_test3'}
2023-11-21 11:48:17,956 - INFO - request processed successfully username='admin' collection_name=None mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/admin_Car rental doc.pdf': %s
2023-11-21 12:14:49,986 - INFO - Unable to poll TPU GCE metadata: HTTPConnectionPool(host='metadata.google.internal', port=80): Max retries exceeded with url: /computeMetadata/v1/instance/attributes/accelerator-type (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7efd24c6cf40>: Failed to establish a new connection: [Errno -2] Name or service not known'))
2023-11-21 12:27:14,382 - INFO - Unable to poll TPU GCE metadata: HTTPConnectionPool(host='metadata.google.internal', port=80): Max retries exceeded with url: /computeMetadata/v1/instance/attributes/accelerator-type (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fbe397a4f70>: Failed to establish a new connection: [Errno -2] Name or service not known'))
2023-11-21 12:31:03,268 - INFO - Unable to poll TPU GCE metadata: HTTPConnectionPool(host='metadata.google.internal', port=80): Max retries exceeded with url: /computeMetadata/v1/instance/attributes/accelerator-type (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7efdc1655090>: Failed to establish a new connection: [Errno -2] Name or service not known'))
2023-11-21 12:42:19,617 - INFO - request processed successfully username='admin' collection_name='Admin_pdf' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files': %s
2023-11-23 11:12:51,049 - INFO - Created a temporary directory at /tmp/tmp1uh5qtym
2023-11-23 11:12:51,049 - INFO - Writing /tmp/tmp1uh5qtym/_remote_module_non_scriptable.py
2023-11-23 11:12:57,415 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 11:23:44,404 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 11:24:08,688 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 12:09:17,103 - INFO - Created a temporary directory at /tmp/tmpn7qlos_4
2023-11-23 12:09:17,104 - INFO - Writing /tmp/tmpn7qlos_4/_remote_module_non_scriptable.py
2023-11-23 12:09:19,414 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:09:40,259 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 12:09:49,126 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 12:23:24,289 - INFO - Created a temporary directory at /tmp/tmp52ctgij0
2023-11-23 12:23:24,290 - INFO - Writing /tmp/tmp52ctgij0/_remote_module_non_scriptable.py
2023-11-23 12:23:26,577 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:23:44,091 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 12:23:52,776 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 12:23:59,341 - INFO - Created a temporary directory at /tmp/tmp50ngzdjy
2023-11-23 12:23:59,341 - INFO - Writing /tmp/tmp50ngzdjy/_remote_module_non_scriptable.py
2023-11-23 12:24:01,677 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:24:24,679 - INFO - Created a temporary directory at /tmp/tmpmqxeqq11
2023-11-23 12:24:24,680 - INFO - Writing /tmp/tmpmqxeqq11/_remote_module_non_scriptable.py
2023-11-23 12:24:27,032 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:24:49,788 - INFO - Created a temporary directory at /tmp/tmp2qwruxb3
2023-11-23 12:24:49,788 - INFO - Writing /tmp/tmp2qwruxb3/_remote_module_non_scriptable.py
2023-11-23 12:24:52,138 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:25:14,908 - INFO - Created a temporary directory at /tmp/tmpnfixem8q
2023-11-23 12:25:14,908 - INFO - Writing /tmp/tmpnfixem8q/_remote_module_non_scriptable.py
2023-11-23 12:25:17,245 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:25:40,456 - INFO - Created a temporary directory at /tmp/tmprmnm6fnz
2023-11-23 12:25:40,456 - INFO - Writing /tmp/tmprmnm6fnz/_remote_module_non_scriptable.py
2023-11-23 12:25:42,829 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:26:05,709 - INFO - Created a temporary directory at /tmp/tmpte34exz3
2023-11-23 12:26:05,709 - INFO - Writing /tmp/tmpte34exz3/_remote_module_non_scriptable.py
2023-11-23 12:26:08,037 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:26:31,138 - INFO - Created a temporary directory at /tmp/tmp4rmjy08y
2023-11-23 12:26:31,138 - INFO - Writing /tmp/tmp4rmjy08y/_remote_module_non_scriptable.py
2023-11-23 12:26:33,432 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:26:56,541 - INFO - Created a temporary directory at /tmp/tmp4p9ddlmp
2023-11-23 12:26:56,541 - INFO - Writing /tmp/tmp4p9ddlmp/_remote_module_non_scriptable.py
2023-11-23 12:26:58,910 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:27:29,210 - INFO - Created a temporary directory at /tmp/tmp1li9fg41
2023-11-23 12:27:29,211 - INFO - Writing /tmp/tmp1li9fg41/_remote_module_non_scriptable.py
2023-11-23 12:27:31,546 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:28:33,686 - INFO - Created a temporary directory at /tmp/tmpfh7j7eez
2023-11-23 12:28:33,686 - INFO - Writing /tmp/tmpfh7j7eez/_remote_module_non_scriptable.py
2023-11-23 12:28:36,034 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:29:38,654 - INFO - Created a temporary directory at /tmp/tmp74ismo3e
2023-11-23 12:29:38,654 - INFO - Writing /tmp/tmp74ismo3e/_remote_module_non_scriptable.py
2023-11-23 12:29:40,982 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:30:42,780 - INFO - Created a temporary directory at /tmp/tmp4dbh3c2j
2023-11-23 12:30:42,780 - INFO - Writing /tmp/tmp4dbh3c2j/_remote_module_non_scriptable.py
2023-11-23 12:30:45,146 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:31:47,314 - INFO - Created a temporary directory at /tmp/tmpjass4rlb
2023-11-23 12:31:47,314 - INFO - Writing /tmp/tmpjass4rlb/_remote_module_non_scriptable.py
2023-11-23 12:31:49,690 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:32:52,257 - INFO - Created a temporary directory at /tmp/tmpo4l9kwnc
2023-11-23 12:32:52,257 - INFO - Writing /tmp/tmpo4l9kwnc/_remote_module_non_scriptable.py
2023-11-23 12:32:54,574 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:33:56,892 - INFO - Created a temporary directory at /tmp/tmpozv5f5k2
2023-11-23 12:33:56,892 - INFO - Writing /tmp/tmpozv5f5k2/_remote_module_non_scriptable.py
2023-11-23 12:33:59,197 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:35:01,320 - INFO - Created a temporary directory at /tmp/tmpyyk7iue2
2023-11-23 12:35:01,321 - INFO - Writing /tmp/tmpyyk7iue2/_remote_module_non_scriptable.py
2023-11-23 12:35:03,681 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:37:10,276 - INFO - Created a temporary directory at /tmp/tmpyfr5vwwk
2023-11-23 12:37:10,276 - INFO - Writing /tmp/tmpyfr5vwwk/_remote_module_non_scriptable.py
2023-11-23 12:37:12,655 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:38:15,069 - INFO - Created a temporary directory at /tmp/tmp3lf146a0
2023-11-23 12:38:15,069 - INFO - Writing /tmp/tmp3lf146a0/_remote_module_non_scriptable.py
2023-11-23 12:38:17,415 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:39:20,351 - INFO - Created a temporary directory at /tmp/tmppq5_4gwz
2023-11-23 12:39:20,351 - INFO - Writing /tmp/tmppq5_4gwz/_remote_module_non_scriptable.py
2023-11-23 12:39:22,661 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:40:25,264 - INFO - Created a temporary directory at /tmp/tmp63psze5p
2023-11-23 12:40:25,265 - INFO - Writing /tmp/tmp63psze5p/_remote_module_non_scriptable.py
2023-11-23 12:40:27,610 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:41:30,310 - INFO - Created a temporary directory at /tmp/tmpys1qln4v
2023-11-23 12:41:30,310 - INFO - Writing /tmp/tmpys1qln4v/_remote_module_non_scriptable.py
2023-11-23 12:41:32,657 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:42:34,582 - INFO - Created a temporary directory at /tmp/tmpx97u88py
2023-11-23 12:42:34,582 - INFO - Writing /tmp/tmpx97u88py/_remote_module_non_scriptable.py
2023-11-23 12:42:36,917 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:43:39,615 - INFO - Created a temporary directory at /tmp/tmpb7v8wpbr
2023-11-23 12:43:39,615 - INFO - Writing /tmp/tmpb7v8wpbr/_remote_module_non_scriptable.py
2023-11-23 12:43:41,966 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:44:44,212 - INFO - Created a temporary directory at /tmp/tmp20e82bof
2023-11-23 12:44:44,212 - INFO - Writing /tmp/tmp20e82bof/_remote_module_non_scriptable.py
2023-11-23 12:44:46,573 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:45:48,322 - INFO - Created a temporary directory at /tmp/tmpfm0muokk
2023-11-23 12:45:48,323 - INFO - Writing /tmp/tmpfm0muokk/_remote_module_non_scriptable.py
2023-11-23 12:45:50,682 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:46:52,508 - INFO - Created a temporary directory at /tmp/tmpnr_tekat
2023-11-23 12:46:52,509 - INFO - Writing /tmp/tmpnr_tekat/_remote_module_non_scriptable.py
2023-11-23 12:46:54,881 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:47:57,043 - INFO - Created a temporary directory at /tmp/tmpb138zrkz
2023-11-23 12:47:57,044 - INFO - Writing /tmp/tmpb138zrkz/_remote_module_non_scriptable.py
2023-11-23 12:47:59,379 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:49:01,198 - INFO - Created a temporary directory at /tmp/tmp26lisu9h
2023-11-23 12:49:01,198 - INFO - Writing /tmp/tmp26lisu9h/_remote_module_non_scriptable.py
2023-11-23 12:49:03,521 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:50:05,792 - INFO - Created a temporary directory at /tmp/tmp61mdubsu
2023-11-23 12:50:05,792 - INFO - Writing /tmp/tmp61mdubsu/_remote_module_non_scriptable.py
2023-11-23 12:50:08,122 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:51:11,560 - INFO - Created a temporary directory at /tmp/tmp_yh7aoo4
2023-11-23 12:51:11,560 - INFO - Writing /tmp/tmp_yh7aoo4/_remote_module_non_scriptable.py
2023-11-23 12:51:13,964 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:52:14,714 - INFO - Created a temporary directory at /tmp/tmp8pyhkrew
2023-11-23 12:52:14,715 - INFO - Writing /tmp/tmp8pyhkrew/_remote_module_non_scriptable.py
2023-11-23 12:52:17,070 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:53:19,247 - INFO - Created a temporary directory at /tmp/tmpdcagsmgx
2023-11-23 12:53:19,247 - INFO - Writing /tmp/tmpdcagsmgx/_remote_module_non_scriptable.py
2023-11-23 12:53:21,603 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:54:24,034 - INFO - Created a temporary directory at /tmp/tmp2uf87urb
2023-11-23 12:54:24,034 - INFO - Writing /tmp/tmp2uf87urb/_remote_module_non_scriptable.py
2023-11-23 12:54:26,392 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:55:28,963 - INFO - Created a temporary directory at /tmp/tmp3awgtdft
2023-11-23 12:55:28,964 - INFO - Writing /tmp/tmp3awgtdft/_remote_module_non_scriptable.py
2023-11-23 12:55:31,325 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:56:32,894 - INFO - Created a temporary directory at /tmp/tmpe_9rfdka
2023-11-23 12:56:32,894 - INFO - Writing /tmp/tmpe_9rfdka/_remote_module_non_scriptable.py
2023-11-23 12:56:35,237 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:57:37,015 - INFO - Created a temporary directory at /tmp/tmphcwlzw3g
2023-11-23 12:57:37,015 - INFO - Writing /tmp/tmphcwlzw3g/_remote_module_non_scriptable.py
2023-11-23 12:57:39,388 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:58:02,698 - INFO - Created a temporary directory at /tmp/tmp6pms8at1
2023-11-23 12:58:02,698 - INFO - Writing /tmp/tmp6pms8at1/_remote_module_non_scriptable.py
2023-11-23 12:58:05,044 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 12:58:22,618 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 12:58:31,300 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 13:13:29,302 - INFO - Created a temporary directory at /tmp/tmpuutsaogr
2023-11-23 13:13:29,302 - INFO - Writing /tmp/tmpuutsaogr/_remote_module_non_scriptable.py
2023-11-23 13:13:31,609 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 13:13:49,189 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 13:13:57,927 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 13:18:05,478 - INFO - Created a temporary directory at /tmp/tmp4cobtbia
2023-11-23 13:18:05,478 - INFO - Writing /tmp/tmp4cobtbia/_remote_module_non_scriptable.py
2023-11-23 13:18:07,744 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 13:18:25,232 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 13:18:33,889 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 13:21:14,851 - INFO - Received requests to /inference endpoint
2023-11-23 13:21:14,953 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 13:21:14,953 - INFO - Received request: {'username': 'admin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-23 13:21:24,101 - INFO - Processed the request successfully
2023-11-23 13:23:15,418 - INFO - Created a temporary directory at /tmp/tmpk2suv46u
2023-11-23 13:23:15,419 - INFO - Writing /tmp/tmpk2suv46u/_remote_module_non_scriptable.py
2023-11-23 13:23:17,554 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 13:23:35,938 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 13:23:44,634 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 13:23:50,599 - INFO - Created a temporary directory at /tmp/tmp5c09l6yy
2023-11-23 13:23:50,599 - INFO - Writing /tmp/tmp5c09l6yy/_remote_module_non_scriptable.py
2023-11-23 13:23:52,784 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 13:24:11,888 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 13:24:20,615 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 13:25:55,503 - INFO - Received requests to /inference endpoint
2023-11-23 13:25:55,604 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 13:25:55,604 - INFO - Received request: {'username': 'admin', 'prompt': 'hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string'}
2023-11-23 13:26:02,572 - INFO - Processed the request successfully
2023-11-23 14:33:21,216 - INFO - Created a temporary directory at /tmp/tmpjb8ykvvk
2023-11-23 14:33:21,216 - INFO - Writing /tmp/tmpjb8ykvvk/_remote_module_non_scriptable.py
2023-11-23 14:33:23,368 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 14:33:40,879 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 14:33:49,607 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 14:33:56,713 - INFO - Created a temporary directory at /tmp/tmpbckzbvbo
2023-11-23 14:33:56,713 - INFO - Writing /tmp/tmpbckzbvbo/_remote_module_non_scriptable.py
2023-11-23 14:33:58,978 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 14:34:16,672 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 14:34:25,394 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 14:42:58,320 - INFO - Received requests to /inference endpoint
2023-11-23 14:42:58,421 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 14:42:58,422 - INFO - Received request: {'username': 'admin', 'prompt': 'hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'llama_70b'}
2023-11-23 14:43:05,361 - INFO - Processed the request successfully
2023-11-23 14:43:12,311 - INFO - Received requests to /inference endpoint
2023-11-23 14:43:12,412 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 14:43:12,413 - INFO - Received request: {'username': 'admin', 'prompt': 'hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'llama_13b'}
2023-11-23 14:43:19,189 - INFO - Processed the request successfully
2023-11-23 15:30:56,689 - INFO - Received requests to /inference endpoint
2023-11-23 15:30:56,790 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 15:30:56,790 - INFO - Received request: {'username': 'alex', 'prompt': 'hi', 'memory': False, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'llama_70b'}
2023-11-23 15:31:03,235 - INFO - Processed the request successfully
2023-11-23 15:39:21,434 - INFO - Received requests to /inference endpoint
2023-11-23 15:39:21,535 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 15:39:21,535 - INFO - Received request: {'username': 'alex', 'prompt': 'Hi', 'memory': False, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-11-23 15:39:27,700 - INFO - Processed the request successfully
2023-11-23 15:39:37,523 - INFO - Received requests to /inference endpoint
2023-11-23 15:39:37,625 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 15:39:37,625 - INFO - Received request: {'username': 'alex', 'prompt': 'Hi', 'memory': False, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-23 15:39:43,538 - INFO - Processed the request successfully
2023-11-23 15:44:58,145 - INFO - Received requests to /inference endpoint
2023-11-23 15:44:58,246 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 15:44:58,246 - INFO - Received request: {'username': 'alex', 'prompt': 'my name is Amin', 'memory': False, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-23 15:45:02,916 - INFO - Processed the request successfully
2023-11-23 15:45:14,247 - INFO - Received requests to /inference endpoint
2023-11-23 15:45:14,348 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 15:45:14,348 - INFO - Received request: {'username': 'alex', 'prompt': 'what was my previous question ?', 'memory': False, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-23 15:45:23,367 - INFO - Processed the request successfully
2023-11-23 15:45:40,795 - INFO - Received requests to /inference endpoint
2023-11-23 15:45:40,896 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 15:45:40,896 - INFO - Received request: {'username': 'alex', 'prompt': 'I told you name, what was my name?', 'memory': False, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-23 15:45:46,943 - INFO - Processed the request successfully
2023-11-23 15:46:04,201 - INFO - Received requests to /inference endpoint
2023-11-23 15:46:04,302 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 15:46:04,302 - INFO - Received request: {'username': 'alex', 'prompt': 'my name is Amin', 'memory': False, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-23 15:46:08,827 - INFO - Processed the request successfully
2023-11-23 15:46:15,746 - INFO - Received requests to /inference endpoint
2023-11-23 15:46:15,847 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 15:46:15,847 - INFO - Received request: {'username': 'alex', 'prompt': 'what was my name?', 'memory': False, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-23 15:46:25,574 - INFO - Processed the request successfully
2023-11-23 16:00:44,051 - INFO - Received requests to /inference endpoint
2023-11-23 16:00:44,152 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 16:00:44,152 - INFO - Received request: {'username': 'alex', 'prompt': 'where is London?', 'memory': False, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-23 16:00:58,162 - INFO - Processed the request successfully
2023-11-23 16:07:32,433 - INFO - Received requests to /inference endpoint
2023-11-23 16:07:32,534 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 16:07:32,534 - INFO - Received request: {'username': 'alex', 'prompt': 'what was my previous question about?', 'memory': False, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-23 16:07:41,463 - INFO - Processed the request successfully
2023-11-23 16:12:21,583 - INFO - Received requests to /inference endpoint
2023-11-23 16:12:21,684 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 16:12:21,684 - INFO - Received request: {'username': 'alex', 'prompt': 'hi', 'memory': False, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-23 16:12:28,046 - INFO - Processed the request successfully
2023-11-23 16:14:54,577 - INFO - Created a temporary directory at /tmp/tmpe76emsue
2023-11-23 16:14:54,577 - INFO - Writing /tmp/tmpe76emsue/_remote_module_non_scriptable.py
2023-11-23 16:14:56,681 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 16:15:14,381 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 16:15:23,087 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 16:15:29,688 - INFO - Created a temporary directory at /tmp/tmpj2a3qw3q
2023-11-23 16:15:29,689 - INFO - Writing /tmp/tmpj2a3qw3q/_remote_module_non_scriptable.py
2023-11-23 16:15:31,851 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-23 16:15:49,766 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-23 16:15:58,513 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-23 16:19:39,870 - INFO - Received requests to /inference endpoint
2023-11-23 16:19:39,971 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 16:19:39,971 - INFO - Received request: {'username': 'admin', 'prompt': 'hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'string'}
2023-11-23 16:19:46,762 - INFO - Processed the request successfully
2023-11-23 16:20:04,412 - INFO - Received requests to /inference endpoint
2023-11-23 16:20:04,514 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 16:20:04,514 - INFO - Received request: {'username': 'admin', 'prompt': 'my name is admin', 'memory': True, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'string'}
2023-11-23 16:20:09,768 - INFO - Processed the request successfully
2023-11-23 16:20:23,309 - INFO - Received requests to /inference endpoint
2023-11-23 16:20:23,410 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 16:20:23,411 - INFO - Received request: {'username': 'admin', 'prompt': 'what was my name?', 'memory': True, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'string'}
2023-11-23 16:20:30,829 - INFO - Processed the request successfully
2023-11-23 16:20:57,682 - INFO - Received requests to /inference endpoint
2023-11-23 16:20:57,783 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 16:20:57,783 - INFO - Received request: {'username': 'admin', 'prompt': 'where is the capital of the UK?', 'memory': True, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'string'}
2023-11-23 16:21:04,300 - INFO - Processed the request successfully
2023-11-23 16:21:23,741 - INFO - Received requests to /inference endpoint
2023-11-23 16:21:23,842 - INFO - Received a batch of request with batch size of: 1 
2023-11-23 16:21:23,842 - INFO - Received request: {'username': 'admin', 'prompt': 'what was my previous question about?', 'memory': True, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'string'}
2023-11-23 16:21:32,897 - INFO - Processed the request successfully
2023-11-24 09:43:45,747 - INFO - Received requests to /inference endpoint
2023-11-24 09:43:45,848 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 09:43:45,848 - INFO - Received request: {'username': 'admin', 'prompt': 'Hi, my name is Admin', 'memory': True, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'Llama_70b'}
2023-11-24 09:43:55,246 - INFO - Processed the request successfully
2023-11-24 09:44:18,755 - INFO - Received requests to /inference endpoint
2023-11-24 09:44:18,856 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 09:44:18,856 - INFO - Received request: {'username': 'admin', 'prompt': 'what was my previous prompt?', 'memory': True, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'Llama_70b'}
2023-11-24 09:44:27,432 - INFO - Processed the request successfully
2023-11-24 09:44:47,855 - INFO - Received requests to /inference endpoint
2023-11-24 09:44:47,957 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 09:44:47,957 - INFO - Received request: {'username': 'admin', 'prompt': 'what was my previous prompt?', 'memory': True, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': 'string', 'llm_model': 'Llama_13b'}
2023-11-24 09:44:56,326 - INFO - Processed the request successfully
2023-11-24 11:57:07,619 - INFO - Received requests to /inference endpoint
2023-11-24 11:57:07,720 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 11:57:07,720 - INFO - Received request: {'username': 'alex', 'prompt': 'where is the capital of France?', 'memory': False, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-11-24 11:57:18,765 - INFO - Processed the request successfully
2023-11-24 11:57:32,035 - INFO - Received requests to /inference endpoint
2023-11-24 11:57:32,136 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 11:57:32,137 - INFO - Received request: {'username': 'alex', 'prompt': 'what was my previous question ?', 'memory': False, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-11-24 11:57:41,574 - INFO - Processed the request successfully
2023-11-24 11:58:00,398 - INFO - Received requests to /inference endpoint
2023-11-24 11:58:00,499 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 11:58:00,499 - INFO - Received request: {'username': 'alex', 'prompt': 'what was my previous prompt?', 'memory': False, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-11-24 11:58:07,056 - INFO - Processed the request successfully
2023-11-24 11:58:22,765 - INFO - Received requests to /inference endpoint
2023-11-24 11:58:22,866 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 11:58:22,866 - INFO - Received request: {'username': 'alex', 'prompt': 'where is the capital of France?', 'memory': False, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-11-24 11:58:33,997 - INFO - Processed the request successfully
2023-11-24 11:58:51,482 - INFO - Received requests to /inference endpoint
2023-11-24 11:58:51,583 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 11:58:51,584 - INFO - Received request: {'username': 'alex', 'prompt': 'what was my previous prompt about?', 'memory': False, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-11-24 11:59:00,208 - INFO - Processed the request successfully
2023-11-24 12:01:15,545 - INFO - Received requests to /inference endpoint
2023-11-24 12:01:15,647 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 12:01:15,647 - INFO - Received request: {'username': 'alex', 'prompt': 'where is the capital of France?', 'memory': True, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-11-24 12:01:20,173 - INFO - Processed the request successfully
2023-11-24 12:01:32,396 - INFO - Received requests to /inference endpoint
2023-11-24 12:01:32,497 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 12:01:32,497 - INFO - Received request: {'username': 'alex', 'prompt': 'What was my previous prompt?', 'memory': True, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-11-24 12:01:41,192 - INFO - Processed the request successfully
2023-11-24 12:06:49,837 - INFO - Received requests to /inference endpoint
2023-11-24 12:06:49,938 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 12:06:49,938 - INFO - Received request: {'username': 'alex', 'prompt': 'did I ask question about paris?', 'memory': True, 'conversation_number': 1, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-11-24 12:06:56,663 - INFO - Processed the request successfully
2023-11-24 12:07:30,597 - INFO - Received requests to /inference endpoint
2023-11-24 12:07:30,698 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 12:07:30,698 - INFO - Received request: {'username': 'alex', 'prompt': 'where is London located?', 'memory': True, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_70b'}
2023-11-24 12:07:41,822 - INFO - Processed the request successfully
2023-11-24 12:08:00,768 - INFO - Received requests to /inference endpoint
2023-11-24 12:08:00,869 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 12:08:00,869 - INFO - Received request: {'username': 'alex', 'prompt': 'did I ask question about London?', 'memory': True, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-24 12:08:06,731 - INFO - Processed the request successfully
2023-11-24 12:08:23,289 - INFO - Received requests to /inference endpoint
2023-11-24 12:08:23,390 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 12:08:23,390 - INFO - Received request: {'username': 'alex', 'prompt': 'did I asked where London is Located?', 'memory': True, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-24 12:08:31,514 - INFO - Processed the request successfully
2023-11-24 12:09:00,315 - INFO - Received requests to /inference endpoint
2023-11-24 12:09:00,416 - INFO - Received a batch of request with batch size of: 1 
2023-11-24 12:09:00,417 - INFO - Received request: {'username': 'alex', 'prompt': 'give me brief of our chats before this prompt', 'memory': True, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-11-24 12:09:23,082 - INFO - Processed the request successfully
2023-11-24 12:58:21,180 - INFO - Created a temporary directory at /tmp/tmpoi9rm3t6
2023-11-24 12:58:21,180 - INFO - Writing /tmp/tmpoi9rm3t6/_remote_module_non_scriptable.py
2023-11-24 12:58:23,306 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 12:58:40,894 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 12:58:49,645 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-24 12:58:56,194 - INFO - Created a temporary directory at /tmp/tmp906n0fra
2023-11-24 12:58:56,194 - INFO - Writing /tmp/tmp906n0fra/_remote_module_non_scriptable.py
2023-11-24 12:58:58,396 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 12:59:16,220 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 12:59:24,937 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-24 13:42:14,482 - INFO - Created a temporary directory at /tmp/tmpxi8mh_ak
2023-11-24 13:42:14,482 - INFO - Writing /tmp/tmpxi8mh_ak/_remote_module_non_scriptable.py
2023-11-24 13:42:16,638 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 13:42:34,329 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 13:42:43,072 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-24 13:42:49,588 - INFO - Created a temporary directory at /tmp/tmpyi32iyuj
2023-11-24 13:42:49,588 - INFO - Writing /tmp/tmpyi32iyuj/_remote_module_non_scriptable.py
2023-11-24 13:42:51,804 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 13:43:09,365 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 13:43:18,074 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-24 13:45:31,246 - INFO - Created a temporary directory at /tmp/tmpzsfn_vvw
2023-11-24 13:45:31,246 - INFO - Writing /tmp/tmpzsfn_vvw/_remote_module_non_scriptable.py
2023-11-24 13:45:33,448 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 13:45:51,250 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 13:45:59,988 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-11-24 13:46:06,278 - INFO - Created a temporary directory at /tmp/tmpjxsa7vpl
2023-11-24 13:46:06,278 - INFO - Writing /tmp/tmpjxsa7vpl/_remote_module_non_scriptable.py
2023-11-24 13:46:08,508 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 16:14:29,257 - INFO - Created a temporary directory at /tmp/tmpwi0t5sf9
2023-11-24 16:14:29,258 - INFO - Writing /tmp/tmpwi0t5sf9/_remote_module_non_scriptable.py
2023-11-24 16:14:31,556 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 16:14:49,192 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 16:15:04,555 - INFO - Created a temporary directory at /tmp/tmpfgxakqhk
2023-11-24 16:15:04,556 - INFO - Writing /tmp/tmpfgxakqhk/_remote_module_non_scriptable.py
2023-11-24 16:16:08,239 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 16:16:12,441 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 16:19:20,847 - INFO - Created a temporary directory at /tmp/tmpbyn56i3p
2023-11-24 16:19:20,847 - INFO - Writing /tmp/tmpbyn56i3p/_remote_module_non_scriptable.py
2023-11-24 16:19:23,271 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 16:19:40,999 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 16:19:56,558 - INFO - Created a temporary directory at /tmp/tmp4mh0e5us
2023-11-24 16:19:56,558 - INFO - Writing /tmp/tmp4mh0e5us/_remote_module_non_scriptable.py
2023-11-24 16:19:58,422 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 16:20:02,922 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 16:24:52,413 - INFO - Created a temporary directory at /tmp/tmpjkss21or
2023-11-24 16:24:52,413 - INFO - Writing /tmp/tmpjkss21or/_remote_module_non_scriptable.py
2023-11-24 16:24:54,738 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 16:25:12,084 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 16:25:26,647 - INFO - Created a temporary directory at /tmp/tmpgiw8zzz6
2023-11-24 16:25:26,647 - INFO - Writing /tmp/tmpgiw8zzz6/_remote_module_non_scriptable.py
2023-11-24 16:25:28,448 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 16:25:32,666 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 16:26:27,492 - INFO - Created a temporary directory at /tmp/tmpgd0se94d
2023-11-24 16:26:27,493 - INFO - Writing /tmp/tmpgd0se94d/_remote_module_non_scriptable.py
2023-11-24 16:26:29,828 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 16:26:47,350 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 16:29:55,708 - INFO - Created a temporary directory at /tmp/tmp6kcast2l
2023-11-24 16:29:55,708 - INFO - Writing /tmp/tmp6kcast2l/_remote_module_non_scriptable.py
2023-11-24 16:29:58,038 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 16:30:15,722 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-24 16:31:35,276 - INFO - Created a temporary directory at /tmp/tmpbs261uga
2023-11-24 16:31:35,277 - INFO - Writing /tmp/tmpbs261uga/_remote_module_non_scriptable.py
2023-11-24 16:31:37,588 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-24 16:31:55,077 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-27 09:52:02,915 - INFO - Created a temporary directory at /tmp/tmpme30l9h9
2023-11-27 09:52:02,915 - INFO - Writing /tmp/tmpme30l9h9/_remote_module_non_scriptable.py
2023-11-27 09:52:05,183 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-27 09:52:22,711 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-27 09:52:37,971 - INFO - Created a temporary directory at /tmp/tmpkazi7fy2
2023-11-27 09:52:37,971 - INFO - Writing /tmp/tmpkazi7fy2/_remote_module_non_scriptable.py
2023-11-27 09:52:39,805 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-27 09:52:43,763 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-27 09:54:02,579 - INFO - Created a temporary directory at /tmp/tmpkj0bmpr9
2023-11-27 09:54:02,580 - INFO - Writing /tmp/tmpkj0bmpr9/_remote_module_non_scriptable.py
2023-11-27 09:54:04,862 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-27 09:54:22,230 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-27 09:54:36,775 - INFO - Created a temporary directory at /tmp/tmpk_d8gkff
2023-11-27 09:54:36,776 - INFO - Writing /tmp/tmpk_d8gkff/_remote_module_non_scriptable.py
2023-11-27 09:54:38,606 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-11-27 09:54:42,469 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-11-27 09:57:00,310 - INFO - request processed successfully username='admin' collection_name='videos' mode='create_collection' vectorDB_type='Weaviate' file_path='string': %s
2023-11-27 10:01:33,265 - ERROR - An error occurred: [Errno 20] Not a directory: '/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/docs/paper.pdf'
2023-11-27 10:09:22,302 - ERROR - An error occurred: [Errno 20] Not a directory: '/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/admin_Car rental doc.pdf'
2023-11-27 10:55:03,854 - ERROR - An error occurred: [Errno 20] Not a directory: '/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/docs/paper.pdf'
2023-11-27 11:00:59,336 - INFO - request processed successfully username='admin' collection_name='Admin_videos' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/f84b2db7d4a8f72d': %s
2023-11-27 11:05:11,372 - INFO - request processed successfully username='admin' collection_name='Admin_videos' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/536b0eacb9e16257': %s
2023-11-27 11:24:52,458 - ERROR - An error occurred: (sqlite3.OperationalError) attempt to write a readonly database
[SQL: UPDATE users SET collection_names=? WHERE users.id = ?]
[parameters: ('admin_Video,admin_Video2,admin_Test,admin_my_collection,admin_xc,admin_vc,admin_ff,admin_None,admin_zz,admin_test1,admin_test2,admin_test3,admin_pdf,A_video,A_web,Admin_ssd,Admin_videos,Admin_amin', 1)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2023-11-27 11:26:12,608 - ERROR - An error occurred: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.OperationalError) attempt to write a readonly database
[SQL: UPDATE users SET collection_names=? WHERE users.id = ?]
[parameters: ('admin_Video,admin_Video2,admin_Test,admin_my_collection,admin_xc,admin_vc,admin_ff,admin_None,admin_zz,admin_test1,admin_test2,admin_test3,admin_pdf,A_video,A_web,Admin_ssd,Admin_videos,Admin_amin', 1)]
(Background on this error at: https://sqlalche.me/e/20/e3q8) (Background on this error at: https://sqlalche.me/e/20/7s2a)
2023-11-27 11:28:24,866 - INFO - request processed successfully username='admin' collection_name='Admin_General_collection' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/59906acd283673a0': %s
2023-11-27 12:27:48,177 - ERROR - An error occurred: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.OperationalError) attempt to write a readonly database
[SQL: UPDATE users SET collection_names=? WHERE users.id = ?]
[parameters: ('admin_Video,admin_Video2,admin_Test,admin_my_collection,admin_xc,admin_vc,admin_ff,admin_None,admin_zz,admin_test1,admin_test2,admin_test3,admin_pdf,A_video,A_web,Admin_ssd,Admin_videos,Admin_amin', 1)]
(Background on this error at: https://sqlalche.me/e/20/e3q8) (Background on this error at: https://sqlalche.me/e/20/7s2a)
2023-11-27 12:33:17,440 - INFO - request processed successfully username='amin' collection_name=None mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/38a6b2c006a2e2d9': %s
2023-12-05 12:49:46,413 - INFO - Created a temporary directory at /tmp/tmpfd17agu7
2023-12-05 12:49:46,413 - INFO - Writing /tmp/tmpfd17agu7/_remote_module_non_scriptable.py
2023-12-05 12:49:50,023 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-05 12:59:22,312 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-05 12:59:38,525 - INFO - Created a temporary directory at /tmp/tmp704ler0i
2023-12-05 12:59:38,525 - INFO - Writing /tmp/tmp704ler0i/_remote_module_non_scriptable.py
2023-12-06 10:14:45,285 - INFO - Created a temporary directory at /tmp/tmpemk428xz
2023-12-06 10:14:45,285 - INFO - Writing /tmp/tmpemk428xz/_remote_module_non_scriptable.py
2023-12-06 10:14:48,704 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-06 10:20:26,244 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-06 10:20:42,262 - INFO - Created a temporary directory at /tmp/tmpilr_7vuk
2023-12-06 10:20:42,263 - INFO - Writing /tmp/tmpilr_7vuk/_remote_module_non_scriptable.py
2023-12-06 10:21:56,387 - INFO - Created a temporary directory at /tmp/tmpuh6vs2_q
2023-12-06 10:21:56,387 - INFO - Writing /tmp/tmpuh6vs2_q/_remote_module_non_scriptable.py
2023-12-06 10:23:24,436 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-06 10:23:41,517 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-06 10:25:34,841 - INFO - Created a temporary directory at /tmp/tmpx6t7m6ts
2023-12-06 10:25:34,841 - INFO - Writing /tmp/tmpx6t7m6ts/_remote_module_non_scriptable.py
2023-12-06 10:25:37,172 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-06 10:27:34,676 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-06 10:27:49,354 - INFO - Created a temporary directory at /tmp/tmp7l33bz1s
2023-12-06 10:27:49,355 - INFO - Writing /tmp/tmp7l33bz1s/_remote_module_non_scriptable.py
2023-12-06 10:27:51,218 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-06 10:27:56,180 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-06 11:04:19,573 - INFO - Received requests to /inference endpoint
2023-12-06 11:04:19,674 - INFO - Received a batch of request with batch size of: 1 
2023-12-06 11:04:19,675 - INFO - Received request: {'username': 'admin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': False, 'collection_name': 'Amin', 'llm_model': 'Llama_70b'}
2023-12-06 11:04:54,237 - INFO - Received requests to /inference endpoint
2023-12-06 11:04:54,338 - INFO - Received a batch of request with batch size of: 1 
2023-12-06 11:04:54,338 - INFO - Received request: {'username': 'admin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': False, 'collection_name': 'General_collection', 'llm_model': 'Llama_70b'}
2023-12-06 11:05:03,064 - INFO - Received requests to /inference endpoint
2023-12-06 11:05:03,165 - INFO - Received a batch of request with batch size of: 1 
2023-12-06 11:05:03,165 - INFO - Received request: {'username': 'admin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': False, 'collection_name': 'General_collection', 'llm_model': 'Llama_70b'}
2023-12-06 11:05:46,516 - INFO - Received requests to /inference endpoint
2023-12-06 11:05:46,617 - INFO - Received a batch of request with batch size of: 1 
2023-12-06 11:05:46,617 - INFO - Received request: {'username': 'admin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': False, 'collection_name': 'General_collection', 'llm_model': 'Llama_70b'}
2023-12-06 11:06:33,448 - INFO - Received requests to /inference endpoint
2023-12-06 11:06:33,549 - INFO - Received a batch of request with batch size of: 1 
2023-12-06 11:06:33,549 - INFO - Received request: {'username': 'amin', 'prompt': 'Hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': False, 'collection_name': 'General_collection', 'llm_model': 'Llama_70b'}
2023-12-06 11:06:33,572 - ERROR - Error processing the request: Error during query: [{'locations': [{'column': 6, 'line': 1}], 'message': 'Cannot query field "General_collection" on type "GetObjectsObj". Did you mean "Admin_General_collection"?', 'path': None}]
2023-12-06 11:17:34,034 - INFO - checking the request/ username='amin' class_name='video' mode='create_class' vectorDB_type='Weaviate' file_path='string': %s
2023-12-06 11:17:34,123 - INFO - checkpoint 1
2023-12-06 11:17:34,123 - INFO - checkpoint 2 amin: %s
2023-12-06 11:17:34,123 - INFO - checkpoint 2 amin_video: %s
2023-12-06 11:24:44,014 - INFO - checking the request/ username='amin' class_name='document' mode='create_class' vectorDB_type='Weaviate' file_path='string': %s
2023-12-06 11:24:44,101 - INFO - checkpoint 1
2023-12-06 11:24:44,101 - INFO - checkpoint 2 amin: %s
2023-12-06 11:24:44,101 - INFO - checkpoint 2 amin_document: %s
2023-12-06 11:24:44,120 - INFO - success: class document created for user amin
2023-12-06 11:31:00,074 - INFO - checking the request/ username='amin' class_name='web' mode='create_class' vectorDB_type='Weaviate' file_path='string': %s
2023-12-06 11:31:00,135 - INFO - checkpoint 1
2023-12-06 11:31:00,135 - INFO - checkpoint 2 amin: %s
2023-12-06 11:31:00,135 - INFO - checkpoint 2 amin_web: %s
2023-12-06 11:31:00,166 - INFO - success: class web created for user amin
2023-12-06 11:46:23,071 - INFO - checking the request/ username='amin' class_name='web2' mode='create_class' vectorDB_type='Weaviate' file_path='string': %s
2023-12-06 11:46:23,132 - INFO - checkpoint 1
2023-12-06 11:46:23,133 - INFO - checkpoint 2 amin: %s
2023-12-06 11:46:23,133 - INFO - checkpoint 2 amin_web2: %s
2023-12-06 11:46:23,177 - INFO - class name added successfully to database
2023-12-06 11:46:23,177 - INFO - success: class web2 created for user amin
2023-12-06 11:52:25,070 - INFO - request processed successfully username='amin' class_name='web2' mode='delete_class' vectorDB_type='Weaviate' file_path='string': %s
2023-12-06 11:52:25,070 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 12:02:08,040 - INFO - Created a temporary directory at /tmp/tmpvmh12ryr
2023-12-06 12:02:08,040 - INFO - Writing /tmp/tmpvmh12ryr/_remote_module_non_scriptable.py
2023-12-06 12:02:10,623 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-06 12:02:45,785 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-06 12:03:01,497 - INFO - Created a temporary directory at /tmp/tmpnuw3eece
2023-12-06 12:03:01,497 - INFO - Writing /tmp/tmpnuw3eece/_remote_module_non_scriptable.py
2023-12-06 12:03:03,336 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-06 12:03:07,744 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-06 12:03:51,534 - INFO - Received requests to /inference endpoint
2023-12-06 12:03:51,635 - INFO - Received a batch of request with batch size of: 1 
2023-12-06 12:03:51,636 - INFO - Received request: {'username': 'amin', 'prompt': 'hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': False, 'collection_name': 'web', 'llm_model': 'Llama_70b'}
2023-12-06 12:04:09,090 - INFO - Received requests to /inference endpoint
2023-12-06 12:04:09,191 - INFO - Received a batch of request with batch size of: 1 
2023-12-06 12:04:09,191 - INFO - Received request: {'username': 'amin', 'prompt': 'hi', 'memory': False, 'conversation_number': 0, 'AI_assistance': False, 'collection_name': 'video', 'llm_model': 'Llama_70b'}
2023-12-06 13:31:34,071 - INFO - checking the request/ username='string' class_name='ee' mode='create_collection' vectorDB_type='Weaviate' file_path='string': %s
2023-12-06 13:31:34,158 - INFO - checkpoint 1
2023-12-06 13:31:34,158 - INFO - checkpoint 2 string: %s
2023-12-06 13:31:34,158 - INFO - checkpoint 2 string_ee: %s
2023-12-06 13:31:34,243 - INFO - class name added successfully to database
2023-12-06 13:31:34,243 - INFO - success: class ee created for user string
2023-12-06 13:33:22,281 - INFO - checking the request/ username='amin' class_name='cd' mode='create_collection' vectorDB_type='Weaviate' file_path=None: %s
2023-12-06 13:33:22,369 - INFO - checkpoint 1
2023-12-06 13:33:22,369 - INFO - checkpoint 2 amin: %s
2023-12-06 13:33:22,369 - INFO - checkpoint 2 amin_cd: %s
2023-12-06 13:33:22,405 - INFO - class name added successfully to database
2023-12-06 13:33:22,405 - INFO - success: class cd created for user amin
2023-12-06 13:33:44,717 - INFO - request processed successfully username='amin' class_name=None mode=None vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/bf47e2ef6b86afed': %s
2023-12-06 13:33:44,717 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 14:00:52,783 - INFO - Created a temporary directory at /tmp/tmphrl8ikoy
2023-12-06 14:00:52,784 - INFO - Writing /tmp/tmphrl8ikoy/_remote_module_non_scriptable.py
2023-12-06 14:00:55,129 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-06 14:01:18,877 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-06 14:01:34,022 - INFO - Created a temporary directory at /tmp/tmpi4v65qg6
2023-12-06 14:01:34,022 - INFO - Writing /tmp/tmpi4v65qg6/_remote_module_non_scriptable.py
2023-12-06 14:01:35,856 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-06 14:01:40,113 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-06 14:03:32,776 - INFO - actors creation successful [Actor(WeaviateEmbedder, 5ae9dee07e49ab95d952437401000000), Actor(WeaviateEmbedder, c394f0a0dbcccd8776b8dd9101000000), Actor(WeaviateEmbedder, 3743bbe20c6b8637fe9468ca01000000)]: %s
2023-12-06 14:03:32,777 - INFO - check 1st step of ray was successful
2023-12-06 14:03:32,777 - INFO - check if ray was successful:
2023-12-06 14:03:32,777 - INFO - check weaviate add data, 
2023-12-06 14:03:32,777 - INFO - request processed successfully username='amin' class_name='web' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/docs': %s
2023-12-06 14:03:34,491 - INFO - Check the data that is being passed [{'page_content': '3 Temporal Early Exits Video Object Detection Pipeline\nWe propose a novel video object detection pipeline to tackle the computational complexity of video\nobject detection. An overview of the proposed pipeline is shown in ﬁgure 2. Given a sequence of\nvideo frames, multiple Temporal Early Exit Modules (TEEM) are used to build the early exit branches.\nEach early exit branch uses features extracted in the early stages of the feature network to identify\nany semantic variation between the features of consecutive video frames. If any semantic variations\nis distinguished by a TEEM, a full computation effort (main branch) will be needed to process the\nvideo frame. On the other hand, the computation process of a video frame without any semantic\nvariations will be terminated at an early exit branch and the detection results from the previous frame\nwill be reused. We describe the implementation details of the proposed pipeline in this section.\n3.1 Scenery Change\nThe key challenge in identifying feature variations is qualifying semantic dissimilarities between\nimages [ 1]. To address this challenge, we introduce a scenery change (SC)metric to quantify\nsemantic variations between video frames. We deﬁne a semantic variation to be only a variation in\nobjects of interest where noisy variations are considered to be nuisance. Given any pairs of video\nframes, a TEEM aims to identify semantic differences between frames. A scenery changes between a\npair of video frames (fri,fri+j), if the maximum motion in the Moving Fields of Interest (MFI) is\ngreater than the variation threshold value τvaras follows,\nSC(fri,fri+j) ={\n1 max( MFI(fri,fri+j))>τvar\n0otherwise. (1)\nWe compute MFI set by measuring intersection over union (IoU) of each object of interest, Ok, across\nframe pairs as follows,\nMFI(ok) ={\n1−IoU(Ofri\nk,Ofri+j\nk )ifOk∈fri∧fri+j\n1 Otherwise. (2)\nEach element in MFI quantiﬁes the variation in an object of interest between a pair of video frames.\nThe scenery change metric distinguishes noisy variations from semantic variations in video frame\npairs. Figure 3 visualizes some examples of scenery change metric for video frame pair.\n3.2 Temporal Early Exit Module\nTo detect a scenery change between frame pairs (fri,fri+j), we propose TEEM which consists of\nAttention andClassiﬁer components. This section describes these components in details. Figure 2\nshows an overview of a TEEM.\nTheAttention component is represented as a function parameterized by θAttenlin equation (3).FAttn\ntakesZl\niandZl\ni+jas inputs, and outputs an attention map AttMaplwhich encodes the semantic\nvariations happened between friandfri+j. In equation (3),Zl\niandZl\ni+jare image features of\nthe previous friand currentfri+jvideo frames, respectively, encoded by convolutional layer lin\nthe feature network. The attention function captures the semantic variations between frame pairs\nin the representation space by subtracting Zl\nifromZl\ni+jin equation (4). Then, by concatenating\nthe subtraction results with the feature representation of the current video frame, Zl\ni+j, a 2D spatial\nattention map associated with the video frame pair is generated in equation (6).\nAttMapl=FAttn(Zl\ni,Zl\ni+j;θAttenl). (3)\nZsub=Zi+j−Zj, (4)\n¯Zc=Conct [Zi:Zsub], (5)\nAttMapl=Sigmoid (Conv(¯Zc)), (6)\n4', 'document_title': 'paper.pdf'}, {'page_content': 'In equations (6)and(5),Conct [; ],Conv ,Sigmoid indicate concatenation, convolutional layer and\nsigmoid function, respectively. Following [ 13], to avoid introducing any form of global normalization,\nwe use sigmoid, instead of softmax, for computing the attention map. The attention map makes\nTEEM learn and focus on the motions of objects of interest between frame pairs. Notably, a TEEM\nrequires storing the latest feature maps Zl\nito be used for successive frames.\nThe generated attention map alongside the feature map of the current video frame Zl\ni+jare fed to\naclassifier to detect any scenery change between frame pairs. The classifier component is a\nclassiﬁer with two outputs which represented as a function FClass, parameterized by θClassin equation\n(7). It takes the attention map, AttMapl, and the feature map of the current video frame Zl\ni+jand\nproduces a class label. The possible labels are changed and unchanged frame. To classify the video\nframe, the AttMaplis multiplied element-wise with the feature map of current video frame to get the\nreﬁned feature maps ZAttin equation (8). The reﬁned feature maps are passed through a convolutional\nlayer and a fully connected layer, equation (9), to produce a class label.\nClsl\ni=FClass(Zl\ni+j,AttMapl;θClass). (7)\nZAtt=AttMapl⊙Zl\ni+j. (8)\nClsl\ni=FC(ReLU (Norm (Conv(Zatt)))). (9)\nIn equation (9)FC, ReLU, Norm and Conv indicate fully connected, ReLU, batch normalization and\nconvolutional layers, respectively. Clsl\niis the output of the TEEM, located at the convolutional layer\nlof feature network. Each video frame can be classiﬁed by a TEEM located in an early exit branch\ninto a changed or unchanged pair.\n3.3 Training Policy\nAn early exit branch, including a TEEM, can be added after any convolutional layer in the feature\nnetwork. However, the location of an early exit branch in the feature network determines its\nperformance in identifying semantic variations. Therefore, a temporal early attention video object\ndetection pipeline have multiple early exits to identify semantic variations. However, each TEEM in\nan early exit branch is regarded as a stand-alone module, hence, trained separately from the backbone\nnetwork. Let (fri,fri+j)be a sampled video frame pair and θbe all parameters in TEEM in early\nexitl. For a target label, P, the objective is to minimize the cross-entropy loss as follows:\nLTEEMk(P,Pθ;θ) =−∑\nkPk·log(Pθk) (10)\nwherekis classiﬁer outputs and,\nPθ=Softmax (S), (11)\nand\nS=FTEEM l(fri,fri+j;θ). (12)\nHere,FTEEM lis the the output of the TEEM in early exit l. In the forward pass of training, a video\nframe pair is passed through the network, including both the main branch and early exits. The\noutput of each early exit is recorded to calculate the error of TEEM in the early exit. In backward\npropagation, the error of each exit is passed back only through the TEEMs and the weights of TEEM\nare updated using gradient descent. Notably, the weights of the main branch are not updated during\ntraining of the TEEMs.\n3.4 Fast inference in a Temporal Early Exit Video Object Detection Pipeline\nAlgorithm 1 summarizes the temporal early exit video object detection pipeline inference algorithm.\nGiven a pre-trained object detection network as the main branch, Fmain, withjearly exit branches,\nthe ﬁrst video frame is processed by the main branch. The inference for successive frames proceeds\nby feeding frames through the feature network of the main branch until reaching the ﬁrst early exit\nbranch. Then, the softmax and entropy of FTEEM in the early exit are calculated to evaluate the\nconﬁdence of the classiﬁer in prediction. If the entropy is less than the threshold γand the class\nlabel with the maximum score (probability) is unchanged, the inference procedure terminates and\n5', 'document_title': 'paper.pdf'}, {'page_content': 'object detection results from the previous frame are returned. If the class label with maximum score\nis changed, then the frame is processed by the main branch. For the entropy greater than γ, the frame\ncontinues to be processed by the feature network till the next early exit branch. If the softmax and\nentropy of all early exits are greater than thresholds, the frame continues to be processed by the main\nbranch.\nAlgorithm 1 Inference algorithm for video object detection\nRequire: Video frames{I}\nEnsure: detection =None\nfori←1to←length (I)do\nifi== 1 then\ndetection←Fmain(Ii)\nreturn detection\nelse\nforj←1toj= #( early exits )do\ny←FTEEM j(Ii,Ii−1)\nS←softmax (y)\ne←entropy (S)\nife<τ and arg max (S) == Unchanged then\nreturn detection\nelse ife<γ and arg max (S) == Changed then\ndetection←Fmain(Ii)\nreturn detection\nelse ife>γ then\npass\nend if\nend for\nend if\nend for\n4 Dataset\nThe CDnet dataset [ 5] provides a realistic, camera-captured, diverse set of videos for change detection.\nThe videos have been selected to cover a wide range of detection challenges and are representative\nof typical indoor and outdoor visual data captured in surveillance, smart environment. This dataset\nconsists of 33 camera-captured videos ( ∼70,000 frames) with boats, trucks, and pedestrians as objects\nof interest. It contains a range of challenging factors, including turbulence, dynamic background,\ncamera jitter challenging weather and pan-tilt-zooming (PTZ). All videos come with ground-truth\nsegmentation of motion areas for each video frame. However, CDnet dataset doesn’t provide labels\nfor the individual instances of moving objects. This makes measuring SC metric challenging and\ninaccurate in scenarios with multiple moving objects. To evaluate those scenarios, we also build\nthe scenery change dataset. Our dataset is based on VIRAT dataset [ 15] which is inspired and used\nby many change detection applications [ 17]. We used Vatic engine [ 27] to label individual moving\nobjects with bounding boxes. The dataset consists of 20 camera-captured videos with pedestrian, car\nand cyclist as objects of interest. We believe our dataset can complement existing change detection\ndatasets like CDnet[5].\n4.1 Dataset Balancing\nTo train TEEMs, video frame pairs need to be randomly sampled from videos. However, random\nsampling of frame pairs causes the unbalanced class dataset (changed scenery and unchanged scenery\nclasses). The balance of classes depends on the interval between sampled video frame pairs. For\nlong sampling interval, the dataset skews to changed scenery frame pairs. On the other hand, a short\nsampling interval leads to skewing to unchanged scenery. As a result of the unbalanced dataset,\nthe classiﬁer of TEEMs tends to ignore small classes, while concentrating on learning to classify\nlarge ones accurately. We tackle the unbalanced dataset problem by proposing the dynamic sampling\ninterval approach. To this end, we divide the semantic variation into ten classes. Each class represents\n6', 'document_title': 'paper.pdf'}]: %s
2023-12-06 14:03:34,492 - INFO - Check the results [{'page_content': '3 Temporal Early Exits Video Object Detection Pipeline\nWe propose a novel video object detection pipeline to tackle the computational complexity of video\nobject detection. An overview of the proposed pipeline is shown in ﬁgure 2. Given a sequence of\nvideo frames, multiple Temporal Early Exit Modules (TEEM) are used to build the early exit branches.\nEach early exit branch uses features extracted in the early stages of the feature network to identify\nany semantic variation between the features of consecutive video frames. If any semantic variations\nis distinguished by a TEEM, a full computation effort (main branch) will be needed to process the\nvideo frame. On the other hand, the computation process of a video frame without any semantic\nvariations will be terminated at an early exit branch and the detection results from the previous frame\nwill be reused. We describe the implementation details of the proposed pipeline in this section.\n3.1 Scenery Change\nThe key challenge in identifying feature variations is qualifying semantic dissimilarities between\nimages [ 1]. To address this challenge, we introduce a scenery change (SC)metric to quantify\nsemantic variations between video frames. We deﬁne a semantic variation to be only a variation in\nobjects of interest where noisy variations are considered to be nuisance. Given any pairs of video\nframes, a TEEM aims to identify semantic differences between frames. A scenery changes between a\npair of video frames (fri,fri+j), if the maximum motion in the Moving Fields of Interest (MFI) is\ngreater than the variation threshold value τvaras follows,\nSC(fri,fri+j) ={\n1 max( MFI(fri,fri+j))>τvar\n0otherwise. (1)\nWe compute MFI set by measuring intersection over union (IoU) of each object of interest, Ok, across\nframe pairs as follows,\nMFI(ok) ={\n1−IoU(Ofri\nk,Ofri+j\nk )ifOk∈fri∧fri+j\n1 Otherwise. (2)\nEach element in MFI quantiﬁes the variation in an object of interest between a pair of video frames.\nThe scenery change metric distinguishes noisy variations from semantic variations in video frame\npairs. Figure 3 visualizes some examples of scenery change metric for video frame pair.\n3.2 Temporal Early Exit Module\nTo detect a scenery change between frame pairs (fri,fri+j), we propose TEEM which consists of\nAttention andClassiﬁer components. This section describes these components in details. Figure 2\nshows an overview of a TEEM.\nTheAttention component is represented as a function parameterized by θAttenlin equation (3).FAttn\ntakesZl\niandZl\ni+jas inputs, and outputs an attention map AttMaplwhich encodes the semantic\nvariations happened between friandfri+j. In equation (3),Zl\niandZl\ni+jare image features of\nthe previous friand currentfri+jvideo frames, respectively, encoded by convolutional layer lin\nthe feature network. The attention function captures the semantic variations between frame pairs\nin the representation space by subtracting Zl\nifromZl\ni+jin equation (4). Then, by concatenating\nthe subtraction results with the feature representation of the current video frame, Zl\ni+j, a 2D spatial\nattention map associated with the video frame pair is generated in equation (6).\nAttMapl=FAttn(Zl\ni,Zl\ni+j;θAttenl). (3)\nZsub=Zi+j−Zj, (4)\n¯Zc=Conct [Zi:Zsub], (5)\nAttMapl=Sigmoid (Conv(¯Zc)), (6)\n4', 'document_title': 'paper.pdf'}, {'page_content': 'In equations (6)and(5),Conct [; ],Conv ,Sigmoid indicate concatenation, convolutional layer and\nsigmoid function, respectively. Following [ 13], to avoid introducing any form of global normalization,\nwe use sigmoid, instead of softmax, for computing the attention map. The attention map makes\nTEEM learn and focus on the motions of objects of interest between frame pairs. Notably, a TEEM\nrequires storing the latest feature maps Zl\nito be used for successive frames.\nThe generated attention map alongside the feature map of the current video frame Zl\ni+jare fed to\naclassifier to detect any scenery change between frame pairs. The classifier component is a\nclassiﬁer with two outputs which represented as a function FClass, parameterized by θClassin equation\n(7). It takes the attention map, AttMapl, and the feature map of the current video frame Zl\ni+jand\nproduces a class label. The possible labels are changed and unchanged frame. To classify the video\nframe, the AttMaplis multiplied element-wise with the feature map of current video frame to get the\nreﬁned feature maps ZAttin equation (8). The reﬁned feature maps are passed through a convolutional\nlayer and a fully connected layer, equation (9), to produce a class label.\nClsl\ni=FClass(Zl\ni+j,AttMapl;θClass). (7)\nZAtt=AttMapl⊙Zl\ni+j. (8)\nClsl\ni=FC(ReLU (Norm (Conv(Zatt)))). (9)\nIn equation (9)FC, ReLU, Norm and Conv indicate fully connected, ReLU, batch normalization and\nconvolutional layers, respectively. Clsl\niis the output of the TEEM, located at the convolutional layer\nlof feature network. Each video frame can be classiﬁed by a TEEM located in an early exit branch\ninto a changed or unchanged pair.\n3.3 Training Policy\nAn early exit branch, including a TEEM, can be added after any convolutional layer in the feature\nnetwork. However, the location of an early exit branch in the feature network determines its\nperformance in identifying semantic variations. Therefore, a temporal early attention video object\ndetection pipeline have multiple early exits to identify semantic variations. However, each TEEM in\nan early exit branch is regarded as a stand-alone module, hence, trained separately from the backbone\nnetwork. Let (fri,fri+j)be a sampled video frame pair and θbe all parameters in TEEM in early\nexitl. For a target label, P, the objective is to minimize the cross-entropy loss as follows:\nLTEEMk(P,Pθ;θ) =−∑\nkPk·log(Pθk) (10)\nwherekis classiﬁer outputs and,\nPθ=Softmax (S), (11)\nand\nS=FTEEM l(fri,fri+j;θ). (12)\nHere,FTEEM lis the the output of the TEEM in early exit l. In the forward pass of training, a video\nframe pair is passed through the network, including both the main branch and early exits. The\noutput of each early exit is recorded to calculate the error of TEEM in the early exit. In backward\npropagation, the error of each exit is passed back only through the TEEMs and the weights of TEEM\nare updated using gradient descent. Notably, the weights of the main branch are not updated during\ntraining of the TEEMs.\n3.4 Fast inference in a Temporal Early Exit Video Object Detection Pipeline\nAlgorithm 1 summarizes the temporal early exit video object detection pipeline inference algorithm.\nGiven a pre-trained object detection network as the main branch, Fmain, withjearly exit branches,\nthe ﬁrst video frame is processed by the main branch. The inference for successive frames proceeds\nby feeding frames through the feature network of the main branch until reaching the ﬁrst early exit\nbranch. Then, the softmax and entropy of FTEEM in the early exit are calculated to evaluate the\nconﬁdence of the classiﬁer in prediction. If the entropy is less than the threshold γand the class\nlabel with the maximum score (probability) is unchanged, the inference procedure terminates and\n5', 'document_title': 'paper.pdf'}, {'page_content': 'object detection results from the previous frame are returned. If the class label with maximum score\nis changed, then the frame is processed by the main branch. For the entropy greater than γ, the frame\ncontinues to be processed by the feature network till the next early exit branch. If the softmax and\nentropy of all early exits are greater than thresholds, the frame continues to be processed by the main\nbranch.\nAlgorithm 1 Inference algorithm for video object detection\nRequire: Video frames{I}\nEnsure: detection =None\nfori←1to←length (I)do\nifi== 1 then\ndetection←Fmain(Ii)\nreturn detection\nelse\nforj←1toj= #( early exits )do\ny←FTEEM j(Ii,Ii−1)\nS←softmax (y)\ne←entropy (S)\nife<τ and arg max (S) == Unchanged then\nreturn detection\nelse ife<γ and arg max (S) == Changed then\ndetection←Fmain(Ii)\nreturn detection\nelse ife>γ then\npass\nend if\nend for\nend if\nend for\n4 Dataset\nThe CDnet dataset [ 5] provides a realistic, camera-captured, diverse set of videos for change detection.\nThe videos have been selected to cover a wide range of detection challenges and are representative\nof typical indoor and outdoor visual data captured in surveillance, smart environment. This dataset\nconsists of 33 camera-captured videos ( ∼70,000 frames) with boats, trucks, and pedestrians as objects\nof interest. It contains a range of challenging factors, including turbulence, dynamic background,\ncamera jitter challenging weather and pan-tilt-zooming (PTZ). All videos come with ground-truth\nsegmentation of motion areas for each video frame. However, CDnet dataset doesn’t provide labels\nfor the individual instances of moving objects. This makes measuring SC metric challenging and\ninaccurate in scenarios with multiple moving objects. To evaluate those scenarios, we also build\nthe scenery change dataset. Our dataset is based on VIRAT dataset [ 15] which is inspired and used\nby many change detection applications [ 17]. We used Vatic engine [ 27] to label individual moving\nobjects with bounding boxes. The dataset consists of 20 camera-captured videos with pedestrian, car\nand cyclist as objects of interest. We believe our dataset can complement existing change detection\ndatasets like CDnet[5].\n4.1 Dataset Balancing\nTo train TEEMs, video frame pairs need to be randomly sampled from videos. However, random\nsampling of frame pairs causes the unbalanced class dataset (changed scenery and unchanged scenery\nclasses). The balance of classes depends on the interval between sampled video frame pairs. For\nlong sampling interval, the dataset skews to changed scenery frame pairs. On the other hand, a short\nsampling interval leads to skewing to unchanged scenery. As a result of the unbalanced dataset,\nthe classiﬁer of TEEMs tends to ignore small classes, while concentrating on learning to classify\nlarge ones accurately. We tackle the unbalanced dataset problem by proposing the dynamic sampling\ninterval approach. To this end, we divide the semantic variation into ten classes. Each class represents\n6', 'document_title': 'paper.pdf'}]: %s
2023-12-06 14:03:35,935 - INFO - Check the data that is being passed [{'page_content': 'Temporal Early Exits for Efﬁcient Video Object\nDetection\nAmin Sabet\nSchool of Electronics and Computer Science\nUniversity of Southampton, UK\nms4r18@soton.ac.ukJonathon Hare\nSchool of Electronics and Computer Science\nUniversity of Southampton, UK\njsh2@ecs.soton.ac.uk,\nBashir Al-Hashimi\nFaculty of Natural and Mathematical Sciences\nKing’s College London, UK\nbashir.al-hashimi@kcl.ac.ukGeoff V . Merrett\nSchool of Electronics and Computer Science\nUniversity of Southampton, UK\ngvm@ecs.soton.ac.uk\nAbstract\nTransferring image-based object detectors to the domain of video remains chal-\nlenging under resource constraints. Previous efforts utilised optical ﬂow to allow\nunchanged features to be propagated, however, the overhead is considerable when\nworking with very slowly changing scenes from applications such as surveillance.\nIn this paper, we propose temporal early exits to reduce the computational complex-\nity of per-frame video object detection. Multiple temporal early exit modules with\nlow computational overhead are inserted at early layers of the backbone network to\nidentify the semantic differences between consecutive frames. Full computation\nis only required if the frame is identiﬁed as having a semantic change to previous\nframes; otherwise, detection results from previous frames are reused. Experiments\non CDnet show that our method signiﬁcantly reduces the computational complexity\nand execution of per-frame video object detection up to 34×compared to existing\nmethods with an acceptable reduction of 2.2% in mAP.\n1 Introduction\nObject detection is one of the fundamental tasks in computer vision and serves as a core approach\nin many practical applications, such as robotics and video surveillance [ 26,2]. Object detection in\nstatic images has achieved remarkable successes in recent years using CNNs [ 7]. However, video\nobject detection has now emerged as a new challenge beyond image data. This is due to the high\ncomputational cost introduced by applying existing image object detection networks on numerous\nindividual video frames. Figure 1-(a) shows the overview of the per-frame video object detection\napproach, where all video frames are processed by a similar CNN. Deploying per-frame video object\ndetection becomes even more challenging for resource and energy-constrained applications.\nDeep optical ﬂow approaches [ 29] tackle the computational complexity challenge of video object\ndetection by taking advantage of temporal information in videos. They exploit feature similarity\nbetween consecutive frames to reduce the expensive feature computation on most frames and improve\nthe speed. Instead of extracting features of all frames by a deep CNN, deep optical ﬂow uses a lighter\nnetwork to extract, propagate and aggregate features of video frames with similar features to previous\nframes. Figure 1-(b) shows the overview of deep optical ﬂow approaches [24].\nFeature similarity between successive video frames occurs often in applications such as facility\nmonitoring and surveillance systems, where the camera is static and there are less frequently moving\nobjects in the videos [ 11]. Whilst reducing computational complexity, optical ﬂow approaches require\nPreprint. Under review.arXiv:2106.11208v1  [cs.CV]  21 Jun 2021', 'document_title': 'paper.pdf'}, {'page_content': 'Figure 1: Comparison video object detection approaches. (a) Conventional approach: applying deep\nCNNs on individual frames. (b) Deep ﬂow estimation: employing lighter ﬂow estimation network\nto propagate features across frames. (c) Proposed pipeline: identifying semantic variation in early\nstages of network and avoiding deep CNN computation for unchanged video frames\nsubstantial computational effort to generate and aggregating feature maps even though the features of\nsuccessive frames remain unchanged.\nTo address the challenge of identifying and processing frames with unchanged features, we propose\na computationally lightweight, Temporal Early Exit Module (TEEM), which identiﬁes semantic\nvariations between consecutive frames. We show that the full computational effort of a network [ 17] is\nnot required to distinguish and detect semantic changes between frames. We then use TEEM to build\na per-frame video object detection pipeline, shown in ﬁgure 1-(C). In this pipeline, a TEEM identiﬁes\nsemantic variation between features of consecutive frames in the very early stages of the feature\nnetwork. Then, the TEEM conditionally activates the deeper layers of the network if a semantic\ndifference is detected between frames. If a frame is identiﬁed to be semantically unchanged, then\nthe object detection results from the previous frame are reused. By identifying unchanged frames at\nearlier stages and thus, avoiding processing video frames, the proposed pipeline signiﬁcantly reduces\nthe computational complexity and speeds up the video object detection.\nThe contributions of this paper are as follows.\n•We propose Temporal Early Exit Modules which exploit the features of the early con-\nvolutional layers to infer a 2D spatial attention map between video frames. The atten-\ntion map encodes the semantic variations between consecutive frames. The attention map is\nthen used to generate reﬁned feature maps, feeding into a classiﬁer to classify frames into\nchanged and unchanged categories. The experiments on CDnet dataset [ 5] show that the\nTEEM classiﬁes frames into changed and unchanged classes with 94% accuracy.\n•We demonstrate a temporal early exit video object detection pipeline that uses TEEMs\nto conditionally process video frames. Full computation effort is required only for pro-\ncessing video frames with a semantic variation to previous frames. The evaluation of the\nproposed pipline on CDnet dataset [ 5] shows up to 34×speed up of per-frame video object\ndetection with less than 2.2% reduction in mAP.\n2 Related Work\nOptical Flow Estimation. Recent optical ﬂow estimation approaches divide all video frames into\nkey frame and non-key frame sets [ 30] [24] [29]. While deep networks are only applied on the\nkey-frames, a lighter ﬂow estimation network is exploited to obtain the features of non-key frames\n(shown in Figure 1), which results in speeding up the algorithm. However, the ﬂow estimation\napproaches rely on selecting the best key-frame, where the features of non-key frames are estimated\nfrom key-frame features. Deep feature ﬂow network (DFF) [ 30] adopts a simple ﬁxed key-frame\nselection scheme and fails to take account of the quality of key-frame and the optical-ﬂow. The\ndifferential network [ 24] proposes a binary differential classiﬁer network to detect the key-frames\nwith classiﬁcation accuracy of up to 76%. In order to address the challenge of ﬁnding the optimal key\nvideo frames, instead of replacing the temporal features with the features of key-frames, PSLA [ 6]\nproposes to progressively update the temporal features through a recursive feature updating network.\nTo summarise, while ﬂow estimation networks speed up video object detection, they require large\nand redundant computations for feature propagation for the majority of unchanged video frames.\n2', 'document_title': 'paper.pdf'}, {'page_content': 'Figure 2: Overview of the proposed temporal early exits video object detection pipeline.TEEMs are\nadded to feature network of a object detection network (main branch) to detect unchanged video\nframes and avoid redundant computations.\n(A)\n(B)\n(C)\nfri objects of interest i fri+j objects of interest i+j MFI (fri,fri+j)\nFigure 3: Scenery change examples, (A) shows a changed scenery with the IoU=0. (B) shows an\nunchanged scenery with the IoU=1 (C) shows a changed scenery with the IoU=0.19\nEarly Exit CNNs. In a similar line of work to ours, early exit classiﬁers promote faster inference\nby allowing classiﬁcation for easy instances to exit the network early. This class of algorithms\nare based on the observation that often the features learned at earlier stages of a deep network can\ncorrectly infer a large subset of the data samples. For example, BranchyNet [ 25] is a neural network\narchitecture which adds side branches to the main branch, the original backbone network, to allow\ncertain test samples to exit early. BranchyNet is trained by the joint optimization of loss functions for\nall exit points. In the follow-up work, conditional deep learning (CDL) [ 16] measures the efﬁciency\nimprovement due to the addition of the linear classiﬁer at each convolutional layer to ﬁnd the optimal\nlayer for early exit branches in the backbone network. Early exit approaches reduce the runtime and\nenergy consumption of image classiﬁcation. However, they have been developed only for image\nclassiﬁcation applications.\nScene Change Detection. Scene change detection is a fundamental problem in the ﬁeld of computer\nvision. The most classical approach for scene change detection is image differencing [ 21,22], which\ngenerates a change map by determining the set of pixels that are ‘signiﬁcantly different’ across two\nimages. Then, a binary mask is generated by thresholding the change map. Although, this method\nhas low computational cost, the raw pixel features are incapable of effectively differentiating between\nsemantic changes and noise. To obtain more discriminative features, image rationing [ 12], change\nvector analysis [ 4,3], Markov random ﬁelds [ 14] and dictionary learning [ 12,10] have been proposed.\nHowever, they are still sensitive to noise and illumination changes [19].\n3', 'document_title': 'paper.pdf'}]: %s
2023-12-06 14:03:35,935 - INFO - Check the results [{'page_content': 'Temporal Early Exits for Efﬁcient Video Object\nDetection\nAmin Sabet\nSchool of Electronics and Computer Science\nUniversity of Southampton, UK\nms4r18@soton.ac.ukJonathon Hare\nSchool of Electronics and Computer Science\nUniversity of Southampton, UK\njsh2@ecs.soton.ac.uk,\nBashir Al-Hashimi\nFaculty of Natural and Mathematical Sciences\nKing’s College London, UK\nbashir.al-hashimi@kcl.ac.ukGeoff V . Merrett\nSchool of Electronics and Computer Science\nUniversity of Southampton, UK\ngvm@ecs.soton.ac.uk\nAbstract\nTransferring image-based object detectors to the domain of video remains chal-\nlenging under resource constraints. Previous efforts utilised optical ﬂow to allow\nunchanged features to be propagated, however, the overhead is considerable when\nworking with very slowly changing scenes from applications such as surveillance.\nIn this paper, we propose temporal early exits to reduce the computational complex-\nity of per-frame video object detection. Multiple temporal early exit modules with\nlow computational overhead are inserted at early layers of the backbone network to\nidentify the semantic differences between consecutive frames. Full computation\nis only required if the frame is identiﬁed as having a semantic change to previous\nframes; otherwise, detection results from previous frames are reused. Experiments\non CDnet show that our method signiﬁcantly reduces the computational complexity\nand execution of per-frame video object detection up to 34×compared to existing\nmethods with an acceptable reduction of 2.2% in mAP.\n1 Introduction\nObject detection is one of the fundamental tasks in computer vision and serves as a core approach\nin many practical applications, such as robotics and video surveillance [ 26,2]. Object detection in\nstatic images has achieved remarkable successes in recent years using CNNs [ 7]. However, video\nobject detection has now emerged as a new challenge beyond image data. This is due to the high\ncomputational cost introduced by applying existing image object detection networks on numerous\nindividual video frames. Figure 1-(a) shows the overview of the per-frame video object detection\napproach, where all video frames are processed by a similar CNN. Deploying per-frame video object\ndetection becomes even more challenging for resource and energy-constrained applications.\nDeep optical ﬂow approaches [ 29] tackle the computational complexity challenge of video object\ndetection by taking advantage of temporal information in videos. They exploit feature similarity\nbetween consecutive frames to reduce the expensive feature computation on most frames and improve\nthe speed. Instead of extracting features of all frames by a deep CNN, deep optical ﬂow uses a lighter\nnetwork to extract, propagate and aggregate features of video frames with similar features to previous\nframes. Figure 1-(b) shows the overview of deep optical ﬂow approaches [24].\nFeature similarity between successive video frames occurs often in applications such as facility\nmonitoring and surveillance systems, where the camera is static and there are less frequently moving\nobjects in the videos [ 11]. Whilst reducing computational complexity, optical ﬂow approaches require\nPreprint. Under review.arXiv:2106.11208v1  [cs.CV]  21 Jun 2021', 'document_title': 'paper.pdf'}, {'page_content': 'Figure 1: Comparison video object detection approaches. (a) Conventional approach: applying deep\nCNNs on individual frames. (b) Deep ﬂow estimation: employing lighter ﬂow estimation network\nto propagate features across frames. (c) Proposed pipeline: identifying semantic variation in early\nstages of network and avoiding deep CNN computation for unchanged video frames\nsubstantial computational effort to generate and aggregating feature maps even though the features of\nsuccessive frames remain unchanged.\nTo address the challenge of identifying and processing frames with unchanged features, we propose\na computationally lightweight, Temporal Early Exit Module (TEEM), which identiﬁes semantic\nvariations between consecutive frames. We show that the full computational effort of a network [ 17] is\nnot required to distinguish and detect semantic changes between frames. We then use TEEM to build\na per-frame video object detection pipeline, shown in ﬁgure 1-(C). In this pipeline, a TEEM identiﬁes\nsemantic variation between features of consecutive frames in the very early stages of the feature\nnetwork. Then, the TEEM conditionally activates the deeper layers of the network if a semantic\ndifference is detected between frames. If a frame is identiﬁed to be semantically unchanged, then\nthe object detection results from the previous frame are reused. By identifying unchanged frames at\nearlier stages and thus, avoiding processing video frames, the proposed pipeline signiﬁcantly reduces\nthe computational complexity and speeds up the video object detection.\nThe contributions of this paper are as follows.\n•We propose Temporal Early Exit Modules which exploit the features of the early con-\nvolutional layers to infer a 2D spatial attention map between video frames. The atten-\ntion map encodes the semantic variations between consecutive frames. The attention map is\nthen used to generate reﬁned feature maps, feeding into a classiﬁer to classify frames into\nchanged and unchanged categories. The experiments on CDnet dataset [ 5] show that the\nTEEM classiﬁes frames into changed and unchanged classes with 94% accuracy.\n•We demonstrate a temporal early exit video object detection pipeline that uses TEEMs\nto conditionally process video frames. Full computation effort is required only for pro-\ncessing video frames with a semantic variation to previous frames. The evaluation of the\nproposed pipline on CDnet dataset [ 5] shows up to 34×speed up of per-frame video object\ndetection with less than 2.2% reduction in mAP.\n2 Related Work\nOptical Flow Estimation. Recent optical ﬂow estimation approaches divide all video frames into\nkey frame and non-key frame sets [ 30] [24] [29]. While deep networks are only applied on the\nkey-frames, a lighter ﬂow estimation network is exploited to obtain the features of non-key frames\n(shown in Figure 1), which results in speeding up the algorithm. However, the ﬂow estimation\napproaches rely on selecting the best key-frame, where the features of non-key frames are estimated\nfrom key-frame features. Deep feature ﬂow network (DFF) [ 30] adopts a simple ﬁxed key-frame\nselection scheme and fails to take account of the quality of key-frame and the optical-ﬂow. The\ndifferential network [ 24] proposes a binary differential classiﬁer network to detect the key-frames\nwith classiﬁcation accuracy of up to 76%. In order to address the challenge of ﬁnding the optimal key\nvideo frames, instead of replacing the temporal features with the features of key-frames, PSLA [ 6]\nproposes to progressively update the temporal features through a recursive feature updating network.\nTo summarise, while ﬂow estimation networks speed up video object detection, they require large\nand redundant computations for feature propagation for the majority of unchanged video frames.\n2', 'document_title': 'paper.pdf'}, {'page_content': 'Figure 2: Overview of the proposed temporal early exits video object detection pipeline.TEEMs are\nadded to feature network of a object detection network (main branch) to detect unchanged video\nframes and avoid redundant computations.\n(A)\n(B)\n(C)\nfri objects of interest i fri+j objects of interest i+j MFI (fri,fri+j)\nFigure 3: Scenery change examples, (A) shows a changed scenery with the IoU=0. (B) shows an\nunchanged scenery with the IoU=1 (C) shows a changed scenery with the IoU=0.19\nEarly Exit CNNs. In a similar line of work to ours, early exit classiﬁers promote faster inference\nby allowing classiﬁcation for easy instances to exit the network early. This class of algorithms\nare based on the observation that often the features learned at earlier stages of a deep network can\ncorrectly infer a large subset of the data samples. For example, BranchyNet [ 25] is a neural network\narchitecture which adds side branches to the main branch, the original backbone network, to allow\ncertain test samples to exit early. BranchyNet is trained by the joint optimization of loss functions for\nall exit points. In the follow-up work, conditional deep learning (CDL) [ 16] measures the efﬁciency\nimprovement due to the addition of the linear classiﬁer at each convolutional layer to ﬁnd the optimal\nlayer for early exit branches in the backbone network. Early exit approaches reduce the runtime and\nenergy consumption of image classiﬁcation. However, they have been developed only for image\nclassiﬁcation applications.\nScene Change Detection. Scene change detection is a fundamental problem in the ﬁeld of computer\nvision. The most classical approach for scene change detection is image differencing [ 21,22], which\ngenerates a change map by determining the set of pixels that are ‘signiﬁcantly different’ across two\nimages. Then, a binary mask is generated by thresholding the change map. Although, this method\nhas low computational cost, the raw pixel features are incapable of effectively differentiating between\nsemantic changes and noise. To obtain more discriminative features, image rationing [ 12], change\nvector analysis [ 4,3], Markov random ﬁelds [ 14] and dictionary learning [ 12,10] have been proposed.\nHowever, they are still sensitive to noise and illumination changes [19].\n3', 'document_title': 'paper.pdf'}]: %s
2023-12-06 14:03:37,448 - INFO - Check the data that is being passed [{'page_content': 'a variation threshold between frame pairs i.e., class one and two represent frame pairs with variation\nthreshold of [0, 0.1) and [0.1, 0.2), respectively. Consequently, starting from a random sampling\ninterval, we sample frame pairs, measure the minimum IoU of objects across frame pairs and classify\nthem into one of the ten classes. As the number of pairs in one class exceeds a threshold, we reject\nthe sampling that falls into the class and adjusts the sampling interval toward the direction which\nincreases classes with a smaller number of samples until all classes are balanced. We also face another\nclass unbalance issue due to videos with different lengths — i.e., some videos have thousands of\nframes while some have a few dozens of frames. We address this problem by upsampling frame pairs\nfrom shorter videos.\n5 Experiments and Results\nIn this section, we describe the experimental setup used to evaluate the performance of the temporal\nearly exit video object detection pipeline. The experimental results are also presented in this section.\n5.1 Implementation and Experimental Setup\nFor the main branch of the video object detection pipeline , Fmain, we use Faster-RCNN [ 20] with\nResnet50 and Resnet101 feature networks trained on MS COCO detection dataset [ 9]. We use\nTEE-Faster-RCNN to refer to this pipeline. Following Residual attention network [ 28], we add early\nexit branches after each Bottleneck and Basic blocks of the feature network. Therefore, TEE-Faster-\nRCNN consists of four early exits. We train TEEMs for 40 epochs using the Adam Optimizer [ 8]\nwith a learning rate of 0.001 and a batch size of 64. In both training and inference, the images have\nshorter sides of 224 pixels. We use 25K and 5k video frame pairs for training and testing, respectively.\nTraining was performed on 4 GPUs and testing run time is measured on a single GTX 1080 GPU. We\nset the entropy to 0.97 at test time. Our model is implemented using PyTorch [ 18], and our code and\ndataset will be made publicly available. We evaluate the accuracy, computational complexity, and run\ntime of TEE-Faster-RCNN.\n5.2 Classiﬁcation Accuracy of Early Exits:\nWe evaluate the accuracy of early exits using classiﬁcation accuracy, precision, recall, and F1 metrics.\nAccuracy metric enumerates the number of correct predicted changed and unchanged frames by\nTEEMs in early exit branches. Precision quantiﬁes the number of frames which predicted as the\nchanged frames and belong to the changed frame class. Recall quantiﬁes the number of changed\nframe predictions made out of all changed frame examples.\nTable 1 shows the measured accuracy, precision, recall and F1 for early exits of TEE-Faster-RCNN\nwith Resnet50 and Resnet101 feature networks. TEE-Faster-RCNN is trained for the variation\nthreshold of 0.4, equation (1). Exit1, Exit2, Exit3, and Exit4 are located after the ﬁrst, second, third,\nand fourth Bottleneck modules in the Faster-RCNN feature network, respectively. The Exit1 (with\nResnet50 feature network) classiﬁes frames into change and unchanged categories with 89% accuracy.\nThe classiﬁcation accuracy increases up to 91% and 93% in Exit2 and Exit3. The observations show\nlower accuracy for Exit4 82% compares to Exit3. We believe that lower accuracy of Exit4 is due to\nthe low-resolution feature maps used by the TEEM in early exit 4 to identify variations.\nExit4 uses 7×7low-resolution but semantically strong feature, encoding very high-level concepts,\nto identify semantic variations across frames. Since the high-level features remain often unchanged\nacross consecutive video frames, Exit4 achieves lower accuracy in classifying video frames with\nsmall semantic variation i.e., when the location of a small object changes slightly between two frames.\nHowever, Exit4 accurately detects unchanged frame pairs as well as frame pairs with signiﬁcant\nvariations.\nCompared to TEEMs, DFF [ 24] detects the key-frames (refers to changed frames) with 76% accuracy,\nyet with high computational cost. DFF concatenates the input frame pairs and sends them as a\nsix-channel depth input through a differential network to get the difference feature map. However,\ninstead of processing input frame pairs together to identify variations, we proposed to process a video\nframe once and store and reuse the intermediate feature maps to build an attention map that encodes\nthe semantic differences between frame pairs with small computational overhead.\n7', 'document_title': 'paper.pdf'}, {'page_content': 'Table 1: Accuracy, precision, recall, and F1 scores measures for TEEMs in Faster-RCNN with\nResnet50 and Resnet101 feature networks.\nResnet50 Resnet101\nAcc Pr Re F1 Acc Pr Re F1\nExit1 0.88 0.89 0.88 0.89 0.89 0.89 0.89 0.89\nExit2 0.93 0.93 0.93 0.93 0.91 0.92 0.92 0.92\nExit3 0.91 0.91 0.92 0.91 0.94 0.92 0.92 0.93\nExit4 0.86 0.82 0.86 0.84 0.85 0.83 0.87 0.85\nfri fri+jTEEM 1TEEM 2TEEM 3TEEM 4\nFigure 4: Class activation maps of video frame pairs show that TEEMs effectively learn to focus only\non the motions of the objects of interest between video frames to identify scenery change.\n5.3 Class Activation Map of TEEMs\nTo visualize the performance of TEEMs, ﬁgure 4 shows class activation map of TEEMs. Each class\nactivation map shows which parts of a video frame have contributed more to the ﬁnal output of\nthe TEEMs. Figure 4 shows that TEEMs effectively learn to focus on the moving ﬁelds of interest\nbetween frame pairs to identify variations. Futhermore, Figure 4 illustrates that TEEM1 and TEEM2\nidentify moving objects in higher resolution because the input feature maps into TEEM1 and TEEM2\nhave high resolution. The resolution of input feature maps into TEEM1 and TEEM2 are 56×56\nand28×28resolution, respectively. However, the class activation map of TEEM3 and TEEM4 are\ncoarse-grain because they use low-resolution feature maps of 14×14and7×7, respectively.\n5.4 Computational Complexity\nTable 2 compares computational cost (number of MAC operations and parameters) and run time\n(frames per second) of processing video frames by the original Faster-RCNN and TEE-Faster-RCNN\nbranches. The ﬁrst row of table 2 indicates the original Faster-RCNN network, including feature\nnetwork, region proposal network, and region-based convolutional neural network.\nExit 1, Exit2, Exit3, and Exit4 refer to computational paths of TEE-Faster-RCNN from the input of\nFaster-RCNN to TEEM1, TEEM2, TEEM3, and TEEM4, respectively. The second column of table 2\nshows the required MAC operations and parameters for each computation path of TEE-Faster-RCNN\nwith Resnet50 feature network. The third columns shows the speed of each early exit. The fourth\ncolumn shows the computational complexity the TEEM added in early exits. The second part of table\n2 shows the same results for TEE-Faster-RCNN with resnet101 feature network.\nThe original Faster-RCNN with Resnet51 feature network requires up to 134G MAC operations\nand 41M parameters to process a video frame. The high computation requirement limits the video\nobject detection speed to 18 fps, 14 fps for the Faster-RCNN with Resnet101 feature network. Using\nthe same network architecture for processing all video frames regardless of the semantic variations\nbetween neighbouring frames leads to the inferior use of limited energy and computational resources.\nHowever, the TEE-Faster-RCNN uses computationally lightweight early exit branches to process\nunchanged video frames. Table 2 shows that early exit branches have substantially less computations\nand memory requirements which speeds up processing video frames. Exit1 branch requires only\n8', 'document_title': 'paper.pdf'}, {'page_content': 'Table 2: Computational complexity and speed of TEE-Faster-RCNN video object detection.\nResnet51 Resnet101\nbranch(Op, Par) Speed TEEM(Opr,Par) branch(Op, Par) Speed TEEM(Opr,Par)\nFR (134G , 41M) 18fps - (181G , 60) 14fps -\nExit1 (1.7G , 0.525M) 628fps (0.94G , 0.3M) (1.7G, 0.525M) 597fps (0.94G , 0.3M)\nExit2 (2.7G , 2.615M) 390fps (0.93G , 1.17M) (3.7G, 2.910M) 400fps (0.93G , 1.17M)\nExit3 (3.5G , 9.733M) 263fps (0.23G , 1.19M) (9.1G, 30.195M) 140fps (0.23G , 1.19M)\nExit4 (5G , 23.808) 218fps (0.25G , 5.129M) (9.9G, 50.289M) 126fps (0.2G , 5.129M)\nTable 3: Comparing the detection accuracy of TEE-Faster-RCNN with per-frame Faster-RCNN\nUpdating ratio mAP mIoU\nFaster-RCNN 1 0.231 0.8\nFixed-Step 7 0.211 0.76\nFixed-Step 10 0.183 0.75\nFixed-Step 20 0.16 0.70\nTEE-Faster-RCNN 20 0.209 0.75\n1.7G MAC operations and uses 0.5 M parameters. Signiﬁcant reduction in computation complexity\nis because of avoided parts of the feature network, region proposal network, and region-based\nconvolutional neural network. This reduction in computations speeds up processing unchanged video\nframes up to 628 fps. The required MAC operation for Exit2, Exit3 and Exit4 branches are 2.7G,\n3.5G, and 5G, respectively. Exit2, Exit3 and Exit4 speed up processing unchanged frames to 390 fps,\n263 fps, and 218 fps, respectively. The fourth column of table 2 shows the required MAC operation\nand number of parameters for TEEMs.\n5.5 Detection Accuracy\nHaving tested the classiﬁcation performance of TEEMs, we then evaluate the mean average precision\n(mAP @0.35:0.05:0.75) and mean intersection over union (mIOU) of TEE-Faster-RCNN video object\ndetection. Table 3 compares the accuracy of TEE-Faster-RCNN detection results with the per-frame\noriginal Faster-RCNN object detection. TEE-Faster-RCNN updates the detection results at the\naverage step of 20. Therefore, for a fair comparison we performed Faster-RCNN with different ﬁxed\nupdating steps. The results reﬂect that the accuracy of TEE-Faster-RCNN cannot compete with the\nper-frame video object detection approach. Whilst, TEE-Faster-RCNN achieves the same accuracy\nof per-frame Faster-RCNN with the ﬁxed updating steps of 7, its updating step of TEE-Faster-RCNN\nis 20 on average. Notably, TEE-Faster-RCNN does not aim to improve the accuracy of detection but\nintroduces a simple yet effective approach to signiﬁcantly reduce the computational complexity of\nvideo object detection for the video applications with less frequent moving objects e.g., the CDnet\ndataset [ 5]. For applications with frequent moving objects such as the ImageNet-VID dataset [ 23],\nmore complex methods like optical ﬂow approaches are needed to achieve better accuracy.\n6 Conclusion\nWe proposes a temporal early exit object detection pipeline to reduce the computational complexity\nof per-frame video object detection. The proposed approach takes advantage of infrequent variation\nbetween features of consecutive video frames to avoid redundant computation. Video frames with\ninvariant features are identiﬁed in the early stages of the network with very low computation effort. For\nthe unchanged video frames, detection results from previous frames are reused. A full computation\neffort is only required if a video frame is identiﬁed with semantic variations compared to previous\nframes. The proposed approach accelerates per-frame video object detection up to 34×with less than\n2.2 % reduction in mAP.\n9', 'document_title': 'paper.pdf'}, {'page_content': 'References\n[1]Pablo F Alcantarilla, Simon Stent, German Ros, Roberto Arroyo, and Riccardo Gherardi. Street-\nview change detection with deconvolutional networks. Autonomous Robots , 42(7):1301–1322,\n2018.\n[2]Ali Borji, Ming-Ming Cheng, Huaizu Jiang, and Jia Li. Salient object detection: A benchmark.\nIEEE transactions on image processing , 24(12):5706–5722, 2015.\n[3]Francesca Bovolo and Lorenzo Bruzzone. A theoretical framework for unsupervised change\ndetection based on change vector analysis in the polar domain. IEEE Transactions on Geoscience\nand Remote Sensing , 45(1):218–236, 2006.\n[4]Lorenzo Bruzzone and D Fernandez Prieto. An adaptive semiparametric and context-based\napproach to unsupervised change detection in multitemporal remote-sensing images. IEEE\nTransactions on image processing , 11(4):452–466, 2002.\n[5]Nil Goyette, Pierre-Marc Jodoin, Fatih Porikli, Janusz Konrad, and Prakash Ishwar. Changede-\ntection. net: A new change detection benchmark dataset. In 2012 IEEE computer society\nconference on computer vision and pattern recognition workshops , pages 1–8. IEEE, 2012.\n[6]Chaoxu Guo, Bin Fan, Jie Gu, Qian Zhang, Shiming Xiang, Veronique Prinet, and Chunhong\nPan. Progressive sparse local attention for video object detection. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision , pages 3909–3918, 2019.\n[7]Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi,\nIan Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, et al. Speed/accuracy trade-offs\nfor modern convolutional object detectors. In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 7310–7311, 2017.\n[8]Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014.\n[9]Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European\nconference on computer vision , pages 740–755. Springer, 2014.\n[10] Xiaoqiang Lu, Yuan Yuan, and Xiangtao Zheng. Joint dictionary learning for multispectral\nchange detection. IEEE transactions on cybernetics , 47(4):884–897, 2016.\n[11] Yuan Luo, Hanxing Zhou, Qin Tan, Xuefeng Chen, and Mingjing Yun. Key frame extraction of\nsurveillance video based on moving object detection and image similarity. Pattern Recognition\nand Image Analysis , 28(2):225–231, 2018.\n[12] H MAHMOUDZADEH. Digital change detection using remotely sensed data for monitoring\ngreen space destruction in tabriz. 2007.\n[13] David Mascharka, Philip Tran, Ryan Soklaski, and Arjun Majumdar. Transparency by design:\nClosing the gap between performance and interpretability in visual reasoning. In Proceedings\nof the IEEE conference on computer vision and pattern recognition , pages 4942–4950, 2018.\n[14] Gabriele Moser, Elena Angiati, and Sebastiano B Serpico. Multiscale unsupervised change\ndetection on optical images by markov random ﬁelds and wavelets. IEEE Geoscience and\nRemote Sensing Letters , 8(4):725–729, 2011.\n[15] Sangmin Oh, Anthony Hoogs, Amitha Perera, Naresh Cuntoor, Chia-Chih Chen, Jong Taek Lee,\nSaurajit Mukherjee, JK Aggarwal, Hyungtae Lee, Larry Davis, et al. A large-scale benchmark\ndataset for event recognition in surveillance video. In CVPR 2011 , pages 3153–3160. IEEE,\n2011.\n[16] Priyadarshini Panda, Abhronil Sengupta, and Kaushik Roy. Conditional deep learning for\nenergy-efﬁcient and enhanced pattern recognition. In 2016 Design, Automation & Test in\nEurope Conference & Exhibition (DATE) , pages 475–480. IEEE, 2016.\n10', 'document_title': 'paper.pdf'}, {'page_content': '[17] Dong Huk Park, Trevor Darrell, and Anna Rohrbach. Robust change captioning. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision , pages 4624–4633, 2019.\n[18] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative\nstyle, high-performance deep learning library. arXiv preprint arXiv:1912.01703 , 2019.\n[19] Richard J Radke, Srinivas Andra, Omar Al-Kofahi, and Badrinath Roysam. Image change\ndetection algorithms: a systematic survey. IEEE transactions on image processing , 14(3):\n294–307, 2005.\n[20] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time\nobject detection with region proposal networks. arXiv preprint arXiv:1506.01497 , 2015.\n[21] Paul Rosin. Thresholding for change detection. In Sixth International Conference on Computer\nVision (IEEE Cat. No. 98CH36271) , pages 274–279. IEEE, 1998.\n[22] Paul L Rosin and Efstathios Ioannidis. Evaluation of global image thresholding for change\ndetection. Pattern recognition letters , 24(14):2345–2356, 2003.\n[23] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual\nrecognition challenge. International journal of computer vision , 115(3):211–252, 2015.\n[24] Jing Shi and Chenliang Xu. Differential network for video object detection.\n[25] Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Branchynet: Fast inference\nvia early exiting from deep neural networks. In 2016 23rd International Conference on Pattern\nRecognition (ICPR) , pages 2464–2469. IEEE, 2016.\n[26] Yonglong Tian, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning strong parts for\npedestrian detection. In Proceedings of the IEEE international conference on computer vision ,\npages 1904–1912, 2015.\n[27] Carl V ondrick, Donald Patterson, and Deva Ramanan. Efﬁciently scaling up crowdsourced\nvideo annotation. International Journal of Computer Vision , pages 1–21. ISSN 0920-5691.\nURL http://dx.doi.org/10.1007/s11263-012-0564-1 . 10.1007/s11263-012-0564-1.\n[28] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang\nWang, and Xiaoou Tang. Residual attention network for image classiﬁcation. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition , pages 3156–3164, 2017.\n[29] Philippe Weinzaepfel, Jerome Revaud, Zaid Harchaoui, and Cordelia Schmid. Deepﬂow: Large\ndisplacement optical ﬂow with deep matching. In Proceedings of the IEEE international\nconference on computer vision , pages 1385–1392, 2013.\n[30] Xizhou Zhu, Yuwen Xiong, Jifeng Dai, Lu Yuan, and Yichen Wei. Deep feature ﬂow for video\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 2349–2358, 2017.\n11', 'document_title': 'paper.pdf'}]: %s
2023-12-06 14:03:37,448 - INFO - Check the results [{'page_content': 'a variation threshold between frame pairs i.e., class one and two represent frame pairs with variation\nthreshold of [0, 0.1) and [0.1, 0.2), respectively. Consequently, starting from a random sampling\ninterval, we sample frame pairs, measure the minimum IoU of objects across frame pairs and classify\nthem into one of the ten classes. As the number of pairs in one class exceeds a threshold, we reject\nthe sampling that falls into the class and adjusts the sampling interval toward the direction which\nincreases classes with a smaller number of samples until all classes are balanced. We also face another\nclass unbalance issue due to videos with different lengths — i.e., some videos have thousands of\nframes while some have a few dozens of frames. We address this problem by upsampling frame pairs\nfrom shorter videos.\n5 Experiments and Results\nIn this section, we describe the experimental setup used to evaluate the performance of the temporal\nearly exit video object detection pipeline. The experimental results are also presented in this section.\n5.1 Implementation and Experimental Setup\nFor the main branch of the video object detection pipeline , Fmain, we use Faster-RCNN [ 20] with\nResnet50 and Resnet101 feature networks trained on MS COCO detection dataset [ 9]. We use\nTEE-Faster-RCNN to refer to this pipeline. Following Residual attention network [ 28], we add early\nexit branches after each Bottleneck and Basic blocks of the feature network. Therefore, TEE-Faster-\nRCNN consists of four early exits. We train TEEMs for 40 epochs using the Adam Optimizer [ 8]\nwith a learning rate of 0.001 and a batch size of 64. In both training and inference, the images have\nshorter sides of 224 pixels. We use 25K and 5k video frame pairs for training and testing, respectively.\nTraining was performed on 4 GPUs and testing run time is measured on a single GTX 1080 GPU. We\nset the entropy to 0.97 at test time. Our model is implemented using PyTorch [ 18], and our code and\ndataset will be made publicly available. We evaluate the accuracy, computational complexity, and run\ntime of TEE-Faster-RCNN.\n5.2 Classiﬁcation Accuracy of Early Exits:\nWe evaluate the accuracy of early exits using classiﬁcation accuracy, precision, recall, and F1 metrics.\nAccuracy metric enumerates the number of correct predicted changed and unchanged frames by\nTEEMs in early exit branches. Precision quantiﬁes the number of frames which predicted as the\nchanged frames and belong to the changed frame class. Recall quantiﬁes the number of changed\nframe predictions made out of all changed frame examples.\nTable 1 shows the measured accuracy, precision, recall and F1 for early exits of TEE-Faster-RCNN\nwith Resnet50 and Resnet101 feature networks. TEE-Faster-RCNN is trained for the variation\nthreshold of 0.4, equation (1). Exit1, Exit2, Exit3, and Exit4 are located after the ﬁrst, second, third,\nand fourth Bottleneck modules in the Faster-RCNN feature network, respectively. The Exit1 (with\nResnet50 feature network) classiﬁes frames into change and unchanged categories with 89% accuracy.\nThe classiﬁcation accuracy increases up to 91% and 93% in Exit2 and Exit3. The observations show\nlower accuracy for Exit4 82% compares to Exit3. We believe that lower accuracy of Exit4 is due to\nthe low-resolution feature maps used by the TEEM in early exit 4 to identify variations.\nExit4 uses 7×7low-resolution but semantically strong feature, encoding very high-level concepts,\nto identify semantic variations across frames. Since the high-level features remain often unchanged\nacross consecutive video frames, Exit4 achieves lower accuracy in classifying video frames with\nsmall semantic variation i.e., when the location of a small object changes slightly between two frames.\nHowever, Exit4 accurately detects unchanged frame pairs as well as frame pairs with signiﬁcant\nvariations.\nCompared to TEEMs, DFF [ 24] detects the key-frames (refers to changed frames) with 76% accuracy,\nyet with high computational cost. DFF concatenates the input frame pairs and sends them as a\nsix-channel depth input through a differential network to get the difference feature map. However,\ninstead of processing input frame pairs together to identify variations, we proposed to process a video\nframe once and store and reuse the intermediate feature maps to build an attention map that encodes\nthe semantic differences between frame pairs with small computational overhead.\n7', 'document_title': 'paper.pdf'}, {'page_content': 'Table 1: Accuracy, precision, recall, and F1 scores measures for TEEMs in Faster-RCNN with\nResnet50 and Resnet101 feature networks.\nResnet50 Resnet101\nAcc Pr Re F1 Acc Pr Re F1\nExit1 0.88 0.89 0.88 0.89 0.89 0.89 0.89 0.89\nExit2 0.93 0.93 0.93 0.93 0.91 0.92 0.92 0.92\nExit3 0.91 0.91 0.92 0.91 0.94 0.92 0.92 0.93\nExit4 0.86 0.82 0.86 0.84 0.85 0.83 0.87 0.85\nfri fri+jTEEM 1TEEM 2TEEM 3TEEM 4\nFigure 4: Class activation maps of video frame pairs show that TEEMs effectively learn to focus only\non the motions of the objects of interest between video frames to identify scenery change.\n5.3 Class Activation Map of TEEMs\nTo visualize the performance of TEEMs, ﬁgure 4 shows class activation map of TEEMs. Each class\nactivation map shows which parts of a video frame have contributed more to the ﬁnal output of\nthe TEEMs. Figure 4 shows that TEEMs effectively learn to focus on the moving ﬁelds of interest\nbetween frame pairs to identify variations. Futhermore, Figure 4 illustrates that TEEM1 and TEEM2\nidentify moving objects in higher resolution because the input feature maps into TEEM1 and TEEM2\nhave high resolution. The resolution of input feature maps into TEEM1 and TEEM2 are 56×56\nand28×28resolution, respectively. However, the class activation map of TEEM3 and TEEM4 are\ncoarse-grain because they use low-resolution feature maps of 14×14and7×7, respectively.\n5.4 Computational Complexity\nTable 2 compares computational cost (number of MAC operations and parameters) and run time\n(frames per second) of processing video frames by the original Faster-RCNN and TEE-Faster-RCNN\nbranches. The ﬁrst row of table 2 indicates the original Faster-RCNN network, including feature\nnetwork, region proposal network, and region-based convolutional neural network.\nExit 1, Exit2, Exit3, and Exit4 refer to computational paths of TEE-Faster-RCNN from the input of\nFaster-RCNN to TEEM1, TEEM2, TEEM3, and TEEM4, respectively. The second column of table 2\nshows the required MAC operations and parameters for each computation path of TEE-Faster-RCNN\nwith Resnet50 feature network. The third columns shows the speed of each early exit. The fourth\ncolumn shows the computational complexity the TEEM added in early exits. The second part of table\n2 shows the same results for TEE-Faster-RCNN with resnet101 feature network.\nThe original Faster-RCNN with Resnet51 feature network requires up to 134G MAC operations\nand 41M parameters to process a video frame. The high computation requirement limits the video\nobject detection speed to 18 fps, 14 fps for the Faster-RCNN with Resnet101 feature network. Using\nthe same network architecture for processing all video frames regardless of the semantic variations\nbetween neighbouring frames leads to the inferior use of limited energy and computational resources.\nHowever, the TEE-Faster-RCNN uses computationally lightweight early exit branches to process\nunchanged video frames. Table 2 shows that early exit branches have substantially less computations\nand memory requirements which speeds up processing video frames. Exit1 branch requires only\n8', 'document_title': 'paper.pdf'}, {'page_content': 'Table 2: Computational complexity and speed of TEE-Faster-RCNN video object detection.\nResnet51 Resnet101\nbranch(Op, Par) Speed TEEM(Opr,Par) branch(Op, Par) Speed TEEM(Opr,Par)\nFR (134G , 41M) 18fps - (181G , 60) 14fps -\nExit1 (1.7G , 0.525M) 628fps (0.94G , 0.3M) (1.7G, 0.525M) 597fps (0.94G , 0.3M)\nExit2 (2.7G , 2.615M) 390fps (0.93G , 1.17M) (3.7G, 2.910M) 400fps (0.93G , 1.17M)\nExit3 (3.5G , 9.733M) 263fps (0.23G , 1.19M) (9.1G, 30.195M) 140fps (0.23G , 1.19M)\nExit4 (5G , 23.808) 218fps (0.25G , 5.129M) (9.9G, 50.289M) 126fps (0.2G , 5.129M)\nTable 3: Comparing the detection accuracy of TEE-Faster-RCNN with per-frame Faster-RCNN\nUpdating ratio mAP mIoU\nFaster-RCNN 1 0.231 0.8\nFixed-Step 7 0.211 0.76\nFixed-Step 10 0.183 0.75\nFixed-Step 20 0.16 0.70\nTEE-Faster-RCNN 20 0.209 0.75\n1.7G MAC operations and uses 0.5 M parameters. Signiﬁcant reduction in computation complexity\nis because of avoided parts of the feature network, region proposal network, and region-based\nconvolutional neural network. This reduction in computations speeds up processing unchanged video\nframes up to 628 fps. The required MAC operation for Exit2, Exit3 and Exit4 branches are 2.7G,\n3.5G, and 5G, respectively. Exit2, Exit3 and Exit4 speed up processing unchanged frames to 390 fps,\n263 fps, and 218 fps, respectively. The fourth column of table 2 shows the required MAC operation\nand number of parameters for TEEMs.\n5.5 Detection Accuracy\nHaving tested the classiﬁcation performance of TEEMs, we then evaluate the mean average precision\n(mAP @0.35:0.05:0.75) and mean intersection over union (mIOU) of TEE-Faster-RCNN video object\ndetection. Table 3 compares the accuracy of TEE-Faster-RCNN detection results with the per-frame\noriginal Faster-RCNN object detection. TEE-Faster-RCNN updates the detection results at the\naverage step of 20. Therefore, for a fair comparison we performed Faster-RCNN with different ﬁxed\nupdating steps. The results reﬂect that the accuracy of TEE-Faster-RCNN cannot compete with the\nper-frame video object detection approach. Whilst, TEE-Faster-RCNN achieves the same accuracy\nof per-frame Faster-RCNN with the ﬁxed updating steps of 7, its updating step of TEE-Faster-RCNN\nis 20 on average. Notably, TEE-Faster-RCNN does not aim to improve the accuracy of detection but\nintroduces a simple yet effective approach to signiﬁcantly reduce the computational complexity of\nvideo object detection for the video applications with less frequent moving objects e.g., the CDnet\ndataset [ 5]. For applications with frequent moving objects such as the ImageNet-VID dataset [ 23],\nmore complex methods like optical ﬂow approaches are needed to achieve better accuracy.\n6 Conclusion\nWe proposes a temporal early exit object detection pipeline to reduce the computational complexity\nof per-frame video object detection. The proposed approach takes advantage of infrequent variation\nbetween features of consecutive video frames to avoid redundant computation. Video frames with\ninvariant features are identiﬁed in the early stages of the network with very low computation effort. For\nthe unchanged video frames, detection results from previous frames are reused. A full computation\neffort is only required if a video frame is identiﬁed with semantic variations compared to previous\nframes. The proposed approach accelerates per-frame video object detection up to 34×with less than\n2.2 % reduction in mAP.\n9', 'document_title': 'paper.pdf'}, {'page_content': 'References\n[1]Pablo F Alcantarilla, Simon Stent, German Ros, Roberto Arroyo, and Riccardo Gherardi. Street-\nview change detection with deconvolutional networks. Autonomous Robots , 42(7):1301–1322,\n2018.\n[2]Ali Borji, Ming-Ming Cheng, Huaizu Jiang, and Jia Li. Salient object detection: A benchmark.\nIEEE transactions on image processing , 24(12):5706–5722, 2015.\n[3]Francesca Bovolo and Lorenzo Bruzzone. A theoretical framework for unsupervised change\ndetection based on change vector analysis in the polar domain. IEEE Transactions on Geoscience\nand Remote Sensing , 45(1):218–236, 2006.\n[4]Lorenzo Bruzzone and D Fernandez Prieto. An adaptive semiparametric and context-based\napproach to unsupervised change detection in multitemporal remote-sensing images. IEEE\nTransactions on image processing , 11(4):452–466, 2002.\n[5]Nil Goyette, Pierre-Marc Jodoin, Fatih Porikli, Janusz Konrad, and Prakash Ishwar. Changede-\ntection. net: A new change detection benchmark dataset. In 2012 IEEE computer society\nconference on computer vision and pattern recognition workshops , pages 1–8. IEEE, 2012.\n[6]Chaoxu Guo, Bin Fan, Jie Gu, Qian Zhang, Shiming Xiang, Veronique Prinet, and Chunhong\nPan. Progressive sparse local attention for video object detection. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision , pages 3909–3918, 2019.\n[7]Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi,\nIan Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, et al. Speed/accuracy trade-offs\nfor modern convolutional object detectors. In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 7310–7311, 2017.\n[8]Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014.\n[9]Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European\nconference on computer vision , pages 740–755. Springer, 2014.\n[10] Xiaoqiang Lu, Yuan Yuan, and Xiangtao Zheng. Joint dictionary learning for multispectral\nchange detection. IEEE transactions on cybernetics , 47(4):884–897, 2016.\n[11] Yuan Luo, Hanxing Zhou, Qin Tan, Xuefeng Chen, and Mingjing Yun. Key frame extraction of\nsurveillance video based on moving object detection and image similarity. Pattern Recognition\nand Image Analysis , 28(2):225–231, 2018.\n[12] H MAHMOUDZADEH. Digital change detection using remotely sensed data for monitoring\ngreen space destruction in tabriz. 2007.\n[13] David Mascharka, Philip Tran, Ryan Soklaski, and Arjun Majumdar. Transparency by design:\nClosing the gap between performance and interpretability in visual reasoning. In Proceedings\nof the IEEE conference on computer vision and pattern recognition , pages 4942–4950, 2018.\n[14] Gabriele Moser, Elena Angiati, and Sebastiano B Serpico. Multiscale unsupervised change\ndetection on optical images by markov random ﬁelds and wavelets. IEEE Geoscience and\nRemote Sensing Letters , 8(4):725–729, 2011.\n[15] Sangmin Oh, Anthony Hoogs, Amitha Perera, Naresh Cuntoor, Chia-Chih Chen, Jong Taek Lee,\nSaurajit Mukherjee, JK Aggarwal, Hyungtae Lee, Larry Davis, et al. A large-scale benchmark\ndataset for event recognition in surveillance video. In CVPR 2011 , pages 3153–3160. IEEE,\n2011.\n[16] Priyadarshini Panda, Abhronil Sengupta, and Kaushik Roy. Conditional deep learning for\nenergy-efﬁcient and enhanced pattern recognition. In 2016 Design, Automation & Test in\nEurope Conference & Exhibition (DATE) , pages 475–480. IEEE, 2016.\n10', 'document_title': 'paper.pdf'}, {'page_content': '[17] Dong Huk Park, Trevor Darrell, and Anna Rohrbach. Robust change captioning. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision , pages 4624–4633, 2019.\n[18] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative\nstyle, high-performance deep learning library. arXiv preprint arXiv:1912.01703 , 2019.\n[19] Richard J Radke, Srinivas Andra, Omar Al-Kofahi, and Badrinath Roysam. Image change\ndetection algorithms: a systematic survey. IEEE transactions on image processing , 14(3):\n294–307, 2005.\n[20] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time\nobject detection with region proposal networks. arXiv preprint arXiv:1506.01497 , 2015.\n[21] Paul Rosin. Thresholding for change detection. In Sixth International Conference on Computer\nVision (IEEE Cat. No. 98CH36271) , pages 274–279. IEEE, 1998.\n[22] Paul L Rosin and Efstathios Ioannidis. Evaluation of global image thresholding for change\ndetection. Pattern recognition letters , 24(14):2345–2356, 2003.\n[23] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual\nrecognition challenge. International journal of computer vision , 115(3):211–252, 2015.\n[24] Jing Shi and Chenliang Xu. Differential network for video object detection.\n[25] Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Branchynet: Fast inference\nvia early exiting from deep neural networks. In 2016 23rd International Conference on Pattern\nRecognition (ICPR) , pages 2464–2469. IEEE, 2016.\n[26] Yonglong Tian, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning strong parts for\npedestrian detection. In Proceedings of the IEEE international conference on computer vision ,\npages 1904–1912, 2015.\n[27] Carl V ondrick, Donald Patterson, and Deva Ramanan. Efﬁciently scaling up crowdsourced\nvideo annotation. International Journal of Computer Vision , pages 1–21. ISSN 0920-5691.\nURL http://dx.doi.org/10.1007/s11263-012-0564-1 . 10.1007/s11263-012-0564-1.\n[28] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang\nWang, and Xiaoou Tang. Residual attention network for image classiﬁcation. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition , pages 3156–3164, 2017.\n[29] Philippe Weinzaepfel, Jerome Revaud, Zaid Harchaoui, and Cordelia Schmid. Deepﬂow: Large\ndisplacement optical ﬂow with deep matching. In Proceedings of the IEEE international\nconference on computer vision , pages 1385–1392, 2013.\n[30] Xizhou Zhu, Yuwen Xiong, Jifeng Dai, Lu Yuan, and Yichen Wei. Deep feature ﬂow for video\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 2349–2358, 2017.\n11', 'document_title': 'paper.pdf'}]: %s
2023-12-06 14:05:36,539 - INFO - Received requests to /inference endpoint
2023-12-06 14:05:36,640 - INFO - Received a batch of request with batch size of: 1 
2023-12-06 14:05:36,640 - INFO - Received request: {'username': 'amin', 'prompt': 'give me a summary of  paper', 'memory': False, 'conversation_number': 0, 'AI_assistance': False, 'collection_name': 'web', 'llm_model': 'Llama_13b'}
2023-12-06 14:06:16,541 - INFO - Processed the request successfully
2023-12-06 15:55:42,728 - INFO - request processed successfully username='amin' class_name=None mode=None vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/39b003ed377ad2b1': %s
2023-12-06 15:55:42,728 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 16:02:33,162 - INFO - request processed successfully username='amin' class_name=None mode=None vectorDB_type='Weaviate' file_path=None: %s
2023-12-06 16:02:33,162 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 16:06:03,313 - INFO - request processed successfully username='amin' class_name=None mode=None vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/739c23f5a50ad4f9': %s
2023-12-06 16:06:03,313 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 16:06:09,981 - INFO - request processed successfully username='amin' class_name=None mode=None vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/b19ae0db2bb367cf': %s
2023-12-06 16:06:09,981 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 16:08:38,388 - INFO - request processed successfully username='amin' class_name=None mode=None vectorDB_type='Weaviate' file_path=None: %s
2023-12-06 16:08:38,388 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 16:09:44,982 - INFO - request processed successfully username='amin' class_name=None mode=None vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/9d5f6182ff1d72bf': %s
2023-12-06 16:09:44,982 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 16:10:10,877 - INFO - request processed successfully username='amin' class_name=None mode=None vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/b547716aa2174f0e': %s
2023-12-06 16:10:10,877 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 16:10:31,046 - INFO - request processed successfully username='amin' class_name=None mode=None vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/a7297a2e0806883d': %s
2023-12-06 16:10:31,046 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 16:10:38,626 - INFO - request processed successfully username='amin' class_name=None mode=None vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/311bea31f43d0826': %s
2023-12-06 16:10:38,626 - ERROR - An error occurred: local variable 'response' referenced before assignment
2023-12-06 16:20:45,570 - INFO - checking the request/ username='amin' class_name='sdee' mode='create_collection' vectorDB_type='Weaviate' file_path=None: %s
2023-12-06 16:20:45,663 - INFO - checkpoint 1
2023-12-06 16:20:45,663 - INFO - checkpoint 2 amin: %s
2023-12-06 16:20:45,663 - INFO - checkpoint 2 amin_sdee: %s
2023-12-06 16:20:45,711 - INFO - class name added successfully to database
2023-12-06 16:20:45,711 - INFO - success: class sdee created for user amin
2023-12-07 10:15:06,924 - INFO - Created a temporary directory at /tmp/tmpymd0ku3l
2023-12-07 10:15:06,924 - INFO - Writing /tmp/tmpymd0ku3l/_remote_module_non_scriptable.py
2023-12-07 10:15:09,289 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-07 10:15:34,232 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-07 10:15:49,009 - INFO - Created a temporary directory at /tmp/tmp8pin0ush
2023-12-07 10:15:49,009 - INFO - Writing /tmp/tmp8pin0ush/_remote_module_non_scriptable.py
2023-12-07 10:15:50,821 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-07 10:15:55,067 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-07 10:16:04,557 - INFO - checking the request/ username='amin' class_name='genAI' mode='create_collection' vectorDB_type='Weaviate' file_path=None: %s
2023-12-07 10:16:04,617 - INFO - checkpoint 1
2023-12-07 10:16:04,617 - INFO - checkpoint 2 amin: %s
2023-12-07 10:16:04,617 - INFO - checkpoint 2 amin_genAI: %s
2023-12-07 10:16:04,659 - INFO - class name added successfully to database
2023-12-07 10:16:04,659 - INFO - success: class genAI created for user amin
2023-12-07 10:17:35,864 - INFO - request received username='amin' class_name='genAI' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/0273b9b7518984f6': %s
2023-12-07 10:17:36,445 - INFO - actors creation successful [Actor(WeaviateEmbedder, 81e0adc786a4f367b581fb0501000000), Actor(WeaviateEmbedder, 550d19d444353d63f639428a01000000), Actor(WeaviateEmbedder, cf13543a67d30513b458ef2901000000)]: %s
2023-12-07 10:17:36,446 - INFO - check 1st step of ray was successful
2023-12-07 10:17:36,446 - INFO - check if ray was successful:
2023-12-07 10:17:36,446 - INFO - check weaviate add data, 
2023-12-07 10:17:36,446 - INFO - request processed successfully username='amin' class_name='genAI' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/0273b9b7518984f6': %s
2023-12-07 10:17:38,260 - INFO - Check the data that is being passed [{'page_content': 'DINOv2: Learning Robust Visual Features\nwithout Supervision\nMaxime Oquab∗∗, Timothée Darcet∗∗, Théo Moutakanni∗∗,\nHuy Vo∗, Marc Szafraniec∗, Vasil Khalidov∗, Pierre Fernandez, Daniel Haziza,\nFrancisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba,\nRussell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat,\nVasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal1,\nPatrick Labatut∗, Armand Joulin∗, Piotr Bojanowski∗\nMeta AI Research1Inria\n∗core team∗∗equal contribution\nAbstract\nThe recent breakthroughs in natural language processing for model pretraining on large\nquantities of data have opened the way for similar foundation models in computer vision.\nThese models could greatly simplify the use of images in any system by producing all-\npurpose visual features, i.e., features that work across image distributions and tasks without\nﬁnetuning. This work shows that existing pretraining methods, especially self-supervised\nmethods, can produce such features if trained on enough curated data from diverse sources.\nWe revisit existing approaches and combine diﬀerent techniques to scale our pretraining in\nterms of data and model size. Most of the technical contributions aim at accelerating and\nstabilizing the training at scale. In terms of data, we propose an automatic pipeline to build\na dedicated, diverse, and curated image dataset instead of uncurated data, as typically done\nin the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy\net al., 2020) with 1B parameters and distill it into a series of smaller models that surpass\nthe best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the\nbenchmarks at image and pixel levels.\n1 Introduction\nLearning task-agnostic pretrained representations have become the standard in Natural Language Process-\ning (NLP) (Radford et al.; Raﬀel et al., 2020; Chowdhery et al., 2022; Hoﬀmann et al., 2022; Touvron et al.,\n2023). One can use these features “as they are”, i.e., without ﬁne-tuning, and achieve performances on down-\nstream tasks that are signiﬁcantly better than those produced by task-speciﬁc models (Brown et al., 2020).\nThis success has been fueled by pretraining on large quantities of raw text using pretext objectives, such as\nlanguage modeling (Radford et al., 2017) or word vectors (Devlin et al., 2018), that require no supervision.\nFollowing this paradigm shift in NLP, we expect similar “foundation” models to appear in computer vi-\nsion (Bommasani et al., 2021). These models should generate visual features that work out of the box on\nany task, both at the image level, e.g., image classiﬁcation, and pixel level, e.g., segmentation. Most promis-\ning eﬀorts towards these foundation models focus on text-guided pretraining, i.e., using a form of textual\nsupervision to guide the training of the features (Joulin et al., 2016; Mahajan et al., 2018; Radford et al.,\n2021). This form of text-guided pretraining limits the information that can be retained about the image\nsince captions only approximate the rich information in images, and complex pixel-level information may\nAll the authors are aﬃliated to Meta, except Julien Mairal who is aﬃliated to Inria. Timothée Darcet and Pierre Fernandez\nhave a co-aﬃliation with Inria. Théo Moutakanni has a co-aﬃliation with Université Paris Saclay. Alaaeldin El-Nouby has a\nco-aﬃliation with Inria and ENS-PSL. Correspondence: {qas, timdarcet, theomoutakanni, ajoulin, bojanowski}@meta.com\n1arXiv:2304.07193v1  [cs.CV]  14 Apr 2023', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 1: Visualization of the ﬁrst PCA components. We compute a PCA between the patches of the\nimages from the same column (a, b, c and d) and show their ﬁrst 3 components. Each component is matched\nto a diﬀerent color channel. Same parts are matched between related images despite changes of pose, style\nor even objects. Background is removed by thresholding the ﬁrst PCA component.\nnot surface with this supervision. Furthermore, these image encoders require aligned text-image corpora and\nhence, do not oﬀer the ﬂexibility of their text counterparts, that is, to learn from raw data alone.\nAn alternative to text-guided pretraining is self-supervised learning (Caron et al., 2018; Chen et al., 2020;\nHe et al., 2021) where features are learned from images alone. These approaches are conceptually closer to\npretext tasks such as language modeling and can capture information at the image and pixel level (Caron\net al., 2021). However, despite their potential to learn all-purposed features, most of the advances in\nself-supervised learning were made in the context of pretraining on a small curated dataset, ImageNet-\n1k (Russakovsky et al., 2015). Some eﬀorts on scaling these approaches beyond ImageNet-1k have been\nattempted (Caron et al., 2019; Goyal et al., 2021; 2022a), but they focused on uncurated datasets, which\ntypically lead to a signiﬁcant drop in the quality of the features. This is explained by the lack of control\nover the data quality and diversity, which are essential to produce good features.\nIn this work, we explore if self-supervised learning has the potential to learn all-purposed visual features if\npretrained on a large quantity of curated data. We revisit existing discriminative self-supervised approaches\nthat learn features at both the image and patch level, such as iBOT (Zhou et al., 2021), and we reconsider\nsomeoftheirdesignchoicesunderthelensofalargerdataset. Mostofourtechnicalcontributionsaretailored\ntoward stabilizing and accelerating discriminative self-supervised learning when scaling in model and data\nsizes. These improvements make our approach around 2 ×faster and require 3 ×less memory than similar\ndiscriminative self-supervised methods, allowing us to leverage longer training with larger batch sizes.\nRegarding pretraining data, we have built an automatic pipeline to ﬁlter and rebalance datasets from an\nextensive collection of uncurated images. This pipeline is inspired by pipelines used in NLP (Wenzek et al.,\n2019), where data similarities are used instead of external metadata and do not require manual annotation.\nA major diﬃculty when dealing with images in the wild is to rebalance concepts and avoid overﬁtting on a\nfew dominant modes. In this work, a naive clustering approach works reasonably well to resolve this issue.\nWe gathered a small but diverse corpus of 142M images to validate our approach.\nFinally, we provide a variety of pretrained visual models, called DINOv2, trained with diﬀerent Vision\nTransformers (ViT) (Dosovitskiy et al., 2016) architectures on our data. We release all the models and\nthe code to retrain DINOv2 on any data. We validate the quality of DINOv2 on various computer vision\nbenchmarks at both image and pixel levels as we scale them, as summarized in Fig. 2. We conclude that self-\n2', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': '101010111012\nflops7578818487Accuracy\nInet-1k\n101010111012\nflops40485664mIoU\nSegmentation\n101010111012\nflops0.91.21.51.8 R-MSE\nMonocular Depth \n101010111012\nflops80848892Accuracy\nClassification\n101010111012\nflops4856647280Accuracy\nFinegrained Classification\n101010111012\nflops30456075mAP\nInstance Retrieval\n101010111012\nflops4050607080Accuracy\nImageNet-{A,R,Sketch}\n101010111012\nflops5055606570Accuracy\nVideo Understanding\nSSL\nWSL\nDINOv2Figure 2: Evolution of performance when scaling in parameters. We show performance on eight\ntypes of vision tasks, as presented in Sec. 7, and average metrics with each type. Features are extracted\nfrom our self-supervised encoders, DINOv2 (dark blue), and we compare them with self-supervised methods\n(pale orange), as well as weakly-supervised methods (dark pink). We report the best-performing weakly-\nsupervised model’s performance as a dashed horizontal line. Our family of models drastically improves over\nthe previous state of the art in self-supervised learning and reaches performance comparable with weakly-\nsupervised features. See Sec. 7 for a detailed analysis.\nsupervised pretraining alone is a good candidate for learning transferable frozen features that are competitive\nwith the best openly available weakly-supervised models.\n2 Related Work\nIntra-image self-supervised training. A ﬁrst family of self-supervised methods focuses on pretext tasks\nbuilt from the image, i.e., extracting a signal from the image to be predicted from the rest of the image.\nThis idea has become prevalent with the work of Doersch et al. (2015), where they train by predicting the\ncontext of a given patch. Many other pretext tasks were introduced based on re-colorizing images (Zhang\net al., 2016), predicting transformations (Gidaris et al., 2018), inpainting (Pathak et al., 2016) or patch\nre-ordering (Noroozi & Favaro, 2016; Misra & Maaten, 2020). Recently, the emergence of patch-based\narchitectures, like ViTs, has led to a revisit of inpainting for pre-training (He et al., 2021; Bao et al., 2021;\nEl-Nouby et al., 2021), potentially in feature space (Assran et al., 2023; Baevski et al., 2022). Of particular\ninterest, He et al. (2021) show that a masked auto-encoder (MAE) learns features that provide substantial\nimprovements when ﬁnetuned on downstream tasks. This property of MAEs has been further validated\non video (Tong et al., 2022), audio (Xu et al., 2022), and across other modalities (Girdhar et al., 2022).\nHowever, their features require supervised ﬁnetuning, while our features perform well out of the box.\nDiscriminativeself-supervisedlearning. Thesecondlineofwork, closertoours, isusingdiscriminative\nsignals between images or groups of images to learn features. This family of methods has roots in early\ndeep learning work (Hadsell et al., 2006) but became popular with the emergence of instance classiﬁcation\nmethods (Dosovitskiy et al., 2014; Bojanowski & Joulin, 2017; Wu et al., 2018). Several improvements\nwere made based either on instance-level objectives (Hénaﬀ et al., 2019; He et al., 2020; Chen & He, 2020;\nChen et al., 2020; Grill et al., 2020; Caron et al., 2021) or clustering (Caron et al., 2018; Asano et al.,\n2020; Caron et al., 2020). These methods provide performant frozen features on standard benchmarks like\nImageNet (Russakovsky et al., 2015), but they are hard to scale to larger model sizes (Chen et al., 2021). In\n3', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Uncur ated Data \nAugment ed Cur ated Data\nCurated Data\n Embedding\n Deduplication\n RetrievalFigure 3: Overview of our data processing pipeline. Images from curated and uncurated data sources\nare ﬁrst mapped to embeddings. Uncurated images are then deduplicated before being matched to curated\nimages. The resulting combination augments the initial dataset through a self-supervised retrieval system.\nthis work, we revisit the training of these approaches in the context of large pretraining datasets and models.\nIn particular, we build on top of Zhou et al. (2021) that we ﬁnd particularly suited for scaling.\nScaling self-supervised pretraining. A growing body of work has focused on the scaling abilities of\nself-supervised learning in terms of data and model size (Caron et al., 2019; Goyal et al., 2019; Tian et al.,\n2021; Goyal et al., 2022a). Most of these works use large quantities of uncurated data to train models\nwithout supervision. They show evidence that discriminative methods scale with data, but because of the\npoor quality of the pretraining data, most of the results are obtained by ﬁnetuning the features. Of particular\ninterest, Goyal et al. (2021) have also shown that these methods beneﬁt from scaling in model size given\nenough pretrained data. This line of work questions the ability of self-supervised methods to work on any\ndata while we focus on producing the best pretrained encoders.\nAutomatic data curation. Our dataset construction borrows from the image retrieval community (Wein-\nzaepfeletal.,2021;Radenovićetal.,2018b;Bermanetal.,2019;Douzeetal.,2009;Toliasetal.,2015;Revaud\net al., 2019). In particular, the use of retrieval to augment the training set has been studied in the context\nof semi-supervised learning (Yalniz et al., 2019). Similarly, others have used hashtags or other metadata to\nﬁlter uncurated datasets (Mahajan et al., 2018; Radford et al., 2021). Unlike this work, we use no metadata\nnor supervision to ﬁlter images and leverage visual similarity between images. Our approach is inspired by\ntext curation pipelines (Wenzek et al., 2019), where a language model is trained on Wikipedia to score texts\nextracted from an uncurated source.\n3 Data Processing\nWe assemble our curated LVD-142M dataset by retrieving, from a large pool of uncurated data, images that\nare close to those in several curated datasets. We describe below the main components in our data pipeline\nincluding the curated/uncurated data sources, the image deduplication step and the retrieval system. Our\npipeline does not require any metadata or text and directly works with images, as shown in Fig. 3. We refer\nthe reader to appendix A for more details on our approach.\nData sources. Our selection of curated datasets is detailed in the appendix (Table 15) and contains\nImageNet-22k, the train split of ImageNet-1k, Google Landmarks and several ﬁne-grained datasets. For the\nuncurated data source, we collect a raw unﬁltered dataset of images from a publicly available repository of\ncrawled web data. From each web page in the repository, we extract URL links of images from <img>tags.\nWe discards URLs that are unsafe or restricted by domains, and post-process the downloaded images (PCA\nhash deduplication, NSFW ﬁltering, and blurring identiﬁable faces). This results in 1.2B unique images.\n4', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Deduplication. We apply the copy detection pipeline of Pizzi et al. (2022) to the uncurated data and\nremove near-duplicate images. This reduces redundancy and increases diversity among images. We also\nremove near-duplicates of images contained in the test or validation set of any benchmark used in this work.\nSelf-supervised image retrieval. We build our curated pretraining dataset by retrieving images from\nour uncurated data source that are close to images in our curated sources. In order to do this, we ﬁrst\ncompute an image embedding using a self-supervised ViT-H/16 network pretrained on ImageNet-22k, and\nuse cosine-similarity as a distance measure between images. Then, we perform k-means clustering of the\nuncurated data. Given a query dataset for retrieval, if it is large enough we retrieve N(typically 4) nearest\nneighbors for each query image. If it is small, we sample Mimages from the cluster corresponding to each\nquery image. We adjust NandMby visual inspection of the retrieval result.\nImplementation Details. The deduplication and retrieval stages of our pipeline rely on the Faiss li-\nbrary (Johnson et al., 2019) to eﬃciently index and compute batch searches of nearest embeddings. In\nparticular, we heavily leverage its support for GPU-accelerated indices, using inverted ﬁle indices with prod-\nuct quantization codes (Jegou et al., 2010). The whole processing is distributed on a compute cluster of 20\nnodes equipped with 8 V100-32GB GPUs and takes less than two days to produce the LVD-142M dataset.\n4 Discriminative Self-supervised Pre-training\nWe learn our features with a discriminative self-supervised method that can be seen as a combination of\nDINO and iBOT losses with the centering of SwAV (Caron et al., 2020). We also add a regularizer to spread\nfeatures and a short high-resolution training phase. We rapidly introduce each of these approaches, but more\ndetails can be found in the related papers, or in our open-sourced code.\n•Image-level objective (Caron et al., 2021). We consider the cross-entropy loss between the\nfeatures extracted from a student and a teacher network. Both features are coming from the class\ntoken of a ViT, obtained from diﬀerent crops of the same image. We learn the parameters of the\nstudent and build the teacher with an exponential moving average of past iterates (He et al., 2020).\n•Patch-level objective (Zhou et al., 2021). We randomly mask some of the input patches given\nto the student, but not to the teacher. We then add a cross-entropy loss between the patch features\nof both networks on each masked patch. This loss is combined with the image-level loss.\n•Untying head weights between both objectives. We observe that tying the weights associated\nwith both objectives makes the model underﬁt at the patch-level while overﬁtting at the image-level.\nUntying these weights resolves this issue and improve the performances at both scales.\n•Sinkhorn-Knopp centering (Caron et al., 2020). Ruan et al. (2022) recommend to replace the\nteacher softmax-centering step of DINO and iBot by the Sinkhorn-Knopp (SK) batch normalization\nof SwAV (Caron et al., 2020). We run the Sinkhorn-Knopp algorithm steps for 3 iterations. For the\nstudent, we apply the softmax normalization.\n•KoLeo regularizer (Sablayrolles et al., 2018). The KoLeo regularizer derives from the\nKozachenko-Leonenko diﬀerential entropy estimator (see Beirlant et al. (1997); Delattre & Fournier\n(2017)) and encourages a uniform span of the features within a batch. Given a set of nvectors\n(x1, . . . , x n), it is deﬁned asLkoleo =−1\nn∑n\ni=1log(dn,i),where dn,i= min j̸=i∥xi−xj∥is the mini-\nmum distance between xiand any other point within the batch. We also ℓ2-normalize the features\nbefore computing this regularizer.\n•Adapting the resolution (Touvron et al., 2019). Increasing image resolution is key to pixel-\nlevel downstream tasks such as segmentation or detection, where small objects disappear at low\nresolutions. However, training at high resolution is time and memory demanding, and instead, we\nincrease the resolution of images to 518×518during a short period at the end of pretraining.\n5', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': '5 Eﬃcient implementation\nWe consider several improvements to train models at a larger scale. We train models on A100 GPUs using\nPyTorch 2.0. The code is also available along with the pretrained models used for feature extraction1.\nThe details of our models are in the appendix, Table 17. With the same hardware, compared to the iBOT\nimplementation, the DINOv2 code runs around 2×faster using only 1/3of the memory.\nFast and memory-eﬃcient attention. We implemented our own version of FlashAttention (Dao et al.,\n2022) to improve memory usage and speed on the self-attention layers. Our version is on par with or\nbetter than the original on all cases considered, while covering more use-cases and hardware. Due to the\nGPU hardware speciﬁcs, the eﬃciency is best when the embedding dimension per head is a multiple of\n64, and the matrix operations are even better when the full embedding dimension is a multiple of 256.\nAs a consequence, our ViT-g architecture slightly diﬀers from the architecture proposed by Zhai et al.\n(2022) in order to maximize compute eﬃciency, and we use an embedding dimension of 1536 with 24 heads\n(64 dim/head), rather than 1408 with 16 heads (88 dim/head). Our experiments did not show signiﬁcant\ndiﬀerences in ﬁnal accuracy, and our ViT-g backbone counts 1.1B parameters.\nNested tensors in self-attention. Our version also allows running in the same forward pass the global\ncrops and the local crops (that have diﬀerent numbers of patch tokens), leading to signiﬁcant compute\neﬃciency gains compared to using separate forward and backward passes as done in prior implementations.\nThe lower-level components of our setup are available in the xFormers library2(Lefaudeux et al. (2022)).\nEﬃcient stochastic depth. We implement an improved version of stochastic depth (Huang et al., 2016)\nthat skips the computation of the dropped residuals rather than masking the result. This saves memory and\ncompute in proportion approximately equal to the drop rate, thanks to speciﬁc fused kernels. With high\ndrop rates ( d= 40%in this work), this allows a drastic improvement in compute eﬃciency and memory\nusage. The implementation consists of randomly shuﬄing the Bsamples over the batch dimension, and\nslicing the ﬁrst (1−d)×Bsamples for the computations in the block.\nFully-Sharded Data Parallel (FSDP). Minimizing our objective with the AdamW optimizer requires\n4 model replicas in ﬂoat32 precision – student, teacher, optimizer ﬁrst moments, optimizer second moments.\nThis sums to 16 GBof memory for a billion-parameter model such as our ViT-g. In order to reduce this\nmemory footprint per GPU, we split the model replicas across GPUs, i.e., sharding 16 GBacross GPUs\nusing the PyTorch implementation of FSDP. Consequently, the model size is not bounded by the memory of\na single GPU but by the total sum of GPU memory across compute nodes. The Pytorch implementation of\nFSDP brings a second advantage, which is to save on the cross-GPU communication costs: the weight shards\nare stored in ﬂoat32 precision as required by the optimizer, but broadcasting weights and reducing gradients\nis done in ﬂoat16 precision for the backbone (MLP heads gradients are reduced in ﬂoat32 to avoid training\ninstabilities). This leads to approximately 50% reduction in communication costs compared to the ﬂoat32\ngradient all-reduce operation used in DistributedDataParallel (DDP), which is used in other self-supervised\npretraining methods (Caron et al., 2021; Zhou et al., 2021). As a consequence, the training procedure\nscales more eﬃciently than DDP with ﬂoat16 autocast when scaling the number of GPU nodes. Overall,\nPytorch-FSDP mixed-precision is superior to DDP with autocast in virtually all cases we encountered.\nModeldistillation. Mostofourtechnicalimprovementstothetrainingloopaimatimprovingthetraining\nof large models over large quantities of data. For smaller models, we distill them from our largest model,\nthe ViT-g, instead of training them from scratch. Knowledge distillation (Hinton et al., 2015) aims at\nreproducing the output of a large model with a smaller model by minimizing some distance between both\noutputs for a set of given inputs. Since our objective function is a form of distillation from the teacher\nnetwork to the student network, we leverage the same training loop with a few exceptions: we use a larger\nmodel as a frozen teacher, keep a spare EMA of the student that we use as our ﬁnal model, remove the\nmasking and stochastic depth, and, apply the iBOT loss on the two global crops. In our ablations, we\n1https://github.com/facebookresearch/dinov2\n2https://github.com/facebookresearch/xformers\n6', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'INet-1k k-NN INet-1k linear\niBOT 72.9 82.3\n+(our reproduction) 74.5 ↑1.6 83.2 ↑0.9\n+LayerScale, Stochastic Depth 75.4 ↑0.9 82.0 ↓1.2\n+128k prototypes 76.6 ↑1.2 81.9 ↓0.1\n+KoLeo 78.9 ↑2.3 82.5 ↑0.6\n+SwiGLU FFN 78.7 ↓0.2 83.1 ↑0.6\n+Patch size 14 78.9 ↑0.2 83.5 ↑0.4\n+Teacher momentum 0.994 79.4 ↑0.5 83.6 ↑0.1\n+Tweak warmup schedules 80.5 ↑1.1 83.8 ↑0.2\n+Batch size 3k 81.7 ↑1.2 84.7 ↑0.9\n+Sinkhorn-Knopp 81.7 = 84.7 =\n+Untying heads = DINOv2 82.0 ↑0.3 84.5 ↓0.2\nTable 1:Ablation study of the training diﬀerences between iBOT and DINOv2. We optimize\nfor k-NN performance, as in our experience, the linear probe performance is lower-bounded by the k-NN\nperformance. Some modiﬁcations, like LayerScale and a high Stochastic Depth (rate= 0.4), incur a decrease\nin linear probe performance, but have the beneﬁts of increasing the stability of training by avoiding NaN loss\nvalues during training. Overall, these modiﬁcations allowed for the next set of improvements to be added.\nExperiments are run using the ViT-Large architecture on ImageNet-22k.\nobserve that this approach achieves better performance than training from scratch, even for a ViT-L. Our\ndistillation method ends up close to the one described by Duval et al. (2023), except we do not modify the\nloss terms for distillation and evaluate the EMA of the student.\n6 Ablation Studies\nWe present a set of ablations to empirically validate diﬀerent components of our pipeline: the technical\nmodiﬁcations described in Sec. 4, the pretraining data and the impact of model distillation. We consider\nvarious downstream tasks that are described in Sec. 7.\n6.1 Improved Training Recipe\nOur approach improves over the iBOT method by combining it with several existing components described\nin Sec. 4. To evaluate their importance, we train multiple models where we successively add components to\na baseline iBOT model. We report the Top-1 accuracy on the validation set of ImageNet-1k with a k-NN\nand a linear linear in Table 1. Generally, we observe that each component improves the performance on\neither k-NN or linear probing and even both in most cases. Only LayerScale and Stochastic Depth incur a\nperformance drop in linear probing but signiﬁcantly improve the training stability in our experience.\n6.2 Pretraining Data Source\nThe quality of features is directly related to the quality of the pretraining data. In this experiment, we\nprobe the impact of LVD-142M compared to ImageNet-22k, a commonly used pretraining dataset, or using\ndirectly raw and uncurated data. For the uncurated dataset, we randomly sample 142million images from\nthe same data source as LVD-142M. We train a ViT-g/14 on each dataset for the same number of iterations.\nWe also include a variant of ImageNet-22k obtained by removing the synsets of ImageNet-1k (INet-22k \\\nINet-1k) for completeness. We report the comparisons in Table 2.\nThe most salient observation is that training on a curated set of images works better on most benchmarks\nthan training on uncurated data. This conﬁrms the beneﬁt of curating data, even in the case of self-\nsupervised pretraining. When compared with models trained on ImageNet-22k, training on LVD-142M is\nalso superior on all the benchmarks but ImageNet-1k. This conﬁrms that training on a more diverse set of\n7', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Training Data INet-1k Im-A ADE-20k Oxford-M\nINet-22k 85.9 73.5 46.6 62.5\nINet-22k\\INet-1k 85.3 70.3 46.2 58.7\nUncurated data 83.3 59.4 48.5 54.3\nLVD-142M 85.8 73.9 47.7 64.6\nTable 2:Ablation of the source of pretraining data. We compare the INet-22k dataset that was used\nin iBOT to our dataset, LVD-142M. Each model is trained for the same number of iterations, that is smaller\nthan in our ﬁnal run. Pretraining on LVD-142M maintains the performance over INet-1k while leading to\nmodels that perform better in other domains.\nL H g848586\nImageNet-1k\nINet-22k\nLVD142M\nL H g747678\nImageNet-V2\nL H g5060\nImageNet-Sketch\nL H g9294\nFood101\nL H g8090\nCars\nL H g3540\nAmsterTime\nL H g2040\nOxford-H\nFigure 4: Model scale versus data scale. Evolution of performance as a function of model size for\ntwo diﬀerent pretraining datasets: ImageNet-22k (14M images) and LVD-142M (142M images). The ViT-g\ntrained on LVD-142M surpasses the ViT-g trained on ImageNet-22k on most benchmarks.\nimages improves the quality of the features in domains that are not covered by this dataset. Overall, the\nconclusion of this ablation is that our dataset provides a good balance of diﬀerent types of images that leads\nto the best performance overall.\n6.3 Model Size and Data\nWe quantify the importance of scaling data with the model size in Fig. 4. As the size of models grow, training\non LVD-142M becomes more beneﬁcial than training on ImageNet-22k. For instance, a ViT-g trained on\nLVD-142M matches the performance on ImageNet-1k of a model trained on ImageNet-22k while signiﬁcantly\noutperforming it on the other benchmarks.\n6.4 Loss Components\nWe validated the proposed technical improvements in Sec. 6.1 by adding them incrementally. This section\nanalyzes the performance hit observed if we ablate speciﬁc loss terms, starting from our best-performing\nmodel. We ablate the importance of the KoLeo loss and the impact of the masked image modeling term.\nFor both, we report performance on ImageNet-1k using a linear classiﬁer, ADE-20k segmentation using a\nlinear classiﬁer, and nearest-neighbor image retrieval on Oxford-M. Table 3a shows the impact of using the\nKoLeo loss. We see that the instance retrieval performance improves by more than 8%, conﬁrming that this\nterm helps spread features in the output space. At the same time, the other metrics do not suﬀer from this\nregularization. In Table 3b, we show the impact of using the masked image modeling term from iBOT. This\nterm is critical for dense prediction tasks, leading to almost 3%performance improvement.\n6.5 Impact of Knowledge Distillation\nFor small architectures, we distill larger models instead of training them from scratch. We use the distillation\nprocedure described in Sec. 5. We evaluate the eﬀectiveness of this approach by comparing a ViT-L/14\ntrained from scratch with one distilled from a ViT-g/14 over 12 benchmarks in Fig. 5. We also report the\nperformance of the ViT-g/14 used for distillation as a topline. The distilled model outperforms the one\ntrained from scratch on 10 out of 12 benchmarks, validating our pretraining approach for small models.\n8', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'KoLeo INet-1k Im-A ADE-20k Oxford-M\n\x15 85.3 70.6 47.2 55.6\n✓ 85.8 72.8 47.1 63.9\n(a) Koleo lossMIM INet-1k Im-A ADE-20k Oxford-M\n\x15 85.3 72.0 44.2 64.3\n✓ 85.8 72.8 47.1 63.9\n(b) MIM objective in iBOT\nTable 3:(a)Eﬀect of the KoLeo loss term. (b)Eﬀect of the iBOT Masked Image Modeling (MIM) loss\nterm. Evaluation performed on ImageNet-{1k,A} (classiﬁcation with linear probe, accuracy %), ADE-20k\n(segmentation with linear layer, mIoU) and Oxford-M (image retrieval, mAP). Each model is trained on the\nsame number of iterations, that is smaller than our ﬁnal run. The KoLeo loss term improves nearest-neighbor\nsearch tasks (e.g. retrieval), and the MIM loss improves patch-level tasks (e.g. segmentation).\nINet-1kFoodCarsiNat18\niNat21\nPlaces 205Oxford-H\nParis-H\nINet-A\nINet-RKitti\nNYUd\nViT-L/14 Scratch\nViT-L/14 Distill\nViT-g/14 Scratch\n84.5 86.3 86.592.894.394.7\n81.890.191.4\n77.880.481.6\n83.185.185.7\n66.067.367.5\n47.7 52.6 52.1\n77.6\n84.482.761.7\n71.3\n75.968.1\n74.1\n78.82.57\n2.5\n2.350.345\n0.333\n0.298\n(a) Comparison on individual metricsArch Method INet-1k Segm. Depth ↓Classif.\nViT-g/14 Scratch 86.5 73.4 1.00 92.1\nViT-L/14 Scratch 84.5 72.2 1.10 90.2\nViT-L/14 Distill 86.3 73.3 1.08 91.2\nArch Method Finegr. Retriev. ARSketch Video\nViT-g/14 Scratch 78.3 75.2 77.0 69.3\nViT-L/14 Scratch 75.8 71.3 69.5 67.3\nViT-L/14 Distill 77.6 76.3 74.5 67.5\n(b) Averaged metrics on 8 vision tasks\nFigure 5: Eﬀectiveness of knowledge distillation. Comparison between a ViT-L trained from scratch\nor distilled from DINOv2 using ViT-g/14. For reference, we also report the performance of the ViT-g/14\nteacher. We show that a ViT-L model distilled from a frozen ViT-g outperforms a the same model trained\nfrom scratch on all benchmarks, sometimes even outperforming the distillation target.\n6.6 Impact of Resolution\nWe measure the impact of changing the resolution during the pretraining on the performance of image and\npatch-level features. We consider models trained from scratch using a ﬁxed resolution of either 224×224\nor416×416, and a model trained from scratch at 224×224, then resumed for 10k more iterations at\n416×416. High-resolution training is compute-intensive, so we conduct this ablation on a small setup: a\nViT-L/16 trained on ImageNet1k. In Fig. 6, we report the performance of a linear probe on ImageNet-1k\nand ADE-20k, evaluated at various resolutions. The model trained on high-resolution images performs best\nacross resolutions, but this comes at a high cost: training at 416is approximate 3×more compute-intensive\nthan training at 224. On the other hand, training at high resolution for only 10k iterations at the end of the\ntraining is almost as good and only requiring a fraction of the compute. As a consequence, we include this\nstep at the end of the training rather than training at a high resolution from scratch.\n7 Results\nIn this section, we present the empirical evaluation of our models on many image understanding tasks. We\nevaluate both global and local image representations, on category and instance-level recognition, semantic\nsegmentation, monocular depth prediction, and action recognition. We detail the list of benchmarks in\nAppendix C. The goal of this evaluation is twofold. First, we show that our self-supervised features outper-\n9', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': '224 336 512 640 768\nresolution78798081828384Accuracy\nImageNet-1k\n224 336 512 640 768\nresolution3941434547mIoU\nADE-20K\n224\n416\n224416\nFigure 6: Role of resolution. Performance of ViT-L/16 trained on ImageNet-1k at ﬁxed resolution (“224”\nand “416”) or trained at 224 then 416 for a short duration (“224 →416”). We train linear classiﬁers on top of\nfrozen features at diﬀerent resolutions and report Top-1 accuracy on ImageNet and mIoU on ADE-20k. We\nobserve that performing SSL training at high resolution for a short duration achieve behavior and results\nclose to training at the same high resolution for the full training, at a fraction of the cost.\nform the current state of the art by a very large margin. Second, we show that they match, or surpass the\nperformance of weakly-supervised ones on a substantial number of tasks.\nBaselines. In our comparisons, we use two kinds of models as baselines. We compare to the best performing\nself-supervised models that are openly available. First, we run our evaluations for MAE (He et al., 2021),\nDINO (Caron et al., 2021), SEERv2 (Goyal et al., 2022a), MSN (Assran et al., 2022), EsViT (Li et al.,\n2022a), Mugs (Zhou et al., 2022) and iBOT (Zhou et al., 2021). When several architectural variants were\nproposed for a given method, we report results for the one that leads to best top-1 accuracy on ImageNet-1k.\nSecond, we report performance of open-source weakly-supervised models such as CLIP (Radford et al., 2021),\nOpenCLIP (Ilharco et al., 2021), and SWAG (Singh et al., 2022). When evaluating models on ImageNet-1k,\nwe report the performance for each of the aforementioned methods. For all other evaluations, we report\nthe four best-performing models amongst SSL ones. Also, for reference, we report the best performing\nOpenCLIP-G for weakly-supervised ones.\n7.1 ImageNet Classiﬁcation\nAs a ﬁrst evaluation, we probe the quality of the holistic image representation produced by the model on the\nImageNet-1k classiﬁcation dataset. We evaluate the quality of features by training a simple classiﬁer over a\nfrozen backbone, and do not perform ﬁnetuning of the backbone weights. Following previous work, we use\na linear model for simplicity, ensuring a reproducible evaluation, despite the fact that classes may not be\nlinearly separable. Because most SSL methods were developped using ImageNet-1k validation performance\nas a debugging signal, we also report the top-1 accuracy on ImageNet-ReaL and ImageNet-V2. In order\nto report this additional validation performance, for all models, we run the evaluation with our code. We\ncompare our frozen features to the best publicly available SSL features in Table 4, regardless of architecture\nor pretraining data. We see the components proposed in this work lead to a very signiﬁcant improvement\n(+4.2%) over the previous state of the art (iBOT ViT-L/16 trained on ImageNet-22k) on linear evaluation.\nAt the same time, we also see that the performance increase on the alternative test sets is larger for our\nmethod, indicating stronger generalization. We describe details of our linear evaluation in Appendix B.3.\nHow far are we from weakly-supervised models? We also want to validate that our features are com-\npetitive with state-of-the-art open-source weakly supervised models. To this end, we compare on ImageNet-\n1k, using the linear evaluation, to three oﬀ-the-shelf methods with several architectural variants. For all\nmodels, we run the linear evaluation using our code, after making sure that our numbers match those re-\nported in technical reports and papers. We show the result of this evaluation in Table 4. We see that our\nbackbone, surpases the performance of OpenCLIP with a ViT-G/14 architecture ( +0.3%) and EVA-CLIP\nwith a ViT-g/14 ( +0.1%). At the same time, we also observe that our performance on the ImageNet-V2 test\n10', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}]: %s
2023-12-07 10:17:38,261 - INFO - Check the results [{'page_content': 'DINOv2: Learning Robust Visual Features\nwithout Supervision\nMaxime Oquab∗∗, Timothée Darcet∗∗, Théo Moutakanni∗∗,\nHuy Vo∗, Marc Szafraniec∗, Vasil Khalidov∗, Pierre Fernandez, Daniel Haziza,\nFrancisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba,\nRussell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat,\nVasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal1,\nPatrick Labatut∗, Armand Joulin∗, Piotr Bojanowski∗\nMeta AI Research1Inria\n∗core team∗∗equal contribution\nAbstract\nThe recent breakthroughs in natural language processing for model pretraining on large\nquantities of data have opened the way for similar foundation models in computer vision.\nThese models could greatly simplify the use of images in any system by producing all-\npurpose visual features, i.e., features that work across image distributions and tasks without\nﬁnetuning. This work shows that existing pretraining methods, especially self-supervised\nmethods, can produce such features if trained on enough curated data from diverse sources.\nWe revisit existing approaches and combine diﬀerent techniques to scale our pretraining in\nterms of data and model size. Most of the technical contributions aim at accelerating and\nstabilizing the training at scale. In terms of data, we propose an automatic pipeline to build\na dedicated, diverse, and curated image dataset instead of uncurated data, as typically done\nin the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy\net al., 2020) with 1B parameters and distill it into a series of smaller models that surpass\nthe best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the\nbenchmarks at image and pixel levels.\n1 Introduction\nLearning task-agnostic pretrained representations have become the standard in Natural Language Process-\ning (NLP) (Radford et al.; Raﬀel et al., 2020; Chowdhery et al., 2022; Hoﬀmann et al., 2022; Touvron et al.,\n2023). One can use these features “as they are”, i.e., without ﬁne-tuning, and achieve performances on down-\nstream tasks that are signiﬁcantly better than those produced by task-speciﬁc models (Brown et al., 2020).\nThis success has been fueled by pretraining on large quantities of raw text using pretext objectives, such as\nlanguage modeling (Radford et al., 2017) or word vectors (Devlin et al., 2018), that require no supervision.\nFollowing this paradigm shift in NLP, we expect similar “foundation” models to appear in computer vi-\nsion (Bommasani et al., 2021). These models should generate visual features that work out of the box on\nany task, both at the image level, e.g., image classiﬁcation, and pixel level, e.g., segmentation. Most promis-\ning eﬀorts towards these foundation models focus on text-guided pretraining, i.e., using a form of textual\nsupervision to guide the training of the features (Joulin et al., 2016; Mahajan et al., 2018; Radford et al.,\n2021). This form of text-guided pretraining limits the information that can be retained about the image\nsince captions only approximate the rich information in images, and complex pixel-level information may\nAll the authors are aﬃliated to Meta, except Julien Mairal who is aﬃliated to Inria. Timothée Darcet and Pierre Fernandez\nhave a co-aﬃliation with Inria. Théo Moutakanni has a co-aﬃliation with Université Paris Saclay. Alaaeldin El-Nouby has a\nco-aﬃliation with Inria and ENS-PSL. Correspondence: {qas, timdarcet, theomoutakanni, ajoulin, bojanowski}@meta.com\n1arXiv:2304.07193v1  [cs.CV]  14 Apr 2023', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 1: Visualization of the ﬁrst PCA components. We compute a PCA between the patches of the\nimages from the same column (a, b, c and d) and show their ﬁrst 3 components. Each component is matched\nto a diﬀerent color channel. Same parts are matched between related images despite changes of pose, style\nor even objects. Background is removed by thresholding the ﬁrst PCA component.\nnot surface with this supervision. Furthermore, these image encoders require aligned text-image corpora and\nhence, do not oﬀer the ﬂexibility of their text counterparts, that is, to learn from raw data alone.\nAn alternative to text-guided pretraining is self-supervised learning (Caron et al., 2018; Chen et al., 2020;\nHe et al., 2021) where features are learned from images alone. These approaches are conceptually closer to\npretext tasks such as language modeling and can capture information at the image and pixel level (Caron\net al., 2021). However, despite their potential to learn all-purposed features, most of the advances in\nself-supervised learning were made in the context of pretraining on a small curated dataset, ImageNet-\n1k (Russakovsky et al., 2015). Some eﬀorts on scaling these approaches beyond ImageNet-1k have been\nattempted (Caron et al., 2019; Goyal et al., 2021; 2022a), but they focused on uncurated datasets, which\ntypically lead to a signiﬁcant drop in the quality of the features. This is explained by the lack of control\nover the data quality and diversity, which are essential to produce good features.\nIn this work, we explore if self-supervised learning has the potential to learn all-purposed visual features if\npretrained on a large quantity of curated data. We revisit existing discriminative self-supervised approaches\nthat learn features at both the image and patch level, such as iBOT (Zhou et al., 2021), and we reconsider\nsomeoftheirdesignchoicesunderthelensofalargerdataset. Mostofourtechnicalcontributionsaretailored\ntoward stabilizing and accelerating discriminative self-supervised learning when scaling in model and data\nsizes. These improvements make our approach around 2 ×faster and require 3 ×less memory than similar\ndiscriminative self-supervised methods, allowing us to leverage longer training with larger batch sizes.\nRegarding pretraining data, we have built an automatic pipeline to ﬁlter and rebalance datasets from an\nextensive collection of uncurated images. This pipeline is inspired by pipelines used in NLP (Wenzek et al.,\n2019), where data similarities are used instead of external metadata and do not require manual annotation.\nA major diﬃculty when dealing with images in the wild is to rebalance concepts and avoid overﬁtting on a\nfew dominant modes. In this work, a naive clustering approach works reasonably well to resolve this issue.\nWe gathered a small but diverse corpus of 142M images to validate our approach.\nFinally, we provide a variety of pretrained visual models, called DINOv2, trained with diﬀerent Vision\nTransformers (ViT) (Dosovitskiy et al., 2016) architectures on our data. We release all the models and\nthe code to retrain DINOv2 on any data. We validate the quality of DINOv2 on various computer vision\nbenchmarks at both image and pixel levels as we scale them, as summarized in Fig. 2. We conclude that self-\n2', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': '101010111012\nflops7578818487Accuracy\nInet-1k\n101010111012\nflops40485664mIoU\nSegmentation\n101010111012\nflops0.91.21.51.8 R-MSE\nMonocular Depth \n101010111012\nflops80848892Accuracy\nClassification\n101010111012\nflops4856647280Accuracy\nFinegrained Classification\n101010111012\nflops30456075mAP\nInstance Retrieval\n101010111012\nflops4050607080Accuracy\nImageNet-{A,R,Sketch}\n101010111012\nflops5055606570Accuracy\nVideo Understanding\nSSL\nWSL\nDINOv2Figure 2: Evolution of performance when scaling in parameters. We show performance on eight\ntypes of vision tasks, as presented in Sec. 7, and average metrics with each type. Features are extracted\nfrom our self-supervised encoders, DINOv2 (dark blue), and we compare them with self-supervised methods\n(pale orange), as well as weakly-supervised methods (dark pink). We report the best-performing weakly-\nsupervised model’s performance as a dashed horizontal line. Our family of models drastically improves over\nthe previous state of the art in self-supervised learning and reaches performance comparable with weakly-\nsupervised features. See Sec. 7 for a detailed analysis.\nsupervised pretraining alone is a good candidate for learning transferable frozen features that are competitive\nwith the best openly available weakly-supervised models.\n2 Related Work\nIntra-image self-supervised training. A ﬁrst family of self-supervised methods focuses on pretext tasks\nbuilt from the image, i.e., extracting a signal from the image to be predicted from the rest of the image.\nThis idea has become prevalent with the work of Doersch et al. (2015), where they train by predicting the\ncontext of a given patch. Many other pretext tasks were introduced based on re-colorizing images (Zhang\net al., 2016), predicting transformations (Gidaris et al., 2018), inpainting (Pathak et al., 2016) or patch\nre-ordering (Noroozi & Favaro, 2016; Misra & Maaten, 2020). Recently, the emergence of patch-based\narchitectures, like ViTs, has led to a revisit of inpainting for pre-training (He et al., 2021; Bao et al., 2021;\nEl-Nouby et al., 2021), potentially in feature space (Assran et al., 2023; Baevski et al., 2022). Of particular\ninterest, He et al. (2021) show that a masked auto-encoder (MAE) learns features that provide substantial\nimprovements when ﬁnetuned on downstream tasks. This property of MAEs has been further validated\non video (Tong et al., 2022), audio (Xu et al., 2022), and across other modalities (Girdhar et al., 2022).\nHowever, their features require supervised ﬁnetuning, while our features perform well out of the box.\nDiscriminativeself-supervisedlearning. Thesecondlineofwork, closertoours, isusingdiscriminative\nsignals between images or groups of images to learn features. This family of methods has roots in early\ndeep learning work (Hadsell et al., 2006) but became popular with the emergence of instance classiﬁcation\nmethods (Dosovitskiy et al., 2014; Bojanowski & Joulin, 2017; Wu et al., 2018). Several improvements\nwere made based either on instance-level objectives (Hénaﬀ et al., 2019; He et al., 2020; Chen & He, 2020;\nChen et al., 2020; Grill et al., 2020; Caron et al., 2021) or clustering (Caron et al., 2018; Asano et al.,\n2020; Caron et al., 2020). These methods provide performant frozen features on standard benchmarks like\nImageNet (Russakovsky et al., 2015), but they are hard to scale to larger model sizes (Chen et al., 2021). In\n3', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Uncur ated Data \nAugment ed Cur ated Data\nCurated Data\n Embedding\n Deduplication\n RetrievalFigure 3: Overview of our data processing pipeline. Images from curated and uncurated data sources\nare ﬁrst mapped to embeddings. Uncurated images are then deduplicated before being matched to curated\nimages. The resulting combination augments the initial dataset through a self-supervised retrieval system.\nthis work, we revisit the training of these approaches in the context of large pretraining datasets and models.\nIn particular, we build on top of Zhou et al. (2021) that we ﬁnd particularly suited for scaling.\nScaling self-supervised pretraining. A growing body of work has focused on the scaling abilities of\nself-supervised learning in terms of data and model size (Caron et al., 2019; Goyal et al., 2019; Tian et al.,\n2021; Goyal et al., 2022a). Most of these works use large quantities of uncurated data to train models\nwithout supervision. They show evidence that discriminative methods scale with data, but because of the\npoor quality of the pretraining data, most of the results are obtained by ﬁnetuning the features. Of particular\ninterest, Goyal et al. (2021) have also shown that these methods beneﬁt from scaling in model size given\nenough pretrained data. This line of work questions the ability of self-supervised methods to work on any\ndata while we focus on producing the best pretrained encoders.\nAutomatic data curation. Our dataset construction borrows from the image retrieval community (Wein-\nzaepfeletal.,2021;Radenovićetal.,2018b;Bermanetal.,2019;Douzeetal.,2009;Toliasetal.,2015;Revaud\net al., 2019). In particular, the use of retrieval to augment the training set has been studied in the context\nof semi-supervised learning (Yalniz et al., 2019). Similarly, others have used hashtags or other metadata to\nﬁlter uncurated datasets (Mahajan et al., 2018; Radford et al., 2021). Unlike this work, we use no metadata\nnor supervision to ﬁlter images and leverage visual similarity between images. Our approach is inspired by\ntext curation pipelines (Wenzek et al., 2019), where a language model is trained on Wikipedia to score texts\nextracted from an uncurated source.\n3 Data Processing\nWe assemble our curated LVD-142M dataset by retrieving, from a large pool of uncurated data, images that\nare close to those in several curated datasets. We describe below the main components in our data pipeline\nincluding the curated/uncurated data sources, the image deduplication step and the retrieval system. Our\npipeline does not require any metadata or text and directly works with images, as shown in Fig. 3. We refer\nthe reader to appendix A for more details on our approach.\nData sources. Our selection of curated datasets is detailed in the appendix (Table 15) and contains\nImageNet-22k, the train split of ImageNet-1k, Google Landmarks and several ﬁne-grained datasets. For the\nuncurated data source, we collect a raw unﬁltered dataset of images from a publicly available repository of\ncrawled web data. From each web page in the repository, we extract URL links of images from <img>tags.\nWe discards URLs that are unsafe or restricted by domains, and post-process the downloaded images (PCA\nhash deduplication, NSFW ﬁltering, and blurring identiﬁable faces). This results in 1.2B unique images.\n4', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Deduplication. We apply the copy detection pipeline of Pizzi et al. (2022) to the uncurated data and\nremove near-duplicate images. This reduces redundancy and increases diversity among images. We also\nremove near-duplicates of images contained in the test or validation set of any benchmark used in this work.\nSelf-supervised image retrieval. We build our curated pretraining dataset by retrieving images from\nour uncurated data source that are close to images in our curated sources. In order to do this, we ﬁrst\ncompute an image embedding using a self-supervised ViT-H/16 network pretrained on ImageNet-22k, and\nuse cosine-similarity as a distance measure between images. Then, we perform k-means clustering of the\nuncurated data. Given a query dataset for retrieval, if it is large enough we retrieve N(typically 4) nearest\nneighbors for each query image. If it is small, we sample Mimages from the cluster corresponding to each\nquery image. We adjust NandMby visual inspection of the retrieval result.\nImplementation Details. The deduplication and retrieval stages of our pipeline rely on the Faiss li-\nbrary (Johnson et al., 2019) to eﬃciently index and compute batch searches of nearest embeddings. In\nparticular, we heavily leverage its support for GPU-accelerated indices, using inverted ﬁle indices with prod-\nuct quantization codes (Jegou et al., 2010). The whole processing is distributed on a compute cluster of 20\nnodes equipped with 8 V100-32GB GPUs and takes less than two days to produce the LVD-142M dataset.\n4 Discriminative Self-supervised Pre-training\nWe learn our features with a discriminative self-supervised method that can be seen as a combination of\nDINO and iBOT losses with the centering of SwAV (Caron et al., 2020). We also add a regularizer to spread\nfeatures and a short high-resolution training phase. We rapidly introduce each of these approaches, but more\ndetails can be found in the related papers, or in our open-sourced code.\n•Image-level objective (Caron et al., 2021). We consider the cross-entropy loss between the\nfeatures extracted from a student and a teacher network. Both features are coming from the class\ntoken of a ViT, obtained from diﬀerent crops of the same image. We learn the parameters of the\nstudent and build the teacher with an exponential moving average of past iterates (He et al., 2020).\n•Patch-level objective (Zhou et al., 2021). We randomly mask some of the input patches given\nto the student, but not to the teacher. We then add a cross-entropy loss between the patch features\nof both networks on each masked patch. This loss is combined with the image-level loss.\n•Untying head weights between both objectives. We observe that tying the weights associated\nwith both objectives makes the model underﬁt at the patch-level while overﬁtting at the image-level.\nUntying these weights resolves this issue and improve the performances at both scales.\n•Sinkhorn-Knopp centering (Caron et al., 2020). Ruan et al. (2022) recommend to replace the\nteacher softmax-centering step of DINO and iBot by the Sinkhorn-Knopp (SK) batch normalization\nof SwAV (Caron et al., 2020). We run the Sinkhorn-Knopp algorithm steps for 3 iterations. For the\nstudent, we apply the softmax normalization.\n•KoLeo regularizer (Sablayrolles et al., 2018). The KoLeo regularizer derives from the\nKozachenko-Leonenko diﬀerential entropy estimator (see Beirlant et al. (1997); Delattre & Fournier\n(2017)) and encourages a uniform span of the features within a batch. Given a set of nvectors\n(x1, . . . , x n), it is deﬁned asLkoleo =−1\nn∑n\ni=1log(dn,i),where dn,i= min j̸=i∥xi−xj∥is the mini-\nmum distance between xiand any other point within the batch. We also ℓ2-normalize the features\nbefore computing this regularizer.\n•Adapting the resolution (Touvron et al., 2019). Increasing image resolution is key to pixel-\nlevel downstream tasks such as segmentation or detection, where small objects disappear at low\nresolutions. However, training at high resolution is time and memory demanding, and instead, we\nincrease the resolution of images to 518×518during a short period at the end of pretraining.\n5', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': '5 Eﬃcient implementation\nWe consider several improvements to train models at a larger scale. We train models on A100 GPUs using\nPyTorch 2.0. The code is also available along with the pretrained models used for feature extraction1.\nThe details of our models are in the appendix, Table 17. With the same hardware, compared to the iBOT\nimplementation, the DINOv2 code runs around 2×faster using only 1/3of the memory.\nFast and memory-eﬃcient attention. We implemented our own version of FlashAttention (Dao et al.,\n2022) to improve memory usage and speed on the self-attention layers. Our version is on par with or\nbetter than the original on all cases considered, while covering more use-cases and hardware. Due to the\nGPU hardware speciﬁcs, the eﬃciency is best when the embedding dimension per head is a multiple of\n64, and the matrix operations are even better when the full embedding dimension is a multiple of 256.\nAs a consequence, our ViT-g architecture slightly diﬀers from the architecture proposed by Zhai et al.\n(2022) in order to maximize compute eﬃciency, and we use an embedding dimension of 1536 with 24 heads\n(64 dim/head), rather than 1408 with 16 heads (88 dim/head). Our experiments did not show signiﬁcant\ndiﬀerences in ﬁnal accuracy, and our ViT-g backbone counts 1.1B parameters.\nNested tensors in self-attention. Our version also allows running in the same forward pass the global\ncrops and the local crops (that have diﬀerent numbers of patch tokens), leading to signiﬁcant compute\neﬃciency gains compared to using separate forward and backward passes as done in prior implementations.\nThe lower-level components of our setup are available in the xFormers library2(Lefaudeux et al. (2022)).\nEﬃcient stochastic depth. We implement an improved version of stochastic depth (Huang et al., 2016)\nthat skips the computation of the dropped residuals rather than masking the result. This saves memory and\ncompute in proportion approximately equal to the drop rate, thanks to speciﬁc fused kernels. With high\ndrop rates ( d= 40%in this work), this allows a drastic improvement in compute eﬃciency and memory\nusage. The implementation consists of randomly shuﬄing the Bsamples over the batch dimension, and\nslicing the ﬁrst (1−d)×Bsamples for the computations in the block.\nFully-Sharded Data Parallel (FSDP). Minimizing our objective with the AdamW optimizer requires\n4 model replicas in ﬂoat32 precision – student, teacher, optimizer ﬁrst moments, optimizer second moments.\nThis sums to 16 GBof memory for a billion-parameter model such as our ViT-g. In order to reduce this\nmemory footprint per GPU, we split the model replicas across GPUs, i.e., sharding 16 GBacross GPUs\nusing the PyTorch implementation of FSDP. Consequently, the model size is not bounded by the memory of\na single GPU but by the total sum of GPU memory across compute nodes. The Pytorch implementation of\nFSDP brings a second advantage, which is to save on the cross-GPU communication costs: the weight shards\nare stored in ﬂoat32 precision as required by the optimizer, but broadcasting weights and reducing gradients\nis done in ﬂoat16 precision for the backbone (MLP heads gradients are reduced in ﬂoat32 to avoid training\ninstabilities). This leads to approximately 50% reduction in communication costs compared to the ﬂoat32\ngradient all-reduce operation used in DistributedDataParallel (DDP), which is used in other self-supervised\npretraining methods (Caron et al., 2021; Zhou et al., 2021). As a consequence, the training procedure\nscales more eﬃciently than DDP with ﬂoat16 autocast when scaling the number of GPU nodes. Overall,\nPytorch-FSDP mixed-precision is superior to DDP with autocast in virtually all cases we encountered.\nModeldistillation. Mostofourtechnicalimprovementstothetrainingloopaimatimprovingthetraining\nof large models over large quantities of data. For smaller models, we distill them from our largest model,\nthe ViT-g, instead of training them from scratch. Knowledge distillation (Hinton et al., 2015) aims at\nreproducing the output of a large model with a smaller model by minimizing some distance between both\noutputs for a set of given inputs. Since our objective function is a form of distillation from the teacher\nnetwork to the student network, we leverage the same training loop with a few exceptions: we use a larger\nmodel as a frozen teacher, keep a spare EMA of the student that we use as our ﬁnal model, remove the\nmasking and stochastic depth, and, apply the iBOT loss on the two global crops. In our ablations, we\n1https://github.com/facebookresearch/dinov2\n2https://github.com/facebookresearch/xformers\n6', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'INet-1k k-NN INet-1k linear\niBOT 72.9 82.3\n+(our reproduction) 74.5 ↑1.6 83.2 ↑0.9\n+LayerScale, Stochastic Depth 75.4 ↑0.9 82.0 ↓1.2\n+128k prototypes 76.6 ↑1.2 81.9 ↓0.1\n+KoLeo 78.9 ↑2.3 82.5 ↑0.6\n+SwiGLU FFN 78.7 ↓0.2 83.1 ↑0.6\n+Patch size 14 78.9 ↑0.2 83.5 ↑0.4\n+Teacher momentum 0.994 79.4 ↑0.5 83.6 ↑0.1\n+Tweak warmup schedules 80.5 ↑1.1 83.8 ↑0.2\n+Batch size 3k 81.7 ↑1.2 84.7 ↑0.9\n+Sinkhorn-Knopp 81.7 = 84.7 =\n+Untying heads = DINOv2 82.0 ↑0.3 84.5 ↓0.2\nTable 1:Ablation study of the training diﬀerences between iBOT and DINOv2. We optimize\nfor k-NN performance, as in our experience, the linear probe performance is lower-bounded by the k-NN\nperformance. Some modiﬁcations, like LayerScale and a high Stochastic Depth (rate= 0.4), incur a decrease\nin linear probe performance, but have the beneﬁts of increasing the stability of training by avoiding NaN loss\nvalues during training. Overall, these modiﬁcations allowed for the next set of improvements to be added.\nExperiments are run using the ViT-Large architecture on ImageNet-22k.\nobserve that this approach achieves better performance than training from scratch, even for a ViT-L. Our\ndistillation method ends up close to the one described by Duval et al. (2023), except we do not modify the\nloss terms for distillation and evaluate the EMA of the student.\n6 Ablation Studies\nWe present a set of ablations to empirically validate diﬀerent components of our pipeline: the technical\nmodiﬁcations described in Sec. 4, the pretraining data and the impact of model distillation. We consider\nvarious downstream tasks that are described in Sec. 7.\n6.1 Improved Training Recipe\nOur approach improves over the iBOT method by combining it with several existing components described\nin Sec. 4. To evaluate their importance, we train multiple models where we successively add components to\na baseline iBOT model. We report the Top-1 accuracy on the validation set of ImageNet-1k with a k-NN\nand a linear linear in Table 1. Generally, we observe that each component improves the performance on\neither k-NN or linear probing and even both in most cases. Only LayerScale and Stochastic Depth incur a\nperformance drop in linear probing but signiﬁcantly improve the training stability in our experience.\n6.2 Pretraining Data Source\nThe quality of features is directly related to the quality of the pretraining data. In this experiment, we\nprobe the impact of LVD-142M compared to ImageNet-22k, a commonly used pretraining dataset, or using\ndirectly raw and uncurated data. For the uncurated dataset, we randomly sample 142million images from\nthe same data source as LVD-142M. We train a ViT-g/14 on each dataset for the same number of iterations.\nWe also include a variant of ImageNet-22k obtained by removing the synsets of ImageNet-1k (INet-22k \\\nINet-1k) for completeness. We report the comparisons in Table 2.\nThe most salient observation is that training on a curated set of images works better on most benchmarks\nthan training on uncurated data. This conﬁrms the beneﬁt of curating data, even in the case of self-\nsupervised pretraining. When compared with models trained on ImageNet-22k, training on LVD-142M is\nalso superior on all the benchmarks but ImageNet-1k. This conﬁrms that training on a more diverse set of\n7', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Training Data INet-1k Im-A ADE-20k Oxford-M\nINet-22k 85.9 73.5 46.6 62.5\nINet-22k\\INet-1k 85.3 70.3 46.2 58.7\nUncurated data 83.3 59.4 48.5 54.3\nLVD-142M 85.8 73.9 47.7 64.6\nTable 2:Ablation of the source of pretraining data. We compare the INet-22k dataset that was used\nin iBOT to our dataset, LVD-142M. Each model is trained for the same number of iterations, that is smaller\nthan in our ﬁnal run. Pretraining on LVD-142M maintains the performance over INet-1k while leading to\nmodels that perform better in other domains.\nL H g848586\nImageNet-1k\nINet-22k\nLVD142M\nL H g747678\nImageNet-V2\nL H g5060\nImageNet-Sketch\nL H g9294\nFood101\nL H g8090\nCars\nL H g3540\nAmsterTime\nL H g2040\nOxford-H\nFigure 4: Model scale versus data scale. Evolution of performance as a function of model size for\ntwo diﬀerent pretraining datasets: ImageNet-22k (14M images) and LVD-142M (142M images). The ViT-g\ntrained on LVD-142M surpasses the ViT-g trained on ImageNet-22k on most benchmarks.\nimages improves the quality of the features in domains that are not covered by this dataset. Overall, the\nconclusion of this ablation is that our dataset provides a good balance of diﬀerent types of images that leads\nto the best performance overall.\n6.3 Model Size and Data\nWe quantify the importance of scaling data with the model size in Fig. 4. As the size of models grow, training\non LVD-142M becomes more beneﬁcial than training on ImageNet-22k. For instance, a ViT-g trained on\nLVD-142M matches the performance on ImageNet-1k of a model trained on ImageNet-22k while signiﬁcantly\noutperforming it on the other benchmarks.\n6.4 Loss Components\nWe validated the proposed technical improvements in Sec. 6.1 by adding them incrementally. This section\nanalyzes the performance hit observed if we ablate speciﬁc loss terms, starting from our best-performing\nmodel. We ablate the importance of the KoLeo loss and the impact of the masked image modeling term.\nFor both, we report performance on ImageNet-1k using a linear classiﬁer, ADE-20k segmentation using a\nlinear classiﬁer, and nearest-neighbor image retrieval on Oxford-M. Table 3a shows the impact of using the\nKoLeo loss. We see that the instance retrieval performance improves by more than 8%, conﬁrming that this\nterm helps spread features in the output space. At the same time, the other metrics do not suﬀer from this\nregularization. In Table 3b, we show the impact of using the masked image modeling term from iBOT. This\nterm is critical for dense prediction tasks, leading to almost 3%performance improvement.\n6.5 Impact of Knowledge Distillation\nFor small architectures, we distill larger models instead of training them from scratch. We use the distillation\nprocedure described in Sec. 5. We evaluate the eﬀectiveness of this approach by comparing a ViT-L/14\ntrained from scratch with one distilled from a ViT-g/14 over 12 benchmarks in Fig. 5. We also report the\nperformance of the ViT-g/14 used for distillation as a topline. The distilled model outperforms the one\ntrained from scratch on 10 out of 12 benchmarks, validating our pretraining approach for small models.\n8', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'KoLeo INet-1k Im-A ADE-20k Oxford-M\n\x15 85.3 70.6 47.2 55.6\n✓ 85.8 72.8 47.1 63.9\n(a) Koleo lossMIM INet-1k Im-A ADE-20k Oxford-M\n\x15 85.3 72.0 44.2 64.3\n✓ 85.8 72.8 47.1 63.9\n(b) MIM objective in iBOT\nTable 3:(a)Eﬀect of the KoLeo loss term. (b)Eﬀect of the iBOT Masked Image Modeling (MIM) loss\nterm. Evaluation performed on ImageNet-{1k,A} (classiﬁcation with linear probe, accuracy %), ADE-20k\n(segmentation with linear layer, mIoU) and Oxford-M (image retrieval, mAP). Each model is trained on the\nsame number of iterations, that is smaller than our ﬁnal run. The KoLeo loss term improves nearest-neighbor\nsearch tasks (e.g. retrieval), and the MIM loss improves patch-level tasks (e.g. segmentation).\nINet-1kFoodCarsiNat18\niNat21\nPlaces 205Oxford-H\nParis-H\nINet-A\nINet-RKitti\nNYUd\nViT-L/14 Scratch\nViT-L/14 Distill\nViT-g/14 Scratch\n84.5 86.3 86.592.894.394.7\n81.890.191.4\n77.880.481.6\n83.185.185.7\n66.067.367.5\n47.7 52.6 52.1\n77.6\n84.482.761.7\n71.3\n75.968.1\n74.1\n78.82.57\n2.5\n2.350.345\n0.333\n0.298\n(a) Comparison on individual metricsArch Method INet-1k Segm. Depth ↓Classif.\nViT-g/14 Scratch 86.5 73.4 1.00 92.1\nViT-L/14 Scratch 84.5 72.2 1.10 90.2\nViT-L/14 Distill 86.3 73.3 1.08 91.2\nArch Method Finegr. Retriev. ARSketch Video\nViT-g/14 Scratch 78.3 75.2 77.0 69.3\nViT-L/14 Scratch 75.8 71.3 69.5 67.3\nViT-L/14 Distill 77.6 76.3 74.5 67.5\n(b) Averaged metrics on 8 vision tasks\nFigure 5: Eﬀectiveness of knowledge distillation. Comparison between a ViT-L trained from scratch\nor distilled from DINOv2 using ViT-g/14. For reference, we also report the performance of the ViT-g/14\nteacher. We show that a ViT-L model distilled from a frozen ViT-g outperforms a the same model trained\nfrom scratch on all benchmarks, sometimes even outperforming the distillation target.\n6.6 Impact of Resolution\nWe measure the impact of changing the resolution during the pretraining on the performance of image and\npatch-level features. We consider models trained from scratch using a ﬁxed resolution of either 224×224\nor416×416, and a model trained from scratch at 224×224, then resumed for 10k more iterations at\n416×416. High-resolution training is compute-intensive, so we conduct this ablation on a small setup: a\nViT-L/16 trained on ImageNet1k. In Fig. 6, we report the performance of a linear probe on ImageNet-1k\nand ADE-20k, evaluated at various resolutions. The model trained on high-resolution images performs best\nacross resolutions, but this comes at a high cost: training at 416is approximate 3×more compute-intensive\nthan training at 224. On the other hand, training at high resolution for only 10k iterations at the end of the\ntraining is almost as good and only requiring a fraction of the compute. As a consequence, we include this\nstep at the end of the training rather than training at a high resolution from scratch.\n7 Results\nIn this section, we present the empirical evaluation of our models on many image understanding tasks. We\nevaluate both global and local image representations, on category and instance-level recognition, semantic\nsegmentation, monocular depth prediction, and action recognition. We detail the list of benchmarks in\nAppendix C. The goal of this evaluation is twofold. First, we show that our self-supervised features outper-\n9', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': '224 336 512 640 768\nresolution78798081828384Accuracy\nImageNet-1k\n224 336 512 640 768\nresolution3941434547mIoU\nADE-20K\n224\n416\n224416\nFigure 6: Role of resolution. Performance of ViT-L/16 trained on ImageNet-1k at ﬁxed resolution (“224”\nand “416”) or trained at 224 then 416 for a short duration (“224 →416”). We train linear classiﬁers on top of\nfrozen features at diﬀerent resolutions and report Top-1 accuracy on ImageNet and mIoU on ADE-20k. We\nobserve that performing SSL training at high resolution for a short duration achieve behavior and results\nclose to training at the same high resolution for the full training, at a fraction of the cost.\nform the current state of the art by a very large margin. Second, we show that they match, or surpass the\nperformance of weakly-supervised ones on a substantial number of tasks.\nBaselines. In our comparisons, we use two kinds of models as baselines. We compare to the best performing\nself-supervised models that are openly available. First, we run our evaluations for MAE (He et al., 2021),\nDINO (Caron et al., 2021), SEERv2 (Goyal et al., 2022a), MSN (Assran et al., 2022), EsViT (Li et al.,\n2022a), Mugs (Zhou et al., 2022) and iBOT (Zhou et al., 2021). When several architectural variants were\nproposed for a given method, we report results for the one that leads to best top-1 accuracy on ImageNet-1k.\nSecond, we report performance of open-source weakly-supervised models such as CLIP (Radford et al., 2021),\nOpenCLIP (Ilharco et al., 2021), and SWAG (Singh et al., 2022). When evaluating models on ImageNet-1k,\nwe report the performance for each of the aforementioned methods. For all other evaluations, we report\nthe four best-performing models amongst SSL ones. Also, for reference, we report the best performing\nOpenCLIP-G for weakly-supervised ones.\n7.1 ImageNet Classiﬁcation\nAs a ﬁrst evaluation, we probe the quality of the holistic image representation produced by the model on the\nImageNet-1k classiﬁcation dataset. We evaluate the quality of features by training a simple classiﬁer over a\nfrozen backbone, and do not perform ﬁnetuning of the backbone weights. Following previous work, we use\na linear model for simplicity, ensuring a reproducible evaluation, despite the fact that classes may not be\nlinearly separable. Because most SSL methods were developped using ImageNet-1k validation performance\nas a debugging signal, we also report the top-1 accuracy on ImageNet-ReaL and ImageNet-V2. In order\nto report this additional validation performance, for all models, we run the evaluation with our code. We\ncompare our frozen features to the best publicly available SSL features in Table 4, regardless of architecture\nor pretraining data. We see the components proposed in this work lead to a very signiﬁcant improvement\n(+4.2%) over the previous state of the art (iBOT ViT-L/16 trained on ImageNet-22k) on linear evaluation.\nAt the same time, we also see that the performance increase on the alternative test sets is larger for our\nmethod, indicating stronger generalization. We describe details of our linear evaluation in Appendix B.3.\nHow far are we from weakly-supervised models? We also want to validate that our features are com-\npetitive with state-of-the-art open-source weakly supervised models. To this end, we compare on ImageNet-\n1k, using the linear evaluation, to three oﬀ-the-shelf methods with several architectural variants. For all\nmodels, we run the linear evaluation using our code, after making sure that our numbers match those re-\nported in technical reports and papers. We show the result of this evaluation in Table 4. We see that our\nbackbone, surpases the performance of OpenCLIP with a ViT-G/14 architecture ( +0.3%) and EVA-CLIP\nwith a ViT-g/14 ( +0.1%). At the same time, we also observe that our performance on the ImageNet-V2 test\n10', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}]: %s
2023-12-07 10:17:39,939 - INFO - Check the data that is being passed [{'page_content': 'kNN linear\nMethod Arch. Data Text sup. val val ReaL V2\nWeakly supervised\nCLIP ViT-L/14 WIT-400M ✓ 79.8 84.3 88.1 75.3\nCLIP ViT-L/14 336WIT-400M ✓ 80.5 85.3 88.8 75.8\nSWAG ViT-H/14 IG3.6B ✓ 82.6 85.7 88.7 77.6\nOpenCLIP ViT-H/14 LAION ✓ 81.7 84.4 88.4 75.5\nOpenCLIP ViT-G/14 LAION ✓ 83.2 86.2 89.4 77.2\nEVA-CLIP ViT-g/14 custom∗✓ 83.5 86.4 89.3 77.4\nSelf-supervised\nMAE ViT-H/14 INet-1k \x15 49.4 76.6 83.3 64.8\nDINO ViT-S/8 INet-1k \x15 78.6 79.2 85.5 68.2\nSEERv2 RG10B IG2B \x15 – 79.8 – –\nMSN ViT-L/7 INet-1k \x15 79.2 80.7 86.0 69.7\nEsViT Swin-B/W=14 INet-1k \x15 79.4 81.3 87.0 70.4\nMugs ViT-L/16 INet-1k \x15 80.2 82.1 86.9 70.8\niBOT ViT-L/16 INet-22k \x15 72.9 82.3 87.5 72.4\nDINOv2ViT-S/14 LVD-142M \x15 79.0 81.1 86.6 70.9\nViT-B/14 LVD-142M \x15 82.1 84.5 88.3 75.1\nViT-L/14 LVD-142M \x15 83.5 86.3 89.5 78.0\nViT-g/14 LVD-142M \x15 83.5 86.5 89.6 78.4\nTable4:LinearevaluationonImageNet-1koffrozenpretrainedfeatures. WereportTop-1accuracy\non the validation set for publicly available models trained on public or private data, and with or without\ntext supervision (text sup.). For reference, we also report the kNN performance on the validation set. We\ncompare across any possible architectures (Arch.), at resolution 224×224unless stated otherwise. The\ndataset used for training EVA-CLIP is a custom mixture, see paper for details (Fang et al., 2023).\nset is signiﬁcantly better ( +1.1%versus EVA-CLIP), indicating better generalization. For the remainder of\nthis section, we report OpenCLIP-G as a reference for weakly-supervised models.\nCan we ﬁnetune the encoders? We question if the ability of our models to produce high quality frozen\nfeatures impact their performance when ﬁnetuned with supervision on a speciﬁc dataset. While this is not\ncore to this paper, this experiment is indicative of whether we have involuntarily specialized our models\nto the setting of linear evaluations of frozen features. To run this sanity check, we apply the ﬁnetuning\npipeline from Touvron et al. (2022), without tweaking hyper-parameters. In Table 5, we show that the\nTop-1 accuracy on the validation set of ImageNet-1k improves by more than +2%when the backbone is\nﬁnetuned. This is true both when using models at resolution 224and448. Further gains can be obtained by\ntuning the hyper-parameters of the ﬁnetuning, but this is beyond the goal of this sanity check. Nonetheless,\nour best ﬁnetuned performance ( 88.9%) is only a couple of percent below ( −2.2%) the absolute state of the\narts ( 91.1%), obtained by Chen et al. (2023). As DINOv2 leads to features that are strong in both the linear\nand ﬁnetuning settings, a strong property of our approach is that ﬁnetuning is optional .\nRobustness analysis. To complement our study, and probe the generalization of our features, we evaluate\nour ImageNet-1k models trained with linear classiﬁcation heads on domain generalization benchmarks. We\nuse the best performing linear classiﬁer as described above and simply run inference on those benchmarks.\nPlease note that most results in the litterature are obtained with models that are ﬁnetuned end-to-end on\nImageNet-1k. We show the result of this experiment in Table 6. When comparing with state-of-the-art SSL\nmethods, our models shows drastically better robustness ( +29.6%on A, +22.1%on R and +23.0%on Sketch\n11', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Arch. Res. Linear Finetuned ∆\nViT-g/14224 86.5 88.5 +2.0\n448 86.7 88.9 +2.2\nTable 5: Supervised ﬁnetuning on ImageNet-1k. We use the pipeline of Touvron et al. (2022) to\nﬁnetune our encoders on ImageNet-1k at resolutions 224×224or448×448. We compare with the accuracy\nobtained with linear probing and observe only modest improvements with ﬁne-tuning: this suggests that\nDINOv2 features already perform well out-of-the-box.\nMethod Arch Data Im-A Im-R Im-C ↓Sketch\nOpenCLIP ViT-G/14 LAION 63.8 87.8 45.366.4\nMAE ViT-H/14 INet-1k 10.2 34.4 61.4 21.9\nDINO ViT-B/8 INet-1k 23.9 37.0 56.6 25.5\niBOT ViT-L/16 INet-22k 41.5 51.0 43.9 38.5\nDINOv2ViT-S/14 LVD-142M 33.5 53.7 54.4 41.2\nViT-B/14 LVD-142M 55.1 63.3 42.7 50.6\nViT-L/14 LVD-142M 71.3 74.4 31.5 59.3\nViT-g/14 LVD-142M 75.978.828.2 62.5\nTable 6:Domain Generalization with a linear probe on top of frozen features at a resolution of 224.\nHigher numbers are better for all benchmarks except Im-C.\ncompared to iBOT). Our model also improves upon the best weakly-supervised model on ImageNet-A while\nlagging behind on R and Sketch.\n7.2 Additional Image and Video classiﬁcation Benchmarks\nIn this section we study the generalization of our features on downstream classiﬁcation benchmarks. We\nconsider two sets of evaluations in that context. On one hand, we use large and ﬁnegrained datasets such\nas iNaturalist and Places205. On the other, we use the 12 image classiﬁcation tasks originally proposed\nin SimCLR (Chen et al., 2020). For iNaturalist 2018, iNaturalist 2021, and Places205, we train a linear\nclassiﬁer with data augmentations as in Sec. 7.1 We report top-1 accuracy for those three datasets in Table 7.\nInterestingly, our model signiﬁcantly outperforms OpenCLIP ViT-G/14 on both variants of iNaturalist\n(+8.6%and+9.7%for 2018 and 2021 respectively), and lags slightly behind on Places 205 ( −2.3%).\nIn a second set of evaluations, we measure the performance of our model on video action recognition even\nthough our features were not trained on videos.. We evaluated features on three datasets, namely UCF-\n101 (Soomro et al., 2012), Kinetics-400 (Kay et al., 2017) and Something-Something v2 (Goyal et al., 2017).\nFor this evaluation, we pick 8evenly spaced frames in the video and train a linear classiﬁer on the average\nof the features for UCF and K-400. For SSv2, we opt for concatenation to retain more temporal information\nthan with feature averaging. For each dataset, we measure average accuracy and report the results in\nTable 7. We see that amongst self-supervised approaches, our model clearly sets a new state of the art.\nMoreover, our model matches the accuracy of the OpenCLIP features on UCF and Kinetics ( +0.1%and\n+0.5%respectively) and clearly outperforms them on SSv2 ( +2.5%). This is particularly interesting, as\nSSv2 requires a much richer understanding of the video frames.\nFinally, in Table 8, we compare selected frozen features on 12 transfer classiﬁcation benchmarks initially\nproposed by Chen et al. (2020). This benchmark covers scenes, objects (food, cars, planes), and textures.\nWe replace the Birdsnap dataset with CUB because the former was not publicly available in its entirety. We\nfollow the experimental protocol as outlined by Chen et al. (2020), namely training a logistic regression on\nprecomputed features. Our model signiﬁcantly outperforms state-of-the-art SSL models, with most notable\ndiﬀerences on Stanford Cars ( +14.8%versus DINO ViT-B/8) and FGVC Aircraft ( +14.8%versus iBOT\n12', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Image classiﬁcation Video classiﬁcation\nFeature Arch iNat2018 iNat2021 Places205 K400 UCF-101 SSv2\nOpenCLIP ViT-G/14 73.0 76.0 69.8 78.3 90.7 35.8\nMAE ViT-H/14 31.0 32.3 52.4 54.2 70.6 29.2\nDINO ViT-B/8 59.6 68.3 60.4 64.5 85.0 32.6\niBOT ViT-L/16 66.3 74.6 64.4 72.6 88.6 38.7\nDINOv2ViT-S/14 69 74.2 62.9 67.8 87 33.1\nViT-B/14 76.4 81.1 66.2 73.2 89.1 34.4\nViT-L/14 80.4 85.1 67.3 76.3 90.5 35.6\nViT-g/14 81.6 85.7 67.5 78.4 91.2 38.3\nTable 7:Linear evaluation on other image and video classiﬁcation. The image benchmarks contain\na large quantity of ﬁne-grained examples about objects or scenes. The video benchmarks cover action\nclassiﬁcation and human-object interaction. All the features are frozen with a linear probe on top.\nFeature Arch Food C10 C100 SUN Cars Aircr VOC DTD Pets Cal101 Flowers CUB Avg\nOpenCLIP ViT-G/14 94.5 98.7 91.0 84.0 96.1 80.289.3 86.0 95.798.199.5 89.9 91.9\nMAE ViT-H/14 78.4 96.1 83.9 63.9 56.1 63.4 84.3 75.4 89.4 95.9 92.3 57.2 78.0\nDINO ViT-B/8 85.1 97.2 86.9 70.3 76.6 70.6 86.7 79.6 93.2 95.4 97.6 81.7 85.1\niBOT ViT-L/16 91.0 99.0 92.8 75.6 71.8 72.4 89.0 80.7 87.7 97.5 99.6 82.1 86.6\nDINOv2ViT-S/14 89.1 97.7 87.5 74.4 81.6 74.0 87.8 80.6 95.1 97.0 99.6 88.1 87.7\nViT-B/14 92.8 98.7 91.3 77.3 88.2 79.4 88.2 83.3 96.2 96.1 99.6 89.6 90.1\nViT-L/14 94.3 99.3 93.4 78.7 90.1 81.5 88.3 84.0 96.6 97.5 99.7 90.5 91.2\nViT-g/14 94.7 99.5 94.4 78.7 91.4 87.289.0 84.5 96.797.699.7 91.6 92.1\nTable 8:Linear evaluation of frozen features on ﬁne-grained benchmarks. Accuracy on 12 bench-\nmarks covering objects, scenes and textures following the evaluation protocol proposed in Chen et al. (2020).\nViT-L/16). Even though these benchmarks favor text-guided pretraining, our features are still competitive\nwith OpenCLIP on most classiﬁcation benchmarks, with the exception of a few datasets, especially SUN\n(−5.3%) and Cars (−4.7%).\n7.3 Instance Recognition\nIn this experiment, we probe our model on the task of instance-level recognition using a non-parametric\napproach. Images from a database are ranked according to their cosine similarity with a query image. We\nevaluated our model and compare to baselines on Paris and Oxford, that are landmark recognition bench-\nmarks. We also evaluated on Met, a dataset of artworks from the Metropolitan museum, and AmsterTime,\ncontaining street view images matched to archival images of Amsterdam. We measure performance by com-\nputing the mean average precision and report our results in Table 9. We see that our features signiﬁcantly\noutperform both SSL ( +41%mAP on Oxford-Hard), and weakly-supervised ( +34%mAP on Oxford-Hard)\nones. Itisinterestingtoseethatourfeaturesperformwellacrosstaskgranularities, bothatthecategory-level\nand instance-level. This is a desirable property for strong oﬀ-the-shelf computer vision features.\n7.4 Dense Recognition Tasks\nWe probe the quality of patch-level features extracted from our network on several dense downstream tasks.\nWeconsidersemanticimagesegmentationandmonoculardepthestimationinseveralsettingsandweconduct\nevaluations on multiple datasets for each.\n13', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Oxford Paris Met AmsterTime\nFeature Arch M H M H GAP GAP- ACC mAP\nOpenCLIP ViT-G/14 50.7 19.7 79.2 60.2 6.5 23.9 34.4 24.6\nMAE ViT-H/14 11.7 2.2 19.9 4.7 7.5 23.5 30.5 4.2\nDINO ViT-B/8 40.1 13.7 65.3 35.3 17.1 37.7 43.9 24.6\niBOT ViT-L/16 39.0 12.7 70.7 47.0 25.1 54.8 58.2 26.7\nDINOv2ViT-S/14 68.8 43.2 84.6 68.5 29.4 54.3 57.7 43.5\nViT-B/14 72.9 49.5 90.3 78.6 36.7 63.5 66.1 45.6\nViT-L/14 75.1 54.0 92.7 83.5 40.0 68.9 71.6 50.0\nViT-g/14 73.6 52.3 92.1 82.6 36.8 73.6 76.5 46.7\nTable 9:Evaluation of frozen features on instance-level recognition. We consider 4 diﬀerent bench-\nmarks and report their main metrics.\nADE20k CityScapes Pascal VOC\n(62.9) (86.9) (89.0)\nMethod Arch. lin. +ms lin. +ms lin. +ms\nOpenCLIP ViT-G/14 39.3 46.0 60.3 70.3 71.4 79.2\nMAE ViT-H/14 33.3 30.7 58.4 61.0 67.6 63.3\nDINO ViT-B/8 31.8 35.2 56.9 66.2 66.4 75.6\niBOT ViT-L/16 44.6 47.5 64.8 74.5 82.3 84.3\nDINOv2ViT-S/14 44.3 47.2 66.6 77.1 81.1 82.6\nViT-B/14 47.3 51.3 69.4 80.0 82.5 84.9\nViT-L/14 47.7 53.1 70.3 80.9 82.1 86.0\nViT-g/14 49.053.071.3 81.0 83.0 86.2\nTable 10: Semantic segmentation on ADE20K, CityScapes and Pascal VOC with frozen features\nand a linear classiﬁer (lin.) and with multiscale (+ms). The absolute state of the art – from Wang et al.\n(2022), Liu et al. (2021) and Chen et al. (2018) respectively – are mentioned at the top of the Table. For\nreference, using the Mask2Former pipeline (Steiner et al., 2021) with a ViT-Adapter (Chen et al., 2022) on\ntop of our frozen ViT-g/14 backbone gives 60.2 mIoU on ADE-20k.\nSemantic segmentation. For our semantic segmentation evaluation, we consider two diﬀerent setups.\nLinear: a linear layer is trained to predict class logits from a patch tokens. It is used to produce a low-\nresolution logit map (eg 32x32 for a model with patch size 16), which is then upsampled to full resolution\n(512x512) to obtain a segmentation map. This procedure is extremely simple but cannot easily produce\nhigh-resolution segmentations. +ms: a boosted version of the linear setup. We concatenate the patch\ntokens of the 4 last layers, use a larger image resolution of 640, and use multiscale test-time augmentations\nto improve the predictions. We report the performance of our model variants as well as the baselines on\nthree datasets under the two setups in Table 10.\nOur models show very good performance on all datasets and for all setups. Interestingly, our evaluation\nusing+msis on par with fully ﬁnetuning MAE with an Upernet decoder ( 53.0versus 53.6mIoU). This is\nsurprising because we use a signiﬁcantly simpler predictor. Also, our best model, when evaluated using the\nboosted recipe, almost matches the state of the art on Pascal VOC ( 86.2versus 89.0mIoU).\nIn a ﬁnal experiment, we freeze our backbone, and plug it into a ViT-Adapter Chen et al. (2022) with a\nMask2former head (Cheng et al., 2022). We tune the weights of the adapter and head, but keep the backbone\nfrozen: only a fraction of the weights are tuned, keeping the training procedure lightweight. We reach 60.2\nmIoU on ADE20k, close to the competitive state of the art, standing at 62.9 mIoU (Wang et al., 2022).\n14', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'NYUd KITTI NYUd →SUN RGB-D\n(0.330) (2.10) (0.421)\nMethod Arch. lin. 1 lin. 4 DPT lin. 1 lin. 4 DPT lin. 1 lin. 4 DPT\nOpenCLIP ViT-G/14 0.541 0.510 0.414 3.57 3.21 2.56 0.537 0.476 0.408\nMAE ViT-H/14 0.517 0.483 0.415 3.66 3.26 2.59 0.545 0.523 0.506\nDINO ViT-B/8 0.555 0.539 0.492 3.81 3.56 2.74 0.553 0.541 0.520\niBOT ViT-L/16 0.417 0.387 0.358 3.31 3.07 2.55 0.447 0.435 0.426\nDINOv2ViT-S/14 0.449 0.417 0.356 3.10 2.86 2.34 0.477 0.431 0.409\nViT-B/14 0.399 0.362 0.317 2.90 2.59 2.23 0.448 0.400 0.377\nViT-L/14 0.384 0.333 0.293 2.78 2.50 2.14 0.429 0.396 0.360\nViT-g/14 0.344 0.298 0.279 2.62 2.35 2.11 0.402 0.362 0.338\nTable 11: Depth estimation with frozen features . We report performance when training a linear\nclassiﬁer on top of one (lin. 1) or four (lin. 4) transformer layers, as well, as the DPT decoder (DPT) of\nRanftl et al. (2021). We report the RMSE metric on the 3 datasets. Lower is better. For reference, we\nreport state-of-the-art results taken from Li et al. (2022b) on each benchmark on top of the Table.\nDepth estimation. In this experiment, we evaluate our patch-level features on three monocular depth\nestimation benchmarks: NYUd, KITTI and zero-shot transfer from NYUd to SUN3d. We follow the evalu-\nation protocol of Li et al. (2022b). We consider three diﬀerent setups for this evaluation. lin. 1: we extract\nthe last layer of the frozen transformer and concatenate the [CLS]token to each patch token. Then we\nbi-linearly upsample the tokens by a factor of 4 to increase the resolution. Finally we train a simple linear\nlayer using a classiﬁcation loss by dividing the depth prediction range in 256 uniformly distributed bins and\nuse a linear normalization following Bhat et al. (2021). lin. 4: we use the same protocol that we use with\none layer, but concatenate the tokens from layers l={3,6,9,12}for ViT-S/B, l={5,12,18,24}for ViT-L,\nandl={10,20,30,40}for ViT-g. DPT: we use the DPT decoder (Ranftl et al., 2021) on top of our frozen\nmodels and setup a regression task. We scale the size of the head following the dimension of the features for\neach architecture. We show results for all baselines, all datasets and all setups in Table 11.\nFrom this table, we see that our features clearly surpass the best SSL and WSL features available. It\nis interesting to see that iBOT features extracted from a ViT-L outperform the ones of OpenCLIP with\na ViT-G. This observation supports the intuition that caption-based feature learning fails to learn subtle\npatterns like this one. Also, our model, with the DPT decoder and frozen backbone, matches or exceeds\nthe performance of the recent work of Li et al. (2022b). Finally, the out-of-domain generalization result on\nSUN-RGBd shows that our features allow very good transfer between domains. A depth prediction module\ntrained on indoor scenes from NYUd generalizes pretty well to the outdoor examples of SUN-RGBd.\n7.5 Qualitative Results\nIn this ﬁnal section of the empirical evaluation of our features, we propose a few qualitative analyses.\nSemantic Segmentation and Depth Estimation. We show some qualitative results for our dense\nprediction evaluations: segmentation on ADE20K in Fig. 7 and depth estimation on NYUd, KITTI and\nSUN RGB-D in Fig. 7. We compare DINOv2 with OpenCLIP with a linear classiﬁer on each dataset. While\nnot perfect, the linear segmentation model using our DINOv2 backbone produces good results and behaves\nmuch better than the OpenCLIP one under this evaluation setup. Indeed, the segmentation mask produced\nby OpenCLIP-G shows many artifacts and disconnected components. The qualitative results on depth\nestimation clearly illustrate the quantitative gap between OpenCLIP and DINOv2. These results highlight\nthat our features, as well as the features extracted from OpenCLIP, are able to linearly separate complex\ninformation such as depth, even though neither was trained with this type of information. However, our\nfeatures lead to a much smoother depth estimation, with less artifacts. Some objects such as the chair on\nthe SUN RGB-D image are completely ignored by OpenCLIP and correctly positioned using our features.\n15', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 7: Segmentation and depth estimation with linear classiﬁers. Examples from ADE20K,\nNYUd, SUN RGB-D and KITTI with a linear probe on frozen OpenCLIP-G and DINOv2-g features.\nFigure 8:Examples of out-of-distribution examples with frozen DINOv2-g features and a linear probe.\nOut-of-distribution generalization. We show a few examples of applying the depth prediction and\nsegmentation linear classiﬁers to out-of-distribution examples in Fig. 8. The qualitative results support our\nclaim that our features transfer between domains. The quality of the depth and segmentation predicted for\npictures of animals, or paintings is very good, even though the domains are very diﬀerent.\nPCA of patch features. We show the results of the principal component analysis (PCA) performed on\nthe patch features extracted by our model. We keep only patches with a positive value after we threshold\nthe ﬁrst component. This procedure turns out to separate the image’s main object from the background. We\ncompute a second PCA on the remaining patches across three images depicting the same category. We color\nthe three ﬁrst components with three diﬀerent colors and present the results in Fig. 1 and 9. There are two\ninteresting observations: ﬁrst, our unsupervised foreground / background detector, based on detecting the\n16', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 9:Morevisualization oftheﬁrst PCAcomponents. We compute the PCA between the patches\nfrom all of the images and show their ﬁrst 3 components. Each component corresponds to a speciﬁc color\nchannel. Same parts are matched between related images depsite changes of pose, style or even objects.\nBackground is removed by removing patches with a negative score of the ﬁrst PCA component.\nhighest variance direction, performs very well and is capable of delineating the boundary of the main object\nin the picture. Second, the other components correspond to "parts" of objects and match well for images of\nthe same category. This is an emerging property – our model was not trained to parse parts of objects.\nPatch matching. Finally, we explore what type of information our patch-level features contain by match-\ning them across images. We start by detecting the foreground object using the procedure described above.\nThen, we compute the euclidean distance between patch features extracted from two images and map them\nby solving an assignment problem. In order to reduce the number of matches, we then apply a non-maximum\nsuppression to keep only the salient ones. In Fig. 10, we show some examples of such matchings.\nWe observe that the features seem to capture information about semantic regions that serve similar purpose\nin diﬀerent objects or animals. For instance, the wing of a plane matches the wing of a bird. We also observe\nthat the model is robust to style (image versus drawing), and to large variation of poses (see the elephant).\n8 Fairness and Bias Analysis\nWe conduct two fairness evaluations of our models. We probe for geographical fairness and potential harmful\nlabel associations. For both evaluations, we experiment with our largest ViT-g model.\n8.1 Geographical Fairness\nWe evaluate geographical fairness on the Dollar Street dataset introduced in De Vries et al. (2019) using\nthe evaluation protocol of Goyal et al. (2022b). This benchmark compares performance across countries and\nincome levels. It contains 16,073 images from 289 households across 54 countries. The task is to recognize\n94 concepts that vary visually between households based on income or location. In Table 12, we compare\nour model with SEERv2 (Goyal et al., 2022a), a model trained on a geographically diverse set of images.\nOur model is slightly fairer across regions and incomes than the SEERv2 model and signiﬁcantly better than\n17', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 10: Matching across images. We match patch-level features between images from diﬀerent do-\nmains, poses and even objects that share similar semantic information. This exhibits the ability of our model\nto transfer across domains and understand relations between similar parts of diﬀerent objects.\nIncome buckets Regions\nMethod Arch. Data low medium high Africa Asia Americas Europe\nSEERv2 RG-10B IG-1B 59.7 78.5 86.6 65.9 76.3 81.1 85.6\nDINOv2 ViT-g/14 LVD-142M 67.4 83.3 90.5 74.0 81.6 86.2 89.7\nTable 12: Geographical fairness and diversity analysis across income buckets and regions.\nthe supervised baseline reported by Goyal et al. (2022a). However, we still observe a signiﬁcant diﬀerence\nbetween regions, particularly in Africa, where our model performance drops by 25.7% compared to Europe.\nThisshowthatourmodelisstillbiasedtowardWesterncountries. Similarly, ourmodelperformssigniﬁcantly\nbetter on high-income households than low-income ones, with a diﬀerence of 31.7%. Despite improvements,\nwe observe signiﬁcant biases in our models toward wealthy households from Western countries.\n8.2 Gender, Skintones and Age\nIn a second set of evaluations, we question how our model classiﬁes images of people of diﬀerent gender, skin\ntone, and age (all self-reported). We follow the protocol of Goyal et al. (2022b), where we train a multiclass\nclassiﬁer on a subset of 619 classes of ImageNet-22k. We group the 619 classes into four broader categories:\nHuman, Possibly Human, Non-Human, or Crime. Non-Human and Crime are considered harmful. Using\nthis classiﬁer, we run inference on 2955 images from the Casual Conversations dataset (Hazirbas et al., 2021)\nand keep all labels in the top-5 that are assigned a probability of 0.1 or more. Because of that, we can assign\n18', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Gender Skintone Age Groups\nModel Assoc.female\ndarkerfemale\nlightermale\ndarkermale\nlighter18-30 30-45 45-70 70+\nSEER Non-Human 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nRG-10B Crime 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nHuman 94.9 95.8 86.6 79.0 90.5 88.3 91.9 82.3\nPossibly-Human 13.6 6.7 65.0 60.2 32.8 37.2 29.4 6.5\nDINOv2 Non-Human 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nViT-g/14 Crime 0.0 0.0 0.2 0.0 0.0 0.1 0.0 0.0\nHuman 97.3 97.7 86.1 84.0 91.2 90.2 93.2 88.7\nPossibly-Human 15.8 17.2 52.2 48.1 35.3 37.3 23.0 9.7\nTable 13: Label association fairness evaluation across gender, skintones and age groups. We\nfollow the protocol proposed by Goyal et al. (2022b) with a slight modiﬁcation. Instead of ﬁnetuning the\nbackbone, we simply learn a linear classiﬁer on the subset of 619 classes of ImageNet-22k.\nModel toGPU TypeGPU PowerGPU-hours PUETotal power Carbon emitted\nReproduce consumption consumption (tCO 2eq)\nDINOv2-g A100-40GB 400W 22,016 1.1 9.7 MWh 3.7\nTable 14: Carbon footprint of reproducing DINOv2. We report the potential carbon emission of\nreproducing DINOv2-g when assuming a power consumption for the A100-40GB of 400W, a PUE of 1.1 and\ncarbon intensity factor of 0.385 kg CO 2e per KWh.\nmultiple classes to each image. We make one modiﬁcation to the original evaluation protocol: we do not\nbackpropagate gradients to the backbone and keep it frozen. We compare our model to SEERv2 in Table 13.\nOur model often classiﬁes images of all groups as Human without large deviations across skin tones. Neither\nSEERv2 nor DINOv2 predict harmful labels from the Non-Human or Crime meta-categories (except for two\ninstances where the background contains bars visually similar to prison bars). We see that our model triggers\nthe Possibly-Human classes often. This class is constructed from objects in ImageNet-22k that are often\nrelated to Humans, such as Scarf, Glasses, or Beard. Our model often predicts the Possibly-Human class\nfor men because of the prevalence of the Beard class. No clear pattern indicates a bias against a particular\ngroup in this study. While this is encouraging, we also acknowledge that a more thorough evaluation of\nbiases may reveal ﬂaws in our model.\n9 Estimating the Environmental Impact of Training our Models\nTraining foundation models consumes a signiﬁcant amount of energy, resulting in carbon dioxide emissions.\nPatterson et al. (2021) propose a methodology to report an estimation of the carbon emitted during the\ntraining of a model based on the speciﬁcs of the data center and its power grid. This computation informs\nthe design of the data center used for the training of models and the choice of location for data centers.\nThis methodology requires to know the speciﬁcs of the data center used for training, which can be complex\nwhen multiple data centers are involved over time. Additionally, these speciﬁcs are most often not in the\ncontrol of the AI practitioner, and hence, this methodology is less helpful when practioners make technical\ndecisions about future trainings. Instead, in this section, we follow an alternative that reports the potential\ncarbon emission of retraining a similar model in an average data center located in the US. This methodology\nwas used in previous work in natural language processing (Strubell et al., 2019; Touvron et al., 2023) to\nestablish an apple-to-apple comparison between pretraining schemes. More precisely, we ﬁx the value of all\nexogenous variables, i.e., the Power Usage Eﬀectiveness (PUE) and carbon intensity factor of a power grid\nto the same values as in Touvron et al. (2023), that is, a PUE of 1.1 and the carbon intensity factor to the\n19', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'US average of 0.385 kg CO 2eq/KWh. We use the same formula as in Patterson et al. (2021) to estimate the\npotential energy consumption and the carbon emission. For the power consumption of an A100-80GB, we\ntake the thermal design power for NVLink systems, which is 400W. We report the potential carbon emission\nof retraining a DINOv2 ViT-g in Table 14. For comparison, retraining an OpenCLIP ViT-L or OpenCLIP\nViT-G would require 22.4 MWh and 118.9 MWh, respectively, if run in the same data center. This is 10 ×\nmore carbon emission. Note that this comparison is not fair to them, since they also train a text encoder in\nparallel, and we thus do not report them in the table. However, it gives a reasonable guidelines for those who\nare interested in training only visual features: in this context, training a self-supervised model is preferable\nin terms of carbon emission. Training a text-guided model still makes sense when planning to reuse the text\nencoder.\nCarbon footprint of the whole project. Additionally, we estimate the footprint of the whole project\nto be between 0.5k and 1k tCO 2eq using the same grid as presented above. This carbon footprint represents\nin the order of 200k GPU-days. The primary sources of emissions are the self-supervised pre-trainings of\nthe models. For example, a single pre-training of a ViT-g model (22k GPU-hours) emits 3.7 tons of CO 2eq,\nwhile a ﬁnetuning on ImageNet-1k (1k GPU-hours) emits 0.2 tons. This estimate only considers the GPUs’\nelectricity consumption and ignores other emissions, such as their manufacturing and disposal.\n10 Future work and Discussion\nIn this work, we present DINOv2, a new series of image encoders pretrained on large curated data with no\nsupervision. This is the ﬁrst SSL work on image data that leads to visual features that close the performance\ngap with (weakly) supervised alternatives across a wide range of benchmarks and without the need for\nﬁnetuning. A few properties emerge from these models, such as an understanding of object parts and scene\ngeometry regardless of the image domains. We expect that more of these properties will emerge at larger\nscales of models and data, akin to instruction emergence in large language models, and plan to continue\nscaling along these axes. This paper also demonstrates that these visual features are compatible with\nclassiﬁers as simple as linear layers - meaning the underlying information is readily available . In future work,\nwe plan to leverage this ability to train a a language-enabled AI system that can process visual features as\nif they were word tokens, and extract the required information to ground the system.\nAcknowledgments.\nWe thank Mathilde Caron for initial discussions that led to this work. We thank Olivia Joulin for the horse\ndrawing used in Fig. 10. We also thank the rest of FAIR and Meta AI for feedback on this work through\nthe entire project.\nReferences\nYuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous clustering\nand representation learning. In ICLR, 2020.\nMahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand\nJoulin, Michael Rabbat, and Nicolas Ballas. Masked siamese networks for label-eﬃcient learning. arXiv\npreprint arXiv:2204.07141 , 2022.\nMahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann\nLeCun, and Nicolas Ballas. Self-supervised learning from images with a joint-embedding predictive archi-\ntecture.arXiv preprint arXiv:2301.08243 , 2023.\nAlexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. Data2vec: A general\nframework for self-supervised learning in speech, vision and language. arXiv preprint arXiv:2202.03555 ,\n2022.\nHangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint\narXiv:2106.08254 , 2021.\n20', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}]: %s
2023-12-07 10:17:39,939 - INFO - Check the results [{'page_content': 'kNN linear\nMethod Arch. Data Text sup. val val ReaL V2\nWeakly supervised\nCLIP ViT-L/14 WIT-400M ✓ 79.8 84.3 88.1 75.3\nCLIP ViT-L/14 336WIT-400M ✓ 80.5 85.3 88.8 75.8\nSWAG ViT-H/14 IG3.6B ✓ 82.6 85.7 88.7 77.6\nOpenCLIP ViT-H/14 LAION ✓ 81.7 84.4 88.4 75.5\nOpenCLIP ViT-G/14 LAION ✓ 83.2 86.2 89.4 77.2\nEVA-CLIP ViT-g/14 custom∗✓ 83.5 86.4 89.3 77.4\nSelf-supervised\nMAE ViT-H/14 INet-1k \x15 49.4 76.6 83.3 64.8\nDINO ViT-S/8 INet-1k \x15 78.6 79.2 85.5 68.2\nSEERv2 RG10B IG2B \x15 – 79.8 – –\nMSN ViT-L/7 INet-1k \x15 79.2 80.7 86.0 69.7\nEsViT Swin-B/W=14 INet-1k \x15 79.4 81.3 87.0 70.4\nMugs ViT-L/16 INet-1k \x15 80.2 82.1 86.9 70.8\niBOT ViT-L/16 INet-22k \x15 72.9 82.3 87.5 72.4\nDINOv2ViT-S/14 LVD-142M \x15 79.0 81.1 86.6 70.9\nViT-B/14 LVD-142M \x15 82.1 84.5 88.3 75.1\nViT-L/14 LVD-142M \x15 83.5 86.3 89.5 78.0\nViT-g/14 LVD-142M \x15 83.5 86.5 89.6 78.4\nTable4:LinearevaluationonImageNet-1koffrozenpretrainedfeatures. WereportTop-1accuracy\non the validation set for publicly available models trained on public or private data, and with or without\ntext supervision (text sup.). For reference, we also report the kNN performance on the validation set. We\ncompare across any possible architectures (Arch.), at resolution 224×224unless stated otherwise. The\ndataset used for training EVA-CLIP is a custom mixture, see paper for details (Fang et al., 2023).\nset is signiﬁcantly better ( +1.1%versus EVA-CLIP), indicating better generalization. For the remainder of\nthis section, we report OpenCLIP-G as a reference for weakly-supervised models.\nCan we ﬁnetune the encoders? We question if the ability of our models to produce high quality frozen\nfeatures impact their performance when ﬁnetuned with supervision on a speciﬁc dataset. While this is not\ncore to this paper, this experiment is indicative of whether we have involuntarily specialized our models\nto the setting of linear evaluations of frozen features. To run this sanity check, we apply the ﬁnetuning\npipeline from Touvron et al. (2022), without tweaking hyper-parameters. In Table 5, we show that the\nTop-1 accuracy on the validation set of ImageNet-1k improves by more than +2%when the backbone is\nﬁnetuned. This is true both when using models at resolution 224and448. Further gains can be obtained by\ntuning the hyper-parameters of the ﬁnetuning, but this is beyond the goal of this sanity check. Nonetheless,\nour best ﬁnetuned performance ( 88.9%) is only a couple of percent below ( −2.2%) the absolute state of the\narts ( 91.1%), obtained by Chen et al. (2023). As DINOv2 leads to features that are strong in both the linear\nand ﬁnetuning settings, a strong property of our approach is that ﬁnetuning is optional .\nRobustness analysis. To complement our study, and probe the generalization of our features, we evaluate\nour ImageNet-1k models trained with linear classiﬁcation heads on domain generalization benchmarks. We\nuse the best performing linear classiﬁer as described above and simply run inference on those benchmarks.\nPlease note that most results in the litterature are obtained with models that are ﬁnetuned end-to-end on\nImageNet-1k. We show the result of this experiment in Table 6. When comparing with state-of-the-art SSL\nmethods, our models shows drastically better robustness ( +29.6%on A, +22.1%on R and +23.0%on Sketch\n11', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Arch. Res. Linear Finetuned ∆\nViT-g/14224 86.5 88.5 +2.0\n448 86.7 88.9 +2.2\nTable 5: Supervised ﬁnetuning on ImageNet-1k. We use the pipeline of Touvron et al. (2022) to\nﬁnetune our encoders on ImageNet-1k at resolutions 224×224or448×448. We compare with the accuracy\nobtained with linear probing and observe only modest improvements with ﬁne-tuning: this suggests that\nDINOv2 features already perform well out-of-the-box.\nMethod Arch Data Im-A Im-R Im-C ↓Sketch\nOpenCLIP ViT-G/14 LAION 63.8 87.8 45.366.4\nMAE ViT-H/14 INet-1k 10.2 34.4 61.4 21.9\nDINO ViT-B/8 INet-1k 23.9 37.0 56.6 25.5\niBOT ViT-L/16 INet-22k 41.5 51.0 43.9 38.5\nDINOv2ViT-S/14 LVD-142M 33.5 53.7 54.4 41.2\nViT-B/14 LVD-142M 55.1 63.3 42.7 50.6\nViT-L/14 LVD-142M 71.3 74.4 31.5 59.3\nViT-g/14 LVD-142M 75.978.828.2 62.5\nTable 6:Domain Generalization with a linear probe on top of frozen features at a resolution of 224.\nHigher numbers are better for all benchmarks except Im-C.\ncompared to iBOT). Our model also improves upon the best weakly-supervised model on ImageNet-A while\nlagging behind on R and Sketch.\n7.2 Additional Image and Video classiﬁcation Benchmarks\nIn this section we study the generalization of our features on downstream classiﬁcation benchmarks. We\nconsider two sets of evaluations in that context. On one hand, we use large and ﬁnegrained datasets such\nas iNaturalist and Places205. On the other, we use the 12 image classiﬁcation tasks originally proposed\nin SimCLR (Chen et al., 2020). For iNaturalist 2018, iNaturalist 2021, and Places205, we train a linear\nclassiﬁer with data augmentations as in Sec. 7.1 We report top-1 accuracy for those three datasets in Table 7.\nInterestingly, our model signiﬁcantly outperforms OpenCLIP ViT-G/14 on both variants of iNaturalist\n(+8.6%and+9.7%for 2018 and 2021 respectively), and lags slightly behind on Places 205 ( −2.3%).\nIn a second set of evaluations, we measure the performance of our model on video action recognition even\nthough our features were not trained on videos.. We evaluated features on three datasets, namely UCF-\n101 (Soomro et al., 2012), Kinetics-400 (Kay et al., 2017) and Something-Something v2 (Goyal et al., 2017).\nFor this evaluation, we pick 8evenly spaced frames in the video and train a linear classiﬁer on the average\nof the features for UCF and K-400. For SSv2, we opt for concatenation to retain more temporal information\nthan with feature averaging. For each dataset, we measure average accuracy and report the results in\nTable 7. We see that amongst self-supervised approaches, our model clearly sets a new state of the art.\nMoreover, our model matches the accuracy of the OpenCLIP features on UCF and Kinetics ( +0.1%and\n+0.5%respectively) and clearly outperforms them on SSv2 ( +2.5%). This is particularly interesting, as\nSSv2 requires a much richer understanding of the video frames.\nFinally, in Table 8, we compare selected frozen features on 12 transfer classiﬁcation benchmarks initially\nproposed by Chen et al. (2020). This benchmark covers scenes, objects (food, cars, planes), and textures.\nWe replace the Birdsnap dataset with CUB because the former was not publicly available in its entirety. We\nfollow the experimental protocol as outlined by Chen et al. (2020), namely training a logistic regression on\nprecomputed features. Our model signiﬁcantly outperforms state-of-the-art SSL models, with most notable\ndiﬀerences on Stanford Cars ( +14.8%versus DINO ViT-B/8) and FGVC Aircraft ( +14.8%versus iBOT\n12', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Image classiﬁcation Video classiﬁcation\nFeature Arch iNat2018 iNat2021 Places205 K400 UCF-101 SSv2\nOpenCLIP ViT-G/14 73.0 76.0 69.8 78.3 90.7 35.8\nMAE ViT-H/14 31.0 32.3 52.4 54.2 70.6 29.2\nDINO ViT-B/8 59.6 68.3 60.4 64.5 85.0 32.6\niBOT ViT-L/16 66.3 74.6 64.4 72.6 88.6 38.7\nDINOv2ViT-S/14 69 74.2 62.9 67.8 87 33.1\nViT-B/14 76.4 81.1 66.2 73.2 89.1 34.4\nViT-L/14 80.4 85.1 67.3 76.3 90.5 35.6\nViT-g/14 81.6 85.7 67.5 78.4 91.2 38.3\nTable 7:Linear evaluation on other image and video classiﬁcation. The image benchmarks contain\na large quantity of ﬁne-grained examples about objects or scenes. The video benchmarks cover action\nclassiﬁcation and human-object interaction. All the features are frozen with a linear probe on top.\nFeature Arch Food C10 C100 SUN Cars Aircr VOC DTD Pets Cal101 Flowers CUB Avg\nOpenCLIP ViT-G/14 94.5 98.7 91.0 84.0 96.1 80.289.3 86.0 95.798.199.5 89.9 91.9\nMAE ViT-H/14 78.4 96.1 83.9 63.9 56.1 63.4 84.3 75.4 89.4 95.9 92.3 57.2 78.0\nDINO ViT-B/8 85.1 97.2 86.9 70.3 76.6 70.6 86.7 79.6 93.2 95.4 97.6 81.7 85.1\niBOT ViT-L/16 91.0 99.0 92.8 75.6 71.8 72.4 89.0 80.7 87.7 97.5 99.6 82.1 86.6\nDINOv2ViT-S/14 89.1 97.7 87.5 74.4 81.6 74.0 87.8 80.6 95.1 97.0 99.6 88.1 87.7\nViT-B/14 92.8 98.7 91.3 77.3 88.2 79.4 88.2 83.3 96.2 96.1 99.6 89.6 90.1\nViT-L/14 94.3 99.3 93.4 78.7 90.1 81.5 88.3 84.0 96.6 97.5 99.7 90.5 91.2\nViT-g/14 94.7 99.5 94.4 78.7 91.4 87.289.0 84.5 96.797.699.7 91.6 92.1\nTable 8:Linear evaluation of frozen features on ﬁne-grained benchmarks. Accuracy on 12 bench-\nmarks covering objects, scenes and textures following the evaluation protocol proposed in Chen et al. (2020).\nViT-L/16). Even though these benchmarks favor text-guided pretraining, our features are still competitive\nwith OpenCLIP on most classiﬁcation benchmarks, with the exception of a few datasets, especially SUN\n(−5.3%) and Cars (−4.7%).\n7.3 Instance Recognition\nIn this experiment, we probe our model on the task of instance-level recognition using a non-parametric\napproach. Images from a database are ranked according to their cosine similarity with a query image. We\nevaluated our model and compare to baselines on Paris and Oxford, that are landmark recognition bench-\nmarks. We also evaluated on Met, a dataset of artworks from the Metropolitan museum, and AmsterTime,\ncontaining street view images matched to archival images of Amsterdam. We measure performance by com-\nputing the mean average precision and report our results in Table 9. We see that our features signiﬁcantly\noutperform both SSL ( +41%mAP on Oxford-Hard), and weakly-supervised ( +34%mAP on Oxford-Hard)\nones. Itisinterestingtoseethatourfeaturesperformwellacrosstaskgranularities, bothatthecategory-level\nand instance-level. This is a desirable property for strong oﬀ-the-shelf computer vision features.\n7.4 Dense Recognition Tasks\nWe probe the quality of patch-level features extracted from our network on several dense downstream tasks.\nWeconsidersemanticimagesegmentationandmonoculardepthestimationinseveralsettingsandweconduct\nevaluations on multiple datasets for each.\n13', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Oxford Paris Met AmsterTime\nFeature Arch M H M H GAP GAP- ACC mAP\nOpenCLIP ViT-G/14 50.7 19.7 79.2 60.2 6.5 23.9 34.4 24.6\nMAE ViT-H/14 11.7 2.2 19.9 4.7 7.5 23.5 30.5 4.2\nDINO ViT-B/8 40.1 13.7 65.3 35.3 17.1 37.7 43.9 24.6\niBOT ViT-L/16 39.0 12.7 70.7 47.0 25.1 54.8 58.2 26.7\nDINOv2ViT-S/14 68.8 43.2 84.6 68.5 29.4 54.3 57.7 43.5\nViT-B/14 72.9 49.5 90.3 78.6 36.7 63.5 66.1 45.6\nViT-L/14 75.1 54.0 92.7 83.5 40.0 68.9 71.6 50.0\nViT-g/14 73.6 52.3 92.1 82.6 36.8 73.6 76.5 46.7\nTable 9:Evaluation of frozen features on instance-level recognition. We consider 4 diﬀerent bench-\nmarks and report their main metrics.\nADE20k CityScapes Pascal VOC\n(62.9) (86.9) (89.0)\nMethod Arch. lin. +ms lin. +ms lin. +ms\nOpenCLIP ViT-G/14 39.3 46.0 60.3 70.3 71.4 79.2\nMAE ViT-H/14 33.3 30.7 58.4 61.0 67.6 63.3\nDINO ViT-B/8 31.8 35.2 56.9 66.2 66.4 75.6\niBOT ViT-L/16 44.6 47.5 64.8 74.5 82.3 84.3\nDINOv2ViT-S/14 44.3 47.2 66.6 77.1 81.1 82.6\nViT-B/14 47.3 51.3 69.4 80.0 82.5 84.9\nViT-L/14 47.7 53.1 70.3 80.9 82.1 86.0\nViT-g/14 49.053.071.3 81.0 83.0 86.2\nTable 10: Semantic segmentation on ADE20K, CityScapes and Pascal VOC with frozen features\nand a linear classiﬁer (lin.) and with multiscale (+ms). The absolute state of the art – from Wang et al.\n(2022), Liu et al. (2021) and Chen et al. (2018) respectively – are mentioned at the top of the Table. For\nreference, using the Mask2Former pipeline (Steiner et al., 2021) with a ViT-Adapter (Chen et al., 2022) on\ntop of our frozen ViT-g/14 backbone gives 60.2 mIoU on ADE-20k.\nSemantic segmentation. For our semantic segmentation evaluation, we consider two diﬀerent setups.\nLinear: a linear layer is trained to predict class logits from a patch tokens. It is used to produce a low-\nresolution logit map (eg 32x32 for a model with patch size 16), which is then upsampled to full resolution\n(512x512) to obtain a segmentation map. This procedure is extremely simple but cannot easily produce\nhigh-resolution segmentations. +ms: a boosted version of the linear setup. We concatenate the patch\ntokens of the 4 last layers, use a larger image resolution of 640, and use multiscale test-time augmentations\nto improve the predictions. We report the performance of our model variants as well as the baselines on\nthree datasets under the two setups in Table 10.\nOur models show very good performance on all datasets and for all setups. Interestingly, our evaluation\nusing+msis on par with fully ﬁnetuning MAE with an Upernet decoder ( 53.0versus 53.6mIoU). This is\nsurprising because we use a signiﬁcantly simpler predictor. Also, our best model, when evaluated using the\nboosted recipe, almost matches the state of the art on Pascal VOC ( 86.2versus 89.0mIoU).\nIn a ﬁnal experiment, we freeze our backbone, and plug it into a ViT-Adapter Chen et al. (2022) with a\nMask2former head (Cheng et al., 2022). We tune the weights of the adapter and head, but keep the backbone\nfrozen: only a fraction of the weights are tuned, keeping the training procedure lightweight. We reach 60.2\nmIoU on ADE20k, close to the competitive state of the art, standing at 62.9 mIoU (Wang et al., 2022).\n14', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'NYUd KITTI NYUd →SUN RGB-D\n(0.330) (2.10) (0.421)\nMethod Arch. lin. 1 lin. 4 DPT lin. 1 lin. 4 DPT lin. 1 lin. 4 DPT\nOpenCLIP ViT-G/14 0.541 0.510 0.414 3.57 3.21 2.56 0.537 0.476 0.408\nMAE ViT-H/14 0.517 0.483 0.415 3.66 3.26 2.59 0.545 0.523 0.506\nDINO ViT-B/8 0.555 0.539 0.492 3.81 3.56 2.74 0.553 0.541 0.520\niBOT ViT-L/16 0.417 0.387 0.358 3.31 3.07 2.55 0.447 0.435 0.426\nDINOv2ViT-S/14 0.449 0.417 0.356 3.10 2.86 2.34 0.477 0.431 0.409\nViT-B/14 0.399 0.362 0.317 2.90 2.59 2.23 0.448 0.400 0.377\nViT-L/14 0.384 0.333 0.293 2.78 2.50 2.14 0.429 0.396 0.360\nViT-g/14 0.344 0.298 0.279 2.62 2.35 2.11 0.402 0.362 0.338\nTable 11: Depth estimation with frozen features . We report performance when training a linear\nclassiﬁer on top of one (lin. 1) or four (lin. 4) transformer layers, as well, as the DPT decoder (DPT) of\nRanftl et al. (2021). We report the RMSE metric on the 3 datasets. Lower is better. For reference, we\nreport state-of-the-art results taken from Li et al. (2022b) on each benchmark on top of the Table.\nDepth estimation. In this experiment, we evaluate our patch-level features on three monocular depth\nestimation benchmarks: NYUd, KITTI and zero-shot transfer from NYUd to SUN3d. We follow the evalu-\nation protocol of Li et al. (2022b). We consider three diﬀerent setups for this evaluation. lin. 1: we extract\nthe last layer of the frozen transformer and concatenate the [CLS]token to each patch token. Then we\nbi-linearly upsample the tokens by a factor of 4 to increase the resolution. Finally we train a simple linear\nlayer using a classiﬁcation loss by dividing the depth prediction range in 256 uniformly distributed bins and\nuse a linear normalization following Bhat et al. (2021). lin. 4: we use the same protocol that we use with\none layer, but concatenate the tokens from layers l={3,6,9,12}for ViT-S/B, l={5,12,18,24}for ViT-L,\nandl={10,20,30,40}for ViT-g. DPT: we use the DPT decoder (Ranftl et al., 2021) on top of our frozen\nmodels and setup a regression task. We scale the size of the head following the dimension of the features for\neach architecture. We show results for all baselines, all datasets and all setups in Table 11.\nFrom this table, we see that our features clearly surpass the best SSL and WSL features available. It\nis interesting to see that iBOT features extracted from a ViT-L outperform the ones of OpenCLIP with\na ViT-G. This observation supports the intuition that caption-based feature learning fails to learn subtle\npatterns like this one. Also, our model, with the DPT decoder and frozen backbone, matches or exceeds\nthe performance of the recent work of Li et al. (2022b). Finally, the out-of-domain generalization result on\nSUN-RGBd shows that our features allow very good transfer between domains. A depth prediction module\ntrained on indoor scenes from NYUd generalizes pretty well to the outdoor examples of SUN-RGBd.\n7.5 Qualitative Results\nIn this ﬁnal section of the empirical evaluation of our features, we propose a few qualitative analyses.\nSemantic Segmentation and Depth Estimation. We show some qualitative results for our dense\nprediction evaluations: segmentation on ADE20K in Fig. 7 and depth estimation on NYUd, KITTI and\nSUN RGB-D in Fig. 7. We compare DINOv2 with OpenCLIP with a linear classiﬁer on each dataset. While\nnot perfect, the linear segmentation model using our DINOv2 backbone produces good results and behaves\nmuch better than the OpenCLIP one under this evaluation setup. Indeed, the segmentation mask produced\nby OpenCLIP-G shows many artifacts and disconnected components. The qualitative results on depth\nestimation clearly illustrate the quantitative gap between OpenCLIP and DINOv2. These results highlight\nthat our features, as well as the features extracted from OpenCLIP, are able to linearly separate complex\ninformation such as depth, even though neither was trained with this type of information. However, our\nfeatures lead to a much smoother depth estimation, with less artifacts. Some objects such as the chair on\nthe SUN RGB-D image are completely ignored by OpenCLIP and correctly positioned using our features.\n15', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 7: Segmentation and depth estimation with linear classiﬁers. Examples from ADE20K,\nNYUd, SUN RGB-D and KITTI with a linear probe on frozen OpenCLIP-G and DINOv2-g features.\nFigure 8:Examples of out-of-distribution examples with frozen DINOv2-g features and a linear probe.\nOut-of-distribution generalization. We show a few examples of applying the depth prediction and\nsegmentation linear classiﬁers to out-of-distribution examples in Fig. 8. The qualitative results support our\nclaim that our features transfer between domains. The quality of the depth and segmentation predicted for\npictures of animals, or paintings is very good, even though the domains are very diﬀerent.\nPCA of patch features. We show the results of the principal component analysis (PCA) performed on\nthe patch features extracted by our model. We keep only patches with a positive value after we threshold\nthe ﬁrst component. This procedure turns out to separate the image’s main object from the background. We\ncompute a second PCA on the remaining patches across three images depicting the same category. We color\nthe three ﬁrst components with three diﬀerent colors and present the results in Fig. 1 and 9. There are two\ninteresting observations: ﬁrst, our unsupervised foreground / background detector, based on detecting the\n16', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 9:Morevisualization oftheﬁrst PCAcomponents. We compute the PCA between the patches\nfrom all of the images and show their ﬁrst 3 components. Each component corresponds to a speciﬁc color\nchannel. Same parts are matched between related images depsite changes of pose, style or even objects.\nBackground is removed by removing patches with a negative score of the ﬁrst PCA component.\nhighest variance direction, performs very well and is capable of delineating the boundary of the main object\nin the picture. Second, the other components correspond to "parts" of objects and match well for images of\nthe same category. This is an emerging property – our model was not trained to parse parts of objects.\nPatch matching. Finally, we explore what type of information our patch-level features contain by match-\ning them across images. We start by detecting the foreground object using the procedure described above.\nThen, we compute the euclidean distance between patch features extracted from two images and map them\nby solving an assignment problem. In order to reduce the number of matches, we then apply a non-maximum\nsuppression to keep only the salient ones. In Fig. 10, we show some examples of such matchings.\nWe observe that the features seem to capture information about semantic regions that serve similar purpose\nin diﬀerent objects or animals. For instance, the wing of a plane matches the wing of a bird. We also observe\nthat the model is robust to style (image versus drawing), and to large variation of poses (see the elephant).\n8 Fairness and Bias Analysis\nWe conduct two fairness evaluations of our models. We probe for geographical fairness and potential harmful\nlabel associations. For both evaluations, we experiment with our largest ViT-g model.\n8.1 Geographical Fairness\nWe evaluate geographical fairness on the Dollar Street dataset introduced in De Vries et al. (2019) using\nthe evaluation protocol of Goyal et al. (2022b). This benchmark compares performance across countries and\nincome levels. It contains 16,073 images from 289 households across 54 countries. The task is to recognize\n94 concepts that vary visually between households based on income or location. In Table 12, we compare\nour model with SEERv2 (Goyal et al., 2022a), a model trained on a geographically diverse set of images.\nOur model is slightly fairer across regions and incomes than the SEERv2 model and signiﬁcantly better than\n17', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 10: Matching across images. We match patch-level features between images from diﬀerent do-\nmains, poses and even objects that share similar semantic information. This exhibits the ability of our model\nto transfer across domains and understand relations between similar parts of diﬀerent objects.\nIncome buckets Regions\nMethod Arch. Data low medium high Africa Asia Americas Europe\nSEERv2 RG-10B IG-1B 59.7 78.5 86.6 65.9 76.3 81.1 85.6\nDINOv2 ViT-g/14 LVD-142M 67.4 83.3 90.5 74.0 81.6 86.2 89.7\nTable 12: Geographical fairness and diversity analysis across income buckets and regions.\nthe supervised baseline reported by Goyal et al. (2022a). However, we still observe a signiﬁcant diﬀerence\nbetween regions, particularly in Africa, where our model performance drops by 25.7% compared to Europe.\nThisshowthatourmodelisstillbiasedtowardWesterncountries. Similarly, ourmodelperformssigniﬁcantly\nbetter on high-income households than low-income ones, with a diﬀerence of 31.7%. Despite improvements,\nwe observe signiﬁcant biases in our models toward wealthy households from Western countries.\n8.2 Gender, Skintones and Age\nIn a second set of evaluations, we question how our model classiﬁes images of people of diﬀerent gender, skin\ntone, and age (all self-reported). We follow the protocol of Goyal et al. (2022b), where we train a multiclass\nclassiﬁer on a subset of 619 classes of ImageNet-22k. We group the 619 classes into four broader categories:\nHuman, Possibly Human, Non-Human, or Crime. Non-Human and Crime are considered harmful. Using\nthis classiﬁer, we run inference on 2955 images from the Casual Conversations dataset (Hazirbas et al., 2021)\nand keep all labels in the top-5 that are assigned a probability of 0.1 or more. Because of that, we can assign\n18', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Gender Skintone Age Groups\nModel Assoc.female\ndarkerfemale\nlightermale\ndarkermale\nlighter18-30 30-45 45-70 70+\nSEER Non-Human 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nRG-10B Crime 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nHuman 94.9 95.8 86.6 79.0 90.5 88.3 91.9 82.3\nPossibly-Human 13.6 6.7 65.0 60.2 32.8 37.2 29.4 6.5\nDINOv2 Non-Human 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nViT-g/14 Crime 0.0 0.0 0.2 0.0 0.0 0.1 0.0 0.0\nHuman 97.3 97.7 86.1 84.0 91.2 90.2 93.2 88.7\nPossibly-Human 15.8 17.2 52.2 48.1 35.3 37.3 23.0 9.7\nTable 13: Label association fairness evaluation across gender, skintones and age groups. We\nfollow the protocol proposed by Goyal et al. (2022b) with a slight modiﬁcation. Instead of ﬁnetuning the\nbackbone, we simply learn a linear classiﬁer on the subset of 619 classes of ImageNet-22k.\nModel toGPU TypeGPU PowerGPU-hours PUETotal power Carbon emitted\nReproduce consumption consumption (tCO 2eq)\nDINOv2-g A100-40GB 400W 22,016 1.1 9.7 MWh 3.7\nTable 14: Carbon footprint of reproducing DINOv2. We report the potential carbon emission of\nreproducing DINOv2-g when assuming a power consumption for the A100-40GB of 400W, a PUE of 1.1 and\ncarbon intensity factor of 0.385 kg CO 2e per KWh.\nmultiple classes to each image. We make one modiﬁcation to the original evaluation protocol: we do not\nbackpropagate gradients to the backbone and keep it frozen. We compare our model to SEERv2 in Table 13.\nOur model often classiﬁes images of all groups as Human without large deviations across skin tones. Neither\nSEERv2 nor DINOv2 predict harmful labels from the Non-Human or Crime meta-categories (except for two\ninstances where the background contains bars visually similar to prison bars). We see that our model triggers\nthe Possibly-Human classes often. This class is constructed from objects in ImageNet-22k that are often\nrelated to Humans, such as Scarf, Glasses, or Beard. Our model often predicts the Possibly-Human class\nfor men because of the prevalence of the Beard class. No clear pattern indicates a bias against a particular\ngroup in this study. While this is encouraging, we also acknowledge that a more thorough evaluation of\nbiases may reveal ﬂaws in our model.\n9 Estimating the Environmental Impact of Training our Models\nTraining foundation models consumes a signiﬁcant amount of energy, resulting in carbon dioxide emissions.\nPatterson et al. (2021) propose a methodology to report an estimation of the carbon emitted during the\ntraining of a model based on the speciﬁcs of the data center and its power grid. This computation informs\nthe design of the data center used for the training of models and the choice of location for data centers.\nThis methodology requires to know the speciﬁcs of the data center used for training, which can be complex\nwhen multiple data centers are involved over time. Additionally, these speciﬁcs are most often not in the\ncontrol of the AI practitioner, and hence, this methodology is less helpful when practioners make technical\ndecisions about future trainings. Instead, in this section, we follow an alternative that reports the potential\ncarbon emission of retraining a similar model in an average data center located in the US. This methodology\nwas used in previous work in natural language processing (Strubell et al., 2019; Touvron et al., 2023) to\nestablish an apple-to-apple comparison between pretraining schemes. More precisely, we ﬁx the value of all\nexogenous variables, i.e., the Power Usage Eﬀectiveness (PUE) and carbon intensity factor of a power grid\nto the same values as in Touvron et al. (2023), that is, a PUE of 1.1 and the carbon intensity factor to the\n19', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'US average of 0.385 kg CO 2eq/KWh. We use the same formula as in Patterson et al. (2021) to estimate the\npotential energy consumption and the carbon emission. For the power consumption of an A100-80GB, we\ntake the thermal design power for NVLink systems, which is 400W. We report the potential carbon emission\nof retraining a DINOv2 ViT-g in Table 14. For comparison, retraining an OpenCLIP ViT-L or OpenCLIP\nViT-G would require 22.4 MWh and 118.9 MWh, respectively, if run in the same data center. This is 10 ×\nmore carbon emission. Note that this comparison is not fair to them, since they also train a text encoder in\nparallel, and we thus do not report them in the table. However, it gives a reasonable guidelines for those who\nare interested in training only visual features: in this context, training a self-supervised model is preferable\nin terms of carbon emission. Training a text-guided model still makes sense when planning to reuse the text\nencoder.\nCarbon footprint of the whole project. Additionally, we estimate the footprint of the whole project\nto be between 0.5k and 1k tCO 2eq using the same grid as presented above. This carbon footprint represents\nin the order of 200k GPU-days. The primary sources of emissions are the self-supervised pre-trainings of\nthe models. For example, a single pre-training of a ViT-g model (22k GPU-hours) emits 3.7 tons of CO 2eq,\nwhile a ﬁnetuning on ImageNet-1k (1k GPU-hours) emits 0.2 tons. This estimate only considers the GPUs’\nelectricity consumption and ignores other emissions, such as their manufacturing and disposal.\n10 Future work and Discussion\nIn this work, we present DINOv2, a new series of image encoders pretrained on large curated data with no\nsupervision. This is the ﬁrst SSL work on image data that leads to visual features that close the performance\ngap with (weakly) supervised alternatives across a wide range of benchmarks and without the need for\nﬁnetuning. A few properties emerge from these models, such as an understanding of object parts and scene\ngeometry regardless of the image domains. We expect that more of these properties will emerge at larger\nscales of models and data, akin to instruction emergence in large language models, and plan to continue\nscaling along these axes. This paper also demonstrates that these visual features are compatible with\nclassiﬁers as simple as linear layers - meaning the underlying information is readily available . In future work,\nwe plan to leverage this ability to train a a language-enabled AI system that can process visual features as\nif they were word tokens, and extract the required information to ground the system.\nAcknowledgments.\nWe thank Mathilde Caron for initial discussions that led to this work. We thank Olivia Joulin for the horse\ndrawing used in Fig. 10. We also thank the rest of FAIR and Meta AI for feedback on this work through\nthe entire project.\nReferences\nYuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous clustering\nand representation learning. In ICLR, 2020.\nMahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand\nJoulin, Michael Rabbat, and Nicolas Ballas. Masked siamese networks for label-eﬃcient learning. arXiv\npreprint arXiv:2204.07141 , 2022.\nMahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann\nLeCun, and Nicolas Ballas. Self-supervised learning from images with a joint-embedding predictive archi-\ntecture.arXiv preprint arXiv:2301.08243 , 2023.\nAlexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. Data2vec: A general\nframework for self-supervised learning in speech, vision and language. arXiv preprint arXiv:2202.03555 ,\n2022.\nHangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint\narXiv:2106.08254 , 2021.\n20', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}]: %s
2023-12-07 10:17:41,687 - INFO - Check the data that is being passed [{'page_content': 'Jan Beirlant, Edward J Dudewicz, László Györﬁ, Edward C Van der Meulen, et al. Nonparametric entropy\nestimation: An overview. International Journal of Mathematical and Statistical Sciences , 6(1):17–39, 1997.\nMaxim Berman, Hervé Jégou, Vedaldi Andrea, Iasonas Kokkinos, and Matthijs Douze. MultiGrain: a uniﬁed\nimage embedding for classes and instances. arXiv preprint arXiv:1902.05509 , 2019.\nLucas Beyer, Olivier J Hénaﬀ, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord. Are we done\nwith imagenet? arXiv preprint arXiv:2006.07159 , 2020.\nShariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. AdaBins: Depth estimation using adaptive\nbins. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . IEEE, jun\n2021. doi: 10.1109/cvpr46437.2021.00400. URL https://doi.org/10.1109%2Fcvpr46437.2021.00400 .\nPiotr Bojanowski and Armand Joulin. Unsupervised learning by predicting noise. In ICML, 2017.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of\nfoundation models. arXiv preprint arXiv:2108.07258 , 2021.\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative components\nwith random forests. In European Conference on Computer Vision , 2014.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\npreprint arXiv:2005.14165 , 2020.\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised\nlearning of visual features. In ECCV, 2018.\nMathilde Caron, Piotr Bojanowski, Julien Mairal, and Armand Joulin. Unsupervised pre-training of image\nfeatures on non-curated data. In ICCV, 2019.\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsuper-\nvised learning of visual features by contrasting cluster assignments. In NeurIPS , 2020.\nMathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand\nJoulin. Emerging properties in self-supervised vision transformers. arXiv preprint arXiv:2104.14294 , 2021.\nLiang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroﬀ, and Hartwig Adam. Encoder-decoder\nwith atrous separable convolution for semantic image segmentation. In Proceedings of the European con-\nference on computer vision (ECCV) , pp. 801–818, 2018.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoﬀrey Hinton. A simple framework for contrastive\nlearning of visual representations. preprint arXiv:2002.05709 , 2020.\nXiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi\nDong, Thang Luong, Cho-Jui Hsieh, et al. Symbolic discovery of optimization algorithms. arXiv preprint\narXiv:2302.06675 , 2023.\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. preprint arXiv:2011.10566 ,\n2020.\nXinleiChen, SainingXie, andKaimingHe. Anempiricalstudyoftrainingself-supervisedvisiontransformers.\narXiv preprint arXiv:2104.02057 , 2021.\nZhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer\nadapter for dense predictions. arXiv preprint arXiv:2205.08534 , 2022.\nBowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention\nmask transformer for universal image segmentation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pp. 1290–1299, 2022.\n21', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language\nmodeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.\nM. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In\nProceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , 2014.\nMarius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,\nUwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understand-\ning. InProceedings of the IEEE conference on computer vision and pattern recognition , pp. 3213–3223,\n2016.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-\neﬃcient exact attention with io-awareness. arXiv preprint arXiv:2205.14135 , 2022.\nTerrance De Vries, Ishan Misra, Changhan Wang, and Laurens Van der Maaten. Does object recognition\nworkforeveryone? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition\nworkshops , pp. 52–59, 2019.\nSylvain Delattre and Nicolas Fournier. On the kozachenko–leonenko entropy estimator. Journal of Statistical\nPlanning and Inference , 185:69–93, 2017.\nJacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert: Pre-trainingofdeepbidirectional\ntransformers for language understanding. preprint arXiv:1810.04805 , 2018.\nJosip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander Kolesnikov,\nJoan Puigcerver, Matthias Minderer, Alexander D’Amour, Dan Moldovan, et al. On robustness and\ntransferabilityofconvolutionalneuralnetworks. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pp. 16458–16468, 2021.\nCarl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context\nprediction. In ICCV, 2015.\nAlexey Dosovitskiy, Jost Tobias Springenberg, Martin A. Riedmiller, and Thomas Brox. Discriminative\nunsupervised feature learning with convolutional neural networks. CoRR, abs/1406.6909, 2014. URL\nhttp://arxiv.org/abs/1406.6909 .\nAlexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Dis-\ncriminative unsupervised feature learning with exemplar convolutional neural networks. TPAMI, 2016.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-\nterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. preprint arXiv:2010.11929 , 2020.\nMatthijs Douze, Hervé Jégou, Harsimrat Sandhawalia, Laurent Amsaleg, and Cordelia Schmid. Evaluation\nof gist descriptors for web-scale image search. In CIVR, 2009.\nQuentin Duval, Ishan Misra, and Nicolas Ballas. A simple recipe for competitive low-compute self supervised\nvision models. arXiv preprint arXiv:2301.09451 , 2023.\nAlaaeldin El-Nouby, Gautier Izacard, Hugo Touvron, Ivan Laptev, Hervé Jegou, and Edouard Grave. Are\nlarge-scale datasets necessary for self-supervised pre-training? arXiv preprint arXiv:2112.10740 , 2021.\nM. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal\nvisual object classes challenge: A retrospective. International Journal of Computer Vision , 111(1):98–136,\nJanuary 2015.\nYuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang,\nand Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. 2023.\n22', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples:\nAn incremental bayesian approach tested on 101 object categories. In 2004 conference on computer vision\nand pattern recognition workshop , pp. 178–178. IEEE, 2004.\nAndreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti\ndataset. The International Journal of Robotics Research , 32(11):1231–1237, 2013.\nSpyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting\nimage rotations, 2018.\nRohit Girdhar, Alaaeldin El-Nouby, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Is-\nhan Misra. Omnimae: Single model masked pretraining on images and videos. arXiv preprint\narXiv:2206.08356 , 2022.\nPriya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. Scaling and benchmarking self-supervised\nvisual representation learning. In ICCV, 2019.\nPriya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat Singh,\nVitaliy Liptchinsky, Ishan Misra, Armand Joulin, et al. Self-supervised pretraining of visual features in\nthe wild. preprint arXiv:2103.01988 , 2021.\nPriya Goyal, Quentin Duval, Isaac Seessel, Mathilde Caron, Mannat Singh, Ishan Misra, Levent Sagun,\nArmand Joulin, and Piotr Bojanowski. Vision models are more robust and fair when pretrained on\nuncurated images without supervision. arXiv preprint arXiv:2202.08360 , 2022a.\nPriya Goyal, Adriana Romero Soriano, Caner Hazirbas, Levent Sagun, and Nicolas Usunier. Fairness in-\ndicators for systematic assessments of visual feature extractors. In 2022 ACM Conference on Fairness,\nAccountability, and Transparency , pp. 70–88, 2022b.\nRaghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, He-\nuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The" something\nsomething" video database for learning and evaluating visual common sense. In Proceedings of the IEEE\ninternational conference on computer vision , pp. 5842–5850, 2017.\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya,\nCarl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray\nKavukcuoglu, Rémi Munos, and Michal Valko. Bootstrap your own latent: A new approach to self-\nsupervised learning. In NeurIPS , 2020.\nRaia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping.\nInCVPR, 2006.\nCanerHazirbas, JoannaBitton, BrianDolhansky, JacquelinePan, AlbertGordo, andCristianCantonFerrer.\nTowards measuring fairness in ai: the casual conversations dataset. IEEE Transactions on Biometrics,\nBehavior, and Identity Science , 4(3):324–332, 2021.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised\nvisual representation learning. In CVPR, 2020.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders\nare scalable vision learners. arXiv preprint arXiv:2111.06377 , 2021.\nOlivierJHénaﬀ, AravindSrinivas, JeﬀreyDeFauw, AliRazavi, CarlDoersch, SMEslami, andAaronvanden\nOord. Data-eﬃcientimagerecognitionwithcontrastivepredictivecoding. preprint arXiv:1905.09272 ,2019.\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions\nand perturbations. In International Conference on Learning Representations , 2019.\n23', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai,\nTyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-\nof-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pp. 8340–8349, 2021.\nGeoﬀrey Hinton, Oriol Vinyals, and Jeﬀ Dean. Distilling the knowledge in a neural network. preprint\narXiv:1503.02531 , 2015.\nJordan Hoﬀmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal\nlarge language models. arXiv preprint arXiv:2203.15556 , 2022.\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochas-\ntic depth. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands,\nOctober 11–14, 2016, Proceedings, Part IV 14 , pp. 646–661. Springer, 2016.\nGabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal\nDave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig\nSchmidt. Openclip. 2021.\nHerve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. IEEE\ntransactions on pattern analysis and machine intelligence , 33(1), 2010.\nJeﬀ Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with GPUs. IEEE Transac-\ntions on Big Data , 7(3):535–547, 2019.\nArmand Joulin, Laurens Van Der Maaten, Allan Jabri, and Nicolas Vasilache. Learning visual features from\nlarge weakly supervised data. In ECCV, 2016.\nWill Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio\nViola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv\npreprint arXiv:1705.06950 , 2017.\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for ﬁne-grained catego-\nrization. In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13) , Sydney,\nAustralia, 2013.\nAlex Krizhevsky, Geoﬀrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\nBenjaminLefaudeux, FranciscoMassa, DianaLiskovich, WenhanXiong, VittorioCaggiano, SeanNaren, Min\nXu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza. xformers: A modular\nand hackable transformer modelling library. https://github.com/facebookresearch/xformers , 2022.\nChunyuan Li, Jianwei Yang, Pengchuan Zhang, Mei Gao, Bin Xiao, Xiyang Dai, Lu Yuan, and Jianfeng\nGao. Eﬃcient self-supervised vision transformers for representation learning. In ICLR, 2022a.\nZhenyu Li, Xuyang Wang, Xianming Liu, and Junjun Jiang. Binsformer: Revisiting adaptive bins for\nmonocular depth estimation. arXiv preprint arXiv:2204.00987 , 2022b.\nHuajun Liu, Fuqiang Liu, Xinyi Fan, and Dong Huang. Polarized self-attention: towards high-quality pixel-\nwise regression. arXiv preprint arXiv:2107.00782 , 2021.\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin\nBharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In ECCV,\n2018.\nS. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classiﬁcation of aircraft.\nTechnical report, 2013.\n24', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. In\nCVPR, 2020.\nMaria-Elena Nilsback and Andrew Zisserman. Automated ﬂower classiﬁcation over a large number of classes.\nInIndian Conference on Computer Vision, Graphics and Image Processing , Dec 2008.\nMehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles.\nIn Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (eds.), Computer Vision – ECCV 2016 , pp.\n69–84, Cham, 2016. Springer International Publishing. ISBN 978-3-319-46466-4.\nOmkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In IEEE\nConference on Computer Vision and Pattern Recognition , 2012.\nDeepak Pathak, Philipp Krähenbühl, Jeﬀ Donahue, Trevor Darrell, and Alexei Efros. Context encoders:\nFeature learning by inpainting. In CVPR, 2016.\nDavid Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David\nSo, Maud Texier, and Jeﬀ Dean. Carbon emissions and large neural network training. arXiv preprint\narXiv:2104.10350 , 2021.\nEd Pizzi, Sreya Dutta Roy, Sugosh Nagavara Ravindra, Priya Goyal, and Matthijs Douze. A self-supervised\ndescriptor for image copy detection. arXiv preprint arXiv:2202.10261 , 2022.\nFilip Radenović, Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondřej Chum. Revisiting oxford and\nparis: Large-scale image retrieval benchmarking. In CVPR, 2018a.\nFilip Radenović, Giorgos Tolias, and Ondřej Chum. Fine-tuning cnn image retrieval with no human anno-\ntation.IEEE transactions on pattern analysis and machine intelligence , 2018b.\nAlec Radford, Jeﬀrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models\nare unsupervised multitask learners.\nAlec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to generate reviews and discovering sentiment.\narXiv preprint arXiv:1704.01444 , 2017.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In International Conference on Machine Learning , pp. 8748–8763. PMLR,\n2021.\nColin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer.\nJ. Mach. Learn. Res. , 21(140):1–67, 2020.\nRené Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Pro-\nceedings of the IEEE/CVF International Conference on Computer Vision , pp. 12179–12188, 2021.\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classiﬁers generalize\nto imagenet? In International Conference on Machine Learning , pp. 5389–5400. PMLR, 2019.\nJerome Revaud, Jon Almazán, Rafael S Rezende, and Cesar Roberto de Souza. Learning with average\nprecision: Training image retrieval with a listwise loss. In ICCV, 2019.\nYangjun Ruan, Saurabh Singh, Warren Morningstar, Alexander A Alemi, Sergey Ioﬀe, Ian Fischer, and\nJoshua V Dillon. Weighted ensemble self-supervised learning. arXiv preprint arXiv:2211.09981 , 2022.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej\nKarpathy, Aditya Khosla, Michael Bernstein, Alexander C Berg, and Li Fei-Fei. Imagenet large scale\nvisual recognition challenge. IJCV, 2015.\n25', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, and Hervé Jégou. Spreading vectors for similarity\nsearch.arXiv preprint arXiv:1806.03198 , 2018.\nNoam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202 , 2020.\nNathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support\ninference from rgbd images. In European conference on computer vision , pp. 746–760. Springer, 2012.\nMannat Singh, Laura Gustafson, Aaron Adcock, Vinicius de Freitas Reis, Bugra Gedik, Raj Prateek\nKosaraju, Dhruv Mahajan, Ross Girshick, Piotr Dollár, and Laurens van der Maaten. Revisiting Weakly\nSupervised Pre-Training of Visual Perception Models. In CVPR, 2022.\nShuranSong, SamuelPLichtenberg, andJianxiongXiao. Sunrgb-d: Argb-dsceneunderstandingbenchmark\nsuite. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 567–576,\n2015.\nKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes\nfrom videos in the wild. arXiv preprint arXiv:1212.0402 , 2012.\nAndreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer.\nHow to train your vit? data, augmentation, and regularization in vision transformers. arXiv preprint\narXiv:2106.10270 , 2021.\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning\nin nlp.arXiv preprint arXiv:1906.02243 , 2019.\nYonglong Tian, Olivier J Henaﬀ, and Aäron van den Oord. Divide and contrast: Self-supervised learning\nfrom uncurated data. In Proceedings of the IEEE/CVF International Conference on Computer Vision ,\npp. 10063–10074, 2021.\nGiorgos Tolias, Ronan Sicre, and Hervé Jégou. Particular object retrieval with integral max-pooling of cnn\nactivations. arXiv preprint arXiv:1511.05879 , 2015.\nZhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-eﬃcient\nlearners for self-supervised video pre-training. arXiv preprint arXiv:2203.12602 , 2022.\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Hervé Jégou. Fixing the train-test resolution discrep-\nancy. In NeurIPS , 2019.\nHugo Touvron, Matthieu Cord, and Hervé Jégou. Deit iii: Revenge of the vit. arXiv preprint\narXiv:2204.07118 , 2022.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. Llama: Open and eﬃcient foundation language models. arXiv preprint\narXiv:2302.13971 , 2023.\nGrant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro\nPerona, and Serge Belongie. The inaturalist species classiﬁcation and detection dataset. In CVPR, 2018.\nGrant Van Horn, Elijah Cole, Sara Beery, Kimberly Wilber, Serge Belongie, and Oisin Mac Aodha. Bench-\nmarking representation learning for natural world image collections. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition , pp. 12884–12893, 2021.\nWenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei\nLu, Hongsheng Li, et al. Internimage: Exploring large-scale vision foundation models with deformable\nconvolutions. arXiv preprint arXiv:2211.05778 , 2022.\nXiaolong Wang, Allan Jabri, and Alexei A Efros. Learning correspondence from the cycle-consistency of\ntime. In CVPR, 2019.\n26', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Philippe Weinzaepfel, Thomas Lucas, Diane Larlus, and Yannis Kalantidis. Learning super-features for\nimage retrieval. In International Conference on Learning Representations , 2021.\nP. Welinder, S. Branson, T. Mita, C. Wah, F. Schroﬀ, S. Belongie, and P. Perona. Caltech-UCSD Birds 200.\nTechnical Report CNS-TR-2010-001, California Institute of Technology, 2010.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand\nJoulin, and Edouard Grave. Ccnet: Extracting high quality monolingual datasets from web crawl data.\narXiv preprint arXiv:1911.00359 , 2019.\nZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric\ninstance discrimination. In CVPR, 2018.\nJ. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recognition from\nabbey to zoo. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition ,\npp. 3485–3492, June 2010. doi: 10.1109/CVPR.2010.5539970.\nHuXu,JunchengLi,AlexeiBaevski,MichaelAuli,WojciechGaluba,FlorianMetze,ChristophFeichtenhofer,\net al. Masked autoencoders that listen. arXiv preprint arXiv:2207.06405 , 2022.\nI Zeki Yalniz, Hervé Jégou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. Billion-scale semi-supervised\nlearning for image classiﬁcation. arXiv preprint arXiv:1905.00546 , 2019.\nBurak Yildiz, Seyran Khademi, Ronald Maria Siebes, and Jan van Gemert. Amstertime: A visual place\nrecognition benchmark dataset for severe domain shift. arXiv preprint arXiv:2203.16291 , 2022.\nNikolaos-Antonios Ypsilantis, Noa Garcia, Guangxing Han, Sarah Ibrahimi, Nanne Van Noord, and Giorgos\nTolias. The met dataset: Instance-level recognition for artworks. In Thirty-ﬁfth Conference on Neural\nInformation Processing Systems Datasets and Benchmarks Track (Round 2) , 2021.\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 12104–12113,\n2022.\nRichard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016.\nBolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep features\nfor scene recognition using places database. In NeurIPS , 2014.\nBolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing\nthroughade20kdataset. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npp. 633–641, 2017.\nJinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert\npre-training with online tokenizer. arXiv preprint arXiv:2111.07832 , 2021.\nPan Zhou, Yichen Zhou, Chenyang Si, Weihao Yu, Teck Khim Ng, and Shuicheng Yan. Mugs: A multi-\ngranular self-supervised learning framework. arXiv preprint arXiv:2203.14415 , 2022.\n27', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'A Data Processing\nA.1 Data selection\nOur selection of datasets for building LVD-142M is detailed in Tab. 15. This collection is intended to provide\nimages covering well various downstream vision tasks both for image-level and dense recognition.\nA.2 Image similarity\nWe employ cosine similarity to compare image features (whether ours or feature generated for deduplication)\nwith the following similarity function m:\nm(s, r) =cosine-similarity (f(s), f(r)) =f(s).f(r)\n∥f(s)∥2∥f(r)∥2\nwhere sandrare a pair of images to compare and fis the model generating features.\nA.3 Deduplication\nSelf-deduplication. To deduplicate our uncurated data source of 1.3B images, we compute and use the\nembeddings generated by Pizzi et al. (2022) and retrieve the k= 64nearest neighbors of each image (using\ncosine similarity). Considering only neighbors with a similarity >0.6, we extract the connected components\nof the associated k-NN graph thanks to a scalable disjoint set data structure implementation. We then only\nkeep one representative for each component of duplicate images. This results in a self-deduplicated data\nsource of 1.1B images.\nRelativededuplication Toreduceredundancyandalsoproperlyevaluatetheperformanceofourfeatures,\nwe discard remaining images of our self-deduplicated data source that are too similar to train and test splits\nof our evaluation datasets. To achieve this, we apply a similar procedure as for self-deduplication, with a\nstricter similarity >0.45, this time identifying the duplicate components (if any) to which each reference\nimage belong and discarding it entirely. This results in a self- and relatively-deduplicated data source of\n744M images.\nA.4 Retrieval\nWe employ two approaches to augment dataset via retrieval: sample-based and cluster-based. The ﬁrst one,\nsample-based, applies to datasets larger than 1M images and consists in collecting a ﬁxed number kof nearest\nimages for each sample image of the dataset to retrieve, eﬀectively trying to multiply by kthe size of the\ndataset. We use k= 4for Google Landmarks v2 and ImageNet-22k but a larger k= 32to make this speciﬁc\nretrieval a core part of our LVD-142M dataset. For smaller datasets, the second approach, cluster-based,\nconsists in ﬁrst clustering our uncurated data source into 100,000separate clusters thanks to a distributed\nk-means implementation. Each cluster should capture diﬀerent types of image concept and contents. We\nthen pick 10,000images from each cluster associated with more than 3images of the retrieved dataset. As\nthis can result in a very large number of retrieved images for some dataset, we restrict such retrievals to a\nmaximum of 1M images to maintain the balance between the diﬀerent datasets within LVD-142M.\nB Implementation Details\nB.1 Unsupervised pre-training\nFor unsupervised pre-training we build on the DINO and iBOT codebases. We use hyperparameters shown\nin Table 16, ViT architectures described in Table 17.\nKoLeo regularization. We apply the KoLeo regularizer with a weight of 0.1 between the class tokens of\nthe ﬁrst global crop, for all samples within a GPU without cross-communication for this step.\n28', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Task Dataset / Split Images Retrieval Retrieved Final\nclassiﬁcation ImageNet-22k / – 14,197,086 as is – 14,197,086\nclassiﬁcation ImageNet-22k / – 14,197,086 sample 56,788,344 56,788,344\nclassiﬁcation ImageNet-1k / train 1,281,167 sample 40,997,344 40,997,344\nﬁne-grained classif. Caltech 101 / train 3,030 cluster 2,630,000 1,000,000\nﬁne-grained classif. CUB-200-2011 / train 5,994 cluster 1,300,000 1,000,000\nﬁne-grained classif. DTD / train1 1,880 cluster 1,580,000 1,000,000\nﬁne-grained classif. FGVC-Aircraft / train 3,334 cluster 1,170,000 1,000,000\nﬁne-grained classif. Flowers-102 / train 1,020 cluster 1,060,000 1,000,000\nﬁne-grained classif. Food-101 / train 75,750 cluster 21,670,000 1,000,000\nﬁne-grained classif. Oxford-IIIT Pet / trainval 3,680 cluster 2,750,000 1,000,000\nﬁne-grained classif. Stanford Cars / train 8,144 cluster 7,220,000 1,000,000\nﬁne-grained classif. SUN397 / train1 19,850 cluster 18,950,000 1,000,000\nﬁne-grained classif. Pascal VOC 2007 / train 2,501 cluster 1,010,000 1,000,000\nsegmentation ADE20K / train 20,210 cluster 20,720,000 1,000,000\nsegmentation Cityscapes / train 2,975 cluster 1,390,000 1,000,000\nsegmentation Pascal VOC 2012 (seg.) / trainaug 1,464 cluster 10,140,000 1,000,000\ndepth estimation Mapillary SLS / train 1,434,262 as is – 1,434,262\ndepth estimation KITTI / train (Eigen) 23,158 cluster 3,700,000 1,000,000\ndepth estimation NYU Depth V2 / train 24,231 cluster 10,850,000 1,000,000\ndepth estimation SUN RGB-D / train 4,829 cluster 4,870,000 1,000,000\nretrieval Google Landmarks v2 / train (clean) 1,580,470 as is – 1,580,470\nretrieval Google Landmarks v2 / train (clean) 1,580,470 sample 6,321,880 6,321,880\nretrieval AmsterTime / new 1,231 cluster 960,000 960,000\nretrieval AmsterTime / old 1,231 cluster 830,000 830,000\nretrieval Met / train 397,121 cluster 62,860,000 1,000,000\nretrieval Revisiting Oxford / base 4,993 cluster 3,680,000 1,000,000\nretrieval Revisiting Paris / base 6,322 cluster 3,660,000 1,000,000\n142,109,386\nTable 15: Composition of our LVD-142M dataset. We report the list of datasets and associated splits\nused to build the dataset, how they were included (as is without retrieval or via sample-based or cluster-based\nretrieval). For retrievals, we indicate the actual number of retrieved images and the ﬁnal number included\nin the dataset.\n29', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Arch. Drop-rate LR Batch size\nDINOv2-S (distilled) ViT-S/14 0 1e-3 2048\nDINOv2-B (distilled) ViT-B/14 0 1e-3 2048\nDINOv2-L (distilled) ViT-L/14 0 1e-3 2048\nDINOv2-L (from scratch) ViT-L/14 0.4 3.5e-4 3072\nDINOv2-g (from scratch) ViT-g/14 0.4 3.5e-4 3072\nTable 16: Training hyperparameters for DINOv2-S, DINOv2-B, DINOv2-L and DINOv2-g. All\nmodels run for 625k iterations with optimizer AdamW, an initial LayerScale value of 1e-5, a weight decay\ncosine schedule from 0.04 to 0.2, a learning rate warmup of 100k iterations, a teacher momentum cosine\nschedule from 0.994 to 1, and we train in ﬂoat16 precision in all cases (except for the DINO heads where we\nreduce the gradients in ﬂoat32).\nArch. Embed dim Heads Blocks FFN layer\nViT-S/14 (distilled) 384 6 12 MLP\nViT-B/14 (distilled) 768 12 18 MLP\nViT-L/14 (distilled) 1024 16 24 MLP\nViT-L/14 (from scratch) 1024 16 24 SwiGLU\nViT-g/14 (from scratch) 1536 24 40 SwiGLU\nTable 17: Architecture details of the ViT-S/B/L/g networks used in this work. We use MLP\nfeed-forward networks for distilled models, and SwiGLU (Shazeer, 2020) when training from scratch.\nEMA update for the teacher. The teacher is initialized with the same state as the student, and is an\nexponential moving average of the student network, with a momentum value in [0.994, 1.0] following a cosine\nschedule. It is updated at the end of every training step.\nB.2 High-Resolution adaptation\nWe initialise the model with the pretrained weights then train it for 10k iterations with the same procedure\nas the original pretraining. All the schedules are kept the same as in the original training, but compressed\nto ﬁt in 10k iterations. All the hyperparameters are kept the same as in the ﬁrst pretraining, except the\nbase learning rate which is reduced.\nB.3 Linear probing evaluation\nFor linear probing we deﬁne 3 evaluation parameters: the learning rate, how many output layers we use,\nwhether we concatenate the average-pooled patch token features with the class token (or use only the\nclass token). We train our linear layer with SGD for 12500 iterations, using random-resized-crop data\naugmentation, and perform the following grid search:\n•learning rate in{0.0001,0.0002,0.0005,0.001,0.002,0.005,0.01,0.02,0.05,0.1,0.2,0.3,0.5}\n•output layers in{1,4}\n•concatenate average-pooled tokens in {yes, no}\nWe then report the highest accuracy value obtained on the validation set as is common practice. Note that\nthis grid search is not expensive, because at each iteration we perform inference on the backbone only once,\nthen feed the output to all linear classiﬁers (each performing a single matrix multiplication).\nC List of benchmarks used for evaluations\nWe show in Table 18 the list of benchmarks and datasets used for evaluation.\n30', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Dataset name Task Citation\nImageNet-1k Image Classiﬁcation (Russakovsky et al., 2015)\nImageNet-V2 Image Classiﬁcation (Recht et al., 2019)\nImageNet-ReaL Image Classiﬁcation (Beyer et al., 2020)\nImageNet-A Image Classiﬁcation (Djolonga et al., 2021)\nImageNet-C Image Classiﬁcation (Hendrycks & Dietterich, 2019)\nImageNet-Rendition Image Classiﬁcation (Hendrycks et al., 2021)\nImageNet-Sketch Image Classiﬁcation (Wang et al., 2019)\nFood-101 Image Classiﬁcation (Bossard et al., 2014)\nCIFAR-10 Image Classiﬁcation (Krizhevsky et al., 2009)\nCIFAR-100 Image Classiﬁcation (Krizhevsky et al., 2009)\nSUN397 Image Classiﬁcation (Xiao et al., 2010)\nStanfordCars Image Classiﬁcation (Krause et al., 2013)\nFGVC-Aircraft Image Classiﬁcation (Maji et al., 2013)\nPascal VOC 2007 Image Classiﬁcation (Everingham et al., 2015)\nDescribable Textures Image Classiﬁcation (Cimpoi et al., 2014)\nOxford Pets Image Classiﬁcation (Parkhi et al., 2012)\nCaltech101 Image Classiﬁcation (Fei-Fei et al., 2004)\nOxford Flowers Image Classiﬁcation (Nilsback & Zisserman, 2008)\nCUB200 Image Classiﬁcation (Welinder et al., 2010)\niNaturalist 2018 Image Classiﬁcation (Van Horn et al., 2018)\niNaturalist 2021 Image Classiﬁcation (Van Horn et al., 2021)\nPlaces-205 Image Classiﬁcation (Zhou et al., 2014)\nUCF101 Video Classiﬁcation (Soomro et al., 2012)\nKinetics-400 Video Classiﬁcation (Kay et al., 2017)\nSomething-Something-V2 Video Classiﬁcation (Goyal et al., 2017)\nRevisiting-Paris Image Retrieval (Radenović et al., 2018a)\nRevisiting-Oxford Image Retrieval (Radenović et al., 2018a)\nMet Image Retrieval (Ypsilantis et al., 2021)\nAmstertime Image Retrieval (Yildiz et al., 2022)\nADE20k Image Segmentation (Zhou et al., 2017)\nCityscapes Image Segmentation (Cordts et al., 2016)\nPascal VOC 2012 Image Segmentation (Everingham et al., 2015)\nNYU-Depth V2 Monocular Depth Estimation (Silberman et al., 2012)\nKITTI Monocular Depth Estimation (Geiger et al., 2013)\nSUN-RGBD Monocular Depth Estimation (Song et al., 2015)\nDollarStreet Fairness Analysis (De Vries et al., 2019)\nCasual Conversations Fairness Analysis (Hazirbas et al., 2021)\nTable 18: List of datasets used for evaluation.\n31', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}]: %s
2023-12-07 10:17:41,687 - INFO - Check the results [{'page_content': 'Jan Beirlant, Edward J Dudewicz, László Györﬁ, Edward C Van der Meulen, et al. Nonparametric entropy\nestimation: An overview. International Journal of Mathematical and Statistical Sciences , 6(1):17–39, 1997.\nMaxim Berman, Hervé Jégou, Vedaldi Andrea, Iasonas Kokkinos, and Matthijs Douze. MultiGrain: a uniﬁed\nimage embedding for classes and instances. arXiv preprint arXiv:1902.05509 , 2019.\nLucas Beyer, Olivier J Hénaﬀ, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord. Are we done\nwith imagenet? arXiv preprint arXiv:2006.07159 , 2020.\nShariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. AdaBins: Depth estimation using adaptive\nbins. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . IEEE, jun\n2021. doi: 10.1109/cvpr46437.2021.00400. URL https://doi.org/10.1109%2Fcvpr46437.2021.00400 .\nPiotr Bojanowski and Armand Joulin. Unsupervised learning by predicting noise. In ICML, 2017.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of\nfoundation models. arXiv preprint arXiv:2108.07258 , 2021.\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative components\nwith random forests. In European Conference on Computer Vision , 2014.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\npreprint arXiv:2005.14165 , 2020.\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised\nlearning of visual features. In ECCV, 2018.\nMathilde Caron, Piotr Bojanowski, Julien Mairal, and Armand Joulin. Unsupervised pre-training of image\nfeatures on non-curated data. In ICCV, 2019.\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsuper-\nvised learning of visual features by contrasting cluster assignments. In NeurIPS , 2020.\nMathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand\nJoulin. Emerging properties in self-supervised vision transformers. arXiv preprint arXiv:2104.14294 , 2021.\nLiang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroﬀ, and Hartwig Adam. Encoder-decoder\nwith atrous separable convolution for semantic image segmentation. In Proceedings of the European con-\nference on computer vision (ECCV) , pp. 801–818, 2018.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoﬀrey Hinton. A simple framework for contrastive\nlearning of visual representations. preprint arXiv:2002.05709 , 2020.\nXiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi\nDong, Thang Luong, Cho-Jui Hsieh, et al. Symbolic discovery of optimization algorithms. arXiv preprint\narXiv:2302.06675 , 2023.\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. preprint arXiv:2011.10566 ,\n2020.\nXinleiChen, SainingXie, andKaimingHe. Anempiricalstudyoftrainingself-supervisedvisiontransformers.\narXiv preprint arXiv:2104.02057 , 2021.\nZhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer\nadapter for dense predictions. arXiv preprint arXiv:2205.08534 , 2022.\nBowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention\nmask transformer for universal image segmentation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pp. 1290–1299, 2022.\n21', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language\nmodeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.\nM. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In\nProceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , 2014.\nMarius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,\nUwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understand-\ning. InProceedings of the IEEE conference on computer vision and pattern recognition , pp. 3213–3223,\n2016.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-\neﬃcient exact attention with io-awareness. arXiv preprint arXiv:2205.14135 , 2022.\nTerrance De Vries, Ishan Misra, Changhan Wang, and Laurens Van der Maaten. Does object recognition\nworkforeveryone? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition\nworkshops , pp. 52–59, 2019.\nSylvain Delattre and Nicolas Fournier. On the kozachenko–leonenko entropy estimator. Journal of Statistical\nPlanning and Inference , 185:69–93, 2017.\nJacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert: Pre-trainingofdeepbidirectional\ntransformers for language understanding. preprint arXiv:1810.04805 , 2018.\nJosip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander Kolesnikov,\nJoan Puigcerver, Matthias Minderer, Alexander D’Amour, Dan Moldovan, et al. On robustness and\ntransferabilityofconvolutionalneuralnetworks. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pp. 16458–16468, 2021.\nCarl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context\nprediction. In ICCV, 2015.\nAlexey Dosovitskiy, Jost Tobias Springenberg, Martin A. Riedmiller, and Thomas Brox. Discriminative\nunsupervised feature learning with convolutional neural networks. CoRR, abs/1406.6909, 2014. URL\nhttp://arxiv.org/abs/1406.6909 .\nAlexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Dis-\ncriminative unsupervised feature learning with exemplar convolutional neural networks. TPAMI, 2016.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-\nterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. preprint arXiv:2010.11929 , 2020.\nMatthijs Douze, Hervé Jégou, Harsimrat Sandhawalia, Laurent Amsaleg, and Cordelia Schmid. Evaluation\nof gist descriptors for web-scale image search. In CIVR, 2009.\nQuentin Duval, Ishan Misra, and Nicolas Ballas. A simple recipe for competitive low-compute self supervised\nvision models. arXiv preprint arXiv:2301.09451 , 2023.\nAlaaeldin El-Nouby, Gautier Izacard, Hugo Touvron, Ivan Laptev, Hervé Jegou, and Edouard Grave. Are\nlarge-scale datasets necessary for self-supervised pre-training? arXiv preprint arXiv:2112.10740 , 2021.\nM. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal\nvisual object classes challenge: A retrospective. International Journal of Computer Vision , 111(1):98–136,\nJanuary 2015.\nYuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang,\nand Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. 2023.\n22', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples:\nAn incremental bayesian approach tested on 101 object categories. In 2004 conference on computer vision\nand pattern recognition workshop , pp. 178–178. IEEE, 2004.\nAndreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti\ndataset. The International Journal of Robotics Research , 32(11):1231–1237, 2013.\nSpyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting\nimage rotations, 2018.\nRohit Girdhar, Alaaeldin El-Nouby, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Is-\nhan Misra. Omnimae: Single model masked pretraining on images and videos. arXiv preprint\narXiv:2206.08356 , 2022.\nPriya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. Scaling and benchmarking self-supervised\nvisual representation learning. In ICCV, 2019.\nPriya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat Singh,\nVitaliy Liptchinsky, Ishan Misra, Armand Joulin, et al. Self-supervised pretraining of visual features in\nthe wild. preprint arXiv:2103.01988 , 2021.\nPriya Goyal, Quentin Duval, Isaac Seessel, Mathilde Caron, Mannat Singh, Ishan Misra, Levent Sagun,\nArmand Joulin, and Piotr Bojanowski. Vision models are more robust and fair when pretrained on\nuncurated images without supervision. arXiv preprint arXiv:2202.08360 , 2022a.\nPriya Goyal, Adriana Romero Soriano, Caner Hazirbas, Levent Sagun, and Nicolas Usunier. Fairness in-\ndicators for systematic assessments of visual feature extractors. In 2022 ACM Conference on Fairness,\nAccountability, and Transparency , pp. 70–88, 2022b.\nRaghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, He-\nuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The" something\nsomething" video database for learning and evaluating visual common sense. In Proceedings of the IEEE\ninternational conference on computer vision , pp. 5842–5850, 2017.\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya,\nCarl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray\nKavukcuoglu, Rémi Munos, and Michal Valko. Bootstrap your own latent: A new approach to self-\nsupervised learning. In NeurIPS , 2020.\nRaia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping.\nInCVPR, 2006.\nCanerHazirbas, JoannaBitton, BrianDolhansky, JacquelinePan, AlbertGordo, andCristianCantonFerrer.\nTowards measuring fairness in ai: the casual conversations dataset. IEEE Transactions on Biometrics,\nBehavior, and Identity Science , 4(3):324–332, 2021.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised\nvisual representation learning. In CVPR, 2020.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders\nare scalable vision learners. arXiv preprint arXiv:2111.06377 , 2021.\nOlivierJHénaﬀ, AravindSrinivas, JeﬀreyDeFauw, AliRazavi, CarlDoersch, SMEslami, andAaronvanden\nOord. Data-eﬃcientimagerecognitionwithcontrastivepredictivecoding. preprint arXiv:1905.09272 ,2019.\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions\nand perturbations. In International Conference on Learning Representations , 2019.\n23', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai,\nTyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-\nof-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pp. 8340–8349, 2021.\nGeoﬀrey Hinton, Oriol Vinyals, and Jeﬀ Dean. Distilling the knowledge in a neural network. preprint\narXiv:1503.02531 , 2015.\nJordan Hoﬀmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal\nlarge language models. arXiv preprint arXiv:2203.15556 , 2022.\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochas-\ntic depth. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands,\nOctober 11–14, 2016, Proceedings, Part IV 14 , pp. 646–661. Springer, 2016.\nGabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal\nDave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig\nSchmidt. Openclip. 2021.\nHerve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. IEEE\ntransactions on pattern analysis and machine intelligence , 33(1), 2010.\nJeﬀ Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with GPUs. IEEE Transac-\ntions on Big Data , 7(3):535–547, 2019.\nArmand Joulin, Laurens Van Der Maaten, Allan Jabri, and Nicolas Vasilache. Learning visual features from\nlarge weakly supervised data. In ECCV, 2016.\nWill Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio\nViola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv\npreprint arXiv:1705.06950 , 2017.\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for ﬁne-grained catego-\nrization. In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13) , Sydney,\nAustralia, 2013.\nAlex Krizhevsky, Geoﬀrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\nBenjaminLefaudeux, FranciscoMassa, DianaLiskovich, WenhanXiong, VittorioCaggiano, SeanNaren, Min\nXu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza. xformers: A modular\nand hackable transformer modelling library. https://github.com/facebookresearch/xformers , 2022.\nChunyuan Li, Jianwei Yang, Pengchuan Zhang, Mei Gao, Bin Xiao, Xiyang Dai, Lu Yuan, and Jianfeng\nGao. Eﬃcient self-supervised vision transformers for representation learning. In ICLR, 2022a.\nZhenyu Li, Xuyang Wang, Xianming Liu, and Junjun Jiang. Binsformer: Revisiting adaptive bins for\nmonocular depth estimation. arXiv preprint arXiv:2204.00987 , 2022b.\nHuajun Liu, Fuqiang Liu, Xinyi Fan, and Dong Huang. Polarized self-attention: towards high-quality pixel-\nwise regression. arXiv preprint arXiv:2107.00782 , 2021.\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin\nBharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In ECCV,\n2018.\nS. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classiﬁcation of aircraft.\nTechnical report, 2013.\n24', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. In\nCVPR, 2020.\nMaria-Elena Nilsback and Andrew Zisserman. Automated ﬂower classiﬁcation over a large number of classes.\nInIndian Conference on Computer Vision, Graphics and Image Processing , Dec 2008.\nMehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles.\nIn Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (eds.), Computer Vision – ECCV 2016 , pp.\n69–84, Cham, 2016. Springer International Publishing. ISBN 978-3-319-46466-4.\nOmkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In IEEE\nConference on Computer Vision and Pattern Recognition , 2012.\nDeepak Pathak, Philipp Krähenbühl, Jeﬀ Donahue, Trevor Darrell, and Alexei Efros. Context encoders:\nFeature learning by inpainting. In CVPR, 2016.\nDavid Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David\nSo, Maud Texier, and Jeﬀ Dean. Carbon emissions and large neural network training. arXiv preprint\narXiv:2104.10350 , 2021.\nEd Pizzi, Sreya Dutta Roy, Sugosh Nagavara Ravindra, Priya Goyal, and Matthijs Douze. A self-supervised\ndescriptor for image copy detection. arXiv preprint arXiv:2202.10261 , 2022.\nFilip Radenović, Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondřej Chum. Revisiting oxford and\nparis: Large-scale image retrieval benchmarking. In CVPR, 2018a.\nFilip Radenović, Giorgos Tolias, and Ondřej Chum. Fine-tuning cnn image retrieval with no human anno-\ntation.IEEE transactions on pattern analysis and machine intelligence , 2018b.\nAlec Radford, Jeﬀrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models\nare unsupervised multitask learners.\nAlec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to generate reviews and discovering sentiment.\narXiv preprint arXiv:1704.01444 , 2017.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In International Conference on Machine Learning , pp. 8748–8763. PMLR,\n2021.\nColin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer.\nJ. Mach. Learn. Res. , 21(140):1–67, 2020.\nRené Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Pro-\nceedings of the IEEE/CVF International Conference on Computer Vision , pp. 12179–12188, 2021.\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classiﬁers generalize\nto imagenet? In International Conference on Machine Learning , pp. 5389–5400. PMLR, 2019.\nJerome Revaud, Jon Almazán, Rafael S Rezende, and Cesar Roberto de Souza. Learning with average\nprecision: Training image retrieval with a listwise loss. In ICCV, 2019.\nYangjun Ruan, Saurabh Singh, Warren Morningstar, Alexander A Alemi, Sergey Ioﬀe, Ian Fischer, and\nJoshua V Dillon. Weighted ensemble self-supervised learning. arXiv preprint arXiv:2211.09981 , 2022.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej\nKarpathy, Aditya Khosla, Michael Bernstein, Alexander C Berg, and Li Fei-Fei. Imagenet large scale\nvisual recognition challenge. IJCV, 2015.\n25', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, and Hervé Jégou. Spreading vectors for similarity\nsearch.arXiv preprint arXiv:1806.03198 , 2018.\nNoam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202 , 2020.\nNathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support\ninference from rgbd images. In European conference on computer vision , pp. 746–760. Springer, 2012.\nMannat Singh, Laura Gustafson, Aaron Adcock, Vinicius de Freitas Reis, Bugra Gedik, Raj Prateek\nKosaraju, Dhruv Mahajan, Ross Girshick, Piotr Dollár, and Laurens van der Maaten. Revisiting Weakly\nSupervised Pre-Training of Visual Perception Models. In CVPR, 2022.\nShuranSong, SamuelPLichtenberg, andJianxiongXiao. Sunrgb-d: Argb-dsceneunderstandingbenchmark\nsuite. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 567–576,\n2015.\nKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes\nfrom videos in the wild. arXiv preprint arXiv:1212.0402 , 2012.\nAndreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer.\nHow to train your vit? data, augmentation, and regularization in vision transformers. arXiv preprint\narXiv:2106.10270 , 2021.\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning\nin nlp.arXiv preprint arXiv:1906.02243 , 2019.\nYonglong Tian, Olivier J Henaﬀ, and Aäron van den Oord. Divide and contrast: Self-supervised learning\nfrom uncurated data. In Proceedings of the IEEE/CVF International Conference on Computer Vision ,\npp. 10063–10074, 2021.\nGiorgos Tolias, Ronan Sicre, and Hervé Jégou. Particular object retrieval with integral max-pooling of cnn\nactivations. arXiv preprint arXiv:1511.05879 , 2015.\nZhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-eﬃcient\nlearners for self-supervised video pre-training. arXiv preprint arXiv:2203.12602 , 2022.\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Hervé Jégou. Fixing the train-test resolution discrep-\nancy. In NeurIPS , 2019.\nHugo Touvron, Matthieu Cord, and Hervé Jégou. Deit iii: Revenge of the vit. arXiv preprint\narXiv:2204.07118 , 2022.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. Llama: Open and eﬃcient foundation language models. arXiv preprint\narXiv:2302.13971 , 2023.\nGrant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro\nPerona, and Serge Belongie. The inaturalist species classiﬁcation and detection dataset. In CVPR, 2018.\nGrant Van Horn, Elijah Cole, Sara Beery, Kimberly Wilber, Serge Belongie, and Oisin Mac Aodha. Bench-\nmarking representation learning for natural world image collections. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition , pp. 12884–12893, 2021.\nWenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei\nLu, Hongsheng Li, et al. Internimage: Exploring large-scale vision foundation models with deformable\nconvolutions. arXiv preprint arXiv:2211.05778 , 2022.\nXiaolong Wang, Allan Jabri, and Alexei A Efros. Learning correspondence from the cycle-consistency of\ntime. In CVPR, 2019.\n26', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Philippe Weinzaepfel, Thomas Lucas, Diane Larlus, and Yannis Kalantidis. Learning super-features for\nimage retrieval. In International Conference on Learning Representations , 2021.\nP. Welinder, S. Branson, T. Mita, C. Wah, F. Schroﬀ, S. Belongie, and P. Perona. Caltech-UCSD Birds 200.\nTechnical Report CNS-TR-2010-001, California Institute of Technology, 2010.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand\nJoulin, and Edouard Grave. Ccnet: Extracting high quality monolingual datasets from web crawl data.\narXiv preprint arXiv:1911.00359 , 2019.\nZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric\ninstance discrimination. In CVPR, 2018.\nJ. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recognition from\nabbey to zoo. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition ,\npp. 3485–3492, June 2010. doi: 10.1109/CVPR.2010.5539970.\nHuXu,JunchengLi,AlexeiBaevski,MichaelAuli,WojciechGaluba,FlorianMetze,ChristophFeichtenhofer,\net al. Masked autoencoders that listen. arXiv preprint arXiv:2207.06405 , 2022.\nI Zeki Yalniz, Hervé Jégou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. Billion-scale semi-supervised\nlearning for image classiﬁcation. arXiv preprint arXiv:1905.00546 , 2019.\nBurak Yildiz, Seyran Khademi, Ronald Maria Siebes, and Jan van Gemert. Amstertime: A visual place\nrecognition benchmark dataset for severe domain shift. arXiv preprint arXiv:2203.16291 , 2022.\nNikolaos-Antonios Ypsilantis, Noa Garcia, Guangxing Han, Sarah Ibrahimi, Nanne Van Noord, and Giorgos\nTolias. The met dataset: Instance-level recognition for artworks. In Thirty-ﬁfth Conference on Neural\nInformation Processing Systems Datasets and Benchmarks Track (Round 2) , 2021.\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 12104–12113,\n2022.\nRichard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016.\nBolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep features\nfor scene recognition using places database. In NeurIPS , 2014.\nBolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing\nthroughade20kdataset. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npp. 633–641, 2017.\nJinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert\npre-training with online tokenizer. arXiv preprint arXiv:2111.07832 , 2021.\nPan Zhou, Yichen Zhou, Chenyang Si, Weihao Yu, Teck Khim Ng, and Shuicheng Yan. Mugs: A multi-\ngranular self-supervised learning framework. arXiv preprint arXiv:2203.14415 , 2022.\n27', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'A Data Processing\nA.1 Data selection\nOur selection of datasets for building LVD-142M is detailed in Tab. 15. This collection is intended to provide\nimages covering well various downstream vision tasks both for image-level and dense recognition.\nA.2 Image similarity\nWe employ cosine similarity to compare image features (whether ours or feature generated for deduplication)\nwith the following similarity function m:\nm(s, r) =cosine-similarity (f(s), f(r)) =f(s).f(r)\n∥f(s)∥2∥f(r)∥2\nwhere sandrare a pair of images to compare and fis the model generating features.\nA.3 Deduplication\nSelf-deduplication. To deduplicate our uncurated data source of 1.3B images, we compute and use the\nembeddings generated by Pizzi et al. (2022) and retrieve the k= 64nearest neighbors of each image (using\ncosine similarity). Considering only neighbors with a similarity >0.6, we extract the connected components\nof the associated k-NN graph thanks to a scalable disjoint set data structure implementation. We then only\nkeep one representative for each component of duplicate images. This results in a self-deduplicated data\nsource of 1.1B images.\nRelativededuplication Toreduceredundancyandalsoproperlyevaluatetheperformanceofourfeatures,\nwe discard remaining images of our self-deduplicated data source that are too similar to train and test splits\nof our evaluation datasets. To achieve this, we apply a similar procedure as for self-deduplication, with a\nstricter similarity >0.45, this time identifying the duplicate components (if any) to which each reference\nimage belong and discarding it entirely. This results in a self- and relatively-deduplicated data source of\n744M images.\nA.4 Retrieval\nWe employ two approaches to augment dataset via retrieval: sample-based and cluster-based. The ﬁrst one,\nsample-based, applies to datasets larger than 1M images and consists in collecting a ﬁxed number kof nearest\nimages for each sample image of the dataset to retrieve, eﬀectively trying to multiply by kthe size of the\ndataset. We use k= 4for Google Landmarks v2 and ImageNet-22k but a larger k= 32to make this speciﬁc\nretrieval a core part of our LVD-142M dataset. For smaller datasets, the second approach, cluster-based,\nconsists in ﬁrst clustering our uncurated data source into 100,000separate clusters thanks to a distributed\nk-means implementation. Each cluster should capture diﬀerent types of image concept and contents. We\nthen pick 10,000images from each cluster associated with more than 3images of the retrieved dataset. As\nthis can result in a very large number of retrieved images for some dataset, we restrict such retrievals to a\nmaximum of 1M images to maintain the balance between the diﬀerent datasets within LVD-142M.\nB Implementation Details\nB.1 Unsupervised pre-training\nFor unsupervised pre-training we build on the DINO and iBOT codebases. We use hyperparameters shown\nin Table 16, ViT architectures described in Table 17.\nKoLeo regularization. We apply the KoLeo regularizer with a weight of 0.1 between the class tokens of\nthe ﬁrst global crop, for all samples within a GPU without cross-communication for this step.\n28', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Task Dataset / Split Images Retrieval Retrieved Final\nclassiﬁcation ImageNet-22k / – 14,197,086 as is – 14,197,086\nclassiﬁcation ImageNet-22k / – 14,197,086 sample 56,788,344 56,788,344\nclassiﬁcation ImageNet-1k / train 1,281,167 sample 40,997,344 40,997,344\nﬁne-grained classif. Caltech 101 / train 3,030 cluster 2,630,000 1,000,000\nﬁne-grained classif. CUB-200-2011 / train 5,994 cluster 1,300,000 1,000,000\nﬁne-grained classif. DTD / train1 1,880 cluster 1,580,000 1,000,000\nﬁne-grained classif. FGVC-Aircraft / train 3,334 cluster 1,170,000 1,000,000\nﬁne-grained classif. Flowers-102 / train 1,020 cluster 1,060,000 1,000,000\nﬁne-grained classif. Food-101 / train 75,750 cluster 21,670,000 1,000,000\nﬁne-grained classif. Oxford-IIIT Pet / trainval 3,680 cluster 2,750,000 1,000,000\nﬁne-grained classif. Stanford Cars / train 8,144 cluster 7,220,000 1,000,000\nﬁne-grained classif. SUN397 / train1 19,850 cluster 18,950,000 1,000,000\nﬁne-grained classif. Pascal VOC 2007 / train 2,501 cluster 1,010,000 1,000,000\nsegmentation ADE20K / train 20,210 cluster 20,720,000 1,000,000\nsegmentation Cityscapes / train 2,975 cluster 1,390,000 1,000,000\nsegmentation Pascal VOC 2012 (seg.) / trainaug 1,464 cluster 10,140,000 1,000,000\ndepth estimation Mapillary SLS / train 1,434,262 as is – 1,434,262\ndepth estimation KITTI / train (Eigen) 23,158 cluster 3,700,000 1,000,000\ndepth estimation NYU Depth V2 / train 24,231 cluster 10,850,000 1,000,000\ndepth estimation SUN RGB-D / train 4,829 cluster 4,870,000 1,000,000\nretrieval Google Landmarks v2 / train (clean) 1,580,470 as is – 1,580,470\nretrieval Google Landmarks v2 / train (clean) 1,580,470 sample 6,321,880 6,321,880\nretrieval AmsterTime / new 1,231 cluster 960,000 960,000\nretrieval AmsterTime / old 1,231 cluster 830,000 830,000\nretrieval Met / train 397,121 cluster 62,860,000 1,000,000\nretrieval Revisiting Oxford / base 4,993 cluster 3,680,000 1,000,000\nretrieval Revisiting Paris / base 6,322 cluster 3,660,000 1,000,000\n142,109,386\nTable 15: Composition of our LVD-142M dataset. We report the list of datasets and associated splits\nused to build the dataset, how they were included (as is without retrieval or via sample-based or cluster-based\nretrieval). For retrievals, we indicate the actual number of retrieved images and the ﬁnal number included\nin the dataset.\n29', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Arch. Drop-rate LR Batch size\nDINOv2-S (distilled) ViT-S/14 0 1e-3 2048\nDINOv2-B (distilled) ViT-B/14 0 1e-3 2048\nDINOv2-L (distilled) ViT-L/14 0 1e-3 2048\nDINOv2-L (from scratch) ViT-L/14 0.4 3.5e-4 3072\nDINOv2-g (from scratch) ViT-g/14 0.4 3.5e-4 3072\nTable 16: Training hyperparameters for DINOv2-S, DINOv2-B, DINOv2-L and DINOv2-g. All\nmodels run for 625k iterations with optimizer AdamW, an initial LayerScale value of 1e-5, a weight decay\ncosine schedule from 0.04 to 0.2, a learning rate warmup of 100k iterations, a teacher momentum cosine\nschedule from 0.994 to 1, and we train in ﬂoat16 precision in all cases (except for the DINO heads where we\nreduce the gradients in ﬂoat32).\nArch. Embed dim Heads Blocks FFN layer\nViT-S/14 (distilled) 384 6 12 MLP\nViT-B/14 (distilled) 768 12 18 MLP\nViT-L/14 (distilled) 1024 16 24 MLP\nViT-L/14 (from scratch) 1024 16 24 SwiGLU\nViT-g/14 (from scratch) 1536 24 40 SwiGLU\nTable 17: Architecture details of the ViT-S/B/L/g networks used in this work. We use MLP\nfeed-forward networks for distilled models, and SwiGLU (Shazeer, 2020) when training from scratch.\nEMA update for the teacher. The teacher is initialized with the same state as the student, and is an\nexponential moving average of the student network, with a momentum value in [0.994, 1.0] following a cosine\nschedule. It is updated at the end of every training step.\nB.2 High-Resolution adaptation\nWe initialise the model with the pretrained weights then train it for 10k iterations with the same procedure\nas the original pretraining. All the schedules are kept the same as in the original training, but compressed\nto ﬁt in 10k iterations. All the hyperparameters are kept the same as in the ﬁrst pretraining, except the\nbase learning rate which is reduced.\nB.3 Linear probing evaluation\nFor linear probing we deﬁne 3 evaluation parameters: the learning rate, how many output layers we use,\nwhether we concatenate the average-pooled patch token features with the class token (or use only the\nclass token). We train our linear layer with SGD for 12500 iterations, using random-resized-crop data\naugmentation, and perform the following grid search:\n•learning rate in{0.0001,0.0002,0.0005,0.001,0.002,0.005,0.01,0.02,0.05,0.1,0.2,0.3,0.5}\n•output layers in{1,4}\n•concatenate average-pooled tokens in {yes, no}\nWe then report the highest accuracy value obtained on the validation set as is common practice. Note that\nthis grid search is not expensive, because at each iteration we perform inference on the backbone only once,\nthen feed the output to all linear classiﬁers (each performing a single matrix multiplication).\nC List of benchmarks used for evaluations\nWe show in Table 18 the list of benchmarks and datasets used for evaluation.\n30', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Dataset name Task Citation\nImageNet-1k Image Classiﬁcation (Russakovsky et al., 2015)\nImageNet-V2 Image Classiﬁcation (Recht et al., 2019)\nImageNet-ReaL Image Classiﬁcation (Beyer et al., 2020)\nImageNet-A Image Classiﬁcation (Djolonga et al., 2021)\nImageNet-C Image Classiﬁcation (Hendrycks & Dietterich, 2019)\nImageNet-Rendition Image Classiﬁcation (Hendrycks et al., 2021)\nImageNet-Sketch Image Classiﬁcation (Wang et al., 2019)\nFood-101 Image Classiﬁcation (Bossard et al., 2014)\nCIFAR-10 Image Classiﬁcation (Krizhevsky et al., 2009)\nCIFAR-100 Image Classiﬁcation (Krizhevsky et al., 2009)\nSUN397 Image Classiﬁcation (Xiao et al., 2010)\nStanfordCars Image Classiﬁcation (Krause et al., 2013)\nFGVC-Aircraft Image Classiﬁcation (Maji et al., 2013)\nPascal VOC 2007 Image Classiﬁcation (Everingham et al., 2015)\nDescribable Textures Image Classiﬁcation (Cimpoi et al., 2014)\nOxford Pets Image Classiﬁcation (Parkhi et al., 2012)\nCaltech101 Image Classiﬁcation (Fei-Fei et al., 2004)\nOxford Flowers Image Classiﬁcation (Nilsback & Zisserman, 2008)\nCUB200 Image Classiﬁcation (Welinder et al., 2010)\niNaturalist 2018 Image Classiﬁcation (Van Horn et al., 2018)\niNaturalist 2021 Image Classiﬁcation (Van Horn et al., 2021)\nPlaces-205 Image Classiﬁcation (Zhou et al., 2014)\nUCF101 Video Classiﬁcation (Soomro et al., 2012)\nKinetics-400 Video Classiﬁcation (Kay et al., 2017)\nSomething-Something-V2 Video Classiﬁcation (Goyal et al., 2017)\nRevisiting-Paris Image Retrieval (Radenović et al., 2018a)\nRevisiting-Oxford Image Retrieval (Radenović et al., 2018a)\nMet Image Retrieval (Ypsilantis et al., 2021)\nAmstertime Image Retrieval (Yildiz et al., 2022)\nADE20k Image Segmentation (Zhou et al., 2017)\nCityscapes Image Segmentation (Cordts et al., 2016)\nPascal VOC 2012 Image Segmentation (Everingham et al., 2015)\nNYU-Depth V2 Monocular Depth Estimation (Silberman et al., 2012)\nKITTI Monocular Depth Estimation (Geiger et al., 2013)\nSUN-RGBD Monocular Depth Estimation (Song et al., 2015)\nDollarStreet Fairness Analysis (De Vries et al., 2019)\nCasual Conversations Fairness Analysis (Hazirbas et al., 2021)\nTable 18: List of datasets used for evaluation.\n31', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}]: %s
2023-12-07 11:00:01,093 - INFO - Received requests to /inference endpoint
2023-12-07 11:00:01,194 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 11:00:01,195 - INFO - Received request: {'username': 'amin', 'prompt': 'give me a brief of the paper', 'memory': True, 'conversation_number': 2, 'AI_assistance': False, 'collection_name': 'genAI', 'llm_model': 'Llama_13b'}
2023-12-07 11:00:06,521 - INFO - Processed the request successfully
2023-12-07 11:00:40,583 - INFO - Received requests to /inference endpoint
2023-12-07 11:00:40,684 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 11:00:40,684 - INFO - Received request: {'username': 'amin', 'prompt': 'Hi how are you doing?', 'memory': True, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-12-07 11:00:46,181 - INFO - Processed the request successfully
2023-12-07 11:01:09,755 - INFO - Received requests to /inference endpoint
2023-12-07 11:01:09,856 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 11:01:09,857 - INFO - Received request: {'username': 'amin', 'prompt': 'give me a summary of the paper', 'memory': True, 'conversation_number': 2, 'AI_assistance': False, 'collection_name': 'genAI', 'llm_model': 'Llama_70b'}
2023-12-07 11:02:43,734 - INFO - Processed the request successfully
2023-12-07 11:03:02,857 - INFO - collection delete: {'success': 'Class amin_\tgenAI has been removed'}: %s
2023-12-07 11:03:09,285 - INFO - collection delete: {'success': 'Class amin_\tsdee has been removed'}: %s
2023-12-07 11:03:15,959 - INFO - collection delete: {'success': 'Class amin_cd has been removed'}: %s
2023-12-07 11:03:26,362 - INFO - collection delete: {'success': 'Class amin_sdee has been removed'}: %s
2023-12-07 11:03:32,158 - INFO - collection delete: {'success': 'Class amin_genAI has been removed'}: %s
2023-12-07 11:03:35,918 - INFO - collection delete: {'success': 'Class amin_rr has been removed'}: %s
2023-12-07 11:03:39,837 - INFO - collection delete: {'success': 'Class amin_file has been removed'}: %s
2023-12-07 11:03:44,517 - INFO - collection delete: {'success': 'Class amin_web has been removed'}: %s
2023-12-07 11:03:49,924 - INFO - collection delete: {'success': 'Class amin_document has been removed'}: %s
2023-12-07 11:03:54,188 - INFO - collection delete: {'success': 'Class amin_video has been removed'}: %s
2023-12-07 11:04:01,826 - INFO - checking the request/ username='amin' class_name='paper' mode='create_collection' vectorDB_type='Weaviate' file_path=None: %s
2023-12-07 11:04:01,904 - INFO - checkpoint 1
2023-12-07 11:04:01,904 - INFO - checkpoint 2 amin: %s
2023-12-07 11:04:01,904 - INFO - checkpoint 2 amin_paper: %s
2023-12-07 11:04:01,927 - INFO - class name added successfully to database
2023-12-07 11:04:01,927 - INFO - success: class paper created for user amin
2023-12-07 11:04:21,627 - INFO - request received username='amin' class_name='paper' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/b411b22ab5006a5c': %s
2023-12-07 11:04:22,559 - INFO - actors creation successful [Actor(WeaviateEmbedder, d55105037d427a4bca3cd49701000000), Actor(WeaviateEmbedder, 555d836fcc0ae02c69101bc501000000), Actor(WeaviateEmbedder, 9c0695adf39668ae9d3195be01000000)]: %s
2023-12-07 11:04:22,560 - INFO - check 1st step of ray was successful
2023-12-07 11:04:22,560 - INFO - check if ray was successful:
2023-12-07 11:04:22,560 - INFO - check weaviate add data, 
2023-12-07 11:04:22,560 - INFO - request processed successfully username='amin' class_name='paper' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/b411b22ab5006a5c': %s
2023-12-07 11:04:24,140 - INFO - Check the data that is being passed [{'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nZhuosheng Zhang1Aston Zhang2Mu Li2Hai Zhao1George Karypis2Alex Smola2\nAbstract\nLarge language models (LLMs) have shown im-\npressive performance on complex reasoning by\nleveraging chain-of-thought (CoT) prompting to\ngenerate intermediate reasoning chains as the ra-\ntionale to infer the answer. However, existing\nCoT studies have focused on the language modal-\nity. We propose Multimodal-CoT that incorpo-\nrates language (text) and vision (images) modal-\nities into a two-stage framework that separates\nrationale generation and answer inference. In this\nway, answer inference can leverage better gen-\nerated rationales that are based on multimodal\ninformation. With Multimodal-CoT, our model\nunder 1 billion parameters outperforms the previ-\nous state-of-the-art LLM (GPT-3.5) by 16 percent-\nage points (75.17% →91.68% accuracy) and even\nsurpasses human performance on the ScienceQA\nbenchmark. Code is publicly available.1\n1. Introduction\nImagine reading a textbook with no ﬁgures or tables. Our\nability to knowledge acquisition is greatly strengthened by\njointly modeling diverse data modalities, such as vision, lan-\nguage, and audio. Recently, large language models (LLMs)\n(Brown et al., 2020; Thoppilan et al., 2022; Rae et al., 2021;\nChowdhery et al., 2022) have shown impressive perfor-\nmance in complex reasoning by generating intermediate\nreasoning steps before inferring the answer. The intriguing\ntechnique is called chain-of-thought (CoT) reasoning (Wei\net al., 2022b; Kojima et al., 2022; Zhang et al., 2022).\nHowever, existing studies related to CoT reasoning are\nlargely isolated in the language modality (Wang et al.,\n2022b; Zhou et al., 2022; Lu et al., 2022b; Fu et al., 2022),\nwith little consideration of multimodal scenarios. To elicit\nCoT reasoning in multimodality, we advocate a Multimodal-\n1Shanghai Jiao Tong University2Amazon Web Services.\nCorrespondence to: Zhuosheng Zhang (work done at Ama-\nzon Web Services) <zhangzs@sjtu.edu.cn >, Aston Zhang\n<az@astonzhang.com >.\n1https://github.com/amazon-science/mm-cot\nOptions:(B) salty(A) softOutputQuestion:Whichpropertydothesetwoobjectshaveincommon?Context: Select the better answer.\nRationale:Lookateachobject.Foreachobject,decideifithasthatproperty.Potatochipshaveasaltytaste.Bothobjectsaresalty.Asoftobjectchangesshapewhenyousqueezeit.Thefriesaresoft,butthecrackerisnot.Thepropertythatbothobjectshaveincommonissalty.Answer:Theansweris(B).VisionLanguageInputFigure 1. Example of the multimodal CoT task.\nCoT paradigm. Given the inputs in different modalities,\nMultimodal-CoT decomposes multi-step problems into in-\ntermediate reasoning steps (rationale) and then infers the\nanswer. Since vision and language are the most popular\nmodalities, we focus on those two modalities in this work.\nAn example is shown in Figure 1. In general, there are two\nways to elicit Multimodal-CoT reasoning as follows: (i)\nprompting LLMs and (ii) ﬁne-tuning small models.2\nThe most immediate way to perform Multimodal-CoT is to\ntransform the input of different modalities into one modality\nand prompt LLMs to perform CoT. For example, it is possi-\nble to extract the caption of an image by a captioning model\nand then concatenate the caption with the original language\ninput to be fed into LLMs (Lu et al., 2022a). However, there\nis severe information loss in the captioning process; thus,\nusing the captions (as opposed to vision features) may suffer\nfrom a lack of mutual synergy in the representation space\nof different modalities.\nTo facilitate the interaction between modalities, another\npotential solution is to ﬁne-tune smaller language models\n(LMs) by fusing multimodal features (Zhang et al., 2023).\nAs this approach allows the ﬂexibility of adjusting model\narchitectures to incorporate multimodal features, we study\nﬁne-tuning models in this work instead of prompting LLMs.\nThe key challenge is that language models under 100 billion\nparameters tend to generate hallucinated rationales that mis-\nlead the answer inference (Ho et al., 2022; Magister et al.,\n2In this work, we refer to small models as models with less\nthan 1 billion parameters (hereinafter dubbed as 1B-models).arXiv:2302.00923v4  [cs.CL]  17 Feb 2023', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nTable 1. Typical CoT techniques (FT: ﬁne-tuning; KD: knowledge distillation). Segment 1: in-context learning techniques; Segment 2:\nﬁne-tuning techniques. To the best of our knowledge, our work is the ﬁrst to study CoT reasoning in different modalities. Besides, we\nfocus on 1B-models, without relying on the outputs of LLMs.\nModels Mutimodal w/o LLM Model / Engine Training CoT Role CoT Source\nZero-Shot-CoT (Kojima et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Template\nFew-Shot-CoT (Wei et al., 2022b) \x17 \x17 PaLM (540B) ICL Reasoning Hand-crafted\nSelf-Consistency-CoT (Wang et al., 2022a) \x17 \x17 Codex (175B) ICL Reasoning Hand-crafted\nLeast-to-Most Prompting (Zhou et al., 2022) \x17 \x17 Codex (175B) ICL Reasoning Hand-crafted\nRetrieval-CoT (Zhang et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Auto-generated\nPromptPG-CoT (Lu et al., 2022b) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Hand-crafted\nAuto-CoT (Zhang et al., 2022) \x17 \x17 Codex (175B) ICL Reasoning Auto-generated\nComplexity-CoT (Fu et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Hand-crafted\nFew-Shot-PoT (Chen et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Hand-crafted\nUniﬁedQA (Lu et al., 2022a) \x17 ✓ T5 (770M) FT Explanation Crawled\nFine-Tuned T5 XXL (Magister et al., 2022) \x17 \x17 T5 (11B) KD Reasoning LLM-generated\nFine-Tune-CoT (Ho et al., 2022) \x17 \x17 GPT-3 (6.7B) KD Reasoning LLM-generated\nMultimodal-CoT (our work) ✓ ✓ T5 (770M) FT Reasoning Crawled\n2022; Ji et al., 2022).\nTo mitigate the challenge of hallucination, we propose\nMultimodal-CoT that incorporates language (text) and vi-\nsion (images) modalities into a two-stage framework that\nseparates rationale generation and answer inference. In\nthis way, answer inference can leverage better generated\nrationales that are based on multimodal information. Our\nexperiments are conducted on the ScienceQA benchmark\n(Lu et al., 2022a), which is the latest multimodal reasoning\nbenchmark with annotated reasoning chains. Experimental\nresults show that our method surpasses the previous state-of-\nthe-art GPT-3.5 model by +16% (75.17% →91.68%) on the\nbenchmark. Our contributions are summarized as follows:\n(i) To the best of our knowledge, this work is the ﬁrst to\nstudy CoT reasoning in different modalities.\n(ii) We propose a two-stage framework by ﬁne-tuning lan-\nguage models to fuse vision and language representations\nto perform Multimodal-CoT. The model is able to generate\ninformative rationales to facilitate inferring ﬁnal answers.\n(iii) Our method achieves new state-of-the-art performance\non the ScienceQA benchmark, outperforming accuracy of\nGPT-3.5 by 16% and even surpassing human performance.\n2. Background\nThis section reviews recent progress of eliciting CoT rea-\nsoning by prompting and ﬁne-tuning language models.\n2.1. CoT Reasoning with LLMs\nRecently, CoT has been widely used to elicit the multi-step\nreasoning abilities of LLMs (Wei et al., 2022b). Concretely,\nCoT techniques encourage the LLM to generate intermedi-\nate reasoning chains for solving a problem. Studies have\nshown that LLMs can perform CoT reasoning with two ma-\njor paradigms of techniques: Zero-Shot-CoT (Kojima et al.,2022) and Few-Shot-CoT (Wei et al., 2022b; Zhang et al.,\n2022). For Zero-Shot-CoT, Kojima et al. (2022) showed that\nLLMs are decent zero-shot reasoners by adding a prompt\nlike “Let’s think step by step” after the test question to in-\nvoke CoT reasoning. For Few-Shot-CoT, a few step-by-step\nreasoning demonstrations are used as conditions for infer-\nence. Each demonstration has a question and a reasoning\nchain that leads to the ﬁnal answer. The demonstrations are\ncommonly obtained by hand-crafting or automatic gener-\nation. The corresponding techniques are thus referred to\nas Manual-CoT (Wei et al., 2022b) and Auto-CoT (Zhang\net al., 2022).\nWith effective demonstrations, Few-Shot-CoT often\nachieves stronger performance than Zero-Shot-CoT and has\nattracted more research interest. Therefore, most recent\nstudies focused on how to improve Few-Shot-CoT. Those\nstudies are categorized into two major research lines: (i)\noptimizing the demonstrations; (ii) optimizing the reasoning\nchains. Table 1 compares typical CoT techniques.\nOptimizing Demonstrations The performance of Few-\nShot-CoT relies on the quality of demonstrations. As re-\nported in Wei et al. (2022b), using demonstrations written\nby different annotators results in dramatic accuracy dispar-\nity in a symbolic reasoning task. Beyond hand-crafting the\ndemonstrations, recent studies have investigated ways to op-\ntimize the demonstration selection process. Notably, Rubin\net al. (2022) retrieved the semantically similar demonstra-\ntions with the test instance. However, this approach shows\na degraded performance when there are mistakes in the rea-\nsoning chains (Zhang et al., 2022). To address the limitation,\nZhang et al. (2022) found that the key is the diversity of\ndemonstration questions and proposed Auto-CoT: (i) par-\ntition questions of a given dataset into a few clusters; (ii)\nsample a representative question from each cluster and gen-\nerate its reasoning chain using Zero-Shot-CoT with simple\nheuristics. In addition, reinforcement learning (RL) and', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\ncomplexity-based selection strategies were also proposed\nto obtain effective demonstrations. Fu et al. (2022) chose\nexamples with complex reasoning chains (i.e., with more\nreasoning steps) as the demonstrations. Lu et al. (2022b)\ntrained an agent to ﬁnd optimal in-context examples from\na candidate pool and maximize the prediction rewards on\ngiven training examples when interacting with GPT-3.5.\nOptimizing Reasoning Chains A notable way to opti-\nmize reasoning chains is problem decomposition. Zhou\net al. (2022) proposed least-to-most prompting to decom-\npose complex problems into sub-problems and then solve\nthese sub-problems sequentially. As a result, solving a\ngiven sub-problem is facilitated by the answers to previ-\nously solved sub-problems. Similarly, Khot et al. (2022)\nused diverse decomposition structures and designed differ-\nent prompts to answer each sub-question. In addition to\nprompting the reasoning chains as natural language texts,\nChen et al. (2022) proposed program-of-thoughts (PoT),\nwhich modeled the reasoning process as a program and\nprompted LLMs to derive the answer by executing the gen-\nerated programs. Another trend is to vote over multiple\nreasoning paths for a test question. Wang et al. (2022a)\nintroduced a self-consistency decoding strategy to sample\nmultiple outputs of LLMs and then took a majority over\nthe ﬁnal answers. Wang et al. (2022b) and Li et al. (2022b)\nintroduced randomness in the input space to produce more\ndiverse outputs for voting.\n2.2. Eliciting CoT Reasoning by Fine-Tuning Models\nA recent interest is eliciting CoT reasoning by ﬁne-tuning\nlanguage models. Lu et al. (2022a) ﬁne-tuned the encoder-\ndecoder T5 model on a large-scale dataset with CoT annota-\ntions. However, a dramatic performance decline is observed\nwhen using CoT to infer the answer, i.e., generating the rea-\nsoning chain before the answer (reasoning). Instead, CoT\nis only used as an explanation after the answer. Magister\net al. (2022) and Ho et al. (2022) employed knowledge\ndistillation by ﬁne-tuning a student model on the chain-of-\nthought outputs generated by a larger teacher model. The\nproposed methods showed performance gains in arithmetic,\ncommonsense, and symbolic reasoning tasks.\nThere is a key challenge in training 1B-models to be CoT\nreasoners. As observed by Wei et al. (2022b), models un-\nder 100 billion parameters tend to produce illogical CoT\nthat leads to wrong answers. In other words, it might be\nharder for 1B-models to generate effective CoT than directly\ngenerating the answer. It becomes even more challenging\nin a multimodal setting where answering the question also\nrequires understanding the multimodal inputs. In the follow-\ning part, we will explore the challenge of Multimodal-CoT\nand investigate how to perform effective multi-step reason-\ning.3. Challenge of Multimodal-CoT\nExisting studies have suggested that the CoT reasoning abil-\nity may emerge in language models at a certain scale, e.g.,\nover 100 billion parameters (Wei et al., 2022a). However,\nit remains an unresolved challenge to elicit such reasoning\nabilities in 1B-models, let alone in the multimodal scenario.\nThis work focuses on 1B-models as they can be ﬁne-tuned\nand deployed with consumer-grade GPUs (e.g., 32G mem-\nory). In this section, we will investigate why 1B-models\nfail at CoT reasoning and study how to design an effective\napproach to overcome the challenge.\n3.1. Towards the Role of CoT\nTo begin with, we ﬁne-tune a text-only baseline for CoT rea-\nsoning on the ScienceQA benchmark (Lu et al., 2022a).\nFollowing Lu et al. (2022a), we adopt UniﬁedQA Base\n(Khashabi et al., 2020) as the backbone language model.3\nOur task is modeled as a text generation problem, where the\nmodel takes the textual information as the input and gener-\nates the output sequence that consists of the rationale and\nthe answer. As an example shown in Figure 1, the model\ntakes the concatenation of tokens of the question text (Q),\nthe context text (C), and multiple options (M) as the input.\nTo study the effect of CoT, we compare the performance\nwith three variants: (i) No-CoT which predicts the answer\ndirectly (QCM→A); (ii) Reasoning where answer inference\nis conditioned to the rationale (QCM →RA); (iii) Explana-\ntion where the rationale is used for explaining the answer\ninference (QCM→AR).\nTable 2. Effects of CoT in the one-stage setting.\nMethod Format Accuracy\nNo-CoT QCM →A 80.40\nReasoning QCM →RA 67.86\nExplanation QCM →AR 69.77\nSurprisingly, we observe a ↓12.54% accuracy decrease\n(80.40%→67.86%) if the model predicts rationales before\nanswers (QCM→RA). The results imply that the rationales\nmight not necessarily contribute to predicting the right an-\nswer. A similar phenomenon was observed in Lu et al.\n(2022a), where the plausible reason might be that the model\nexceeds the maximum token limits before obtaining the\nrequired answer or stops generating the prediction early.\nHowever, we ﬁnd that the maximum length of the gener-\nated outputs (RA) is always less than 400 tokens, which\nis below the length limit of language models (i.e., 512 in\nUniﬁedQA Base). Therefore, it deserves a more in-depth\ninvestigation into why the rationales harm answer inference.\n3UniﬁedQA (Khashabi et al., 2020) is adopted as it is the best\nﬁne-tuning model in Lu et al. (2022a). Model information and\nimplementation details are presented in Appendix B.1.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nGeneratedRationale:Magnetscanpullorpushoneachotherwithouttouching.Whenmagnetsattract,theypulltogether.Whenmagnetsrepel,theypushapart.Whetheramagnetattractsorrepelsothermagnetsdependsonthepositionsofitspoles,orends.Everymagnethastwopoles,callednorthandsouth.Herearesomeexamplesofmagnets.ThenorthpoleofeachmagnetismarkedN,andthesouthpoleismarkedS.Ifdifferentpolesareclosesttoeachother,themagnetsattract.Themagnetsinthepairbelowattract.Ifthesamepolesareclosesttoeachother,themagnetsrepel.Themagnetsinbothpairsbelowrepel.Willthesemagnetsattractorrepel?Tofindout,lookatwhichpolesareclosesttoeachother.Thesouthpoleofonemagnetisclosesttothesouthpoleoftheothermagnet.Polesthatarethesamerepel.So,thesemagnetswillrepeleachother.Answer:Theansweris(B).Options:(B) repel(A) attractProblem\nBaselineQuestion:Willthesemagnetsattractorrepeleachother?Context:Twomagnetsareplacedasshown.Hint:Magnetsthatattractpulltogether.Magnetsthatrepelpushapart.GoldRationale:Magnetscanpullorpushoneachotherwithouttouching.Whenmagnetsattract,theypulltogether.Whenmagnetsrepel,theypushapart.Whetheramagnetattractsorrepelsothermagnetsdependsonthepositionsofitspoles,orends.Everymagnethastwopoles,callednorthandsouth.Herearesomeexamplesofmagnets.ThenorthpoleofeachmagnetismarkedN,andthesouthpoleismarkedS.Ifdifferentpolesareclosesttoeachother,themagnetsattract.Themagnetsinthepairbelowattract.Ifthesamepolesareclosesttoeachother,themagnetsrepel.Themagnetsinbothpairsbelowrepel.Willthesemagnetsattractorrepel?Tofindout,lookatwhichpolesareclosesttoeachother.Thenorthpoleofonemagnetisclosesttothesouthpoleoftheothermagnet.Polesthataredifferentattract.So,thesemagnetswillattracteachother.Answer:Theansweris(A).GeneratedRationale:Magnetscanpullorpushoneachotherwithouttouching.Whenmagnetsattract,theypulltogether.Whenmagnetsrepel,theypushapart.Whetheramagnetattractsorrepelsothermagnetsdependsonthepositionsofitspoles,orends.Everymagnethastwopoles,callednorthandsouth.Herearesomeexamplesofmagnets.ThenorthpoleofeachmagnetismarkedN,andthesouthpoleismarkedS.Ifdifferentpolesareclosesttoeachother,themagnetsattract.Themagnetsinthepairbelowattract.Ifthesamepolesareclosesttoeachother,themagnetsrepel.Themagnetsinbothpairsbelowrepel.Willthesemagnetsattractorrepel?Tofindout,lookatwhichpolesareclosesttoeachother.Thenorthpoleofonemagnetisclosesttothesouthpoleoftheothermagnet.Polesthataredifferentattract.So,thesemagnetswillattracteachother.Answer:Theansweris(A).+ Vision Features\nVision\nFigure 2. Example of the two-stage framework without vision features (baseline) and with vision features (ours) for generating rationales\nand predicting answers. The upper part presents the problem details with a gold rationale, and the lower part shows the outputs of the\nbaseline and our method incorporated with vision features. We observe that the baseline fails to predict the right answer due to the\nmisleading by hallucinated rationales. More examples are shown in Appendix A.1.\n3.2. Misleading by Hallucinated Rationales\nTo dive into how the rationales affect the answer prediction,\nwe separate the CoT problem into two stages, rationale\ngeneration andanswer inference . We report the RougeL\nscore and accuracy for the rationale generation and answer\ninference, respectively. Table 3 shows the results based\non the two-stage framework. Although the two-stage base-\nline model achieves a 91.76 RougeL score of the rationale\ngeneration, the answer inference accuracy is only 70.53%.\nCompared with the QCM →A variant (80.40%) in Table 2,\nthe result shows that the generated rationale in the two-stage\nframework does not improve answer accuracy.\nTable 3. Two-stage setting of (i) rationale generation (RougeL) and\n(ii) answer inference (Accuracy).\nMethod (i) QCM →R (ii) QCMR→A\nTwo-Stage Framework 91.76 70.53\nw/ Captions 91.85 71.12\nw/ Vision Features 96.97 84.91\nThen, we randomly sample 50 error cases and ﬁnd that the\nmodel tends to generate hallucinated rationales that mislead\nthe answer inference. As an example shown in Figure 2, the\nmodel (left part) hallucinates that, “ The south pole of one\nmagnet is closest to the south pole of the other magnet ”, due\nto the lack of reference to the vision content. We ﬁnd that\nsuch mistakes occur at a ratio of 64% among the error cases\nOthers(36%)Resolved (62.5%)Unresolved(37.5%)Hallucination(64%)(a) ratio of hallucination mistakes(b) correction rate w/ vision features  Figure 3. The ratio of hallucination mistakes (a) and correction\nrate w/ vision features (b).\n(Figure 3(a)).\n3.3. Multimodality Contributes to Effective Rationales\nWe speculate that such a phenomenon of hallucination is\ndue to a lack of necessary vision contexts for performing\neffective Multimodal-CoT. To inject vision information, a\nsimple way is to transform the paired image into a caption\n(Lu et al., 2022a) and then append the caption in the input of\nboth stages. However, as shown in Table 3, using captions\nonly yields marginal performance gains ( ↑0.59%). Then,\nwe explore an advanced technique by incorporating vision\nfeatures into the language model. Concretely, we feed the\npaired image to the DETR model (Carion et al., 2020) to\nextract vision features. Then we fuse the vision features', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nVisionLanguageRationale GenerationQuestion:Whichpropertydothesetwoobjectshaveincommon?Context: Select the better answer.Lookateachobject.Foreachobject,decideifithasthatproperty.Potatochipshaveasaltytaste.Bothobjectsaresalty.Asoftobjectchangesshapewhenyousqueezeit.Thefriesaresoft,butthecrackerisnot.Thepropertythatbothobjectshaveincommonissalty.\nOptions:(B) salty(A) softRationaleAnswer InferenceTheansweris(B).Answer\nFigure 4. Overview of our Multimodal-CoT framework. Multimodal-CoT consists of two stages: (i) rationale generation and (ii) answer\ninference. Both stages share the same model architecture but differ in the input and output. In the ﬁrst stage, we feed the model with\nlanguage and vision inputs to generate rationales. In the second stage, we append the original language input with the rationale generated\nfrom the ﬁrst stage. Then, we feed the updated language input with the original vision input to the model to infer the answer.\nwith the encoded language representations before feeding\nto the decoder (more details will be presented in Section\n4). Interestingly, with vision features, the RougeL score of\nthe rationale generation has boosted to 96.97% (QCM →R),\nwhich correspondingly contributes to better answer accuracy\nof 84.91% (QCMR →A). With those effective rationales,\nthe phenomenon of hallucination is mitigated — 62.5%\nhallucination mistakes in Section 3.2 have been corrected\n(Figure 3(b)), as an example shown in Figure 2 (right part).4\nThe analysis so far compellingly shows that vision features\nare indeed beneﬁcial for generating effective rationales and\ncontributing to accurate answer inference. As the two-stage\nmethod (QCMR→A) in Table 3 achieves better performance\nthan all the one-stage method in Table 2, we choose the two-\nstage method in our Multimodal-CoT framework.\n4. Multimodal-CoT\nBased on the observations and discussions in Section 3, we\npropose Multimodal-CoT to incorporate language (text) and\nvision (images) modalities into a two-stage framework. In\nthis section, we will ﬁrst overview the procedure of the\nframework and then elaborate on the technical design of the\nmodel architecture.\n4.1. Framework Overview\nMultimodal-CoT consists of two training stages: (i) ratio-\nnale generation and (ii) answer inference. Both stages share\nthe same model architecture but differ in the input Xand\noutputY. The overall procedure is illustrated in Figure 4.\nWe will take vision-language as an example to show how\nMultimodal-CoT works.\n4The left mistakes are mainly about map understanding, which\nrequires more advanced vision features. We will discuss them in\nSection 6.4.In the rationale generation stage, we feed the model with\nX={X1\nlanguage,Xvision}whereX1\nlanguage represents the lan-\nguage input in the ﬁrst stage and Xvision represents the vision\ninput, i.e., the image. For example, Xcan be instantiated as\na concatenation of question, context, and options of a multi-\nple choice reasoning problem (Lu et al., 2022a) as shown in\nFigure 4. The goal is to learn a rationale generation model\nR=F(X)whereRis the rationale.\nIn the answer inference stage, the rationale Ris appended\nto the original language input X1\nlanguage to construct the lan-\nguage input in the second stage, X2\nlanguage =X1\nlanguage◦R\nwhere◦denotes concatenation. Then, we feed the updated\ninputX′={X2\nlanguage,Xvision}to the answer inference\nmodel to infer the ﬁnal answer A=F(X′).\nIn both stages, we train two models with the same archi-\ntecture independently. They take the annotated elements\n(e.g.,X→R,XR→A, respectively) from the training\nset for supervised learning. During inference, given X, the\nrationales for the test sets are generated using the model\ntrained in the ﬁrst stage; they are used in the second stage\nfor answer inference.\n4.2. Model Architecture\nGiven the language input Xlanguage∈{X1\nlanguage,X2\nlanguage}\nand the vision input Xvision, we compute the probability of\ngenerating target text Y(either the rationale or the answer\nin Figure 4) of length Nby\np(Y|Xlanguage,Xvision ) =N∏\ni=1pθ(Yi|Xlanguage,Xvision,Y<i),\n(1)\nwherepθ(Yi|Xlanguage,Xvision,Y<i)is implemented with\na Transformer-based network (Vaswani et al., 2017). The\nnetwork has three major procedures: encoding, interaction,', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nAlgorithm 1 Multimodal-CoT\nInput: Language input X1\nlanguage , vision input Xvision\nOutput: Generated rationale R, inferred answer A\n1: Construct the input X={Xlanguage,X vision}\n2: Generate rationale R=F(X)using the model F(·)\n3:Append the rationale Rto the original language input\nX2\nlanguage =X1\nlanguage◦R.\n4: Construct new input X′={X2\nlanguage,X vision}\n5:Infer the answer Aby conditioning on the new input, A=\nF(X′).\n6:procedure F(X)\n7: Encode the language and vision inputs Hlanguage andHvision,\nrespectively\n8: Build the interaction between language and vision features\nby attention Hattn\nvision\n9: FuseHlanguage andHattn\nvision by a gated fusion mechanism to\nhaveHfuse\n10: FeedHfuseto the decoder to obtain the target prediction Y\n11: return Y\n12:end procedure\nand decoding. Speciﬁcally, we feed the language text into\na Transformer encoder to obtain a textual representation,\nwhich is then interacted and fused with the vision represen-\ntation before being fed into the Transformer decoder.\nEncoding The modelF(X)takes both the language and\nvision inputs and obtains the text representation Hlanguage\nand the image feature Hvision by the following functions:\nHlanguage =LanguageEncoder (Xlanguage ), (2)\nHvision =Wh·VisionExtractor (Xvision ),(3)\nwhere LanguageEncoder( ·) is implemented as a Trans-\nformer model. We use the hidden states of the last layer\nin the Transformer encoder as the language representation\nHlanguage∈Rn×dwherendenotes the length of the lan-\nguage input, and dis the hidden dimension. Meanwhile,\nVisionExtractor(·) is used to vectorize the input image into\nvision features. Inspired by the recent success of Vision\nTransformers (Dosovitskiy et al., 2021), we fetch the patch-\nlevel features by off-the-shelf vision extraction models,5\nsuch as DETR (Carion et al., 2020). After obtaining the\npatch-level vision features, we apply a learnable projection\nmatrixWhto convert the shape of VisionExtractor (Xvision )\ninto that ofHlanguage ; thus we have Hvision∈Rm×dwhere\nmis the number of patches.\nInteraction After obtaining language and vision represen-\ntations, we use a single-head attention network to correlate\ntext tokens with image patches, where the query ( Q), key\n(K) and value (V) areHlanguage ,Hvision andHvision, respec-\n5The parameters of the vision extraction are frozen.tively. The attention output Hattn\nvision∈Rn×dis deﬁned as:\nHattn\nvision =Softmax (QK⊤\n√dk)V, (4)\nwheredkis the same as the dimension of Hlanguage because\na single head is used.\nThen, we apply the gated fusion mechanism (Zhang et al.,\n2020; Wu et al., 2021; Li et al., 2022a) to fuse Hlanguage and\nHvision. The fused output Hfuse∈Rn×dis obtained by:\nλ=Sigmoid (WlHlanguage +WvHattn\nvision ),(5)\nHfuse = (1−λ)·Hlanguage +λ·Hattn\nvision, (6)\nwhereWlandWvare learnable parameters.\nDecoding Finally, the fused output Hfuseis fed into the\nTransformer decoder to predict the target Y. The complete\nprocedure of Multimodal-CoT is shown in Algorithm 1.\n5. Experiments\nThis section will present the benchmark dataset, the imple-\nmentation of our technique, and the baselines for compar-\nisons. Then, we will report our main results and ﬁndings.\n5.1. Dataset\nOur method is evaluated on the ScienceQA benchmark (Lu\net al., 2022a). ScienceQA is the ﬁrst large-scale multimodal\nscience question dataset that annotates the answers with de-\ntailed lectures and explanations. It contains 21k multimodal\nmultiple choice questions with rich domain diversity across\n3 subjects, 26 topics, 127 categories, and 379 skills. The\nbenchmark dataset is split into training, validation, and test\nsplits with 12726, 4241, and 4241 examples, respectively.\n5.2. Implementation\nThe following part presents the experimental settings of\nMultimodal-CoT and the baseline methods.\nExperimental Settings As the Multimodal-CoT task re-\nquires generating the reasoning chains and leveraging the\nvision features, we use the T5 encoder-decoder architec-\nture (Raffel et al., 2020). Speciﬁcally, we adopt UniﬁedQA\n(Khashabi et al., 2020) to initialize our models in the two\nstages because it achieves the best ﬁne-tuning results in\nLu et al. (2022a). To verify the generality of our approach\nacross different LMs, we also employ FLAN-T5 (Chung\net al., 2022) as the backbone in Section 6.3. As using im-\nage captions does not yield signiﬁcant performance gains in\nSection 3.3, we did not use the captions. We ﬁne-tune the\nmodels up to 20 epochs, with a learning rate of 5e-5. The', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}]: %s
2023-12-07 11:04:24,141 - INFO - Check the results [{'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nZhuosheng Zhang1Aston Zhang2Mu Li2Hai Zhao1George Karypis2Alex Smola2\nAbstract\nLarge language models (LLMs) have shown im-\npressive performance on complex reasoning by\nleveraging chain-of-thought (CoT) prompting to\ngenerate intermediate reasoning chains as the ra-\ntionale to infer the answer. However, existing\nCoT studies have focused on the language modal-\nity. We propose Multimodal-CoT that incorpo-\nrates language (text) and vision (images) modal-\nities into a two-stage framework that separates\nrationale generation and answer inference. In this\nway, answer inference can leverage better gen-\nerated rationales that are based on multimodal\ninformation. With Multimodal-CoT, our model\nunder 1 billion parameters outperforms the previ-\nous state-of-the-art LLM (GPT-3.5) by 16 percent-\nage points (75.17% →91.68% accuracy) and even\nsurpasses human performance on the ScienceQA\nbenchmark. Code is publicly available.1\n1. Introduction\nImagine reading a textbook with no ﬁgures or tables. Our\nability to knowledge acquisition is greatly strengthened by\njointly modeling diverse data modalities, such as vision, lan-\nguage, and audio. Recently, large language models (LLMs)\n(Brown et al., 2020; Thoppilan et al., 2022; Rae et al., 2021;\nChowdhery et al., 2022) have shown impressive perfor-\nmance in complex reasoning by generating intermediate\nreasoning steps before inferring the answer. The intriguing\ntechnique is called chain-of-thought (CoT) reasoning (Wei\net al., 2022b; Kojima et al., 2022; Zhang et al., 2022).\nHowever, existing studies related to CoT reasoning are\nlargely isolated in the language modality (Wang et al.,\n2022b; Zhou et al., 2022; Lu et al., 2022b; Fu et al., 2022),\nwith little consideration of multimodal scenarios. To elicit\nCoT reasoning in multimodality, we advocate a Multimodal-\n1Shanghai Jiao Tong University2Amazon Web Services.\nCorrespondence to: Zhuosheng Zhang (work done at Ama-\nzon Web Services) <zhangzs@sjtu.edu.cn >, Aston Zhang\n<az@astonzhang.com >.\n1https://github.com/amazon-science/mm-cot\nOptions:(B) salty(A) softOutputQuestion:Whichpropertydothesetwoobjectshaveincommon?Context: Select the better answer.\nRationale:Lookateachobject.Foreachobject,decideifithasthatproperty.Potatochipshaveasaltytaste.Bothobjectsaresalty.Asoftobjectchangesshapewhenyousqueezeit.Thefriesaresoft,butthecrackerisnot.Thepropertythatbothobjectshaveincommonissalty.Answer:Theansweris(B).VisionLanguageInputFigure 1. Example of the multimodal CoT task.\nCoT paradigm. Given the inputs in different modalities,\nMultimodal-CoT decomposes multi-step problems into in-\ntermediate reasoning steps (rationale) and then infers the\nanswer. Since vision and language are the most popular\nmodalities, we focus on those two modalities in this work.\nAn example is shown in Figure 1. In general, there are two\nways to elicit Multimodal-CoT reasoning as follows: (i)\nprompting LLMs and (ii) ﬁne-tuning small models.2\nThe most immediate way to perform Multimodal-CoT is to\ntransform the input of different modalities into one modality\nand prompt LLMs to perform CoT. For example, it is possi-\nble to extract the caption of an image by a captioning model\nand then concatenate the caption with the original language\ninput to be fed into LLMs (Lu et al., 2022a). However, there\nis severe information loss in the captioning process; thus,\nusing the captions (as opposed to vision features) may suffer\nfrom a lack of mutual synergy in the representation space\nof different modalities.\nTo facilitate the interaction between modalities, another\npotential solution is to ﬁne-tune smaller language models\n(LMs) by fusing multimodal features (Zhang et al., 2023).\nAs this approach allows the ﬂexibility of adjusting model\narchitectures to incorporate multimodal features, we study\nﬁne-tuning models in this work instead of prompting LLMs.\nThe key challenge is that language models under 100 billion\nparameters tend to generate hallucinated rationales that mis-\nlead the answer inference (Ho et al., 2022; Magister et al.,\n2In this work, we refer to small models as models with less\nthan 1 billion parameters (hereinafter dubbed as 1B-models).arXiv:2302.00923v4  [cs.CL]  17 Feb 2023', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nTable 1. Typical CoT techniques (FT: ﬁne-tuning; KD: knowledge distillation). Segment 1: in-context learning techniques; Segment 2:\nﬁne-tuning techniques. To the best of our knowledge, our work is the ﬁrst to study CoT reasoning in different modalities. Besides, we\nfocus on 1B-models, without relying on the outputs of LLMs.\nModels Mutimodal w/o LLM Model / Engine Training CoT Role CoT Source\nZero-Shot-CoT (Kojima et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Template\nFew-Shot-CoT (Wei et al., 2022b) \x17 \x17 PaLM (540B) ICL Reasoning Hand-crafted\nSelf-Consistency-CoT (Wang et al., 2022a) \x17 \x17 Codex (175B) ICL Reasoning Hand-crafted\nLeast-to-Most Prompting (Zhou et al., 2022) \x17 \x17 Codex (175B) ICL Reasoning Hand-crafted\nRetrieval-CoT (Zhang et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Auto-generated\nPromptPG-CoT (Lu et al., 2022b) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Hand-crafted\nAuto-CoT (Zhang et al., 2022) \x17 \x17 Codex (175B) ICL Reasoning Auto-generated\nComplexity-CoT (Fu et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Hand-crafted\nFew-Shot-PoT (Chen et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Hand-crafted\nUniﬁedQA (Lu et al., 2022a) \x17 ✓ T5 (770M) FT Explanation Crawled\nFine-Tuned T5 XXL (Magister et al., 2022) \x17 \x17 T5 (11B) KD Reasoning LLM-generated\nFine-Tune-CoT (Ho et al., 2022) \x17 \x17 GPT-3 (6.7B) KD Reasoning LLM-generated\nMultimodal-CoT (our work) ✓ ✓ T5 (770M) FT Reasoning Crawled\n2022; Ji et al., 2022).\nTo mitigate the challenge of hallucination, we propose\nMultimodal-CoT that incorporates language (text) and vi-\nsion (images) modalities into a two-stage framework that\nseparates rationale generation and answer inference. In\nthis way, answer inference can leverage better generated\nrationales that are based on multimodal information. Our\nexperiments are conducted on the ScienceQA benchmark\n(Lu et al., 2022a), which is the latest multimodal reasoning\nbenchmark with annotated reasoning chains. Experimental\nresults show that our method surpasses the previous state-of-\nthe-art GPT-3.5 model by +16% (75.17% →91.68%) on the\nbenchmark. Our contributions are summarized as follows:\n(i) To the best of our knowledge, this work is the ﬁrst to\nstudy CoT reasoning in different modalities.\n(ii) We propose a two-stage framework by ﬁne-tuning lan-\nguage models to fuse vision and language representations\nto perform Multimodal-CoT. The model is able to generate\ninformative rationales to facilitate inferring ﬁnal answers.\n(iii) Our method achieves new state-of-the-art performance\non the ScienceQA benchmark, outperforming accuracy of\nGPT-3.5 by 16% and even surpassing human performance.\n2. Background\nThis section reviews recent progress of eliciting CoT rea-\nsoning by prompting and ﬁne-tuning language models.\n2.1. CoT Reasoning with LLMs\nRecently, CoT has been widely used to elicit the multi-step\nreasoning abilities of LLMs (Wei et al., 2022b). Concretely,\nCoT techniques encourage the LLM to generate intermedi-\nate reasoning chains for solving a problem. Studies have\nshown that LLMs can perform CoT reasoning with two ma-\njor paradigms of techniques: Zero-Shot-CoT (Kojima et al.,2022) and Few-Shot-CoT (Wei et al., 2022b; Zhang et al.,\n2022). For Zero-Shot-CoT, Kojima et al. (2022) showed that\nLLMs are decent zero-shot reasoners by adding a prompt\nlike “Let’s think step by step” after the test question to in-\nvoke CoT reasoning. For Few-Shot-CoT, a few step-by-step\nreasoning demonstrations are used as conditions for infer-\nence. Each demonstration has a question and a reasoning\nchain that leads to the ﬁnal answer. The demonstrations are\ncommonly obtained by hand-crafting or automatic gener-\nation. The corresponding techniques are thus referred to\nas Manual-CoT (Wei et al., 2022b) and Auto-CoT (Zhang\net al., 2022).\nWith effective demonstrations, Few-Shot-CoT often\nachieves stronger performance than Zero-Shot-CoT and has\nattracted more research interest. Therefore, most recent\nstudies focused on how to improve Few-Shot-CoT. Those\nstudies are categorized into two major research lines: (i)\noptimizing the demonstrations; (ii) optimizing the reasoning\nchains. Table 1 compares typical CoT techniques.\nOptimizing Demonstrations The performance of Few-\nShot-CoT relies on the quality of demonstrations. As re-\nported in Wei et al. (2022b), using demonstrations written\nby different annotators results in dramatic accuracy dispar-\nity in a symbolic reasoning task. Beyond hand-crafting the\ndemonstrations, recent studies have investigated ways to op-\ntimize the demonstration selection process. Notably, Rubin\net al. (2022) retrieved the semantically similar demonstra-\ntions with the test instance. However, this approach shows\na degraded performance when there are mistakes in the rea-\nsoning chains (Zhang et al., 2022). To address the limitation,\nZhang et al. (2022) found that the key is the diversity of\ndemonstration questions and proposed Auto-CoT: (i) par-\ntition questions of a given dataset into a few clusters; (ii)\nsample a representative question from each cluster and gen-\nerate its reasoning chain using Zero-Shot-CoT with simple\nheuristics. In addition, reinforcement learning (RL) and', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\ncomplexity-based selection strategies were also proposed\nto obtain effective demonstrations. Fu et al. (2022) chose\nexamples with complex reasoning chains (i.e., with more\nreasoning steps) as the demonstrations. Lu et al. (2022b)\ntrained an agent to ﬁnd optimal in-context examples from\na candidate pool and maximize the prediction rewards on\ngiven training examples when interacting with GPT-3.5.\nOptimizing Reasoning Chains A notable way to opti-\nmize reasoning chains is problem decomposition. Zhou\net al. (2022) proposed least-to-most prompting to decom-\npose complex problems into sub-problems and then solve\nthese sub-problems sequentially. As a result, solving a\ngiven sub-problem is facilitated by the answers to previ-\nously solved sub-problems. Similarly, Khot et al. (2022)\nused diverse decomposition structures and designed differ-\nent prompts to answer each sub-question. In addition to\nprompting the reasoning chains as natural language texts,\nChen et al. (2022) proposed program-of-thoughts (PoT),\nwhich modeled the reasoning process as a program and\nprompted LLMs to derive the answer by executing the gen-\nerated programs. Another trend is to vote over multiple\nreasoning paths for a test question. Wang et al. (2022a)\nintroduced a self-consistency decoding strategy to sample\nmultiple outputs of LLMs and then took a majority over\nthe ﬁnal answers. Wang et al. (2022b) and Li et al. (2022b)\nintroduced randomness in the input space to produce more\ndiverse outputs for voting.\n2.2. Eliciting CoT Reasoning by Fine-Tuning Models\nA recent interest is eliciting CoT reasoning by ﬁne-tuning\nlanguage models. Lu et al. (2022a) ﬁne-tuned the encoder-\ndecoder T5 model on a large-scale dataset with CoT annota-\ntions. However, a dramatic performance decline is observed\nwhen using CoT to infer the answer, i.e., generating the rea-\nsoning chain before the answer (reasoning). Instead, CoT\nis only used as an explanation after the answer. Magister\net al. (2022) and Ho et al. (2022) employed knowledge\ndistillation by ﬁne-tuning a student model on the chain-of-\nthought outputs generated by a larger teacher model. The\nproposed methods showed performance gains in arithmetic,\ncommonsense, and symbolic reasoning tasks.\nThere is a key challenge in training 1B-models to be CoT\nreasoners. As observed by Wei et al. (2022b), models un-\nder 100 billion parameters tend to produce illogical CoT\nthat leads to wrong answers. In other words, it might be\nharder for 1B-models to generate effective CoT than directly\ngenerating the answer. It becomes even more challenging\nin a multimodal setting where answering the question also\nrequires understanding the multimodal inputs. In the follow-\ning part, we will explore the challenge of Multimodal-CoT\nand investigate how to perform effective multi-step reason-\ning.3. Challenge of Multimodal-CoT\nExisting studies have suggested that the CoT reasoning abil-\nity may emerge in language models at a certain scale, e.g.,\nover 100 billion parameters (Wei et al., 2022a). However,\nit remains an unresolved challenge to elicit such reasoning\nabilities in 1B-models, let alone in the multimodal scenario.\nThis work focuses on 1B-models as they can be ﬁne-tuned\nand deployed with consumer-grade GPUs (e.g., 32G mem-\nory). In this section, we will investigate why 1B-models\nfail at CoT reasoning and study how to design an effective\napproach to overcome the challenge.\n3.1. Towards the Role of CoT\nTo begin with, we ﬁne-tune a text-only baseline for CoT rea-\nsoning on the ScienceQA benchmark (Lu et al., 2022a).\nFollowing Lu et al. (2022a), we adopt UniﬁedQA Base\n(Khashabi et al., 2020) as the backbone language model.3\nOur task is modeled as a text generation problem, where the\nmodel takes the textual information as the input and gener-\nates the output sequence that consists of the rationale and\nthe answer. As an example shown in Figure 1, the model\ntakes the concatenation of tokens of the question text (Q),\nthe context text (C), and multiple options (M) as the input.\nTo study the effect of CoT, we compare the performance\nwith three variants: (i) No-CoT which predicts the answer\ndirectly (QCM→A); (ii) Reasoning where answer inference\nis conditioned to the rationale (QCM →RA); (iii) Explana-\ntion where the rationale is used for explaining the answer\ninference (QCM→AR).\nTable 2. Effects of CoT in the one-stage setting.\nMethod Format Accuracy\nNo-CoT QCM →A 80.40\nReasoning QCM →RA 67.86\nExplanation QCM →AR 69.77\nSurprisingly, we observe a ↓12.54% accuracy decrease\n(80.40%→67.86%) if the model predicts rationales before\nanswers (QCM→RA). The results imply that the rationales\nmight not necessarily contribute to predicting the right an-\nswer. A similar phenomenon was observed in Lu et al.\n(2022a), where the plausible reason might be that the model\nexceeds the maximum token limits before obtaining the\nrequired answer or stops generating the prediction early.\nHowever, we ﬁnd that the maximum length of the gener-\nated outputs (RA) is always less than 400 tokens, which\nis below the length limit of language models (i.e., 512 in\nUniﬁedQA Base). Therefore, it deserves a more in-depth\ninvestigation into why the rationales harm answer inference.\n3UniﬁedQA (Khashabi et al., 2020) is adopted as it is the best\nﬁne-tuning model in Lu et al. (2022a). Model information and\nimplementation details are presented in Appendix B.1.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nGeneratedRationale:Magnetscanpullorpushoneachotherwithouttouching.Whenmagnetsattract,theypulltogether.Whenmagnetsrepel,theypushapart.Whetheramagnetattractsorrepelsothermagnetsdependsonthepositionsofitspoles,orends.Everymagnethastwopoles,callednorthandsouth.Herearesomeexamplesofmagnets.ThenorthpoleofeachmagnetismarkedN,andthesouthpoleismarkedS.Ifdifferentpolesareclosesttoeachother,themagnetsattract.Themagnetsinthepairbelowattract.Ifthesamepolesareclosesttoeachother,themagnetsrepel.Themagnetsinbothpairsbelowrepel.Willthesemagnetsattractorrepel?Tofindout,lookatwhichpolesareclosesttoeachother.Thesouthpoleofonemagnetisclosesttothesouthpoleoftheothermagnet.Polesthatarethesamerepel.So,thesemagnetswillrepeleachother.Answer:Theansweris(B).Options:(B) repel(A) attractProblem\nBaselineQuestion:Willthesemagnetsattractorrepeleachother?Context:Twomagnetsareplacedasshown.Hint:Magnetsthatattractpulltogether.Magnetsthatrepelpushapart.GoldRationale:Magnetscanpullorpushoneachotherwithouttouching.Whenmagnetsattract,theypulltogether.Whenmagnetsrepel,theypushapart.Whetheramagnetattractsorrepelsothermagnetsdependsonthepositionsofitspoles,orends.Everymagnethastwopoles,callednorthandsouth.Herearesomeexamplesofmagnets.ThenorthpoleofeachmagnetismarkedN,andthesouthpoleismarkedS.Ifdifferentpolesareclosesttoeachother,themagnetsattract.Themagnetsinthepairbelowattract.Ifthesamepolesareclosesttoeachother,themagnetsrepel.Themagnetsinbothpairsbelowrepel.Willthesemagnetsattractorrepel?Tofindout,lookatwhichpolesareclosesttoeachother.Thenorthpoleofonemagnetisclosesttothesouthpoleoftheothermagnet.Polesthataredifferentattract.So,thesemagnetswillattracteachother.Answer:Theansweris(A).GeneratedRationale:Magnetscanpullorpushoneachotherwithouttouching.Whenmagnetsattract,theypulltogether.Whenmagnetsrepel,theypushapart.Whetheramagnetattractsorrepelsothermagnetsdependsonthepositionsofitspoles,orends.Everymagnethastwopoles,callednorthandsouth.Herearesomeexamplesofmagnets.ThenorthpoleofeachmagnetismarkedN,andthesouthpoleismarkedS.Ifdifferentpolesareclosesttoeachother,themagnetsattract.Themagnetsinthepairbelowattract.Ifthesamepolesareclosesttoeachother,themagnetsrepel.Themagnetsinbothpairsbelowrepel.Willthesemagnetsattractorrepel?Tofindout,lookatwhichpolesareclosesttoeachother.Thenorthpoleofonemagnetisclosesttothesouthpoleoftheothermagnet.Polesthataredifferentattract.So,thesemagnetswillattracteachother.Answer:Theansweris(A).+ Vision Features\nVision\nFigure 2. Example of the two-stage framework without vision features (baseline) and with vision features (ours) for generating rationales\nand predicting answers. The upper part presents the problem details with a gold rationale, and the lower part shows the outputs of the\nbaseline and our method incorporated with vision features. We observe that the baseline fails to predict the right answer due to the\nmisleading by hallucinated rationales. More examples are shown in Appendix A.1.\n3.2. Misleading by Hallucinated Rationales\nTo dive into how the rationales affect the answer prediction,\nwe separate the CoT problem into two stages, rationale\ngeneration andanswer inference . We report the RougeL\nscore and accuracy for the rationale generation and answer\ninference, respectively. Table 3 shows the results based\non the two-stage framework. Although the two-stage base-\nline model achieves a 91.76 RougeL score of the rationale\ngeneration, the answer inference accuracy is only 70.53%.\nCompared with the QCM →A variant (80.40%) in Table 2,\nthe result shows that the generated rationale in the two-stage\nframework does not improve answer accuracy.\nTable 3. Two-stage setting of (i) rationale generation (RougeL) and\n(ii) answer inference (Accuracy).\nMethod (i) QCM →R (ii) QCMR→A\nTwo-Stage Framework 91.76 70.53\nw/ Captions 91.85 71.12\nw/ Vision Features 96.97 84.91\nThen, we randomly sample 50 error cases and ﬁnd that the\nmodel tends to generate hallucinated rationales that mislead\nthe answer inference. As an example shown in Figure 2, the\nmodel (left part) hallucinates that, “ The south pole of one\nmagnet is closest to the south pole of the other magnet ”, due\nto the lack of reference to the vision content. We ﬁnd that\nsuch mistakes occur at a ratio of 64% among the error cases\nOthers(36%)Resolved (62.5%)Unresolved(37.5%)Hallucination(64%)(a) ratio of hallucination mistakes(b) correction rate w/ vision features  Figure 3. The ratio of hallucination mistakes (a) and correction\nrate w/ vision features (b).\n(Figure 3(a)).\n3.3. Multimodality Contributes to Effective Rationales\nWe speculate that such a phenomenon of hallucination is\ndue to a lack of necessary vision contexts for performing\neffective Multimodal-CoT. To inject vision information, a\nsimple way is to transform the paired image into a caption\n(Lu et al., 2022a) and then append the caption in the input of\nboth stages. However, as shown in Table 3, using captions\nonly yields marginal performance gains ( ↑0.59%). Then,\nwe explore an advanced technique by incorporating vision\nfeatures into the language model. Concretely, we feed the\npaired image to the DETR model (Carion et al., 2020) to\nextract vision features. Then we fuse the vision features', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nVisionLanguageRationale GenerationQuestion:Whichpropertydothesetwoobjectshaveincommon?Context: Select the better answer.Lookateachobject.Foreachobject,decideifithasthatproperty.Potatochipshaveasaltytaste.Bothobjectsaresalty.Asoftobjectchangesshapewhenyousqueezeit.Thefriesaresoft,butthecrackerisnot.Thepropertythatbothobjectshaveincommonissalty.\nOptions:(B) salty(A) softRationaleAnswer InferenceTheansweris(B).Answer\nFigure 4. Overview of our Multimodal-CoT framework. Multimodal-CoT consists of two stages: (i) rationale generation and (ii) answer\ninference. Both stages share the same model architecture but differ in the input and output. In the ﬁrst stage, we feed the model with\nlanguage and vision inputs to generate rationales. In the second stage, we append the original language input with the rationale generated\nfrom the ﬁrst stage. Then, we feed the updated language input with the original vision input to the model to infer the answer.\nwith the encoded language representations before feeding\nto the decoder (more details will be presented in Section\n4). Interestingly, with vision features, the RougeL score of\nthe rationale generation has boosted to 96.97% (QCM →R),\nwhich correspondingly contributes to better answer accuracy\nof 84.91% (QCMR →A). With those effective rationales,\nthe phenomenon of hallucination is mitigated — 62.5%\nhallucination mistakes in Section 3.2 have been corrected\n(Figure 3(b)), as an example shown in Figure 2 (right part).4\nThe analysis so far compellingly shows that vision features\nare indeed beneﬁcial for generating effective rationales and\ncontributing to accurate answer inference. As the two-stage\nmethod (QCMR→A) in Table 3 achieves better performance\nthan all the one-stage method in Table 2, we choose the two-\nstage method in our Multimodal-CoT framework.\n4. Multimodal-CoT\nBased on the observations and discussions in Section 3, we\npropose Multimodal-CoT to incorporate language (text) and\nvision (images) modalities into a two-stage framework. In\nthis section, we will ﬁrst overview the procedure of the\nframework and then elaborate on the technical design of the\nmodel architecture.\n4.1. Framework Overview\nMultimodal-CoT consists of two training stages: (i) ratio-\nnale generation and (ii) answer inference. Both stages share\nthe same model architecture but differ in the input Xand\noutputY. The overall procedure is illustrated in Figure 4.\nWe will take vision-language as an example to show how\nMultimodal-CoT works.\n4The left mistakes are mainly about map understanding, which\nrequires more advanced vision features. We will discuss them in\nSection 6.4.In the rationale generation stage, we feed the model with\nX={X1\nlanguage,Xvision}whereX1\nlanguage represents the lan-\nguage input in the ﬁrst stage and Xvision represents the vision\ninput, i.e., the image. For example, Xcan be instantiated as\na concatenation of question, context, and options of a multi-\nple choice reasoning problem (Lu et al., 2022a) as shown in\nFigure 4. The goal is to learn a rationale generation model\nR=F(X)whereRis the rationale.\nIn the answer inference stage, the rationale Ris appended\nto the original language input X1\nlanguage to construct the lan-\nguage input in the second stage, X2\nlanguage =X1\nlanguage◦R\nwhere◦denotes concatenation. Then, we feed the updated\ninputX′={X2\nlanguage,Xvision}to the answer inference\nmodel to infer the ﬁnal answer A=F(X′).\nIn both stages, we train two models with the same archi-\ntecture independently. They take the annotated elements\n(e.g.,X→R,XR→A, respectively) from the training\nset for supervised learning. During inference, given X, the\nrationales for the test sets are generated using the model\ntrained in the ﬁrst stage; they are used in the second stage\nfor answer inference.\n4.2. Model Architecture\nGiven the language input Xlanguage∈{X1\nlanguage,X2\nlanguage}\nand the vision input Xvision, we compute the probability of\ngenerating target text Y(either the rationale or the answer\nin Figure 4) of length Nby\np(Y|Xlanguage,Xvision ) =N∏\ni=1pθ(Yi|Xlanguage,Xvision,Y<i),\n(1)\nwherepθ(Yi|Xlanguage,Xvision,Y<i)is implemented with\na Transformer-based network (Vaswani et al., 2017). The\nnetwork has three major procedures: encoding, interaction,', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nAlgorithm 1 Multimodal-CoT\nInput: Language input X1\nlanguage , vision input Xvision\nOutput: Generated rationale R, inferred answer A\n1: Construct the input X={Xlanguage,X vision}\n2: Generate rationale R=F(X)using the model F(·)\n3:Append the rationale Rto the original language input\nX2\nlanguage =X1\nlanguage◦R.\n4: Construct new input X′={X2\nlanguage,X vision}\n5:Infer the answer Aby conditioning on the new input, A=\nF(X′).\n6:procedure F(X)\n7: Encode the language and vision inputs Hlanguage andHvision,\nrespectively\n8: Build the interaction between language and vision features\nby attention Hattn\nvision\n9: FuseHlanguage andHattn\nvision by a gated fusion mechanism to\nhaveHfuse\n10: FeedHfuseto the decoder to obtain the target prediction Y\n11: return Y\n12:end procedure\nand decoding. Speciﬁcally, we feed the language text into\na Transformer encoder to obtain a textual representation,\nwhich is then interacted and fused with the vision represen-\ntation before being fed into the Transformer decoder.\nEncoding The modelF(X)takes both the language and\nvision inputs and obtains the text representation Hlanguage\nand the image feature Hvision by the following functions:\nHlanguage =LanguageEncoder (Xlanguage ), (2)\nHvision =Wh·VisionExtractor (Xvision ),(3)\nwhere LanguageEncoder( ·) is implemented as a Trans-\nformer model. We use the hidden states of the last layer\nin the Transformer encoder as the language representation\nHlanguage∈Rn×dwherendenotes the length of the lan-\nguage input, and dis the hidden dimension. Meanwhile,\nVisionExtractor(·) is used to vectorize the input image into\nvision features. Inspired by the recent success of Vision\nTransformers (Dosovitskiy et al., 2021), we fetch the patch-\nlevel features by off-the-shelf vision extraction models,5\nsuch as DETR (Carion et al., 2020). After obtaining the\npatch-level vision features, we apply a learnable projection\nmatrixWhto convert the shape of VisionExtractor (Xvision )\ninto that ofHlanguage ; thus we have Hvision∈Rm×dwhere\nmis the number of patches.\nInteraction After obtaining language and vision represen-\ntations, we use a single-head attention network to correlate\ntext tokens with image patches, where the query ( Q), key\n(K) and value (V) areHlanguage ,Hvision andHvision, respec-\n5The parameters of the vision extraction are frozen.tively. The attention output Hattn\nvision∈Rn×dis deﬁned as:\nHattn\nvision =Softmax (QK⊤\n√dk)V, (4)\nwheredkis the same as the dimension of Hlanguage because\na single head is used.\nThen, we apply the gated fusion mechanism (Zhang et al.,\n2020; Wu et al., 2021; Li et al., 2022a) to fuse Hlanguage and\nHvision. The fused output Hfuse∈Rn×dis obtained by:\nλ=Sigmoid (WlHlanguage +WvHattn\nvision ),(5)\nHfuse = (1−λ)·Hlanguage +λ·Hattn\nvision, (6)\nwhereWlandWvare learnable parameters.\nDecoding Finally, the fused output Hfuseis fed into the\nTransformer decoder to predict the target Y. The complete\nprocedure of Multimodal-CoT is shown in Algorithm 1.\n5. Experiments\nThis section will present the benchmark dataset, the imple-\nmentation of our technique, and the baselines for compar-\nisons. Then, we will report our main results and ﬁndings.\n5.1. Dataset\nOur method is evaluated on the ScienceQA benchmark (Lu\net al., 2022a). ScienceQA is the ﬁrst large-scale multimodal\nscience question dataset that annotates the answers with de-\ntailed lectures and explanations. It contains 21k multimodal\nmultiple choice questions with rich domain diversity across\n3 subjects, 26 topics, 127 categories, and 379 skills. The\nbenchmark dataset is split into training, validation, and test\nsplits with 12726, 4241, and 4241 examples, respectively.\n5.2. Implementation\nThe following part presents the experimental settings of\nMultimodal-CoT and the baseline methods.\nExperimental Settings As the Multimodal-CoT task re-\nquires generating the reasoning chains and leveraging the\nvision features, we use the T5 encoder-decoder architec-\nture (Raffel et al., 2020). Speciﬁcally, we adopt UniﬁedQA\n(Khashabi et al., 2020) to initialize our models in the two\nstages because it achieves the best ﬁne-tuning results in\nLu et al. (2022a). To verify the generality of our approach\nacross different LMs, we also employ FLAN-T5 (Chung\net al., 2022) as the backbone in Section 6.3. As using im-\nage captions does not yield signiﬁcant performance gains in\nSection 3.3, we did not use the captions. We ﬁne-tune the\nmodels up to 20 epochs, with a learning rate of 5e-5. The', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}]: %s
2023-12-07 11:04:25,696 - INFO - Check the data that is being passed [{'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nTable 4. Main results (%). Size = backbone model size. Question classes: NAT = natural science, SOC = social science, LAN = language\nscience, TXT = text context, IMG = image context, NO = no context, G1-6 = grades 1-6, G7-12 = grades 7-12. Results except ours are\ntaken from Lu et al. (2022a). Segment 1: Human performance; Segment 2: VQA baselines; Segment 3: UniﬁedQA baselines; Segment 4:\nGPT-3.5 baselines; Segment 5: Our Multimodal-CoT results. Results in bold are the best performance.\nModel Size NAT SOC LAN TXT IMG NO G1-6 G7-12 Avg\nHuman -90.23 84.97 87.48 89.60 87.50 88.10 91.59 82.42 88.40\nMCAN (Yu et al., 2019) 95M 56.08 46.23 58.09 59.43 51.17 55.40 51.65 59.72 54.54\nTop-Down (Anderson et al., 2018) 70M 59.50 54.33 61.82 62.90 54.88 59.79 57.27 62.16 59.02\nBAN (Kim et al., 2018) 112M 60.88 46.57 66.64 62.61 52.60 65.51 56.83 63.94 59.37\nDFAF (Gao et al., 2019) 74M 64.03 48.82 63.55 65.88 54.49 64.11 57.12 67.17 60.72\nViLT (Kim et al., 2021) 113M 60.48 63.89 60.27 63.20 61.38 57.00 60.72 61.90 61.14\nPatch-TRM (Lu et al., 2021) 90M 65.19 46.79 65.55 66.96 55.28 64.95 58.04 67.50 61.42\nVisualBERT (Li et al., 2019) 111M 59.33 69.18 61.18 62.71 62.17 58.54 62.96 59.92 61.87\nUniﬁedQA Base (Khashabi et al., 2020) 223M 68.16 69.18 74.91 63.78 61.38 77.84 72.98 65.00 70.12\nUniﬁedQA Base w/ CoT (Lu et al., 2022a) 223M 71.00 76.04 78.91 66.42 66.53 81.81 77.06 68.82 74.11\nGPT-3.5 (Chen et al., 2020) 175B 74.64 69.74 76.00 74.44 67.28 77.42 76.80 68.89 73.97\nGPT-3.5 w/ CoT (Lu et al., 2022a) 175B 75.44 70.87 78.09 74.68 67.43 79.93 78.23 69.68 75.17\nMutimodal-CoT Base 223M 87.52 77.17 85.82 87.88 82.90 86.83 84.65 85.37 84.91\nMutimodal-CoT Large 738M 95.91 82.00 90.82 95.26 88.80 92.89 92.44 90.31 91.68\nTable 5. Ablation results of Multimodal-CoT.\nModel NAT SOC LAN TXT IMG NO G1-6 G7-12 Avg\nMultimodal-CoT 87.52 77.17 85.82 87.88 82.90 86.83 84.65 85.37 84.91\nw/o Two-Stage Framework 80.99 87.40 81.91 80.25 78.83 83.62 82.78 82.20 82.57\nw/o Vision Features 71.09 70.75 69.18 71.16 65.84 71.57 71.00 69.68 70.53\nmaximum input sequence length is 512. The batch sizes for\nthe base and large models are 16 and 8, respectively. Our\nexperiments are run on 4 NVIDIA Tesla V100 32G GPUs.\nBaseline Models Following Lu et al. (2022a), our base-\nlines include (i) Visual question answering (VQA) models\n(Anderson et al., 2018; Kim et al., 2018; Yu et al., 2019;\nGao et al., 2019; Kim et al., 2021; Lu et al., 2021; Li et al.,\n2019); (ii) Text-to-text LM models. (Khashabi et al., 2020);\n(iii) GPT-3.5 models (Chen et al., 2020). More details are\npresented in Appendix B.1.\n5.3. Main Results\nTable 4 shows the main results. Mutimodal-CoT Large out-\nperforms GPT-3.5 by 16.51% (75.17% →91.68%) and sur-\npasses human performance. Speciﬁcally, among the 8\nquestion classes, Mutimodal-CoT Large achieves a 21.37%\n(67.43%→88.80%) performance gain for the questions with\npaired images (IMG). Compared with existing UniﬁedQA\nand GPT-3.5 methods that leverage image captions in the\ncontext to provide vision semantics, the results indicate that\nusing image features is more effective. In addition, our\ntwo-stage framework contributes to the superior results ac-\ncording to our ablation study results in Table 5. Overall,\nthe results verify the effectiveness of multimodality and the\npotential of achieving CoT reasoning with 1B-models via\nour two-stage framework.12345678910405060708090\nEpochAccuracyOne-stage Baseline One-stage Multimodal\nTwo-Stage Baseline Two-Stage Multimodal\nFigure 5. Accuracy curve of the No-CoT baseline and Multimodal-\nCoT variants across epochs.\n6. Analysis\nThe following analysis will investigate how Multimodal-\nCoT works and discuss contribution factors and limitations.\nWe use models under the base size for analysis unless\notherwise stated.\n6.1. Multimodality Boosts Convergence\nFigure 5 shows the evaluation accuracy curve of the baseline\nand Multimodal-CoT in different training epochs. “One-\nstage” is based on the QCM →A input-output format as it', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nachieves the best performance in Table 2 and “Two-stage”\nis our two-stage framework. We ﬁnd that the two-stage\nmethods achieve relatively higher accuracy at the beginning\nthan the one-stage baselines that generate the answer directly\nwithout CoT. However, without the vision features, the two-\nstage baseline could not yield better results as the training\ngoes on due to the low-quality rationales (as observed in\nSection 3). In contrast, using vision features helps generate\nmore effective rationales that contribute to better answer\naccuracy in our two-stage multimodal variant.\n6.2. Using Different Vision Features\nDifferent vision features may affect the model performance.\nWe compare three widely-used types of vision features,\nCLIP (Radford et al., 2021), DETR (Carion et al., 2020),\nand ResNet (He et al., 2016). CLIP and DETR are patch-like\nfeatures where DETR is based on object detection. For the\nResNet features, we repeat the pooled features of ResNet-\n50 to the same length with the text sequence to imitate the\npatch-like features, where each patch is the same as the\npooled image features. More details of the vision features\nare presented in Appendix B.2.\nTable 6. Accuracy (%) of using different vision features.\nMethod One-stage Two-Stage\nw/ CLIP 81.21 84.81\nw/ DETR 82.57 84.91\nw/ ResNet 80.97 84.77\nTable 6 shows the comparative results of vision features. We\nobserve that using vision features generally achieves better\nperformance than the language only baseline. Speciﬁcally,\nDETR achieves relatively better performance in general.\nTherefore, we use DETR by default in Multimodal-CoT.\n6.3. General Effectiveness Across Backbone Models\nTo test the generality of the beneﬁts of our approach to\nother backbone models, we alter the underlying LMs to\nother variants in different sizes or types. As shown in Table\n7, our approach is generally effective for the widely-used\nbackbone models.\nTable 7. Accuracy (%) with different backbone language models.\nMethod Size Language Only Mutimodal-CoT\nUniﬁedQA Base 223M 80.40 84.91\nUniﬁedQA Large 738M 83.60 91.68\nFLAN-T5 Base 248M 83.42 85.85\nFLAN-T5 Large 783M 85.19 93.02\n6.4. Error Analysis\nTo better understand the behavior of Multimodal-CoT and\nfacilitate future studies, we manually investigate randomly\nselected examples generated by our approach. Table 8 sum-marizes the categorization results generated by Multimodal-\nCoT. We randomly picked up 50 samples whose answers\nwere correct and 50 samples whose answers were incor-\nrect. The corresponding examples from each category are\npresented in Appendix C.\nTable 8. Categorization analysis of Multimodal-CoT.\nAnswer CoT Category Percentage (%)\nCorrectCoT is correct 90\nCoT is incorrect 10\nIncorrectCommonsense Mistake 82\nLogical Mistake 12\nCoT is correct 6\nWe ﬁnd that the correct samples (i.e., whose answers are cor-\nrect) contain a certain amount of incorrect chain-of-thought\n(10%). The results indicate that CoT may not always beneﬁt\nthe answer inference, and the model is robust to some extent\n— it can predict the correct answer by ignoring incorrect\nrationales. For incorrect samples (i.e., whose answers are\nincorrect), commonsense mistake in the CoT is the most\nfrequent error type (88%). The model often makes com-\nmonsense mistakes when answering the questions requires\ncommonsense knowledge, e.g., understand maps and count-\ning numbers in the images (Figure 9), and utilizing the\nalphabet (Figure 10). The other type of mistake is a logical\nmistake (12%), with contradictions in the reasoning chains\n(Figure 11). In addition, there are cases with incorrect an-\nswers while their CoT are correct (6%) but might not be\nnecessarily related to answer options (Figure 12).\nThe analysis indicates that there are prospective directions\nfor future studies. It is possible to improve Multimodal-\nCoT by (i) incorporating more informative vision features\nand improving language-vision interaction to be capable of\nunderstanding maps and counting numbers; (ii) injecting\ncommonsense knowledge; (iii) applying a ﬁltering mecha-\nnism, e.g., using only the effective CoT to infer the answer\nand get rid of irrelevant CoT.\n7. Conclusion\nWe formally study the problem of multimodal CoT. We pro-\npose Multimodal-CoT that incorporates language and vision\nmodalities into a two-stage framework that separates ratio-\nnale generation and answer inference, so answer inference\ncan leverage better generated rationales from multimodal in-\nformation. With Multimodal-CoT, we show that our method\nsurpasses GPT-3.5 by 16 percentage points in accuracy on\nthe ScienceQA benchmark. Our error analysis shows that\nit is the potential to leverage more effective vision features,\ninject commonsense knowledge, and apply ﬁltering mecha-\nnisms to improve CoT reasoning in future studies.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nReferences\nAnderson, P., He, X., Buehler, C., Teney, D., Johnson, M.,\nGould, S., and Zhang, L. Bottom-up and top-down atten-\ntion for image captioning and visual question answering.\nIn2018 IEEE Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2018, Salt Lake City, UT, USA,\nJune 18-22, 2018 , pp. 6077–6086. IEEE Computer Soci-\nety, 2018. doi: 10.1109/CVPR.2018.00636.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,\nHenighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,\nJ., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,\nGray, S., Chess, B., Clark, J., Berner, C., McCandlish,\nS., Radford, A., Sutskever, I., and Amodei, D. Language\nmodels are few-shot learners. In Larochelle, H., Ranzato,\nM., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances\nin Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020.\nCarion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov,\nA., and Zagoruyko, S. End-to-end object detection with\ntransformers. In Computer Vision–ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23–28, 2020,\nProceedings, Part I , pp. 213–229, 2020.\nChen, T., Kornblith, S., Swersky, K., Norouzi, M., and\nHinton, G. E. Big self-supervised models are strong\nsemi-supervised learners. In Larochelle, H., Ranzato, M.,\nHadsell, R., Balcan, M., and Lin, H. (eds.), Advances\nin Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020.\nChen, W., Ma, X., Wang, X., and Cohen, W. W. Program\nof thoughts prompting: Disentangling computation from\nreasoning for numerical reasoning tasks. ArXiv preprint ,\nabs/2211.12588, 2022.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton,\nC., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,\nS., Maynez, J., Rao, A., Barnes, P., Tay, Y ., Shazeer,\nN., Prabhakaran, V ., Reif, E., Du, N., Hutchinson, B.,\nPope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G.,\nYin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S.,\nMichalewski, H., Garcia, X., Misra, V ., Robinson, K., Fe-\ndus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph,\nB., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal,\nS., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M.,\nLewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee,\nK., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O.,\nCatasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean,J., Petrov, S., and Fiedel, N. Palm: Scaling language mod-\neling with pathways. ArXiv preprint , abs/2204.02311,\n2022.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y .,\nFedus, W., Li, E., Wang, X., Dehghani, M., Brahma,\nS., et al. Scaling instruction-ﬁnetuned language models.\narXiv preprint arXiv:2210.11416 , 2022.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\nHeigold, G., Gelly, S., et al. An image is worth 16x16\nwords: Transformers for image recognition at scale. In\nThe International Conference on Learning Representa-\ntions (ICLR) , 2021.\nFu, Y ., Peng, H., Sabharwal, A., Clark, P., and Khot, T.\nComplexity-based prompting for multi-step reasoning.\nArXiv preprint , abs/2210.00720, 2022.\nGao, P., Jiang, Z., You, H., Lu, P., Hoi, S. C. H., Wang, X.,\nand Li, H. Dynamic fusion with intra- and inter-modality\nattention ﬂow for visual question answering. In IEEE\nConference on Computer Vision and Pattern Recognition,\nCVPR 2019, Long Beach, CA, USA, June 16-20, 2019 , pp.\n6639–6648. Computer Vision Foundation / IEEE, 2019.\ndoi: 10.1109/CVPR.2019.00680.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\nlearning for image recognition. In 2016 IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2016,\nLas Vegas, NV, USA, June 27-30, 2016 , pp. 770–778.\nIEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.\n90.\nHo, N., Schmid, L., and Yun, S.-Y . Large language models\nare reasoning teachers. arXiv preprint arXiv:2212.10071 ,\n2022.\nJi, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y ., Ishii, E.,\nBang, Y ., Madotto, A., and Fung, P. Survey of halluci-\nnation in natural language generation. ACM Computing\nSurveys , 2022.\nKhashabi, D., Min, S., Khot, T., Sabharwal, A., Tafjord,\nO., Clark, P., and Hajishirzi, H. UNIFIEDQA: Crossing\nformat boundaries with a single QA system. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2020 , pp. 1896–1907, Online, 2020. Association\nfor Computational Linguistics. doi: 10.18653/v1/2020.\nﬁndings-emnlp.171.\nKhot, T., Trivedi, H., Finlayson, M., Fu, Y ., Richardson, K.,\nClark, P., and Sabharwal, A. Decomposed prompting:\nA modular approach for solving complex tasks. ArXiv\npreprint , abs/2210.02406, 2022.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nKim, J., Jun, J., and Zhang, B. Bilinear attention networks.\nIn Bengio, S., Wallach, H. M., Larochelle, H., Grauman,\nK., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances\nin Neural Information Processing Systems 31: Annual\nConference on Neural Information Processing Systems\n2018, NeurIPS 2018, December 3-8, 2018, Montr ´eal,\nCanada , pp. 1571–1581, 2018.\nKim, W., Son, B., and Kim, I. Vilt: Vision-and-language\ntransformer without convolution or region supervision.\nInProceedings of the 38th International Conference on\nMachine Learning (ICML) , pp. 5583–5594, 2021.\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y ., and Iwasawa,\nY . Large language models are zero-shot reasoners. ArXiv\npreprint , abs/2205.11916, 2022.\nLi, B., Lv, C., Zhou, Z., Zhou, T., Xiao, T., Ma, A., and Zhu,\nJ. On vision features in multimodal machine translation.\nInProceedings of the 60th Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: Long\nPapers) , pp. 6327–6337, 2022a.\nLi, L. H., Yatskar, M., Yin, D., Hsieh, C.-J., and Chang,\nK.-W. Visualbert: A simple and performant baseline for\nvision and language. ArXiv preprint , abs/1908.03557,\n2019.\nLi, Y ., Lin, Z., Zhang, S., Fu, Q., Chen, B., Lou, J.-G., and\nChen, W. On the advance of making language models\nbetter reasoners. ArXiv preprint , abs/2206.02336, 2022b.\nLu, P., Qiu, L., Chen, J., Xia, T., Zhao, Y ., Zhang, W., Yu,\nZ., Liang, X., and Zhu, S.-C. Iconqa: A new benchmark\nfor abstract diagram understanding and visual language\nreasoning. In The 35th Conference on Neural Information\nProcessing Systems (NeurIPS) Track on Datasets and\nBenchmarks , 2021.\nLu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu,\nS.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to\nexplain: Multimodal reasoning via thought chains for sci-\nence question answering. ArXiv preprint , abs/2209.09513,\n2022a.\nLu, P., Qiu, L., Chang, K.-W., Wu, Y . N., Zhu, S.-C., Ra-\njpurohit, T., Clark, P., and Kalyan, A. Dynamic prompt\nlearning via policy gradient for semi-structured mathemat-\nical reasoning. ArXiv preprint , abs/2209.14610, 2022b.\nMagister, L. C., Mallinson, J., Adamek, J., Malmi, E., and\nSeveryn, A. Teaching small language models to reason.\nArXiv preprint , abs/2212.08410, 2022.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\net al. Learning transferable visual models from natural\nlanguage supervision. In International Conference on\nMachine Learning , pp. 8748–8763. PMLR, 2021.Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J.,\nSong, F., Aslanides, J., Henderson, S., Ring, R., Young,\nS., Rutherford, E., Hennigan, T., Menick, J., Cassirer, A.,\nPowell, R., Driessche, G. v. d., Hendricks, L. A., Rauh,\nM., Huang, P.-S., Glaese, A., Welbl, J., Dathathri, S.,\nHuang, S., Uesato, J., Mellor, J., Higgins, I., Creswell,\nA., McAleese, N., Wu, A., Elsen, E., Jayakumar, S.,\nBuchatskaya, E., Budden, D., Sutherland, E., Simonyan,\nK., Paganini, M., Sifre, L., Martens, L., Li, X. L., Kun-\ncoro, A., Nematzadeh, A., Gribovskaya, E., Donato, D.,\nLazaridou, A., Mensch, A., Lespiau, J.-B., Tsimpoukelli,\nM., Grigorev, N., Fritz, D., Sottiaux, T., Pajarskas, M.,\nPohlen, T., Gong, Z., Toyama, D., d’Autume, C. d. M.,\nLi, Y ., Terzi, T., Mikulik, V ., Babuschkin, I., Clark, A.,\nCasas, D. d. L., Guy, A., Jones, C., Bradbury, J., Johnson,\nM., Hechtman, B., Weidinger, L., Gabriel, I., Isaac, W.,\nLockhart, E., Osindero, S., Rimell, L., Dyer, C., Vinyals,\nO., Ayoub, K., Stanway, J., Bennett, L., Hassabis, D.,\nKavukcuoglu, K., and Irving, G. Scaling language mod-\nels: Methods, analysis & insights from training gopher.\nArXiv preprint , abs/2112.11446, 2021.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring the\nlimits of transfer learning with a uniﬁed text-to-text trans-\nformer. Journal of Machine Learning Research (JMLR) ,\n21:1–67, 2020.\nRubin, O., Herzig, J., and Berant, J. Learning to re-\ntrieve prompts for in-context learning. In Proceedings\nof the 2022 Conference of the North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies , pp. 2655–2671, 2022. doi:\n10.18653/v1/2022.naacl-main.191.\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kul-\nshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L.,\nDu, Y ., Li, Y ., Lee, H., Zheng, H. S., Ghafouri, A., Mene-\ngali, M., Huang, Y ., Krikun, M., Lepikhin, D., Qin, J.,\nChen, D., Xu, Y ., Chen, Z., Roberts, A., Bosma, M.,\nZhao, V ., Zhou, Y ., Chang, C.-C., Krivokon, I., Rusch,\nW., Pickett, M., Srinivasan, P., Man, L., Meier-Hellstern,\nK., Morris, M. R., Doshi, T., Santos, R. D., Duke, T.,\nSoraker, J., Zevenbergen, B., Prabhakaran, V ., Diaz, M.,\nHutchinson, B., Olson, K., Molina, A., Hoffman-John, E.,\nLee, J., Aroyo, L., Rajakumar, R., Butryna, A., Lamm,\nM., Kuzmina, V ., Fenton, J., Cohen, A., Bernstein, R.,\nKurzweil, R., Aguera-Arcas, B., Cui, C., Croak, M., Chi,\nE., and Le, Q. Lamda: Language models for dialog\napplications. ArXiv preprint , abs/2201.08239, 2022.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need. In Guyon, I., von Luxburg, U., Bengio,\nS., Wallach, H. M., Fergus, R., Vishwanathan, S. V . N.,\nand Garnett, R. (eds.), Advances in Neural Information', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nProcessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA , pp. 5998–6008, 2017.\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E.,\nand Zhou, D. Self-consistency improves chain of\nthought reasoning in language models. ArXiv preprint ,\nabs/2203.11171, 2022a.\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and\nZhou, D. Rationale-augmented ensembles in language\nmodels. ArXiv preprint , abs/2207.00747, 2022b.\nWei, J., Tay, Y ., Bommasani, R., Raffel, C., Zoph, B.,\nBorgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Met-\nzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang,\nP., Dean, J., and Fedus, W. Emergent abilities of large\nlanguage models. Transactions on Machine Learning\nResearch , 2022a. Survey Certiﬁcation.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E.,\nLe, Q., and Zhou, D. Chain of thought prompting elic-\nits reasoning in large language models. ArXiv preprint ,\nabs/2201.11903, 2022b.\nWu, Z., Kong, L., Bi, W., Li, X., and Kao, B. Good for\nmisconceived reasons: An empirical revisiting on the\nneed for visual context in multimodal machine transla-\ntion. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers) , pp. 6153–6166, Online,\n2021. Association for Computational Linguistics. doi:\n10.18653/v1/2021.acl-long.480.\nYu, Z., Yu, J., Cui, Y ., Tao, D., and Tian, Q. Deep modu-\nlar co-attention networks for visual question answering.\nInIEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2019, Long Beach, CA, USA, June\n16-20, 2019 , pp. 6281–6290. Computer Vision Founda-\ntion / IEEE, 2019. doi: 10.1109/CVPR.2019.00644.\nZhang, Z., Chen, K., Wang, R., Utiyama, M., Sumita, E., Li,\nZ., and Zhao, H. Neural machine translation with univer-\nsal visual representation. In 8th International Conference\non Learning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020 . OpenReview.net, 2020.\nZhang, Z., Zhang, A., Li, M., and Smola, A. Automatic\nchain of thought prompting in large language models.\nArXiv preprint , abs/2210.03493, 2022.\nZhang, Z., Chen, K., Wang, R., Utiyama, M., Sumita, E., Li,\nZ., and Zhao, H. Universal multimodal representation for\nlanguage understanding. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence , pp. 1–18, 2023. doi:\n10.1109/TPAMI.2023.3234170.Zhou, D., Sch ¨arli, N., Hou, L., Wei, J., Scales, N., Wang,\nX., Schuurmans, D., Bousquet, O., Le, Q., and Chi, E.\nLeast-to-most prompting enables complex reasoning in\nlarge language models. ArXiv preprint , abs/2205.10625,\n2022.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nA. Extended analysis for the challenge of Multimodal-CoT\nA.1. More Examples of Misleading by Hallucinated Rationales\nAccording to our case studies (Section 3.2), we ﬁnd that the baseline tends to generate hallucinated rationales. We provide\nfurther examples as shown in Figure 6.\nOptions:(B) lichen(A) mushroomProblem\nBaselineQuestion:Basedonthearrows,whichofthefollowingorganismsisaconsumer?Context:BelowisafoodwebfromatundraecosysteminNunavut,aterritoryinNorthernCanada.Afoodwebmodelshowthemattereatenbyorganismsmovesthroughanecosystem.Thearrowsinafoodwebrepresenthowmattermovesbetweenorganismsinanecosystem.GoldRationale:Afoodwebisamodel…Anorganisminafoodwebcanhavemorethanonearrowpointingfromit.Thisshowsthattheorganismiseatenbymorethanoneotherorganisminthefoodweb.Anorganisminafoodwebcanalsohavemorethanonearrowpointingtoit.Thisshowsthattheorganismeatsmorethanoneotherorganisminthefoodweb.Consumerseatotherorganisms.So,therearearrowsinafoodwebthatpointfromotherorganismstoconsumers.Themushroomhasarrowspointingtoitfromthebarren-groundcaribouandthegrizzlybear.So,themushroomisaconsumer.Thelichendoesnothaveanyarrowspointingtoit.So,thelichenisaproducer,notaconsumer.Answer:Theansweris(A).Rationale:Afoodwebisamodel…Thisshowsthattheorganismiseatenbymorethanoneotherorganisminthefoodweb.Anorganisminafoodwebcanalsohavemorethanonearrowpointingtoit.Thisshowsthattheorganismeatsmorethanoneotherorganisminthefoodweb.Consumerseatproducers.So,inafoodweb,consumershavearrowspointingtothemfromproducers.Themushroomhasanarrowpointingtoitfromthemushroom.Themushroomisnotaconsumer,sothemushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Answer:Theansweris(B).Rationale:Afoodwebisamodel…Anorganisminafoodwebcanhavemorethanonearrowpointingfromit.Thisshowsthattheorganismiseatenbymorethanoneotherorganisminthefoodweb.Anorganisminafoodwebcanalsohavemorethanonearrowpointingtoit.Thisshowsthattheorganismeatsmorethanoneotherorganisminthefoodweb.Consumerseatproducersorganisms.So,inarearrowsafoodwebthatpointfromotherorganismstoconsumers.Themushroomhasarrowspointingtoitfromthen-browmushroomibou.thelichenzzlybear.So,themushroomisaconsumer.Thelichenhasnothaveanyarrowspointingtoit.So,themushroomisaconsumer.Lichenisnotaconsumer.Answer:Theansweris(B).Vision\n+ Vision Features\nOptions:(B) sample A(A) neither; the samples have the same temperatureProblem\nBaselineQuestion:Comparetheaveragekineticenergiesoftheparticlesineachsample.Whichsamplehasthehighertemperature?Context:Thediagramsbelowshowtwopuresamplesofgasinidenticalclosed,rigidcontainers.Eachcoloredballrepresentsonegasparticle.Bothsampleshavethesamenumberofparticles.GoldRationale:Thetemperatureofasubstancedependsontheaveragekineticenergyoftheparticlesinthesubstance.Thehighertheaveragekineticenergyoftheparticles,thehigherthetemperatureofthesubstance.Thekineticenergyofaparticleisdeterminedbyitsmassandspeed.Forapuresubstance,thegreaterthemassofeachparticleinthesubstanceandthehighertheaveragespeedoftheparticles,thehighertheiraveragekineticenergy.EachparticleinsampleAhasmoremassthaneachparticleinsampleB.TheparticlesinsampleAalso+ Vision FeaturesVision(C) sample B\nhaveahigheraveragespeedthantheparticlesinsampleB.So,theparticlesinsampleAhaveahigheraveragekineticenergythantheparticlesinsampleB.BecausetheparticlesinsampleAhavethehigheraveragekineticenergy,sampleAmusthavethehighertemperature.Answer:Theansweris(B).Rationale:Thetemperatureofasubstancedependsontheaveragekineticenergyoftheparticlesinthesubstance.Thehighertheaveragekineticenergyoftheparticles,thehigherthetemperatureofthesubstance.Thekineticenergyofaparticleisdeterminedbyitsmassandspeed.Forapuresubstance,thegreaterthemassofeachparticleinthesubstanceandthehighertheaveragespeedoftheparticles,thehighertheiraveragekineticenergy.EachparticleinsampleBhasmoremassthaneachparticleinsampleA.TheparticlesinsampleBalsohaveahigheraveragespeedthantheparticlesinsampleA.So,theparticlesinsampleBhaveahigheraveragekineticenergythantheparticlesinsampleA.BecausetheparticlesinsampleBhavethehigheraveragekineticenergy,sampleBmusthavethehighertemperature.Answer:Theansweris(C).Rationale:Thetemperatureofasubstancedependsontheaveragekineticenergyoftheparticlesinthesubstance.Thehighertheaveragekineticenergyoftheparticles,thehigherthetemperatureofthesubstance.Thekineticenergyofaparticleisdeterminedbyitsmassandspeed.Forapuresubstance,thegreaterthemassofeachparticleinthesubstanceandthehighertheaveragespeedoftheparticles,thehighertheiraveragekineticenergy.EachparticleinsampleAhasmoremassthaneachparticleinsampleB.TheparticlesinsampleAalsohaveahigheraveragespeedthantheparticlesinsampleB.So,theparticlesinsampleAhaveahigheraveragekineticenergythantheparticlesinsampleB.BecausetheparticlesinsampleAhavethehigheraveragekineticenergy,sampleAmusthavethehighertemperature.Answer:Theansweris(B).\nFigure 6. Examples of the two-stage framework without vision features (baseline) and with vision features (ours) for generating rationales\nand predicting answers. The upper part presents the problem details, and the lower part shows the outputs of the baseline and our method.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}]: %s
2023-12-07 11:04:25,697 - INFO - Check the results [{'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nTable 4. Main results (%). Size = backbone model size. Question classes: NAT = natural science, SOC = social science, LAN = language\nscience, TXT = text context, IMG = image context, NO = no context, G1-6 = grades 1-6, G7-12 = grades 7-12. Results except ours are\ntaken from Lu et al. (2022a). Segment 1: Human performance; Segment 2: VQA baselines; Segment 3: UniﬁedQA baselines; Segment 4:\nGPT-3.5 baselines; Segment 5: Our Multimodal-CoT results. Results in bold are the best performance.\nModel Size NAT SOC LAN TXT IMG NO G1-6 G7-12 Avg\nHuman -90.23 84.97 87.48 89.60 87.50 88.10 91.59 82.42 88.40\nMCAN (Yu et al., 2019) 95M 56.08 46.23 58.09 59.43 51.17 55.40 51.65 59.72 54.54\nTop-Down (Anderson et al., 2018) 70M 59.50 54.33 61.82 62.90 54.88 59.79 57.27 62.16 59.02\nBAN (Kim et al., 2018) 112M 60.88 46.57 66.64 62.61 52.60 65.51 56.83 63.94 59.37\nDFAF (Gao et al., 2019) 74M 64.03 48.82 63.55 65.88 54.49 64.11 57.12 67.17 60.72\nViLT (Kim et al., 2021) 113M 60.48 63.89 60.27 63.20 61.38 57.00 60.72 61.90 61.14\nPatch-TRM (Lu et al., 2021) 90M 65.19 46.79 65.55 66.96 55.28 64.95 58.04 67.50 61.42\nVisualBERT (Li et al., 2019) 111M 59.33 69.18 61.18 62.71 62.17 58.54 62.96 59.92 61.87\nUniﬁedQA Base (Khashabi et al., 2020) 223M 68.16 69.18 74.91 63.78 61.38 77.84 72.98 65.00 70.12\nUniﬁedQA Base w/ CoT (Lu et al., 2022a) 223M 71.00 76.04 78.91 66.42 66.53 81.81 77.06 68.82 74.11\nGPT-3.5 (Chen et al., 2020) 175B 74.64 69.74 76.00 74.44 67.28 77.42 76.80 68.89 73.97\nGPT-3.5 w/ CoT (Lu et al., 2022a) 175B 75.44 70.87 78.09 74.68 67.43 79.93 78.23 69.68 75.17\nMutimodal-CoT Base 223M 87.52 77.17 85.82 87.88 82.90 86.83 84.65 85.37 84.91\nMutimodal-CoT Large 738M 95.91 82.00 90.82 95.26 88.80 92.89 92.44 90.31 91.68\nTable 5. Ablation results of Multimodal-CoT.\nModel NAT SOC LAN TXT IMG NO G1-6 G7-12 Avg\nMultimodal-CoT 87.52 77.17 85.82 87.88 82.90 86.83 84.65 85.37 84.91\nw/o Two-Stage Framework 80.99 87.40 81.91 80.25 78.83 83.62 82.78 82.20 82.57\nw/o Vision Features 71.09 70.75 69.18 71.16 65.84 71.57 71.00 69.68 70.53\nmaximum input sequence length is 512. The batch sizes for\nthe base and large models are 16 and 8, respectively. Our\nexperiments are run on 4 NVIDIA Tesla V100 32G GPUs.\nBaseline Models Following Lu et al. (2022a), our base-\nlines include (i) Visual question answering (VQA) models\n(Anderson et al., 2018; Kim et al., 2018; Yu et al., 2019;\nGao et al., 2019; Kim et al., 2021; Lu et al., 2021; Li et al.,\n2019); (ii) Text-to-text LM models. (Khashabi et al., 2020);\n(iii) GPT-3.5 models (Chen et al., 2020). More details are\npresented in Appendix B.1.\n5.3. Main Results\nTable 4 shows the main results. Mutimodal-CoT Large out-\nperforms GPT-3.5 by 16.51% (75.17% →91.68%) and sur-\npasses human performance. Speciﬁcally, among the 8\nquestion classes, Mutimodal-CoT Large achieves a 21.37%\n(67.43%→88.80%) performance gain for the questions with\npaired images (IMG). Compared with existing UniﬁedQA\nand GPT-3.5 methods that leverage image captions in the\ncontext to provide vision semantics, the results indicate that\nusing image features is more effective. In addition, our\ntwo-stage framework contributes to the superior results ac-\ncording to our ablation study results in Table 5. Overall,\nthe results verify the effectiveness of multimodality and the\npotential of achieving CoT reasoning with 1B-models via\nour two-stage framework.12345678910405060708090\nEpochAccuracyOne-stage Baseline One-stage Multimodal\nTwo-Stage Baseline Two-Stage Multimodal\nFigure 5. Accuracy curve of the No-CoT baseline and Multimodal-\nCoT variants across epochs.\n6. Analysis\nThe following analysis will investigate how Multimodal-\nCoT works and discuss contribution factors and limitations.\nWe use models under the base size for analysis unless\notherwise stated.\n6.1. Multimodality Boosts Convergence\nFigure 5 shows the evaluation accuracy curve of the baseline\nand Multimodal-CoT in different training epochs. “One-\nstage” is based on the QCM →A input-output format as it', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nachieves the best performance in Table 2 and “Two-stage”\nis our two-stage framework. We ﬁnd that the two-stage\nmethods achieve relatively higher accuracy at the beginning\nthan the one-stage baselines that generate the answer directly\nwithout CoT. However, without the vision features, the two-\nstage baseline could not yield better results as the training\ngoes on due to the low-quality rationales (as observed in\nSection 3). In contrast, using vision features helps generate\nmore effective rationales that contribute to better answer\naccuracy in our two-stage multimodal variant.\n6.2. Using Different Vision Features\nDifferent vision features may affect the model performance.\nWe compare three widely-used types of vision features,\nCLIP (Radford et al., 2021), DETR (Carion et al., 2020),\nand ResNet (He et al., 2016). CLIP and DETR are patch-like\nfeatures where DETR is based on object detection. For the\nResNet features, we repeat the pooled features of ResNet-\n50 to the same length with the text sequence to imitate the\npatch-like features, where each patch is the same as the\npooled image features. More details of the vision features\nare presented in Appendix B.2.\nTable 6. Accuracy (%) of using different vision features.\nMethod One-stage Two-Stage\nw/ CLIP 81.21 84.81\nw/ DETR 82.57 84.91\nw/ ResNet 80.97 84.77\nTable 6 shows the comparative results of vision features. We\nobserve that using vision features generally achieves better\nperformance than the language only baseline. Speciﬁcally,\nDETR achieves relatively better performance in general.\nTherefore, we use DETR by default in Multimodal-CoT.\n6.3. General Effectiveness Across Backbone Models\nTo test the generality of the beneﬁts of our approach to\nother backbone models, we alter the underlying LMs to\nother variants in different sizes or types. As shown in Table\n7, our approach is generally effective for the widely-used\nbackbone models.\nTable 7. Accuracy (%) with different backbone language models.\nMethod Size Language Only Mutimodal-CoT\nUniﬁedQA Base 223M 80.40 84.91\nUniﬁedQA Large 738M 83.60 91.68\nFLAN-T5 Base 248M 83.42 85.85\nFLAN-T5 Large 783M 85.19 93.02\n6.4. Error Analysis\nTo better understand the behavior of Multimodal-CoT and\nfacilitate future studies, we manually investigate randomly\nselected examples generated by our approach. Table 8 sum-marizes the categorization results generated by Multimodal-\nCoT. We randomly picked up 50 samples whose answers\nwere correct and 50 samples whose answers were incor-\nrect. The corresponding examples from each category are\npresented in Appendix C.\nTable 8. Categorization analysis of Multimodal-CoT.\nAnswer CoT Category Percentage (%)\nCorrectCoT is correct 90\nCoT is incorrect 10\nIncorrectCommonsense Mistake 82\nLogical Mistake 12\nCoT is correct 6\nWe ﬁnd that the correct samples (i.e., whose answers are cor-\nrect) contain a certain amount of incorrect chain-of-thought\n(10%). The results indicate that CoT may not always beneﬁt\nthe answer inference, and the model is robust to some extent\n— it can predict the correct answer by ignoring incorrect\nrationales. For incorrect samples (i.e., whose answers are\nincorrect), commonsense mistake in the CoT is the most\nfrequent error type (88%). The model often makes com-\nmonsense mistakes when answering the questions requires\ncommonsense knowledge, e.g., understand maps and count-\ning numbers in the images (Figure 9), and utilizing the\nalphabet (Figure 10). The other type of mistake is a logical\nmistake (12%), with contradictions in the reasoning chains\n(Figure 11). In addition, there are cases with incorrect an-\nswers while their CoT are correct (6%) but might not be\nnecessarily related to answer options (Figure 12).\nThe analysis indicates that there are prospective directions\nfor future studies. It is possible to improve Multimodal-\nCoT by (i) incorporating more informative vision features\nand improving language-vision interaction to be capable of\nunderstanding maps and counting numbers; (ii) injecting\ncommonsense knowledge; (iii) applying a ﬁltering mecha-\nnism, e.g., using only the effective CoT to infer the answer\nand get rid of irrelevant CoT.\n7. Conclusion\nWe formally study the problem of multimodal CoT. We pro-\npose Multimodal-CoT that incorporates language and vision\nmodalities into a two-stage framework that separates ratio-\nnale generation and answer inference, so answer inference\ncan leverage better generated rationales from multimodal in-\nformation. With Multimodal-CoT, we show that our method\nsurpasses GPT-3.5 by 16 percentage points in accuracy on\nthe ScienceQA benchmark. Our error analysis shows that\nit is the potential to leverage more effective vision features,\ninject commonsense knowledge, and apply ﬁltering mecha-\nnisms to improve CoT reasoning in future studies.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nReferences\nAnderson, P., He, X., Buehler, C., Teney, D., Johnson, M.,\nGould, S., and Zhang, L. Bottom-up and top-down atten-\ntion for image captioning and visual question answering.\nIn2018 IEEE Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2018, Salt Lake City, UT, USA,\nJune 18-22, 2018 , pp. 6077–6086. IEEE Computer Soci-\nety, 2018. doi: 10.1109/CVPR.2018.00636.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,\nHenighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,\nJ., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,\nGray, S., Chess, B., Clark, J., Berner, C., McCandlish,\nS., Radford, A., Sutskever, I., and Amodei, D. Language\nmodels are few-shot learners. In Larochelle, H., Ranzato,\nM., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances\nin Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020.\nCarion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov,\nA., and Zagoruyko, S. End-to-end object detection with\ntransformers. In Computer Vision–ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23–28, 2020,\nProceedings, Part I , pp. 213–229, 2020.\nChen, T., Kornblith, S., Swersky, K., Norouzi, M., and\nHinton, G. E. Big self-supervised models are strong\nsemi-supervised learners. In Larochelle, H., Ranzato, M.,\nHadsell, R., Balcan, M., and Lin, H. (eds.), Advances\nin Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020.\nChen, W., Ma, X., Wang, X., and Cohen, W. W. Program\nof thoughts prompting: Disentangling computation from\nreasoning for numerical reasoning tasks. ArXiv preprint ,\nabs/2211.12588, 2022.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton,\nC., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,\nS., Maynez, J., Rao, A., Barnes, P., Tay, Y ., Shazeer,\nN., Prabhakaran, V ., Reif, E., Du, N., Hutchinson, B.,\nPope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G.,\nYin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S.,\nMichalewski, H., Garcia, X., Misra, V ., Robinson, K., Fe-\ndus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph,\nB., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal,\nS., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M.,\nLewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee,\nK., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O.,\nCatasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean,J., Petrov, S., and Fiedel, N. Palm: Scaling language mod-\neling with pathways. ArXiv preprint , abs/2204.02311,\n2022.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y .,\nFedus, W., Li, E., Wang, X., Dehghani, M., Brahma,\nS., et al. Scaling instruction-ﬁnetuned language models.\narXiv preprint arXiv:2210.11416 , 2022.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\nHeigold, G., Gelly, S., et al. An image is worth 16x16\nwords: Transformers for image recognition at scale. In\nThe International Conference on Learning Representa-\ntions (ICLR) , 2021.\nFu, Y ., Peng, H., Sabharwal, A., Clark, P., and Khot, T.\nComplexity-based prompting for multi-step reasoning.\nArXiv preprint , abs/2210.00720, 2022.\nGao, P., Jiang, Z., You, H., Lu, P., Hoi, S. C. H., Wang, X.,\nand Li, H. Dynamic fusion with intra- and inter-modality\nattention ﬂow for visual question answering. In IEEE\nConference on Computer Vision and Pattern Recognition,\nCVPR 2019, Long Beach, CA, USA, June 16-20, 2019 , pp.\n6639–6648. Computer Vision Foundation / IEEE, 2019.\ndoi: 10.1109/CVPR.2019.00680.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\nlearning for image recognition. In 2016 IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2016,\nLas Vegas, NV, USA, June 27-30, 2016 , pp. 770–778.\nIEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.\n90.\nHo, N., Schmid, L., and Yun, S.-Y . Large language models\nare reasoning teachers. arXiv preprint arXiv:2212.10071 ,\n2022.\nJi, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y ., Ishii, E.,\nBang, Y ., Madotto, A., and Fung, P. Survey of halluci-\nnation in natural language generation. ACM Computing\nSurveys , 2022.\nKhashabi, D., Min, S., Khot, T., Sabharwal, A., Tafjord,\nO., Clark, P., and Hajishirzi, H. UNIFIEDQA: Crossing\nformat boundaries with a single QA system. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2020 , pp. 1896–1907, Online, 2020. Association\nfor Computational Linguistics. doi: 10.18653/v1/2020.\nﬁndings-emnlp.171.\nKhot, T., Trivedi, H., Finlayson, M., Fu, Y ., Richardson, K.,\nClark, P., and Sabharwal, A. Decomposed prompting:\nA modular approach for solving complex tasks. ArXiv\npreprint , abs/2210.02406, 2022.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nKim, J., Jun, J., and Zhang, B. Bilinear attention networks.\nIn Bengio, S., Wallach, H. M., Larochelle, H., Grauman,\nK., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances\nin Neural Information Processing Systems 31: Annual\nConference on Neural Information Processing Systems\n2018, NeurIPS 2018, December 3-8, 2018, Montr ´eal,\nCanada , pp. 1571–1581, 2018.\nKim, W., Son, B., and Kim, I. Vilt: Vision-and-language\ntransformer without convolution or region supervision.\nInProceedings of the 38th International Conference on\nMachine Learning (ICML) , pp. 5583–5594, 2021.\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y ., and Iwasawa,\nY . Large language models are zero-shot reasoners. ArXiv\npreprint , abs/2205.11916, 2022.\nLi, B., Lv, C., Zhou, Z., Zhou, T., Xiao, T., Ma, A., and Zhu,\nJ. On vision features in multimodal machine translation.\nInProceedings of the 60th Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: Long\nPapers) , pp. 6327–6337, 2022a.\nLi, L. H., Yatskar, M., Yin, D., Hsieh, C.-J., and Chang,\nK.-W. Visualbert: A simple and performant baseline for\nvision and language. ArXiv preprint , abs/1908.03557,\n2019.\nLi, Y ., Lin, Z., Zhang, S., Fu, Q., Chen, B., Lou, J.-G., and\nChen, W. On the advance of making language models\nbetter reasoners. ArXiv preprint , abs/2206.02336, 2022b.\nLu, P., Qiu, L., Chen, J., Xia, T., Zhao, Y ., Zhang, W., Yu,\nZ., Liang, X., and Zhu, S.-C. Iconqa: A new benchmark\nfor abstract diagram understanding and visual language\nreasoning. In The 35th Conference on Neural Information\nProcessing Systems (NeurIPS) Track on Datasets and\nBenchmarks , 2021.\nLu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu,\nS.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to\nexplain: Multimodal reasoning via thought chains for sci-\nence question answering. ArXiv preprint , abs/2209.09513,\n2022a.\nLu, P., Qiu, L., Chang, K.-W., Wu, Y . N., Zhu, S.-C., Ra-\njpurohit, T., Clark, P., and Kalyan, A. Dynamic prompt\nlearning via policy gradient for semi-structured mathemat-\nical reasoning. ArXiv preprint , abs/2209.14610, 2022b.\nMagister, L. C., Mallinson, J., Adamek, J., Malmi, E., and\nSeveryn, A. Teaching small language models to reason.\nArXiv preprint , abs/2212.08410, 2022.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\net al. Learning transferable visual models from natural\nlanguage supervision. In International Conference on\nMachine Learning , pp. 8748–8763. PMLR, 2021.Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J.,\nSong, F., Aslanides, J., Henderson, S., Ring, R., Young,\nS., Rutherford, E., Hennigan, T., Menick, J., Cassirer, A.,\nPowell, R., Driessche, G. v. d., Hendricks, L. A., Rauh,\nM., Huang, P.-S., Glaese, A., Welbl, J., Dathathri, S.,\nHuang, S., Uesato, J., Mellor, J., Higgins, I., Creswell,\nA., McAleese, N., Wu, A., Elsen, E., Jayakumar, S.,\nBuchatskaya, E., Budden, D., Sutherland, E., Simonyan,\nK., Paganini, M., Sifre, L., Martens, L., Li, X. L., Kun-\ncoro, A., Nematzadeh, A., Gribovskaya, E., Donato, D.,\nLazaridou, A., Mensch, A., Lespiau, J.-B., Tsimpoukelli,\nM., Grigorev, N., Fritz, D., Sottiaux, T., Pajarskas, M.,\nPohlen, T., Gong, Z., Toyama, D., d’Autume, C. d. M.,\nLi, Y ., Terzi, T., Mikulik, V ., Babuschkin, I., Clark, A.,\nCasas, D. d. L., Guy, A., Jones, C., Bradbury, J., Johnson,\nM., Hechtman, B., Weidinger, L., Gabriel, I., Isaac, W.,\nLockhart, E., Osindero, S., Rimell, L., Dyer, C., Vinyals,\nO., Ayoub, K., Stanway, J., Bennett, L., Hassabis, D.,\nKavukcuoglu, K., and Irving, G. Scaling language mod-\nels: Methods, analysis & insights from training gopher.\nArXiv preprint , abs/2112.11446, 2021.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring the\nlimits of transfer learning with a uniﬁed text-to-text trans-\nformer. Journal of Machine Learning Research (JMLR) ,\n21:1–67, 2020.\nRubin, O., Herzig, J., and Berant, J. Learning to re-\ntrieve prompts for in-context learning. In Proceedings\nof the 2022 Conference of the North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies , pp. 2655–2671, 2022. doi:\n10.18653/v1/2022.naacl-main.191.\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kul-\nshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L.,\nDu, Y ., Li, Y ., Lee, H., Zheng, H. S., Ghafouri, A., Mene-\ngali, M., Huang, Y ., Krikun, M., Lepikhin, D., Qin, J.,\nChen, D., Xu, Y ., Chen, Z., Roberts, A., Bosma, M.,\nZhao, V ., Zhou, Y ., Chang, C.-C., Krivokon, I., Rusch,\nW., Pickett, M., Srinivasan, P., Man, L., Meier-Hellstern,\nK., Morris, M. R., Doshi, T., Santos, R. D., Duke, T.,\nSoraker, J., Zevenbergen, B., Prabhakaran, V ., Diaz, M.,\nHutchinson, B., Olson, K., Molina, A., Hoffman-John, E.,\nLee, J., Aroyo, L., Rajakumar, R., Butryna, A., Lamm,\nM., Kuzmina, V ., Fenton, J., Cohen, A., Bernstein, R.,\nKurzweil, R., Aguera-Arcas, B., Cui, C., Croak, M., Chi,\nE., and Le, Q. Lamda: Language models for dialog\napplications. ArXiv preprint , abs/2201.08239, 2022.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need. In Guyon, I., von Luxburg, U., Bengio,\nS., Wallach, H. M., Fergus, R., Vishwanathan, S. V . N.,\nand Garnett, R. (eds.), Advances in Neural Information', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nProcessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA , pp. 5998–6008, 2017.\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E.,\nand Zhou, D. Self-consistency improves chain of\nthought reasoning in language models. ArXiv preprint ,\nabs/2203.11171, 2022a.\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and\nZhou, D. Rationale-augmented ensembles in language\nmodels. ArXiv preprint , abs/2207.00747, 2022b.\nWei, J., Tay, Y ., Bommasani, R., Raffel, C., Zoph, B.,\nBorgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Met-\nzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang,\nP., Dean, J., and Fedus, W. Emergent abilities of large\nlanguage models. Transactions on Machine Learning\nResearch , 2022a. Survey Certiﬁcation.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E.,\nLe, Q., and Zhou, D. Chain of thought prompting elic-\nits reasoning in large language models. ArXiv preprint ,\nabs/2201.11903, 2022b.\nWu, Z., Kong, L., Bi, W., Li, X., and Kao, B. Good for\nmisconceived reasons: An empirical revisiting on the\nneed for visual context in multimodal machine transla-\ntion. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers) , pp. 6153–6166, Online,\n2021. Association for Computational Linguistics. doi:\n10.18653/v1/2021.acl-long.480.\nYu, Z., Yu, J., Cui, Y ., Tao, D., and Tian, Q. Deep modu-\nlar co-attention networks for visual question answering.\nInIEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2019, Long Beach, CA, USA, June\n16-20, 2019 , pp. 6281–6290. Computer Vision Founda-\ntion / IEEE, 2019. doi: 10.1109/CVPR.2019.00644.\nZhang, Z., Chen, K., Wang, R., Utiyama, M., Sumita, E., Li,\nZ., and Zhao, H. Neural machine translation with univer-\nsal visual representation. In 8th International Conference\non Learning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020 . OpenReview.net, 2020.\nZhang, Z., Zhang, A., Li, M., and Smola, A. Automatic\nchain of thought prompting in large language models.\nArXiv preprint , abs/2210.03493, 2022.\nZhang, Z., Chen, K., Wang, R., Utiyama, M., Sumita, E., Li,\nZ., and Zhao, H. Universal multimodal representation for\nlanguage understanding. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence , pp. 1–18, 2023. doi:\n10.1109/TPAMI.2023.3234170.Zhou, D., Sch ¨arli, N., Hou, L., Wei, J., Scales, N., Wang,\nX., Schuurmans, D., Bousquet, O., Le, Q., and Chi, E.\nLeast-to-most prompting enables complex reasoning in\nlarge language models. ArXiv preprint , abs/2205.10625,\n2022.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nA. Extended analysis for the challenge of Multimodal-CoT\nA.1. More Examples of Misleading by Hallucinated Rationales\nAccording to our case studies (Section 3.2), we ﬁnd that the baseline tends to generate hallucinated rationales. We provide\nfurther examples as shown in Figure 6.\nOptions:(B) lichen(A) mushroomProblem\nBaselineQuestion:Basedonthearrows,whichofthefollowingorganismsisaconsumer?Context:BelowisafoodwebfromatundraecosysteminNunavut,aterritoryinNorthernCanada.Afoodwebmodelshowthemattereatenbyorganismsmovesthroughanecosystem.Thearrowsinafoodwebrepresenthowmattermovesbetweenorganismsinanecosystem.GoldRationale:Afoodwebisamodel…Anorganisminafoodwebcanhavemorethanonearrowpointingfromit.Thisshowsthattheorganismiseatenbymorethanoneotherorganisminthefoodweb.Anorganisminafoodwebcanalsohavemorethanonearrowpointingtoit.Thisshowsthattheorganismeatsmorethanoneotherorganisminthefoodweb.Consumerseatotherorganisms.So,therearearrowsinafoodwebthatpointfromotherorganismstoconsumers.Themushroomhasarrowspointingtoitfromthebarren-groundcaribouandthegrizzlybear.So,themushroomisaconsumer.Thelichendoesnothaveanyarrowspointingtoit.So,thelichenisaproducer,notaconsumer.Answer:Theansweris(A).Rationale:Afoodwebisamodel…Thisshowsthattheorganismiseatenbymorethanoneotherorganisminthefoodweb.Anorganisminafoodwebcanalsohavemorethanonearrowpointingtoit.Thisshowsthattheorganismeatsmorethanoneotherorganisminthefoodweb.Consumerseatproducers.So,inafoodweb,consumershavearrowspointingtothemfromproducers.Themushroomhasanarrowpointingtoitfromthemushroom.Themushroomisnotaconsumer,sothemushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Answer:Theansweris(B).Rationale:Afoodwebisamodel…Anorganisminafoodwebcanhavemorethanonearrowpointingfromit.Thisshowsthattheorganismiseatenbymorethanoneotherorganisminthefoodweb.Anorganisminafoodwebcanalsohavemorethanonearrowpointingtoit.Thisshowsthattheorganismeatsmorethanoneotherorganisminthefoodweb.Consumerseatproducersorganisms.So,inarearrowsafoodwebthatpointfromotherorganismstoconsumers.Themushroomhasarrowspointingtoitfromthen-browmushroomibou.thelichenzzlybear.So,themushroomisaconsumer.Thelichenhasnothaveanyarrowspointingtoit.So,themushroomisaconsumer.Lichenisnotaconsumer.Answer:Theansweris(B).Vision\n+ Vision Features\nOptions:(B) sample A(A) neither; the samples have the same temperatureProblem\nBaselineQuestion:Comparetheaveragekineticenergiesoftheparticlesineachsample.Whichsamplehasthehighertemperature?Context:Thediagramsbelowshowtwopuresamplesofgasinidenticalclosed,rigidcontainers.Eachcoloredballrepresentsonegasparticle.Bothsampleshavethesamenumberofparticles.GoldRationale:Thetemperatureofasubstancedependsontheaveragekineticenergyoftheparticlesinthesubstance.Thehighertheaveragekineticenergyoftheparticles,thehigherthetemperatureofthesubstance.Thekineticenergyofaparticleisdeterminedbyitsmassandspeed.Forapuresubstance,thegreaterthemassofeachparticleinthesubstanceandthehighertheaveragespeedoftheparticles,thehighertheiraveragekineticenergy.EachparticleinsampleAhasmoremassthaneachparticleinsampleB.TheparticlesinsampleAalso+ Vision FeaturesVision(C) sample B\nhaveahigheraveragespeedthantheparticlesinsampleB.So,theparticlesinsampleAhaveahigheraveragekineticenergythantheparticlesinsampleB.BecausetheparticlesinsampleAhavethehigheraveragekineticenergy,sampleAmusthavethehighertemperature.Answer:Theansweris(B).Rationale:Thetemperatureofasubstancedependsontheaveragekineticenergyoftheparticlesinthesubstance.Thehighertheaveragekineticenergyoftheparticles,thehigherthetemperatureofthesubstance.Thekineticenergyofaparticleisdeterminedbyitsmassandspeed.Forapuresubstance,thegreaterthemassofeachparticleinthesubstanceandthehighertheaveragespeedoftheparticles,thehighertheiraveragekineticenergy.EachparticleinsampleBhasmoremassthaneachparticleinsampleA.TheparticlesinsampleBalsohaveahigheraveragespeedthantheparticlesinsampleA.So,theparticlesinsampleBhaveahigheraveragekineticenergythantheparticlesinsampleA.BecausetheparticlesinsampleBhavethehigheraveragekineticenergy,sampleBmusthavethehighertemperature.Answer:Theansweris(C).Rationale:Thetemperatureofasubstancedependsontheaveragekineticenergyoftheparticlesinthesubstance.Thehighertheaveragekineticenergyoftheparticles,thehigherthetemperatureofthesubstance.Thekineticenergyofaparticleisdeterminedbyitsmassandspeed.Forapuresubstance,thegreaterthemassofeachparticleinthesubstanceandthehighertheaveragespeedoftheparticles,thehighertheiraveragekineticenergy.EachparticleinsampleAhasmoremassthaneachparticleinsampleB.TheparticlesinsampleAalsohaveahigheraveragespeedthantheparticlesinsampleB.So,theparticlesinsampleAhaveahigheraveragekineticenergythantheparticlesinsampleB.BecausetheparticlesinsampleAhavethehigheraveragekineticenergy,sampleAmusthavethehighertemperature.Answer:Theansweris(B).\nFigure 6. Examples of the two-stage framework without vision features (baseline) and with vision features (ours) for generating rationales\nand predicting answers. The upper part presents the problem details, and the lower part shows the outputs of the baseline and our method.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}]: %s
2023-12-07 11:04:27,170 - INFO - Check the data that is being passed [{'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nA.2. Two-Stage Training Performance with Different Sizes of LMs.\nIn Section 3, we obverse that incorporating vision features helps generate more effective rationales, thus leading to improved\nanswer accuracy. Besides incorporating vision features, it is possible to scale the LM size to mitigate the issue of incorrect\nrationales. Figure 7 shows the answer accuracy with UniﬁedQA Base and UniﬁedQA Large . When using a larger LM, the\naccuracy of the baseline (w/o vision features) is boosted. The result indicates that scaling the LM is possible to mitigate the\nissue of incorrect rationales. However, the performance is still much inferior to using vision features. The result further\nveriﬁes the effectiveness of our Multimodal-CoT with different sizes of LMs.\nbase large6580100\n70.5384.9182.9791.68Accuracy (%)w/o Vision Modality w/ Vision Modality\nFigure 7. Answer accuracy with different sizes of LMs.\nB. Experimental Details\nB.1. Baseline Methods\nFollowing Lu et al. (2022a), our baselines include three types of methods:\n(i) Visual question answering (VQA) models (Yu et al., 2019; Anderson et al., 2018; Kim et al., 2018; Gao et al., 2019; Lu\net al., 2021; Li et al., 2019). The VQA baselines take the question, the context, and choices as the textual input, take the\nimage as the vision input, and predict the score distribution over choice candidates via a linear classiﬁer.\n(ii) Text-to-text LM models. UniﬁedQA (Khashabi et al., 2020) is adopted as it is the best ﬁne-tuning model in Lu et al.\n(2022a). UniﬁedQA takes the textual information as the input and outputs the answer option. The image is converted into a\ncaption extracted by an image captioning model based on ViT and GPT-2.6UniﬁedQA treats our task as a text generation\nproblem. In Lu et al. (2022a), it is trained to generate a target answer text, i.e., one of the candidate options. Then, the most\nsimilar option is selected as the ﬁnal prediction to evaluate the question answering accuracy.\n(iii) GPT-3.5 models (Chen et al., 2020) based on the text-davinci-002 engine. The inference is based on the few-shot\nprompting, where two in-context examples from the training set are concatenated before the test instance.\nFor UniﬁedQA and GPT-3.5, CoT is applied after the answer (Lu et al., 2022a). Besides the above baselines, we develop a\nstronger baseline with a slight modiﬁcation of the output format of UniﬁedQA. Instead of predicting the answer texts, our\nbaseline directly predicts the choice, e.g., the answer is B . This setting helps our baseline achieve better results than the\nexisting UniﬁedQA. Therefore, we use the stronger method as the language only baseline for analysis.\nB.2. Details of Vision Features\nIn Section 6.2, we compared four types of vision features, CLIP (Radford et al., 2021), DETR (Carion et al., 2020), and\nResNet (He et al., 2016). The speciﬁc models are: (i) CLIP: RN101;7(ii) DETR: detr resnet101 dc5;8(iii) ResNet: we use\n6https://huggingface.co/nlpconnect/vit-gpt2-image-captioning .\n7https://github.com/jianjieluo/OpenAI-CLIP-Feature .\n8https://github.com/facebookresearch/detr .', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nthe averaged pooled features of a pre-trained ResNet50 CNN. Table 9 presents the dimension of the vision features (after the\nfunction VisionExtractor( ·) in Eq. 3). For ResNet-50, we repeat the pooled features of ResNet-50 to the same length as the\ntext sequence to imitate the patch-like features, where each patch is the same as the pooled image features.\nTable 9. Dimension of vision features\nMethod Dimension\nCLIP (49, 2048)\nDETR (100, 256)\nResNet (512, 2048)\nC. Examples of Case Studies\nTo better understand the behavior of Multimodal-CoT, we manually investigate randomly selected examples generated by\nour approach. Table 8 summarizes the categorization results generated by Multimodal-CoT. We randomly picked up 50\nsamples whose prediction results were correct and 50 samples whose prediction results were incorrect.\nWe ﬁnd that the correct samples contain a certain amount of incorrect chain-of-thought. As shown in Figure 8(b), the model\ngenerates the incorrect rationale, “ Animals cannot their food by digesting other organisms ” but the predicted answer is\ncorrect. The result indicates that CoT may not always beneﬁt the answer inference, and the model is robust to some extent —\nit can predict the correct answer by ignoring incorrect rationales.\nFor incorrect samples, commonsense mistake is the most frequent error type. The model also makes commonsense mistakes\nwhen answering the questions requires commonsense knowledge, e.g., understand maps and counting numbers in the\nimages (Figure 9), and utilizing the alphabet (Figure 10). The other type of mistake is the logical mistake, where there are\ncontradictions in the reasoning chains (Figure 11). In addition, there are cases that the CoT is correct but might not be\nnecessarily related to answer options; thus the model chooses the incorrect answer.\nThe analysis indicates that there are prospective directions for future studies. On the one hand, it is possible to improve the\nquality of CoT by (i) using more ﬁne-grained interaction of language and vision features; and (ii) injecting commonsense\nknowledge. On the other hand, applying a ﬁltering mechanism to using only the effective CoT to infer the answer and\neliminate irrelevant CoT.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Birds,mammals,fish,reptiles,andamphibiansaregroupsofanimals.Theanimalsineachgrouphavetraitsincommon.Scientistssortanimalsintogroupsbasedontraitstheyhaveincommon.Thisprocessiscalledclassification.Atigersharkisafish.Itlivesunderwater.Ithasfins,notlimbs.Tigersharkscannocturnal.Theymeansthattheyareactivemostlyatnight.Ared-tailedhawkisabird.Ithasfeathers,twowings,andabeak.Red-tailedhawkslivetheirbeteethakstopushforpre.birds,and,repamptiles…Answer:Theansweris(B).Options:(A) red-tailed hawkProblem\nPredictionQuestion:Selectthebirdbelow.Context:Birdshavefeathers,twowings,andabeak.Aperegrinefalconisanexampleofabird.Rationale:Birds,mammals,fish,reptiles,andamphibiansaregroupsofanimals.Theanimalsineachgrouphavetraitsincommon.Scientistssortanimalsintogroupsbasedontraitstheyhaveincommon.Thisprocessiscalledclassification.Atigersharkisafish.Itlivesunderwater.Ithasfins,notlimbs.Tigersharkscannocturnal.Theymeansthattheyareactivemostlyatnight.Ared-tailedhawkisabird.Ithasfeathers,twowings,andabeak.Red-tailedhawkslivetheirbeteethakstopushforpre.birds,and,repamptiles…Answer:Theansweris(A).Vision(B) tiger shark\n(a)CoTiscorrect\nRationale:Leopardthepast,scientistsclassifiedlivingorganismsintotwogroups:plantsandanimals.Overthepast300years,scientistshavediscoveredmanymoretypesoforganisms.Today,manyscientistsclassifyorganismsintosixbroadgroups,calledkingdoms.Organismsineachkingdomhavespecifictraits.Thetablebelowshowssometraitsusedtodescribeeachkingdom.|Bacteria|Archaea|Protists|Fungi|Animals|Plants.Howmanycellsdotheyhave?|one|one|oneormany|oneormany|many|many.Dotheircellshaveanucleus?|no|no|yes|yes|yes|yesnCantheircellsmakefood?|somespeciescan|somespeciescan|somespeciescan|no|no|yesLeoparduswiediiisananimal.Animalcellscannotmaketheirownfood.Animalscannottheirfoodbydigestingotherorganisms.Answer:Theansweris(B).Options:(A) noProblem\nPredictionQuestion:CanLeoparduswiediicellsmaketheirownfood?Context:ThisorganismisLeoparduswiedii.Itisamemberoftheanimalkingdom.Leoparduswiediiiscommonlycalledamargay.MargaysliveintheforestinCentralandSouthAmerica.Margaysarenocturnal.Anocturnalanimalsleepsmostofthedayandisawakeatnight.Rationale:Inthepast,scientistsclassifiedlivingorganismsintotwogroups:plantsandanimals.Overthepast300years,scientistshavediscoveredmanymoretypesoforganisms.Today,manyscientistsclassifyorganismsintosixbroadgroups,calledkingdoms.Organismsineachkingdomhavespecifictraits.Thetablebelowshowssometraitsusedtodescribeeachkingdom.|Bacteria|Archaea|Protists|Fungi|Animals|Plants.Howmanycellsdotheyhave?|one|one|oneormany|oneormany|many|manyDotheircellshaveanucleus?|no|no|yes|yes|yes|yesCantheircellsmakefood?|somespeciescan|somespeciescan|somespeciescan|no|no|yesLeoparduswiediiisananimal.Animalcellscannotmaketheirownfood.Animalsgettheirfoodbydigestingotherorganisms.Answer:Theansweris(A).Vision(B) yes\n(b)CoTisincorrect\nFigure 8. Examples of answers are correct while the CoT is correct (a) or incorrect (b).', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Oceansarehugebodiesofsaltwater.Theworldhasfiveoceans.Alloftheoceansareconnected,makingoneworldocean.ThisistheIndianOcean.Answer:Theansweris(B).Options:(A) the Atlantic OceanProblem\nPredictionQuestion:Whichoceanishighlighted?Context:N/ARationale:Oceansarehugebodiesofsaltwater.Theworldhasfiveoceans.Alloftheoceansareconnected,makingoneworldocean.ThisisthePacificOcean.Answer:Theansweris(C).Vision(B) the Indian Ocean\n(a)Mistakesaboutmaps.\nRationale:Inasolution,soluteparticlesmoveandspreadthroughoutthesolvent.Thediagrambelowshowshowasolutioncanchangeovertime.Soluteparticlesmovefromtheareawheretheyareatahigherconcentrationtotheareawheretheyareatalowerconcentration…Lookatthediagramagain.Itshowsyouhowthesolutionchangedduringtheprocessofdiffusion.Beforethesoluteparticlesreachedequilibrium,therewere5soluteparticlesontheleftsideofthemembraneand4soluteparticlesontherightsideofthemembrane.Whenthesoluteparticlesreachedequilibrium,therewere5soluteparticlesoneachsideofthemembrane.Therewas1moresoluteparticleontherightsideofthemembranethanbefore.So,forthesoluteparticlestoreachequilibrium,moresoluteparticlesmusthavemovedacrossthemembranetotherightthantotheright.Answer:Theansweris(B).Options:(A) to the left than to the rightProblem\nPredictionQuestion:Completethetexttodescribethediagram.Soluteparticlesmovedinbothdirectionsacrossthepermeablemembrane.Butmoresoluteparticlesmovedacrossthemembrane().Whentherewasanequalconcentrationonbothsides,theparticlesreachedequilibrium.Context:Thediagrambelowshowsasolutionwithonesolute.Eachsoluteparticleisrepresentedbyagreenball.Thesolutionfillsaclosedcontainerthatisdividedinhalfbyamembrane.Themembrane,representedbyadottedline,ispermeabletothesoluteparticles.Thediagramshowshowthesolutioncanchangeovertimeduringtheprocessofdiffusion.Rationale:Inasolution,soluteparticlesmoveandspreadthroughoutthesolvent.Thediagrambelowshowshowasolutioncanchangeovertime.Soluteparticlesmovefromtheareawheretheyareatahigherconcentrationtotheareawheretheyareatalowerconcentration…Lookatthediagramagain.Itshowsyouhowthesolutionchangedduringtheprocessofdiffusion.Beforethesoluteparticlesreachedequilibrium,therewere3soluteparticlesontheleftsideofthemembraneand5soluteparticlesontherightsideofthemembrane.Whenthesoluteparticlesreachedequilibrium,therewere4soluteparticlesoneachsideofthemembrane.Therewas1moresoluteparticleontheleftsideofthemembranethanbefore.So,forthesoluteparticlestoreachequilibrium,moresoluteparticlesmusthavemovedacrossthemembranetotheleftthantotheright.Answer:Theansweris(A).Vision\n(b)Mistakesaboutcountingnumbersintheimage.(C) the Pacific Ocean(D) the Southern Ocean\n(B) to the right than to the left\nFigure 9. Examples of commonsense mistakes about understanding maps and counting numbers.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Guidewordsappearoneachpageofadictionary.Theytellyouthefirstwordandlastwordonthepage.Theotherwordsonthepagecomebetweentheguidewordsinalphabeticalorder.Toputwordsinalphabeticalorder,puttheminorderbytheirfirstletters.Ifthefirstlettersarethesame,lookatthesecondletters.Ifthesecondlettersarethesame,lookatthethirdletters,andsoon.Ifonewordisshorter,andtherearenomoreletterstocompare,thentheshorterwordcomesfirstinalphabeticalorder.Forexample,becomesbeforebed.Putthewordsinalphabeticalorder.Sincehardshipisbetweentheguidewordshelping-hunter,itwouldbefoundonthatpage.Answer:Theansweris(B).Options:(A) hospitalProblem\nPredictionQuestion:Whichwordwouldyoufindonadictionarypagewiththefollowingguidewords?helping-hunterContext:Thediagrambelowshowsasolutionwithonesolute.Eachsoluteparticleisrepresentedbyagreenball.Thesolutionfillsaclosedcontainerthatisdividedinhalfbyamembrane.Themembrane,representedbyadottedline,ispermeabletothesoluteparticles.Thediagramshowshowthesolutioncanchangeovertimeduringtheprocessofdiffusion.Rationale:Guidewordsappearoneachpageofadictionary.Theytellyouthefirstwordandlastwordonthepage.Theotherwordsonthepagecomebetweentheguidewordsinalphabeticalorder.Toputwordsinalphabeticalorder,puttheminorderbytheirfirstletters.Ifthefirstlettersarethesame,lookatthesecondletters.Ifthesecondlettersarethesame,lookatthethirdletters,andsoon.Ifonewordisshorter,andtherearenomoreletterstocompare,thentheshorterwordcomesfirstinalphabeticalorder.Forexample,becomesbeforebed.Putthewordsinalphabeticalorder.Sincehospitalisbetweentheguidewordshelping-hunter,itwouldbefoundonthatpage.Answer:Theansweris(A).(B) hardshipVisionN/A\nFigure 10. Examples of commonsense mistakes about utilizing alphabet.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': "Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Measurementsarewrittenwithbothanumberandaunit.Theunitcomesafterthenumber.Theunitshowswhatthenumbermeans.TemperaturecanbewrittenwithunitsofdegreesFahrenheit(°F)orCelsius(°C).Usethelistbelowtocomparethetwounits.212°F|Waterboils|100°C98.6°F|Bodytemperature|37°C68°F|Roomtemperature|20°C32°F|Waterfreezes|0°C.Thebetterestimateforthetemperatureoftheaironahotdayis36°F.36°Fistoohot.Answer:Theansweris(A).Options:(A) 36°FProblem\nPredictionQuestion:Whatisthetemperatureoftheaironahotday?Context:Selectthebetterestimate.Rationale:Measurementsarewrittenwithbothanumberandaunit.Theunitcomesafterthenumber.Theunitshowswhatthenumbermeans.TemperaturecanbewrittenwithunitsofdegreesFahrenheit(°F)orCelsius(°C).Usethelistbelowtocomparethetwounits.212°F|Waterboils|100°C98.6°F|Bodytemperature|37°C68°F|Roomtemperature|20°C32°F|Waterfreezes|0°C.Thebetterestimateforthetemperatureoftheaironahotdayis36°C.36°Fistoocold.Answer:Theansweris(B).(B) 36°CVisionN/A\nRationale::Asentenceisagroupofwordsthatexpressesacompletethought.ThebandI'minhasbeenrehearsingdailybecausewehaveaconcertintwoweeks.Arun-onsentenceisformedwhentwosentencesareruntogether,joinedbyjustacommaorbynopunctuationatall.Ifonlyacommaisused,therun-oniscalledacommasplice…ThebandI'minhasbeenrehearsingdailysincewehaveaconcertintwoweeks.Thisisnotarun-onsentence.Itisformedacommaspliceformedfromtwosentencesruntogether,joinedwithoutjustacomma.Right-handednessiscontrolledbygenetics,interestinglyenough,thesamegenesarealsoresponsibleforthebrainbecomingmorespecializedatcertaintasks.Answer:Theansweris(B).Options:(A) yesProblem\nPredictionQuestion:Isthisarun-onsentence?Right-handednessiscontrolledbygenetics,interestinglyenough,thesamegenesarealsoresponsibleforthebrainbecomingmorespecializedatcertaintasks.Context:N/ARationale:Asentenceisagroupofwordsthatexpressesacompletethought.ThebandI'minhasbeenrehearsingdailybecausewehaveaconcertintwoweeks.Arun-onsentenceisformedwhentwosentencesareruntogether,joinedbyjustacommaorbynopunctuationatall.Ifonlyacommaisused,therun-oniscalledacommasplice…ThebandI'minhasbeenrehearsingdailysincewehaveaconcertintwoweeks.Thisisarun-onsentence.Itisacommaspliceformedfromtwosentencesruntogether,joinedbyjustacomma.Right-handednessiscontrolledbygenetics,interestinglyenough,thesamegenesarealsoresponsibleforthebrainbecomingmorespecializedatcertaintasks.Answer:Theansweris(A).(B) noVisionN/A(a)Logicalmistakewherethemodelfailsatcomparisons.\n(b)Logicalmistakewherethethereisacontradictionintherationale.\nFigure 11. Examples of logical mistakes.", 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': "Multimodal Chain-of-Thought Reasoning in Language Models\nOptions:(A) black stripes on its skinProblem\nPredictionRationale:Thewayanorganismlooksoractsiscalledatrait.Scientistsusefossilstolearnmoreaboutthetraitsofancientorganisms.Fossilscanpreservetheremainsofbodypartsandactivities.Afossilofabodypart,suchasatailorawing,cantellyouwhatanorganismlookedlike.Afossilofanorganism'sactivities,suchasaburroworafootprint,cantellyouabouttheorganism'sbehavior.Herearethreeexamplesoffossilsandthetraitsthatyoucanobservefromthem:Thisisafossilofananimal.Thisfossiltellsyouthattheanimalhadaspiral-shapedshell.Thisisafossilofaplant.Thisfossiltellsyouthattheplanthadsmallleavesarrangedinabranchedpattern.Thisisafossilofananimal'sfootprint.Thisfossiltellsyouthattheanimalcouldwalkonland.Anorganism'sfossilmaynotshowalloftheorganism'straits.Thisisbecausemostbodypartsaredestroyedduringfossilformation.Whenanorganism'sbodyturnsintoafossil,onlyafewbodypartsareusuallypreserved.Answer:Theansweris(C).(B) large fins on its bodyVision(C) a long, thin bodyQuestion:WhichtraitdidPalaeopythonhave?Selectthetraityoucanobserveonthefossil.Context:ThispictureshowsafossilofanancientanimalcalledPalaeopython.Palaeopythonlivedintreesandcouldgrowmorethansixfeetlong.\nRationale:Thewayanorganismlooksoractsiscalledatrait.Scientistsusefossilstolearnmoreaboutthetraitsofancientorganisms.Fossilscanpreservetheremainsofbodypartsandactivities.Afossilofabodypart,suchasatailorawing,cantellyouwhatanorganismlookedlike.Afossilofanorganism'sactivities,suchasaburroworafootprint,cantellyouabouttheorganism'sbehavior.Herearethreeexamplesoffossilsandthetraitsthatyoucanobservefromthem:Thisisafossilofananimal.Thisfossiltellsyouthattheanimalhadaspiral-shapedshell.Thisisafossilofaplant.Thisfossiltellsyouthattheplanthadsmallleavesarrangedinabranchedpattern.Thisisafossilofananimal'sfootprint.Thisfossiltellsyouthattheanimalcouldwalkonland.Anorganism'sfossilmaynotshowalloftheorganism'straits.Thisisbecausemostbodypartsaredestroyedduringfossilformation.Whenanorganism'sbodyturnsintoafossil,onlyafewbodypartsareusuallypreserved.Answer:Theansweris(B).\nFigure 12. Examples of answers are incorrect while the CoT is correct.", 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}]: %s
2023-12-07 11:04:27,170 - INFO - Check the results [{'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nA.2. Two-Stage Training Performance with Different Sizes of LMs.\nIn Section 3, we obverse that incorporating vision features helps generate more effective rationales, thus leading to improved\nanswer accuracy. Besides incorporating vision features, it is possible to scale the LM size to mitigate the issue of incorrect\nrationales. Figure 7 shows the answer accuracy with UniﬁedQA Base and UniﬁedQA Large . When using a larger LM, the\naccuracy of the baseline (w/o vision features) is boosted. The result indicates that scaling the LM is possible to mitigate the\nissue of incorrect rationales. However, the performance is still much inferior to using vision features. The result further\nveriﬁes the effectiveness of our Multimodal-CoT with different sizes of LMs.\nbase large6580100\n70.5384.9182.9791.68Accuracy (%)w/o Vision Modality w/ Vision Modality\nFigure 7. Answer accuracy with different sizes of LMs.\nB. Experimental Details\nB.1. Baseline Methods\nFollowing Lu et al. (2022a), our baselines include three types of methods:\n(i) Visual question answering (VQA) models (Yu et al., 2019; Anderson et al., 2018; Kim et al., 2018; Gao et al., 2019; Lu\net al., 2021; Li et al., 2019). The VQA baselines take the question, the context, and choices as the textual input, take the\nimage as the vision input, and predict the score distribution over choice candidates via a linear classiﬁer.\n(ii) Text-to-text LM models. UniﬁedQA (Khashabi et al., 2020) is adopted as it is the best ﬁne-tuning model in Lu et al.\n(2022a). UniﬁedQA takes the textual information as the input and outputs the answer option. The image is converted into a\ncaption extracted by an image captioning model based on ViT and GPT-2.6UniﬁedQA treats our task as a text generation\nproblem. In Lu et al. (2022a), it is trained to generate a target answer text, i.e., one of the candidate options. Then, the most\nsimilar option is selected as the ﬁnal prediction to evaluate the question answering accuracy.\n(iii) GPT-3.5 models (Chen et al., 2020) based on the text-davinci-002 engine. The inference is based on the few-shot\nprompting, where two in-context examples from the training set are concatenated before the test instance.\nFor UniﬁedQA and GPT-3.5, CoT is applied after the answer (Lu et al., 2022a). Besides the above baselines, we develop a\nstronger baseline with a slight modiﬁcation of the output format of UniﬁedQA. Instead of predicting the answer texts, our\nbaseline directly predicts the choice, e.g., the answer is B . This setting helps our baseline achieve better results than the\nexisting UniﬁedQA. Therefore, we use the stronger method as the language only baseline for analysis.\nB.2. Details of Vision Features\nIn Section 6.2, we compared four types of vision features, CLIP (Radford et al., 2021), DETR (Carion et al., 2020), and\nResNet (He et al., 2016). The speciﬁc models are: (i) CLIP: RN101;7(ii) DETR: detr resnet101 dc5;8(iii) ResNet: we use\n6https://huggingface.co/nlpconnect/vit-gpt2-image-captioning .\n7https://github.com/jianjieluo/OpenAI-CLIP-Feature .\n8https://github.com/facebookresearch/detr .', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nthe averaged pooled features of a pre-trained ResNet50 CNN. Table 9 presents the dimension of the vision features (after the\nfunction VisionExtractor( ·) in Eq. 3). For ResNet-50, we repeat the pooled features of ResNet-50 to the same length as the\ntext sequence to imitate the patch-like features, where each patch is the same as the pooled image features.\nTable 9. Dimension of vision features\nMethod Dimension\nCLIP (49, 2048)\nDETR (100, 256)\nResNet (512, 2048)\nC. Examples of Case Studies\nTo better understand the behavior of Multimodal-CoT, we manually investigate randomly selected examples generated by\nour approach. Table 8 summarizes the categorization results generated by Multimodal-CoT. We randomly picked up 50\nsamples whose prediction results were correct and 50 samples whose prediction results were incorrect.\nWe ﬁnd that the correct samples contain a certain amount of incorrect chain-of-thought. As shown in Figure 8(b), the model\ngenerates the incorrect rationale, “ Animals cannot their food by digesting other organisms ” but the predicted answer is\ncorrect. The result indicates that CoT may not always beneﬁt the answer inference, and the model is robust to some extent —\nit can predict the correct answer by ignoring incorrect rationales.\nFor incorrect samples, commonsense mistake is the most frequent error type. The model also makes commonsense mistakes\nwhen answering the questions requires commonsense knowledge, e.g., understand maps and counting numbers in the\nimages (Figure 9), and utilizing the alphabet (Figure 10). The other type of mistake is the logical mistake, where there are\ncontradictions in the reasoning chains (Figure 11). In addition, there are cases that the CoT is correct but might not be\nnecessarily related to answer options; thus the model chooses the incorrect answer.\nThe analysis indicates that there are prospective directions for future studies. On the one hand, it is possible to improve the\nquality of CoT by (i) using more ﬁne-grained interaction of language and vision features; and (ii) injecting commonsense\nknowledge. On the other hand, applying a ﬁltering mechanism to using only the effective CoT to infer the answer and\neliminate irrelevant CoT.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Birds,mammals,fish,reptiles,andamphibiansaregroupsofanimals.Theanimalsineachgrouphavetraitsincommon.Scientistssortanimalsintogroupsbasedontraitstheyhaveincommon.Thisprocessiscalledclassification.Atigersharkisafish.Itlivesunderwater.Ithasfins,notlimbs.Tigersharkscannocturnal.Theymeansthattheyareactivemostlyatnight.Ared-tailedhawkisabird.Ithasfeathers,twowings,andabeak.Red-tailedhawkslivetheirbeteethakstopushforpre.birds,and,repamptiles…Answer:Theansweris(B).Options:(A) red-tailed hawkProblem\nPredictionQuestion:Selectthebirdbelow.Context:Birdshavefeathers,twowings,andabeak.Aperegrinefalconisanexampleofabird.Rationale:Birds,mammals,fish,reptiles,andamphibiansaregroupsofanimals.Theanimalsineachgrouphavetraitsincommon.Scientistssortanimalsintogroupsbasedontraitstheyhaveincommon.Thisprocessiscalledclassification.Atigersharkisafish.Itlivesunderwater.Ithasfins,notlimbs.Tigersharkscannocturnal.Theymeansthattheyareactivemostlyatnight.Ared-tailedhawkisabird.Ithasfeathers,twowings,andabeak.Red-tailedhawkslivetheirbeteethakstopushforpre.birds,and,repamptiles…Answer:Theansweris(A).Vision(B) tiger shark\n(a)CoTiscorrect\nRationale:Leopardthepast,scientistsclassifiedlivingorganismsintotwogroups:plantsandanimals.Overthepast300years,scientistshavediscoveredmanymoretypesoforganisms.Today,manyscientistsclassifyorganismsintosixbroadgroups,calledkingdoms.Organismsineachkingdomhavespecifictraits.Thetablebelowshowssometraitsusedtodescribeeachkingdom.|Bacteria|Archaea|Protists|Fungi|Animals|Plants.Howmanycellsdotheyhave?|one|one|oneormany|oneormany|many|many.Dotheircellshaveanucleus?|no|no|yes|yes|yes|yesnCantheircellsmakefood?|somespeciescan|somespeciescan|somespeciescan|no|no|yesLeoparduswiediiisananimal.Animalcellscannotmaketheirownfood.Animalscannottheirfoodbydigestingotherorganisms.Answer:Theansweris(B).Options:(A) noProblem\nPredictionQuestion:CanLeoparduswiediicellsmaketheirownfood?Context:ThisorganismisLeoparduswiedii.Itisamemberoftheanimalkingdom.Leoparduswiediiiscommonlycalledamargay.MargaysliveintheforestinCentralandSouthAmerica.Margaysarenocturnal.Anocturnalanimalsleepsmostofthedayandisawakeatnight.Rationale:Inthepast,scientistsclassifiedlivingorganismsintotwogroups:plantsandanimals.Overthepast300years,scientistshavediscoveredmanymoretypesoforganisms.Today,manyscientistsclassifyorganismsintosixbroadgroups,calledkingdoms.Organismsineachkingdomhavespecifictraits.Thetablebelowshowssometraitsusedtodescribeeachkingdom.|Bacteria|Archaea|Protists|Fungi|Animals|Plants.Howmanycellsdotheyhave?|one|one|oneormany|oneormany|many|manyDotheircellshaveanucleus?|no|no|yes|yes|yes|yesCantheircellsmakefood?|somespeciescan|somespeciescan|somespeciescan|no|no|yesLeoparduswiediiisananimal.Animalcellscannotmaketheirownfood.Animalsgettheirfoodbydigestingotherorganisms.Answer:Theansweris(A).Vision(B) yes\n(b)CoTisincorrect\nFigure 8. Examples of answers are correct while the CoT is correct (a) or incorrect (b).', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Oceansarehugebodiesofsaltwater.Theworldhasfiveoceans.Alloftheoceansareconnected,makingoneworldocean.ThisistheIndianOcean.Answer:Theansweris(B).Options:(A) the Atlantic OceanProblem\nPredictionQuestion:Whichoceanishighlighted?Context:N/ARationale:Oceansarehugebodiesofsaltwater.Theworldhasfiveoceans.Alloftheoceansareconnected,makingoneworldocean.ThisisthePacificOcean.Answer:Theansweris(C).Vision(B) the Indian Ocean\n(a)Mistakesaboutmaps.\nRationale:Inasolution,soluteparticlesmoveandspreadthroughoutthesolvent.Thediagrambelowshowshowasolutioncanchangeovertime.Soluteparticlesmovefromtheareawheretheyareatahigherconcentrationtotheareawheretheyareatalowerconcentration…Lookatthediagramagain.Itshowsyouhowthesolutionchangedduringtheprocessofdiffusion.Beforethesoluteparticlesreachedequilibrium,therewere5soluteparticlesontheleftsideofthemembraneand4soluteparticlesontherightsideofthemembrane.Whenthesoluteparticlesreachedequilibrium,therewere5soluteparticlesoneachsideofthemembrane.Therewas1moresoluteparticleontherightsideofthemembranethanbefore.So,forthesoluteparticlestoreachequilibrium,moresoluteparticlesmusthavemovedacrossthemembranetotherightthantotheright.Answer:Theansweris(B).Options:(A) to the left than to the rightProblem\nPredictionQuestion:Completethetexttodescribethediagram.Soluteparticlesmovedinbothdirectionsacrossthepermeablemembrane.Butmoresoluteparticlesmovedacrossthemembrane().Whentherewasanequalconcentrationonbothsides,theparticlesreachedequilibrium.Context:Thediagrambelowshowsasolutionwithonesolute.Eachsoluteparticleisrepresentedbyagreenball.Thesolutionfillsaclosedcontainerthatisdividedinhalfbyamembrane.Themembrane,representedbyadottedline,ispermeabletothesoluteparticles.Thediagramshowshowthesolutioncanchangeovertimeduringtheprocessofdiffusion.Rationale:Inasolution,soluteparticlesmoveandspreadthroughoutthesolvent.Thediagrambelowshowshowasolutioncanchangeovertime.Soluteparticlesmovefromtheareawheretheyareatahigherconcentrationtotheareawheretheyareatalowerconcentration…Lookatthediagramagain.Itshowsyouhowthesolutionchangedduringtheprocessofdiffusion.Beforethesoluteparticlesreachedequilibrium,therewere3soluteparticlesontheleftsideofthemembraneand5soluteparticlesontherightsideofthemembrane.Whenthesoluteparticlesreachedequilibrium,therewere4soluteparticlesoneachsideofthemembrane.Therewas1moresoluteparticleontheleftsideofthemembranethanbefore.So,forthesoluteparticlestoreachequilibrium,moresoluteparticlesmusthavemovedacrossthemembranetotheleftthantotheright.Answer:Theansweris(A).Vision\n(b)Mistakesaboutcountingnumbersintheimage.(C) the Pacific Ocean(D) the Southern Ocean\n(B) to the right than to the left\nFigure 9. Examples of commonsense mistakes about understanding maps and counting numbers.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Guidewordsappearoneachpageofadictionary.Theytellyouthefirstwordandlastwordonthepage.Theotherwordsonthepagecomebetweentheguidewordsinalphabeticalorder.Toputwordsinalphabeticalorder,puttheminorderbytheirfirstletters.Ifthefirstlettersarethesame,lookatthesecondletters.Ifthesecondlettersarethesame,lookatthethirdletters,andsoon.Ifonewordisshorter,andtherearenomoreletterstocompare,thentheshorterwordcomesfirstinalphabeticalorder.Forexample,becomesbeforebed.Putthewordsinalphabeticalorder.Sincehardshipisbetweentheguidewordshelping-hunter,itwouldbefoundonthatpage.Answer:Theansweris(B).Options:(A) hospitalProblem\nPredictionQuestion:Whichwordwouldyoufindonadictionarypagewiththefollowingguidewords?helping-hunterContext:Thediagrambelowshowsasolutionwithonesolute.Eachsoluteparticleisrepresentedbyagreenball.Thesolutionfillsaclosedcontainerthatisdividedinhalfbyamembrane.Themembrane,representedbyadottedline,ispermeabletothesoluteparticles.Thediagramshowshowthesolutioncanchangeovertimeduringtheprocessofdiffusion.Rationale:Guidewordsappearoneachpageofadictionary.Theytellyouthefirstwordandlastwordonthepage.Theotherwordsonthepagecomebetweentheguidewordsinalphabeticalorder.Toputwordsinalphabeticalorder,puttheminorderbytheirfirstletters.Ifthefirstlettersarethesame,lookatthesecondletters.Ifthesecondlettersarethesame,lookatthethirdletters,andsoon.Ifonewordisshorter,andtherearenomoreletterstocompare,thentheshorterwordcomesfirstinalphabeticalorder.Forexample,becomesbeforebed.Putthewordsinalphabeticalorder.Sincehospitalisbetweentheguidewordshelping-hunter,itwouldbefoundonthatpage.Answer:Theansweris(A).(B) hardshipVisionN/A\nFigure 10. Examples of commonsense mistakes about utilizing alphabet.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': "Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Measurementsarewrittenwithbothanumberandaunit.Theunitcomesafterthenumber.Theunitshowswhatthenumbermeans.TemperaturecanbewrittenwithunitsofdegreesFahrenheit(°F)orCelsius(°C).Usethelistbelowtocomparethetwounits.212°F|Waterboils|100°C98.6°F|Bodytemperature|37°C68°F|Roomtemperature|20°C32°F|Waterfreezes|0°C.Thebetterestimateforthetemperatureoftheaironahotdayis36°F.36°Fistoohot.Answer:Theansweris(A).Options:(A) 36°FProblem\nPredictionQuestion:Whatisthetemperatureoftheaironahotday?Context:Selectthebetterestimate.Rationale:Measurementsarewrittenwithbothanumberandaunit.Theunitcomesafterthenumber.Theunitshowswhatthenumbermeans.TemperaturecanbewrittenwithunitsofdegreesFahrenheit(°F)orCelsius(°C).Usethelistbelowtocomparethetwounits.212°F|Waterboils|100°C98.6°F|Bodytemperature|37°C68°F|Roomtemperature|20°C32°F|Waterfreezes|0°C.Thebetterestimateforthetemperatureoftheaironahotdayis36°C.36°Fistoocold.Answer:Theansweris(B).(B) 36°CVisionN/A\nRationale::Asentenceisagroupofwordsthatexpressesacompletethought.ThebandI'minhasbeenrehearsingdailybecausewehaveaconcertintwoweeks.Arun-onsentenceisformedwhentwosentencesareruntogether,joinedbyjustacommaorbynopunctuationatall.Ifonlyacommaisused,therun-oniscalledacommasplice…ThebandI'minhasbeenrehearsingdailysincewehaveaconcertintwoweeks.Thisisnotarun-onsentence.Itisformedacommaspliceformedfromtwosentencesruntogether,joinedwithoutjustacomma.Right-handednessiscontrolledbygenetics,interestinglyenough,thesamegenesarealsoresponsibleforthebrainbecomingmorespecializedatcertaintasks.Answer:Theansweris(B).Options:(A) yesProblem\nPredictionQuestion:Isthisarun-onsentence?Right-handednessiscontrolledbygenetics,interestinglyenough,thesamegenesarealsoresponsibleforthebrainbecomingmorespecializedatcertaintasks.Context:N/ARationale:Asentenceisagroupofwordsthatexpressesacompletethought.ThebandI'minhasbeenrehearsingdailybecausewehaveaconcertintwoweeks.Arun-onsentenceisformedwhentwosentencesareruntogether,joinedbyjustacommaorbynopunctuationatall.Ifonlyacommaisused,therun-oniscalledacommasplice…ThebandI'minhasbeenrehearsingdailysincewehaveaconcertintwoweeks.Thisisarun-onsentence.Itisacommaspliceformedfromtwosentencesruntogether,joinedbyjustacomma.Right-handednessiscontrolledbygenetics,interestinglyenough,thesamegenesarealsoresponsibleforthebrainbecomingmorespecializedatcertaintasks.Answer:Theansweris(A).(B) noVisionN/A(a)Logicalmistakewherethemodelfailsatcomparisons.\n(b)Logicalmistakewherethethereisacontradictionintherationale.\nFigure 11. Examples of logical mistakes.", 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': "Multimodal Chain-of-Thought Reasoning in Language Models\nOptions:(A) black stripes on its skinProblem\nPredictionRationale:Thewayanorganismlooksoractsiscalledatrait.Scientistsusefossilstolearnmoreaboutthetraitsofancientorganisms.Fossilscanpreservetheremainsofbodypartsandactivities.Afossilofabodypart,suchasatailorawing,cantellyouwhatanorganismlookedlike.Afossilofanorganism'sactivities,suchasaburroworafootprint,cantellyouabouttheorganism'sbehavior.Herearethreeexamplesoffossilsandthetraitsthatyoucanobservefromthem:Thisisafossilofananimal.Thisfossiltellsyouthattheanimalhadaspiral-shapedshell.Thisisafossilofaplant.Thisfossiltellsyouthattheplanthadsmallleavesarrangedinabranchedpattern.Thisisafossilofananimal'sfootprint.Thisfossiltellsyouthattheanimalcouldwalkonland.Anorganism'sfossilmaynotshowalloftheorganism'straits.Thisisbecausemostbodypartsaredestroyedduringfossilformation.Whenanorganism'sbodyturnsintoafossil,onlyafewbodypartsareusuallypreserved.Answer:Theansweris(C).(B) large fins on its bodyVision(C) a long, thin bodyQuestion:WhichtraitdidPalaeopythonhave?Selectthetraityoucanobserveonthefossil.Context:ThispictureshowsafossilofanancientanimalcalledPalaeopython.Palaeopythonlivedintreesandcouldgrowmorethansixfeetlong.\nRationale:Thewayanorganismlooksoractsiscalledatrait.Scientistsusefossilstolearnmoreaboutthetraitsofancientorganisms.Fossilscanpreservetheremainsofbodypartsandactivities.Afossilofabodypart,suchasatailorawing,cantellyouwhatanorganismlookedlike.Afossilofanorganism'sactivities,suchasaburroworafootprint,cantellyouabouttheorganism'sbehavior.Herearethreeexamplesoffossilsandthetraitsthatyoucanobservefromthem:Thisisafossilofananimal.Thisfossiltellsyouthattheanimalhadaspiral-shapedshell.Thisisafossilofaplant.Thisfossiltellsyouthattheplanthadsmallleavesarrangedinabranchedpattern.Thisisafossilofananimal'sfootprint.Thisfossiltellsyouthattheanimalcouldwalkonland.Anorganism'sfossilmaynotshowalloftheorganism'straits.Thisisbecausemostbodypartsaredestroyedduringfossilformation.Whenanorganism'sbodyturnsintoafossil,onlyafewbodypartsareusuallypreserved.Answer:Theansweris(B).\nFigure 12. Examples of answers are incorrect while the CoT is correct.", 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}]: %s
2023-12-07 11:05:15,512 - INFO - Received requests to /inference endpoint
2023-12-07 11:05:15,613 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 11:05:15,614 - INFO - Received request: {'username': 'amin', 'prompt': 'provide me with a brief of the multimodal paper', 'memory': True, 'conversation_number': 2, 'AI_assistance': False, 'collection_name': 'paper', 'llm_model': 'Llama_13b'}
2023-12-07 11:05:16,297 - ERROR - Error processing the request: CUDA out of memory. Tried to allocate 9.63 GiB (GPU 0; 79.15 GiB total capacity; 12.17 GiB already allocated; 6.68 GiB free; 14.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2023-12-07 11:07:24,969 - INFO - Created a temporary directory at /tmp/tmpc4ht3skb
2023-12-07 11:07:24,969 - INFO - Writing /tmp/tmpc4ht3skb/_remote_module_non_scriptable.py
2023-12-07 11:07:26,772 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-07 11:07:31,016 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-07 11:09:29,302 - INFO - Received requests to /inference endpoint
2023-12-07 11:09:29,403 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 11:09:29,403 - INFO - Received request: {'username': 'amin', 'prompt': 'give me  a summary of the multimodal paper.', 'memory': True, 'conversation_number': 2, 'AI_assistance': False, 'collection_name': 'paper', 'llm_model': 'Llama_13b'}
2023-12-07 11:10:17,286 - INFO - Processed the request successfully
2023-12-07 11:12:05,089 - INFO - Received requests to /inference endpoint
2023-12-07 11:12:05,190 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 11:12:05,190 - INFO - Received request: {'username': 'amin', 'prompt': 'Give me a summary of the paper', 'memory': False, 'conversation_number': 0, 'AI_assistance': False, 'collection_name': 'paper', 'llm_model': 'Llama_13b'}
2023-12-07 11:12:11,281 - INFO - Processed the request successfully
2023-12-07 11:12:40,279 - INFO - Received requests to /inference endpoint
2023-12-07 11:12:40,380 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 11:12:40,380 - INFO - Received request: {'username': 'amin', 'prompt': 'Where is the capital of the uk', 'memory': False, 'conversation_number': 0, 'AI_assistance': True, 'collection_name': 'paper', 'llm_model': 'Llama_13b'}
2023-12-07 11:12:46,206 - INFO - Processed the request successfully
2023-12-07 11:13:41,232 - INFO - request received username='amin' class_name='paper ' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/fea4c21b92fc08de': %s
2023-12-07 11:13:41,860 - INFO - actors creation successful [Actor(WeaviateEmbedder, 34edfad8c270d94e909ad5c201000000), Actor(WeaviateEmbedder, 890f822bab4c5b167f50312401000000), Actor(WeaviateEmbedder, fd8635f546cf87bf4f811b9b01000000)]: %s
2023-12-07 11:13:41,861 - INFO - check 1st step of ray was successful
2023-12-07 11:13:41,862 - INFO - check if ray was successful:
2023-12-07 11:13:41,862 - INFO - check weaviate add data, 
2023-12-07 11:13:41,862 - INFO - request processed successfully username='amin' class_name='paper ' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/fea4c21b92fc08de': %s
2023-12-07 11:13:43,198 - INFO - Check the data that is being passed [{'page_content': 'DINOv2: Learning Robust Visual Features\nwithout Supervision\nMaxime Oquab∗∗, Timothée Darcet∗∗, Théo Moutakanni∗∗,\nHuy Vo∗, Marc Szafraniec∗, Vasil Khalidov∗, Pierre Fernandez, Daniel Haziza,\nFrancisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba,\nRussell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat,\nVasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal1,\nPatrick Labatut∗, Armand Joulin∗, Piotr Bojanowski∗\nMeta AI Research1Inria\n∗core team∗∗equal contribution\nAbstract\nThe recent breakthroughs in natural language processing for model pretraining on large\nquantities of data have opened the way for similar foundation models in computer vision.\nThese models could greatly simplify the use of images in any system by producing all-\npurpose visual features, i.e., features that work across image distributions and tasks without\nﬁnetuning. This work shows that existing pretraining methods, especially self-supervised\nmethods, can produce such features if trained on enough curated data from diverse sources.\nWe revisit existing approaches and combine diﬀerent techniques to scale our pretraining in\nterms of data and model size. Most of the technical contributions aim at accelerating and\nstabilizing the training at scale. In terms of data, we propose an automatic pipeline to build\na dedicated, diverse, and curated image dataset instead of uncurated data, as typically done\nin the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy\net al., 2020) with 1B parameters and distill it into a series of smaller models that surpass\nthe best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the\nbenchmarks at image and pixel levels.\n1 Introduction\nLearning task-agnostic pretrained representations have become the standard in Natural Language Process-\ning (NLP) (Radford et al.; Raﬀel et al., 2020; Chowdhery et al., 2022; Hoﬀmann et al., 2022; Touvron et al.,\n2023). One can use these features “as they are”, i.e., without ﬁne-tuning, and achieve performances on down-\nstream tasks that are signiﬁcantly better than those produced by task-speciﬁc models (Brown et al., 2020).\nThis success has been fueled by pretraining on large quantities of raw text using pretext objectives, such as\nlanguage modeling (Radford et al., 2017) or word vectors (Devlin et al., 2018), that require no supervision.\nFollowing this paradigm shift in NLP, we expect similar “foundation” models to appear in computer vi-\nsion (Bommasani et al., 2021). These models should generate visual features that work out of the box on\nany task, both at the image level, e.g., image classiﬁcation, and pixel level, e.g., segmentation. Most promis-\ning eﬀorts towards these foundation models focus on text-guided pretraining, i.e., using a form of textual\nsupervision to guide the training of the features (Joulin et al., 2016; Mahajan et al., 2018; Radford et al.,\n2021). This form of text-guided pretraining limits the information that can be retained about the image\nsince captions only approximate the rich information in images, and complex pixel-level information may\nAll the authors are aﬃliated to Meta, except Julien Mairal who is aﬃliated to Inria. Timothée Darcet and Pierre Fernandez\nhave a co-aﬃliation with Inria. Théo Moutakanni has a co-aﬃliation with Université Paris Saclay. Alaaeldin El-Nouby has a\nco-aﬃliation with Inria and ENS-PSL. Correspondence: {qas, timdarcet, theomoutakanni, ajoulin, bojanowski}@meta.com\n1arXiv:2304.07193v1  [cs.CV]  14 Apr 2023', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 1: Visualization of the ﬁrst PCA components. We compute a PCA between the patches of the\nimages from the same column (a, b, c and d) and show their ﬁrst 3 components. Each component is matched\nto a diﬀerent color channel. Same parts are matched between related images despite changes of pose, style\nor even objects. Background is removed by thresholding the ﬁrst PCA component.\nnot surface with this supervision. Furthermore, these image encoders require aligned text-image corpora and\nhence, do not oﬀer the ﬂexibility of their text counterparts, that is, to learn from raw data alone.\nAn alternative to text-guided pretraining is self-supervised learning (Caron et al., 2018; Chen et al., 2020;\nHe et al., 2021) where features are learned from images alone. These approaches are conceptually closer to\npretext tasks such as language modeling and can capture information at the image and pixel level (Caron\net al., 2021). However, despite their potential to learn all-purposed features, most of the advances in\nself-supervised learning were made in the context of pretraining on a small curated dataset, ImageNet-\n1k (Russakovsky et al., 2015). Some eﬀorts on scaling these approaches beyond ImageNet-1k have been\nattempted (Caron et al., 2019; Goyal et al., 2021; 2022a), but they focused on uncurated datasets, which\ntypically lead to a signiﬁcant drop in the quality of the features. This is explained by the lack of control\nover the data quality and diversity, which are essential to produce good features.\nIn this work, we explore if self-supervised learning has the potential to learn all-purposed visual features if\npretrained on a large quantity of curated data. We revisit existing discriminative self-supervised approaches\nthat learn features at both the image and patch level, such as iBOT (Zhou et al., 2021), and we reconsider\nsomeoftheirdesignchoicesunderthelensofalargerdataset. Mostofourtechnicalcontributionsaretailored\ntoward stabilizing and accelerating discriminative self-supervised learning when scaling in model and data\nsizes. These improvements make our approach around 2 ×faster and require 3 ×less memory than similar\ndiscriminative self-supervised methods, allowing us to leverage longer training with larger batch sizes.\nRegarding pretraining data, we have built an automatic pipeline to ﬁlter and rebalance datasets from an\nextensive collection of uncurated images. This pipeline is inspired by pipelines used in NLP (Wenzek et al.,\n2019), where data similarities are used instead of external metadata and do not require manual annotation.\nA major diﬃculty when dealing with images in the wild is to rebalance concepts and avoid overﬁtting on a\nfew dominant modes. In this work, a naive clustering approach works reasonably well to resolve this issue.\nWe gathered a small but diverse corpus of 142M images to validate our approach.\nFinally, we provide a variety of pretrained visual models, called DINOv2, trained with diﬀerent Vision\nTransformers (ViT) (Dosovitskiy et al., 2016) architectures on our data. We release all the models and\nthe code to retrain DINOv2 on any data. We validate the quality of DINOv2 on various computer vision\nbenchmarks at both image and pixel levels as we scale them, as summarized in Fig. 2. We conclude that self-\n2', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': '101010111012\nflops7578818487Accuracy\nInet-1k\n101010111012\nflops40485664mIoU\nSegmentation\n101010111012\nflops0.91.21.51.8 R-MSE\nMonocular Depth \n101010111012\nflops80848892Accuracy\nClassification\n101010111012\nflops4856647280Accuracy\nFinegrained Classification\n101010111012\nflops30456075mAP\nInstance Retrieval\n101010111012\nflops4050607080Accuracy\nImageNet-{A,R,Sketch}\n101010111012\nflops5055606570Accuracy\nVideo Understanding\nSSL\nWSL\nDINOv2Figure 2: Evolution of performance when scaling in parameters. We show performance on eight\ntypes of vision tasks, as presented in Sec. 7, and average metrics with each type. Features are extracted\nfrom our self-supervised encoders, DINOv2 (dark blue), and we compare them with self-supervised methods\n(pale orange), as well as weakly-supervised methods (dark pink). We report the best-performing weakly-\nsupervised model’s performance as a dashed horizontal line. Our family of models drastically improves over\nthe previous state of the art in self-supervised learning and reaches performance comparable with weakly-\nsupervised features. See Sec. 7 for a detailed analysis.\nsupervised pretraining alone is a good candidate for learning transferable frozen features that are competitive\nwith the best openly available weakly-supervised models.\n2 Related Work\nIntra-image self-supervised training. A ﬁrst family of self-supervised methods focuses on pretext tasks\nbuilt from the image, i.e., extracting a signal from the image to be predicted from the rest of the image.\nThis idea has become prevalent with the work of Doersch et al. (2015), where they train by predicting the\ncontext of a given patch. Many other pretext tasks were introduced based on re-colorizing images (Zhang\net al., 2016), predicting transformations (Gidaris et al., 2018), inpainting (Pathak et al., 2016) or patch\nre-ordering (Noroozi & Favaro, 2016; Misra & Maaten, 2020). Recently, the emergence of patch-based\narchitectures, like ViTs, has led to a revisit of inpainting for pre-training (He et al., 2021; Bao et al., 2021;\nEl-Nouby et al., 2021), potentially in feature space (Assran et al., 2023; Baevski et al., 2022). Of particular\ninterest, He et al. (2021) show that a masked auto-encoder (MAE) learns features that provide substantial\nimprovements when ﬁnetuned on downstream tasks. This property of MAEs has been further validated\non video (Tong et al., 2022), audio (Xu et al., 2022), and across other modalities (Girdhar et al., 2022).\nHowever, their features require supervised ﬁnetuning, while our features perform well out of the box.\nDiscriminativeself-supervisedlearning. Thesecondlineofwork, closertoours, isusingdiscriminative\nsignals between images or groups of images to learn features. This family of methods has roots in early\ndeep learning work (Hadsell et al., 2006) but became popular with the emergence of instance classiﬁcation\nmethods (Dosovitskiy et al., 2014; Bojanowski & Joulin, 2017; Wu et al., 2018). Several improvements\nwere made based either on instance-level objectives (Hénaﬀ et al., 2019; He et al., 2020; Chen & He, 2020;\nChen et al., 2020; Grill et al., 2020; Caron et al., 2021) or clustering (Caron et al., 2018; Asano et al.,\n2020; Caron et al., 2020). These methods provide performant frozen features on standard benchmarks like\nImageNet (Russakovsky et al., 2015), but they are hard to scale to larger model sizes (Chen et al., 2021). In\n3', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Uncur ated Data \nAugment ed Cur ated Data\nCurated Data\n Embedding\n Deduplication\n RetrievalFigure 3: Overview of our data processing pipeline. Images from curated and uncurated data sources\nare ﬁrst mapped to embeddings. Uncurated images are then deduplicated before being matched to curated\nimages. The resulting combination augments the initial dataset through a self-supervised retrieval system.\nthis work, we revisit the training of these approaches in the context of large pretraining datasets and models.\nIn particular, we build on top of Zhou et al. (2021) that we ﬁnd particularly suited for scaling.\nScaling self-supervised pretraining. A growing body of work has focused on the scaling abilities of\nself-supervised learning in terms of data and model size (Caron et al., 2019; Goyal et al., 2019; Tian et al.,\n2021; Goyal et al., 2022a). Most of these works use large quantities of uncurated data to train models\nwithout supervision. They show evidence that discriminative methods scale with data, but because of the\npoor quality of the pretraining data, most of the results are obtained by ﬁnetuning the features. Of particular\ninterest, Goyal et al. (2021) have also shown that these methods beneﬁt from scaling in model size given\nenough pretrained data. This line of work questions the ability of self-supervised methods to work on any\ndata while we focus on producing the best pretrained encoders.\nAutomatic data curation. Our dataset construction borrows from the image retrieval community (Wein-\nzaepfeletal.,2021;Radenovićetal.,2018b;Bermanetal.,2019;Douzeetal.,2009;Toliasetal.,2015;Revaud\net al., 2019). In particular, the use of retrieval to augment the training set has been studied in the context\nof semi-supervised learning (Yalniz et al., 2019). Similarly, others have used hashtags or other metadata to\nﬁlter uncurated datasets (Mahajan et al., 2018; Radford et al., 2021). Unlike this work, we use no metadata\nnor supervision to ﬁlter images and leverage visual similarity between images. Our approach is inspired by\ntext curation pipelines (Wenzek et al., 2019), where a language model is trained on Wikipedia to score texts\nextracted from an uncurated source.\n3 Data Processing\nWe assemble our curated LVD-142M dataset by retrieving, from a large pool of uncurated data, images that\nare close to those in several curated datasets. We describe below the main components in our data pipeline\nincluding the curated/uncurated data sources, the image deduplication step and the retrieval system. Our\npipeline does not require any metadata or text and directly works with images, as shown in Fig. 3. We refer\nthe reader to appendix A for more details on our approach.\nData sources. Our selection of curated datasets is detailed in the appendix (Table 15) and contains\nImageNet-22k, the train split of ImageNet-1k, Google Landmarks and several ﬁne-grained datasets. For the\nuncurated data source, we collect a raw unﬁltered dataset of images from a publicly available repository of\ncrawled web data. From each web page in the repository, we extract URL links of images from <img>tags.\nWe discards URLs that are unsafe or restricted by domains, and post-process the downloaded images (PCA\nhash deduplication, NSFW ﬁltering, and blurring identiﬁable faces). This results in 1.2B unique images.\n4', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Deduplication. We apply the copy detection pipeline of Pizzi et al. (2022) to the uncurated data and\nremove near-duplicate images. This reduces redundancy and increases diversity among images. We also\nremove near-duplicates of images contained in the test or validation set of any benchmark used in this work.\nSelf-supervised image retrieval. We build our curated pretraining dataset by retrieving images from\nour uncurated data source that are close to images in our curated sources. In order to do this, we ﬁrst\ncompute an image embedding using a self-supervised ViT-H/16 network pretrained on ImageNet-22k, and\nuse cosine-similarity as a distance measure between images. Then, we perform k-means clustering of the\nuncurated data. Given a query dataset for retrieval, if it is large enough we retrieve N(typically 4) nearest\nneighbors for each query image. If it is small, we sample Mimages from the cluster corresponding to each\nquery image. We adjust NandMby visual inspection of the retrieval result.\nImplementation Details. The deduplication and retrieval stages of our pipeline rely on the Faiss li-\nbrary (Johnson et al., 2019) to eﬃciently index and compute batch searches of nearest embeddings. In\nparticular, we heavily leverage its support for GPU-accelerated indices, using inverted ﬁle indices with prod-\nuct quantization codes (Jegou et al., 2010). The whole processing is distributed on a compute cluster of 20\nnodes equipped with 8 V100-32GB GPUs and takes less than two days to produce the LVD-142M dataset.\n4 Discriminative Self-supervised Pre-training\nWe learn our features with a discriminative self-supervised method that can be seen as a combination of\nDINO and iBOT losses with the centering of SwAV (Caron et al., 2020). We also add a regularizer to spread\nfeatures and a short high-resolution training phase. We rapidly introduce each of these approaches, but more\ndetails can be found in the related papers, or in our open-sourced code.\n•Image-level objective (Caron et al., 2021). We consider the cross-entropy loss between the\nfeatures extracted from a student and a teacher network. Both features are coming from the class\ntoken of a ViT, obtained from diﬀerent crops of the same image. We learn the parameters of the\nstudent and build the teacher with an exponential moving average of past iterates (He et al., 2020).\n•Patch-level objective (Zhou et al., 2021). We randomly mask some of the input patches given\nto the student, but not to the teacher. We then add a cross-entropy loss between the patch features\nof both networks on each masked patch. This loss is combined with the image-level loss.\n•Untying head weights between both objectives. We observe that tying the weights associated\nwith both objectives makes the model underﬁt at the patch-level while overﬁtting at the image-level.\nUntying these weights resolves this issue and improve the performances at both scales.\n•Sinkhorn-Knopp centering (Caron et al., 2020). Ruan et al. (2022) recommend to replace the\nteacher softmax-centering step of DINO and iBot by the Sinkhorn-Knopp (SK) batch normalization\nof SwAV (Caron et al., 2020). We run the Sinkhorn-Knopp algorithm steps for 3 iterations. For the\nstudent, we apply the softmax normalization.\n•KoLeo regularizer (Sablayrolles et al., 2018). The KoLeo regularizer derives from the\nKozachenko-Leonenko diﬀerential entropy estimator (see Beirlant et al. (1997); Delattre & Fournier\n(2017)) and encourages a uniform span of the features within a batch. Given a set of nvectors\n(x1, . . . , x n), it is deﬁned asLkoleo =−1\nn∑n\ni=1log(dn,i),where dn,i= min j̸=i∥xi−xj∥is the mini-\nmum distance between xiand any other point within the batch. We also ℓ2-normalize the features\nbefore computing this regularizer.\n•Adapting the resolution (Touvron et al., 2019). Increasing image resolution is key to pixel-\nlevel downstream tasks such as segmentation or detection, where small objects disappear at low\nresolutions. However, training at high resolution is time and memory demanding, and instead, we\nincrease the resolution of images to 518×518during a short period at the end of pretraining.\n5', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': '5 Eﬃcient implementation\nWe consider several improvements to train models at a larger scale. We train models on A100 GPUs using\nPyTorch 2.0. The code is also available along with the pretrained models used for feature extraction1.\nThe details of our models are in the appendix, Table 17. With the same hardware, compared to the iBOT\nimplementation, the DINOv2 code runs around 2×faster using only 1/3of the memory.\nFast and memory-eﬃcient attention. We implemented our own version of FlashAttention (Dao et al.,\n2022) to improve memory usage and speed on the self-attention layers. Our version is on par with or\nbetter than the original on all cases considered, while covering more use-cases and hardware. Due to the\nGPU hardware speciﬁcs, the eﬃciency is best when the embedding dimension per head is a multiple of\n64, and the matrix operations are even better when the full embedding dimension is a multiple of 256.\nAs a consequence, our ViT-g architecture slightly diﬀers from the architecture proposed by Zhai et al.\n(2022) in order to maximize compute eﬃciency, and we use an embedding dimension of 1536 with 24 heads\n(64 dim/head), rather than 1408 with 16 heads (88 dim/head). Our experiments did not show signiﬁcant\ndiﬀerences in ﬁnal accuracy, and our ViT-g backbone counts 1.1B parameters.\nNested tensors in self-attention. Our version also allows running in the same forward pass the global\ncrops and the local crops (that have diﬀerent numbers of patch tokens), leading to signiﬁcant compute\neﬃciency gains compared to using separate forward and backward passes as done in prior implementations.\nThe lower-level components of our setup are available in the xFormers library2(Lefaudeux et al. (2022)).\nEﬃcient stochastic depth. We implement an improved version of stochastic depth (Huang et al., 2016)\nthat skips the computation of the dropped residuals rather than masking the result. This saves memory and\ncompute in proportion approximately equal to the drop rate, thanks to speciﬁc fused kernels. With high\ndrop rates ( d= 40%in this work), this allows a drastic improvement in compute eﬃciency and memory\nusage. The implementation consists of randomly shuﬄing the Bsamples over the batch dimension, and\nslicing the ﬁrst (1−d)×Bsamples for the computations in the block.\nFully-Sharded Data Parallel (FSDP). Minimizing our objective with the AdamW optimizer requires\n4 model replicas in ﬂoat32 precision – student, teacher, optimizer ﬁrst moments, optimizer second moments.\nThis sums to 16 GBof memory for a billion-parameter model such as our ViT-g. In order to reduce this\nmemory footprint per GPU, we split the model replicas across GPUs, i.e., sharding 16 GBacross GPUs\nusing the PyTorch implementation of FSDP. Consequently, the model size is not bounded by the memory of\na single GPU but by the total sum of GPU memory across compute nodes. The Pytorch implementation of\nFSDP brings a second advantage, which is to save on the cross-GPU communication costs: the weight shards\nare stored in ﬂoat32 precision as required by the optimizer, but broadcasting weights and reducing gradients\nis done in ﬂoat16 precision for the backbone (MLP heads gradients are reduced in ﬂoat32 to avoid training\ninstabilities). This leads to approximately 50% reduction in communication costs compared to the ﬂoat32\ngradient all-reduce operation used in DistributedDataParallel (DDP), which is used in other self-supervised\npretraining methods (Caron et al., 2021; Zhou et al., 2021). As a consequence, the training procedure\nscales more eﬃciently than DDP with ﬂoat16 autocast when scaling the number of GPU nodes. Overall,\nPytorch-FSDP mixed-precision is superior to DDP with autocast in virtually all cases we encountered.\nModeldistillation. Mostofourtechnicalimprovementstothetrainingloopaimatimprovingthetraining\nof large models over large quantities of data. For smaller models, we distill them from our largest model,\nthe ViT-g, instead of training them from scratch. Knowledge distillation (Hinton et al., 2015) aims at\nreproducing the output of a large model with a smaller model by minimizing some distance between both\noutputs for a set of given inputs. Since our objective function is a form of distillation from the teacher\nnetwork to the student network, we leverage the same training loop with a few exceptions: we use a larger\nmodel as a frozen teacher, keep a spare EMA of the student that we use as our ﬁnal model, remove the\nmasking and stochastic depth, and, apply the iBOT loss on the two global crops. In our ablations, we\n1https://github.com/facebookresearch/dinov2\n2https://github.com/facebookresearch/xformers\n6', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'INet-1k k-NN INet-1k linear\niBOT 72.9 82.3\n+(our reproduction) 74.5 ↑1.6 83.2 ↑0.9\n+LayerScale, Stochastic Depth 75.4 ↑0.9 82.0 ↓1.2\n+128k prototypes 76.6 ↑1.2 81.9 ↓0.1\n+KoLeo 78.9 ↑2.3 82.5 ↑0.6\n+SwiGLU FFN 78.7 ↓0.2 83.1 ↑0.6\n+Patch size 14 78.9 ↑0.2 83.5 ↑0.4\n+Teacher momentum 0.994 79.4 ↑0.5 83.6 ↑0.1\n+Tweak warmup schedules 80.5 ↑1.1 83.8 ↑0.2\n+Batch size 3k 81.7 ↑1.2 84.7 ↑0.9\n+Sinkhorn-Knopp 81.7 = 84.7 =\n+Untying heads = DINOv2 82.0 ↑0.3 84.5 ↓0.2\nTable 1:Ablation study of the training diﬀerences between iBOT and DINOv2. We optimize\nfor k-NN performance, as in our experience, the linear probe performance is lower-bounded by the k-NN\nperformance. Some modiﬁcations, like LayerScale and a high Stochastic Depth (rate= 0.4), incur a decrease\nin linear probe performance, but have the beneﬁts of increasing the stability of training by avoiding NaN loss\nvalues during training. Overall, these modiﬁcations allowed for the next set of improvements to be added.\nExperiments are run using the ViT-Large architecture on ImageNet-22k.\nobserve that this approach achieves better performance than training from scratch, even for a ViT-L. Our\ndistillation method ends up close to the one described by Duval et al. (2023), except we do not modify the\nloss terms for distillation and evaluate the EMA of the student.\n6 Ablation Studies\nWe present a set of ablations to empirically validate diﬀerent components of our pipeline: the technical\nmodiﬁcations described in Sec. 4, the pretraining data and the impact of model distillation. We consider\nvarious downstream tasks that are described in Sec. 7.\n6.1 Improved Training Recipe\nOur approach improves over the iBOT method by combining it with several existing components described\nin Sec. 4. To evaluate their importance, we train multiple models where we successively add components to\na baseline iBOT model. We report the Top-1 accuracy on the validation set of ImageNet-1k with a k-NN\nand a linear linear in Table 1. Generally, we observe that each component improves the performance on\neither k-NN or linear probing and even both in most cases. Only LayerScale and Stochastic Depth incur a\nperformance drop in linear probing but signiﬁcantly improve the training stability in our experience.\n6.2 Pretraining Data Source\nThe quality of features is directly related to the quality of the pretraining data. In this experiment, we\nprobe the impact of LVD-142M compared to ImageNet-22k, a commonly used pretraining dataset, or using\ndirectly raw and uncurated data. For the uncurated dataset, we randomly sample 142million images from\nthe same data source as LVD-142M. We train a ViT-g/14 on each dataset for the same number of iterations.\nWe also include a variant of ImageNet-22k obtained by removing the synsets of ImageNet-1k (INet-22k \\\nINet-1k) for completeness. We report the comparisons in Table 2.\nThe most salient observation is that training on a curated set of images works better on most benchmarks\nthan training on uncurated data. This conﬁrms the beneﬁt of curating data, even in the case of self-\nsupervised pretraining. When compared with models trained on ImageNet-22k, training on LVD-142M is\nalso superior on all the benchmarks but ImageNet-1k. This conﬁrms that training on a more diverse set of\n7', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Training Data INet-1k Im-A ADE-20k Oxford-M\nINet-22k 85.9 73.5 46.6 62.5\nINet-22k\\INet-1k 85.3 70.3 46.2 58.7\nUncurated data 83.3 59.4 48.5 54.3\nLVD-142M 85.8 73.9 47.7 64.6\nTable 2:Ablation of the source of pretraining data. We compare the INet-22k dataset that was used\nin iBOT to our dataset, LVD-142M. Each model is trained for the same number of iterations, that is smaller\nthan in our ﬁnal run. Pretraining on LVD-142M maintains the performance over INet-1k while leading to\nmodels that perform better in other domains.\nL H g848586\nImageNet-1k\nINet-22k\nLVD142M\nL H g747678\nImageNet-V2\nL H g5060\nImageNet-Sketch\nL H g9294\nFood101\nL H g8090\nCars\nL H g3540\nAmsterTime\nL H g2040\nOxford-H\nFigure 4: Model scale versus data scale. Evolution of performance as a function of model size for\ntwo diﬀerent pretraining datasets: ImageNet-22k (14M images) and LVD-142M (142M images). The ViT-g\ntrained on LVD-142M surpasses the ViT-g trained on ImageNet-22k on most benchmarks.\nimages improves the quality of the features in domains that are not covered by this dataset. Overall, the\nconclusion of this ablation is that our dataset provides a good balance of diﬀerent types of images that leads\nto the best performance overall.\n6.3 Model Size and Data\nWe quantify the importance of scaling data with the model size in Fig. 4. As the size of models grow, training\non LVD-142M becomes more beneﬁcial than training on ImageNet-22k. For instance, a ViT-g trained on\nLVD-142M matches the performance on ImageNet-1k of a model trained on ImageNet-22k while signiﬁcantly\noutperforming it on the other benchmarks.\n6.4 Loss Components\nWe validated the proposed technical improvements in Sec. 6.1 by adding them incrementally. This section\nanalyzes the performance hit observed if we ablate speciﬁc loss terms, starting from our best-performing\nmodel. We ablate the importance of the KoLeo loss and the impact of the masked image modeling term.\nFor both, we report performance on ImageNet-1k using a linear classiﬁer, ADE-20k segmentation using a\nlinear classiﬁer, and nearest-neighbor image retrieval on Oxford-M. Table 3a shows the impact of using the\nKoLeo loss. We see that the instance retrieval performance improves by more than 8%, conﬁrming that this\nterm helps spread features in the output space. At the same time, the other metrics do not suﬀer from this\nregularization. In Table 3b, we show the impact of using the masked image modeling term from iBOT. This\nterm is critical for dense prediction tasks, leading to almost 3%performance improvement.\n6.5 Impact of Knowledge Distillation\nFor small architectures, we distill larger models instead of training them from scratch. We use the distillation\nprocedure described in Sec. 5. We evaluate the eﬀectiveness of this approach by comparing a ViT-L/14\ntrained from scratch with one distilled from a ViT-g/14 over 12 benchmarks in Fig. 5. We also report the\nperformance of the ViT-g/14 used for distillation as a topline. The distilled model outperforms the one\ntrained from scratch on 10 out of 12 benchmarks, validating our pretraining approach for small models.\n8', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'KoLeo INet-1k Im-A ADE-20k Oxford-M\n\x15 85.3 70.6 47.2 55.6\n✓ 85.8 72.8 47.1 63.9\n(a) Koleo lossMIM INet-1k Im-A ADE-20k Oxford-M\n\x15 85.3 72.0 44.2 64.3\n✓ 85.8 72.8 47.1 63.9\n(b) MIM objective in iBOT\nTable 3:(a)Eﬀect of the KoLeo loss term. (b)Eﬀect of the iBOT Masked Image Modeling (MIM) loss\nterm. Evaluation performed on ImageNet-{1k,A} (classiﬁcation with linear probe, accuracy %), ADE-20k\n(segmentation with linear layer, mIoU) and Oxford-M (image retrieval, mAP). Each model is trained on the\nsame number of iterations, that is smaller than our ﬁnal run. The KoLeo loss term improves nearest-neighbor\nsearch tasks (e.g. retrieval), and the MIM loss improves patch-level tasks (e.g. segmentation).\nINet-1kFoodCarsiNat18\niNat21\nPlaces 205Oxford-H\nParis-H\nINet-A\nINet-RKitti\nNYUd\nViT-L/14 Scratch\nViT-L/14 Distill\nViT-g/14 Scratch\n84.5 86.3 86.592.894.394.7\n81.890.191.4\n77.880.481.6\n83.185.185.7\n66.067.367.5\n47.7 52.6 52.1\n77.6\n84.482.761.7\n71.3\n75.968.1\n74.1\n78.82.57\n2.5\n2.350.345\n0.333\n0.298\n(a) Comparison on individual metricsArch Method INet-1k Segm. Depth ↓Classif.\nViT-g/14 Scratch 86.5 73.4 1.00 92.1\nViT-L/14 Scratch 84.5 72.2 1.10 90.2\nViT-L/14 Distill 86.3 73.3 1.08 91.2\nArch Method Finegr. Retriev. ARSketch Video\nViT-g/14 Scratch 78.3 75.2 77.0 69.3\nViT-L/14 Scratch 75.8 71.3 69.5 67.3\nViT-L/14 Distill 77.6 76.3 74.5 67.5\n(b) Averaged metrics on 8 vision tasks\nFigure 5: Eﬀectiveness of knowledge distillation. Comparison between a ViT-L trained from scratch\nor distilled from DINOv2 using ViT-g/14. For reference, we also report the performance of the ViT-g/14\nteacher. We show that a ViT-L model distilled from a frozen ViT-g outperforms a the same model trained\nfrom scratch on all benchmarks, sometimes even outperforming the distillation target.\n6.6 Impact of Resolution\nWe measure the impact of changing the resolution during the pretraining on the performance of image and\npatch-level features. We consider models trained from scratch using a ﬁxed resolution of either 224×224\nor416×416, and a model trained from scratch at 224×224, then resumed for 10k more iterations at\n416×416. High-resolution training is compute-intensive, so we conduct this ablation on a small setup: a\nViT-L/16 trained on ImageNet1k. In Fig. 6, we report the performance of a linear probe on ImageNet-1k\nand ADE-20k, evaluated at various resolutions. The model trained on high-resolution images performs best\nacross resolutions, but this comes at a high cost: training at 416is approximate 3×more compute-intensive\nthan training at 224. On the other hand, training at high resolution for only 10k iterations at the end of the\ntraining is almost as good and only requiring a fraction of the compute. As a consequence, we include this\nstep at the end of the training rather than training at a high resolution from scratch.\n7 Results\nIn this section, we present the empirical evaluation of our models on many image understanding tasks. We\nevaluate both global and local image representations, on category and instance-level recognition, semantic\nsegmentation, monocular depth prediction, and action recognition. We detail the list of benchmarks in\nAppendix C. The goal of this evaluation is twofold. First, we show that our self-supervised features outper-\n9', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': '224 336 512 640 768\nresolution78798081828384Accuracy\nImageNet-1k\n224 336 512 640 768\nresolution3941434547mIoU\nADE-20K\n224\n416\n224416\nFigure 6: Role of resolution. Performance of ViT-L/16 trained on ImageNet-1k at ﬁxed resolution (“224”\nand “416”) or trained at 224 then 416 for a short duration (“224 →416”). We train linear classiﬁers on top of\nfrozen features at diﬀerent resolutions and report Top-1 accuracy on ImageNet and mIoU on ADE-20k. We\nobserve that performing SSL training at high resolution for a short duration achieve behavior and results\nclose to training at the same high resolution for the full training, at a fraction of the cost.\nform the current state of the art by a very large margin. Second, we show that they match, or surpass the\nperformance of weakly-supervised ones on a substantial number of tasks.\nBaselines. In our comparisons, we use two kinds of models as baselines. We compare to the best performing\nself-supervised models that are openly available. First, we run our evaluations for MAE (He et al., 2021),\nDINO (Caron et al., 2021), SEERv2 (Goyal et al., 2022a), MSN (Assran et al., 2022), EsViT (Li et al.,\n2022a), Mugs (Zhou et al., 2022) and iBOT (Zhou et al., 2021). When several architectural variants were\nproposed for a given method, we report results for the one that leads to best top-1 accuracy on ImageNet-1k.\nSecond, we report performance of open-source weakly-supervised models such as CLIP (Radford et al., 2021),\nOpenCLIP (Ilharco et al., 2021), and SWAG (Singh et al., 2022). When evaluating models on ImageNet-1k,\nwe report the performance for each of the aforementioned methods. For all other evaluations, we report\nthe four best-performing models amongst SSL ones. Also, for reference, we report the best performing\nOpenCLIP-G for weakly-supervised ones.\n7.1 ImageNet Classiﬁcation\nAs a ﬁrst evaluation, we probe the quality of the holistic image representation produced by the model on the\nImageNet-1k classiﬁcation dataset. We evaluate the quality of features by training a simple classiﬁer over a\nfrozen backbone, and do not perform ﬁnetuning of the backbone weights. Following previous work, we use\na linear model for simplicity, ensuring a reproducible evaluation, despite the fact that classes may not be\nlinearly separable. Because most SSL methods were developped using ImageNet-1k validation performance\nas a debugging signal, we also report the top-1 accuracy on ImageNet-ReaL and ImageNet-V2. In order\nto report this additional validation performance, for all models, we run the evaluation with our code. We\ncompare our frozen features to the best publicly available SSL features in Table 4, regardless of architecture\nor pretraining data. We see the components proposed in this work lead to a very signiﬁcant improvement\n(+4.2%) over the previous state of the art (iBOT ViT-L/16 trained on ImageNet-22k) on linear evaluation.\nAt the same time, we also see that the performance increase on the alternative test sets is larger for our\nmethod, indicating stronger generalization. We describe details of our linear evaluation in Appendix B.3.\nHow far are we from weakly-supervised models? We also want to validate that our features are com-\npetitive with state-of-the-art open-source weakly supervised models. To this end, we compare on ImageNet-\n1k, using the linear evaluation, to three oﬀ-the-shelf methods with several architectural variants. For all\nmodels, we run the linear evaluation using our code, after making sure that our numbers match those re-\nported in technical reports and papers. We show the result of this evaluation in Table 4. We see that our\nbackbone, surpases the performance of OpenCLIP with a ViT-G/14 architecture ( +0.3%) and EVA-CLIP\nwith a ViT-g/14 ( +0.1%). At the same time, we also observe that our performance on the ImageNet-V2 test\n10', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}]: %s
2023-12-07 11:13:43,199 - INFO - Check the results [{'page_content': 'DINOv2: Learning Robust Visual Features\nwithout Supervision\nMaxime Oquab∗∗, Timothée Darcet∗∗, Théo Moutakanni∗∗,\nHuy Vo∗, Marc Szafraniec∗, Vasil Khalidov∗, Pierre Fernandez, Daniel Haziza,\nFrancisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba,\nRussell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat,\nVasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal1,\nPatrick Labatut∗, Armand Joulin∗, Piotr Bojanowski∗\nMeta AI Research1Inria\n∗core team∗∗equal contribution\nAbstract\nThe recent breakthroughs in natural language processing for model pretraining on large\nquantities of data have opened the way for similar foundation models in computer vision.\nThese models could greatly simplify the use of images in any system by producing all-\npurpose visual features, i.e., features that work across image distributions and tasks without\nﬁnetuning. This work shows that existing pretraining methods, especially self-supervised\nmethods, can produce such features if trained on enough curated data from diverse sources.\nWe revisit existing approaches and combine diﬀerent techniques to scale our pretraining in\nterms of data and model size. Most of the technical contributions aim at accelerating and\nstabilizing the training at scale. In terms of data, we propose an automatic pipeline to build\na dedicated, diverse, and curated image dataset instead of uncurated data, as typically done\nin the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy\net al., 2020) with 1B parameters and distill it into a series of smaller models that surpass\nthe best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the\nbenchmarks at image and pixel levels.\n1 Introduction\nLearning task-agnostic pretrained representations have become the standard in Natural Language Process-\ning (NLP) (Radford et al.; Raﬀel et al., 2020; Chowdhery et al., 2022; Hoﬀmann et al., 2022; Touvron et al.,\n2023). One can use these features “as they are”, i.e., without ﬁne-tuning, and achieve performances on down-\nstream tasks that are signiﬁcantly better than those produced by task-speciﬁc models (Brown et al., 2020).\nThis success has been fueled by pretraining on large quantities of raw text using pretext objectives, such as\nlanguage modeling (Radford et al., 2017) or word vectors (Devlin et al., 2018), that require no supervision.\nFollowing this paradigm shift in NLP, we expect similar “foundation” models to appear in computer vi-\nsion (Bommasani et al., 2021). These models should generate visual features that work out of the box on\nany task, both at the image level, e.g., image classiﬁcation, and pixel level, e.g., segmentation. Most promis-\ning eﬀorts towards these foundation models focus on text-guided pretraining, i.e., using a form of textual\nsupervision to guide the training of the features (Joulin et al., 2016; Mahajan et al., 2018; Radford et al.,\n2021). This form of text-guided pretraining limits the information that can be retained about the image\nsince captions only approximate the rich information in images, and complex pixel-level information may\nAll the authors are aﬃliated to Meta, except Julien Mairal who is aﬃliated to Inria. Timothée Darcet and Pierre Fernandez\nhave a co-aﬃliation with Inria. Théo Moutakanni has a co-aﬃliation with Université Paris Saclay. Alaaeldin El-Nouby has a\nco-aﬃliation with Inria and ENS-PSL. Correspondence: {qas, timdarcet, theomoutakanni, ajoulin, bojanowski}@meta.com\n1arXiv:2304.07193v1  [cs.CV]  14 Apr 2023', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 1: Visualization of the ﬁrst PCA components. We compute a PCA between the patches of the\nimages from the same column (a, b, c and d) and show their ﬁrst 3 components. Each component is matched\nto a diﬀerent color channel. Same parts are matched between related images despite changes of pose, style\nor even objects. Background is removed by thresholding the ﬁrst PCA component.\nnot surface with this supervision. Furthermore, these image encoders require aligned text-image corpora and\nhence, do not oﬀer the ﬂexibility of their text counterparts, that is, to learn from raw data alone.\nAn alternative to text-guided pretraining is self-supervised learning (Caron et al., 2018; Chen et al., 2020;\nHe et al., 2021) where features are learned from images alone. These approaches are conceptually closer to\npretext tasks such as language modeling and can capture information at the image and pixel level (Caron\net al., 2021). However, despite their potential to learn all-purposed features, most of the advances in\nself-supervised learning were made in the context of pretraining on a small curated dataset, ImageNet-\n1k (Russakovsky et al., 2015). Some eﬀorts on scaling these approaches beyond ImageNet-1k have been\nattempted (Caron et al., 2019; Goyal et al., 2021; 2022a), but they focused on uncurated datasets, which\ntypically lead to a signiﬁcant drop in the quality of the features. This is explained by the lack of control\nover the data quality and diversity, which are essential to produce good features.\nIn this work, we explore if self-supervised learning has the potential to learn all-purposed visual features if\npretrained on a large quantity of curated data. We revisit existing discriminative self-supervised approaches\nthat learn features at both the image and patch level, such as iBOT (Zhou et al., 2021), and we reconsider\nsomeoftheirdesignchoicesunderthelensofalargerdataset. Mostofourtechnicalcontributionsaretailored\ntoward stabilizing and accelerating discriminative self-supervised learning when scaling in model and data\nsizes. These improvements make our approach around 2 ×faster and require 3 ×less memory than similar\ndiscriminative self-supervised methods, allowing us to leverage longer training with larger batch sizes.\nRegarding pretraining data, we have built an automatic pipeline to ﬁlter and rebalance datasets from an\nextensive collection of uncurated images. This pipeline is inspired by pipelines used in NLP (Wenzek et al.,\n2019), where data similarities are used instead of external metadata and do not require manual annotation.\nA major diﬃculty when dealing with images in the wild is to rebalance concepts and avoid overﬁtting on a\nfew dominant modes. In this work, a naive clustering approach works reasonably well to resolve this issue.\nWe gathered a small but diverse corpus of 142M images to validate our approach.\nFinally, we provide a variety of pretrained visual models, called DINOv2, trained with diﬀerent Vision\nTransformers (ViT) (Dosovitskiy et al., 2016) architectures on our data. We release all the models and\nthe code to retrain DINOv2 on any data. We validate the quality of DINOv2 on various computer vision\nbenchmarks at both image and pixel levels as we scale them, as summarized in Fig. 2. We conclude that self-\n2', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': '101010111012\nflops7578818487Accuracy\nInet-1k\n101010111012\nflops40485664mIoU\nSegmentation\n101010111012\nflops0.91.21.51.8 R-MSE\nMonocular Depth \n101010111012\nflops80848892Accuracy\nClassification\n101010111012\nflops4856647280Accuracy\nFinegrained Classification\n101010111012\nflops30456075mAP\nInstance Retrieval\n101010111012\nflops4050607080Accuracy\nImageNet-{A,R,Sketch}\n101010111012\nflops5055606570Accuracy\nVideo Understanding\nSSL\nWSL\nDINOv2Figure 2: Evolution of performance when scaling in parameters. We show performance on eight\ntypes of vision tasks, as presented in Sec. 7, and average metrics with each type. Features are extracted\nfrom our self-supervised encoders, DINOv2 (dark blue), and we compare them with self-supervised methods\n(pale orange), as well as weakly-supervised methods (dark pink). We report the best-performing weakly-\nsupervised model’s performance as a dashed horizontal line. Our family of models drastically improves over\nthe previous state of the art in self-supervised learning and reaches performance comparable with weakly-\nsupervised features. See Sec. 7 for a detailed analysis.\nsupervised pretraining alone is a good candidate for learning transferable frozen features that are competitive\nwith the best openly available weakly-supervised models.\n2 Related Work\nIntra-image self-supervised training. A ﬁrst family of self-supervised methods focuses on pretext tasks\nbuilt from the image, i.e., extracting a signal from the image to be predicted from the rest of the image.\nThis idea has become prevalent with the work of Doersch et al. (2015), where they train by predicting the\ncontext of a given patch. Many other pretext tasks were introduced based on re-colorizing images (Zhang\net al., 2016), predicting transformations (Gidaris et al., 2018), inpainting (Pathak et al., 2016) or patch\nre-ordering (Noroozi & Favaro, 2016; Misra & Maaten, 2020). Recently, the emergence of patch-based\narchitectures, like ViTs, has led to a revisit of inpainting for pre-training (He et al., 2021; Bao et al., 2021;\nEl-Nouby et al., 2021), potentially in feature space (Assran et al., 2023; Baevski et al., 2022). Of particular\ninterest, He et al. (2021) show that a masked auto-encoder (MAE) learns features that provide substantial\nimprovements when ﬁnetuned on downstream tasks. This property of MAEs has been further validated\non video (Tong et al., 2022), audio (Xu et al., 2022), and across other modalities (Girdhar et al., 2022).\nHowever, their features require supervised ﬁnetuning, while our features perform well out of the box.\nDiscriminativeself-supervisedlearning. Thesecondlineofwork, closertoours, isusingdiscriminative\nsignals between images or groups of images to learn features. This family of methods has roots in early\ndeep learning work (Hadsell et al., 2006) but became popular with the emergence of instance classiﬁcation\nmethods (Dosovitskiy et al., 2014; Bojanowski & Joulin, 2017; Wu et al., 2018). Several improvements\nwere made based either on instance-level objectives (Hénaﬀ et al., 2019; He et al., 2020; Chen & He, 2020;\nChen et al., 2020; Grill et al., 2020; Caron et al., 2021) or clustering (Caron et al., 2018; Asano et al.,\n2020; Caron et al., 2020). These methods provide performant frozen features on standard benchmarks like\nImageNet (Russakovsky et al., 2015), but they are hard to scale to larger model sizes (Chen et al., 2021). In\n3', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Uncur ated Data \nAugment ed Cur ated Data\nCurated Data\n Embedding\n Deduplication\n RetrievalFigure 3: Overview of our data processing pipeline. Images from curated and uncurated data sources\nare ﬁrst mapped to embeddings. Uncurated images are then deduplicated before being matched to curated\nimages. The resulting combination augments the initial dataset through a self-supervised retrieval system.\nthis work, we revisit the training of these approaches in the context of large pretraining datasets and models.\nIn particular, we build on top of Zhou et al. (2021) that we ﬁnd particularly suited for scaling.\nScaling self-supervised pretraining. A growing body of work has focused on the scaling abilities of\nself-supervised learning in terms of data and model size (Caron et al., 2019; Goyal et al., 2019; Tian et al.,\n2021; Goyal et al., 2022a). Most of these works use large quantities of uncurated data to train models\nwithout supervision. They show evidence that discriminative methods scale with data, but because of the\npoor quality of the pretraining data, most of the results are obtained by ﬁnetuning the features. Of particular\ninterest, Goyal et al. (2021) have also shown that these methods beneﬁt from scaling in model size given\nenough pretrained data. This line of work questions the ability of self-supervised methods to work on any\ndata while we focus on producing the best pretrained encoders.\nAutomatic data curation. Our dataset construction borrows from the image retrieval community (Wein-\nzaepfeletal.,2021;Radenovićetal.,2018b;Bermanetal.,2019;Douzeetal.,2009;Toliasetal.,2015;Revaud\net al., 2019). In particular, the use of retrieval to augment the training set has been studied in the context\nof semi-supervised learning (Yalniz et al., 2019). Similarly, others have used hashtags or other metadata to\nﬁlter uncurated datasets (Mahajan et al., 2018; Radford et al., 2021). Unlike this work, we use no metadata\nnor supervision to ﬁlter images and leverage visual similarity between images. Our approach is inspired by\ntext curation pipelines (Wenzek et al., 2019), where a language model is trained on Wikipedia to score texts\nextracted from an uncurated source.\n3 Data Processing\nWe assemble our curated LVD-142M dataset by retrieving, from a large pool of uncurated data, images that\nare close to those in several curated datasets. We describe below the main components in our data pipeline\nincluding the curated/uncurated data sources, the image deduplication step and the retrieval system. Our\npipeline does not require any metadata or text and directly works with images, as shown in Fig. 3. We refer\nthe reader to appendix A for more details on our approach.\nData sources. Our selection of curated datasets is detailed in the appendix (Table 15) and contains\nImageNet-22k, the train split of ImageNet-1k, Google Landmarks and several ﬁne-grained datasets. For the\nuncurated data source, we collect a raw unﬁltered dataset of images from a publicly available repository of\ncrawled web data. From each web page in the repository, we extract URL links of images from <img>tags.\nWe discards URLs that are unsafe or restricted by domains, and post-process the downloaded images (PCA\nhash deduplication, NSFW ﬁltering, and blurring identiﬁable faces). This results in 1.2B unique images.\n4', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Deduplication. We apply the copy detection pipeline of Pizzi et al. (2022) to the uncurated data and\nremove near-duplicate images. This reduces redundancy and increases diversity among images. We also\nremove near-duplicates of images contained in the test or validation set of any benchmark used in this work.\nSelf-supervised image retrieval. We build our curated pretraining dataset by retrieving images from\nour uncurated data source that are close to images in our curated sources. In order to do this, we ﬁrst\ncompute an image embedding using a self-supervised ViT-H/16 network pretrained on ImageNet-22k, and\nuse cosine-similarity as a distance measure between images. Then, we perform k-means clustering of the\nuncurated data. Given a query dataset for retrieval, if it is large enough we retrieve N(typically 4) nearest\nneighbors for each query image. If it is small, we sample Mimages from the cluster corresponding to each\nquery image. We adjust NandMby visual inspection of the retrieval result.\nImplementation Details. The deduplication and retrieval stages of our pipeline rely on the Faiss li-\nbrary (Johnson et al., 2019) to eﬃciently index and compute batch searches of nearest embeddings. In\nparticular, we heavily leverage its support for GPU-accelerated indices, using inverted ﬁle indices with prod-\nuct quantization codes (Jegou et al., 2010). The whole processing is distributed on a compute cluster of 20\nnodes equipped with 8 V100-32GB GPUs and takes less than two days to produce the LVD-142M dataset.\n4 Discriminative Self-supervised Pre-training\nWe learn our features with a discriminative self-supervised method that can be seen as a combination of\nDINO and iBOT losses with the centering of SwAV (Caron et al., 2020). We also add a regularizer to spread\nfeatures and a short high-resolution training phase. We rapidly introduce each of these approaches, but more\ndetails can be found in the related papers, or in our open-sourced code.\n•Image-level objective (Caron et al., 2021). We consider the cross-entropy loss between the\nfeatures extracted from a student and a teacher network. Both features are coming from the class\ntoken of a ViT, obtained from diﬀerent crops of the same image. We learn the parameters of the\nstudent and build the teacher with an exponential moving average of past iterates (He et al., 2020).\n•Patch-level objective (Zhou et al., 2021). We randomly mask some of the input patches given\nto the student, but not to the teacher. We then add a cross-entropy loss between the patch features\nof both networks on each masked patch. This loss is combined with the image-level loss.\n•Untying head weights between both objectives. We observe that tying the weights associated\nwith both objectives makes the model underﬁt at the patch-level while overﬁtting at the image-level.\nUntying these weights resolves this issue and improve the performances at both scales.\n•Sinkhorn-Knopp centering (Caron et al., 2020). Ruan et al. (2022) recommend to replace the\nteacher softmax-centering step of DINO and iBot by the Sinkhorn-Knopp (SK) batch normalization\nof SwAV (Caron et al., 2020). We run the Sinkhorn-Knopp algorithm steps for 3 iterations. For the\nstudent, we apply the softmax normalization.\n•KoLeo regularizer (Sablayrolles et al., 2018). The KoLeo regularizer derives from the\nKozachenko-Leonenko diﬀerential entropy estimator (see Beirlant et al. (1997); Delattre & Fournier\n(2017)) and encourages a uniform span of the features within a batch. Given a set of nvectors\n(x1, . . . , x n), it is deﬁned asLkoleo =−1\nn∑n\ni=1log(dn,i),where dn,i= min j̸=i∥xi−xj∥is the mini-\nmum distance between xiand any other point within the batch. We also ℓ2-normalize the features\nbefore computing this regularizer.\n•Adapting the resolution (Touvron et al., 2019). Increasing image resolution is key to pixel-\nlevel downstream tasks such as segmentation or detection, where small objects disappear at low\nresolutions. However, training at high resolution is time and memory demanding, and instead, we\nincrease the resolution of images to 518×518during a short period at the end of pretraining.\n5', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': '5 Eﬃcient implementation\nWe consider several improvements to train models at a larger scale. We train models on A100 GPUs using\nPyTorch 2.0. The code is also available along with the pretrained models used for feature extraction1.\nThe details of our models are in the appendix, Table 17. With the same hardware, compared to the iBOT\nimplementation, the DINOv2 code runs around 2×faster using only 1/3of the memory.\nFast and memory-eﬃcient attention. We implemented our own version of FlashAttention (Dao et al.,\n2022) to improve memory usage and speed on the self-attention layers. Our version is on par with or\nbetter than the original on all cases considered, while covering more use-cases and hardware. Due to the\nGPU hardware speciﬁcs, the eﬃciency is best when the embedding dimension per head is a multiple of\n64, and the matrix operations are even better when the full embedding dimension is a multiple of 256.\nAs a consequence, our ViT-g architecture slightly diﬀers from the architecture proposed by Zhai et al.\n(2022) in order to maximize compute eﬃciency, and we use an embedding dimension of 1536 with 24 heads\n(64 dim/head), rather than 1408 with 16 heads (88 dim/head). Our experiments did not show signiﬁcant\ndiﬀerences in ﬁnal accuracy, and our ViT-g backbone counts 1.1B parameters.\nNested tensors in self-attention. Our version also allows running in the same forward pass the global\ncrops and the local crops (that have diﬀerent numbers of patch tokens), leading to signiﬁcant compute\neﬃciency gains compared to using separate forward and backward passes as done in prior implementations.\nThe lower-level components of our setup are available in the xFormers library2(Lefaudeux et al. (2022)).\nEﬃcient stochastic depth. We implement an improved version of stochastic depth (Huang et al., 2016)\nthat skips the computation of the dropped residuals rather than masking the result. This saves memory and\ncompute in proportion approximately equal to the drop rate, thanks to speciﬁc fused kernels. With high\ndrop rates ( d= 40%in this work), this allows a drastic improvement in compute eﬃciency and memory\nusage. The implementation consists of randomly shuﬄing the Bsamples over the batch dimension, and\nslicing the ﬁrst (1−d)×Bsamples for the computations in the block.\nFully-Sharded Data Parallel (FSDP). Minimizing our objective with the AdamW optimizer requires\n4 model replicas in ﬂoat32 precision – student, teacher, optimizer ﬁrst moments, optimizer second moments.\nThis sums to 16 GBof memory for a billion-parameter model such as our ViT-g. In order to reduce this\nmemory footprint per GPU, we split the model replicas across GPUs, i.e., sharding 16 GBacross GPUs\nusing the PyTorch implementation of FSDP. Consequently, the model size is not bounded by the memory of\na single GPU but by the total sum of GPU memory across compute nodes. The Pytorch implementation of\nFSDP brings a second advantage, which is to save on the cross-GPU communication costs: the weight shards\nare stored in ﬂoat32 precision as required by the optimizer, but broadcasting weights and reducing gradients\nis done in ﬂoat16 precision for the backbone (MLP heads gradients are reduced in ﬂoat32 to avoid training\ninstabilities). This leads to approximately 50% reduction in communication costs compared to the ﬂoat32\ngradient all-reduce operation used in DistributedDataParallel (DDP), which is used in other self-supervised\npretraining methods (Caron et al., 2021; Zhou et al., 2021). As a consequence, the training procedure\nscales more eﬃciently than DDP with ﬂoat16 autocast when scaling the number of GPU nodes. Overall,\nPytorch-FSDP mixed-precision is superior to DDP with autocast in virtually all cases we encountered.\nModeldistillation. Mostofourtechnicalimprovementstothetrainingloopaimatimprovingthetraining\nof large models over large quantities of data. For smaller models, we distill them from our largest model,\nthe ViT-g, instead of training them from scratch. Knowledge distillation (Hinton et al., 2015) aims at\nreproducing the output of a large model with a smaller model by minimizing some distance between both\noutputs for a set of given inputs. Since our objective function is a form of distillation from the teacher\nnetwork to the student network, we leverage the same training loop with a few exceptions: we use a larger\nmodel as a frozen teacher, keep a spare EMA of the student that we use as our ﬁnal model, remove the\nmasking and stochastic depth, and, apply the iBOT loss on the two global crops. In our ablations, we\n1https://github.com/facebookresearch/dinov2\n2https://github.com/facebookresearch/xformers\n6', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'INet-1k k-NN INet-1k linear\niBOT 72.9 82.3\n+(our reproduction) 74.5 ↑1.6 83.2 ↑0.9\n+LayerScale, Stochastic Depth 75.4 ↑0.9 82.0 ↓1.2\n+128k prototypes 76.6 ↑1.2 81.9 ↓0.1\n+KoLeo 78.9 ↑2.3 82.5 ↑0.6\n+SwiGLU FFN 78.7 ↓0.2 83.1 ↑0.6\n+Patch size 14 78.9 ↑0.2 83.5 ↑0.4\n+Teacher momentum 0.994 79.4 ↑0.5 83.6 ↑0.1\n+Tweak warmup schedules 80.5 ↑1.1 83.8 ↑0.2\n+Batch size 3k 81.7 ↑1.2 84.7 ↑0.9\n+Sinkhorn-Knopp 81.7 = 84.7 =\n+Untying heads = DINOv2 82.0 ↑0.3 84.5 ↓0.2\nTable 1:Ablation study of the training diﬀerences between iBOT and DINOv2. We optimize\nfor k-NN performance, as in our experience, the linear probe performance is lower-bounded by the k-NN\nperformance. Some modiﬁcations, like LayerScale and a high Stochastic Depth (rate= 0.4), incur a decrease\nin linear probe performance, but have the beneﬁts of increasing the stability of training by avoiding NaN loss\nvalues during training. Overall, these modiﬁcations allowed for the next set of improvements to be added.\nExperiments are run using the ViT-Large architecture on ImageNet-22k.\nobserve that this approach achieves better performance than training from scratch, even for a ViT-L. Our\ndistillation method ends up close to the one described by Duval et al. (2023), except we do not modify the\nloss terms for distillation and evaluate the EMA of the student.\n6 Ablation Studies\nWe present a set of ablations to empirically validate diﬀerent components of our pipeline: the technical\nmodiﬁcations described in Sec. 4, the pretraining data and the impact of model distillation. We consider\nvarious downstream tasks that are described in Sec. 7.\n6.1 Improved Training Recipe\nOur approach improves over the iBOT method by combining it with several existing components described\nin Sec. 4. To evaluate their importance, we train multiple models where we successively add components to\na baseline iBOT model. We report the Top-1 accuracy on the validation set of ImageNet-1k with a k-NN\nand a linear linear in Table 1. Generally, we observe that each component improves the performance on\neither k-NN or linear probing and even both in most cases. Only LayerScale and Stochastic Depth incur a\nperformance drop in linear probing but signiﬁcantly improve the training stability in our experience.\n6.2 Pretraining Data Source\nThe quality of features is directly related to the quality of the pretraining data. In this experiment, we\nprobe the impact of LVD-142M compared to ImageNet-22k, a commonly used pretraining dataset, or using\ndirectly raw and uncurated data. For the uncurated dataset, we randomly sample 142million images from\nthe same data source as LVD-142M. We train a ViT-g/14 on each dataset for the same number of iterations.\nWe also include a variant of ImageNet-22k obtained by removing the synsets of ImageNet-1k (INet-22k \\\nINet-1k) for completeness. We report the comparisons in Table 2.\nThe most salient observation is that training on a curated set of images works better on most benchmarks\nthan training on uncurated data. This conﬁrms the beneﬁt of curating data, even in the case of self-\nsupervised pretraining. When compared with models trained on ImageNet-22k, training on LVD-142M is\nalso superior on all the benchmarks but ImageNet-1k. This conﬁrms that training on a more diverse set of\n7', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Training Data INet-1k Im-A ADE-20k Oxford-M\nINet-22k 85.9 73.5 46.6 62.5\nINet-22k\\INet-1k 85.3 70.3 46.2 58.7\nUncurated data 83.3 59.4 48.5 54.3\nLVD-142M 85.8 73.9 47.7 64.6\nTable 2:Ablation of the source of pretraining data. We compare the INet-22k dataset that was used\nin iBOT to our dataset, LVD-142M. Each model is trained for the same number of iterations, that is smaller\nthan in our ﬁnal run. Pretraining on LVD-142M maintains the performance over INet-1k while leading to\nmodels that perform better in other domains.\nL H g848586\nImageNet-1k\nINet-22k\nLVD142M\nL H g747678\nImageNet-V2\nL H g5060\nImageNet-Sketch\nL H g9294\nFood101\nL H g8090\nCars\nL H g3540\nAmsterTime\nL H g2040\nOxford-H\nFigure 4: Model scale versus data scale. Evolution of performance as a function of model size for\ntwo diﬀerent pretraining datasets: ImageNet-22k (14M images) and LVD-142M (142M images). The ViT-g\ntrained on LVD-142M surpasses the ViT-g trained on ImageNet-22k on most benchmarks.\nimages improves the quality of the features in domains that are not covered by this dataset. Overall, the\nconclusion of this ablation is that our dataset provides a good balance of diﬀerent types of images that leads\nto the best performance overall.\n6.3 Model Size and Data\nWe quantify the importance of scaling data with the model size in Fig. 4. As the size of models grow, training\non LVD-142M becomes more beneﬁcial than training on ImageNet-22k. For instance, a ViT-g trained on\nLVD-142M matches the performance on ImageNet-1k of a model trained on ImageNet-22k while signiﬁcantly\noutperforming it on the other benchmarks.\n6.4 Loss Components\nWe validated the proposed technical improvements in Sec. 6.1 by adding them incrementally. This section\nanalyzes the performance hit observed if we ablate speciﬁc loss terms, starting from our best-performing\nmodel. We ablate the importance of the KoLeo loss and the impact of the masked image modeling term.\nFor both, we report performance on ImageNet-1k using a linear classiﬁer, ADE-20k segmentation using a\nlinear classiﬁer, and nearest-neighbor image retrieval on Oxford-M. Table 3a shows the impact of using the\nKoLeo loss. We see that the instance retrieval performance improves by more than 8%, conﬁrming that this\nterm helps spread features in the output space. At the same time, the other metrics do not suﬀer from this\nregularization. In Table 3b, we show the impact of using the masked image modeling term from iBOT. This\nterm is critical for dense prediction tasks, leading to almost 3%performance improvement.\n6.5 Impact of Knowledge Distillation\nFor small architectures, we distill larger models instead of training them from scratch. We use the distillation\nprocedure described in Sec. 5. We evaluate the eﬀectiveness of this approach by comparing a ViT-L/14\ntrained from scratch with one distilled from a ViT-g/14 over 12 benchmarks in Fig. 5. We also report the\nperformance of the ViT-g/14 used for distillation as a topline. The distilled model outperforms the one\ntrained from scratch on 10 out of 12 benchmarks, validating our pretraining approach for small models.\n8', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'KoLeo INet-1k Im-A ADE-20k Oxford-M\n\x15 85.3 70.6 47.2 55.6\n✓ 85.8 72.8 47.1 63.9\n(a) Koleo lossMIM INet-1k Im-A ADE-20k Oxford-M\n\x15 85.3 72.0 44.2 64.3\n✓ 85.8 72.8 47.1 63.9\n(b) MIM objective in iBOT\nTable 3:(a)Eﬀect of the KoLeo loss term. (b)Eﬀect of the iBOT Masked Image Modeling (MIM) loss\nterm. Evaluation performed on ImageNet-{1k,A} (classiﬁcation with linear probe, accuracy %), ADE-20k\n(segmentation with linear layer, mIoU) and Oxford-M (image retrieval, mAP). Each model is trained on the\nsame number of iterations, that is smaller than our ﬁnal run. The KoLeo loss term improves nearest-neighbor\nsearch tasks (e.g. retrieval), and the MIM loss improves patch-level tasks (e.g. segmentation).\nINet-1kFoodCarsiNat18\niNat21\nPlaces 205Oxford-H\nParis-H\nINet-A\nINet-RKitti\nNYUd\nViT-L/14 Scratch\nViT-L/14 Distill\nViT-g/14 Scratch\n84.5 86.3 86.592.894.394.7\n81.890.191.4\n77.880.481.6\n83.185.185.7\n66.067.367.5\n47.7 52.6 52.1\n77.6\n84.482.761.7\n71.3\n75.968.1\n74.1\n78.82.57\n2.5\n2.350.345\n0.333\n0.298\n(a) Comparison on individual metricsArch Method INet-1k Segm. Depth ↓Classif.\nViT-g/14 Scratch 86.5 73.4 1.00 92.1\nViT-L/14 Scratch 84.5 72.2 1.10 90.2\nViT-L/14 Distill 86.3 73.3 1.08 91.2\nArch Method Finegr. Retriev. ARSketch Video\nViT-g/14 Scratch 78.3 75.2 77.0 69.3\nViT-L/14 Scratch 75.8 71.3 69.5 67.3\nViT-L/14 Distill 77.6 76.3 74.5 67.5\n(b) Averaged metrics on 8 vision tasks\nFigure 5: Eﬀectiveness of knowledge distillation. Comparison between a ViT-L trained from scratch\nor distilled from DINOv2 using ViT-g/14. For reference, we also report the performance of the ViT-g/14\nteacher. We show that a ViT-L model distilled from a frozen ViT-g outperforms a the same model trained\nfrom scratch on all benchmarks, sometimes even outperforming the distillation target.\n6.6 Impact of Resolution\nWe measure the impact of changing the resolution during the pretraining on the performance of image and\npatch-level features. We consider models trained from scratch using a ﬁxed resolution of either 224×224\nor416×416, and a model trained from scratch at 224×224, then resumed for 10k more iterations at\n416×416. High-resolution training is compute-intensive, so we conduct this ablation on a small setup: a\nViT-L/16 trained on ImageNet1k. In Fig. 6, we report the performance of a linear probe on ImageNet-1k\nand ADE-20k, evaluated at various resolutions. The model trained on high-resolution images performs best\nacross resolutions, but this comes at a high cost: training at 416is approximate 3×more compute-intensive\nthan training at 224. On the other hand, training at high resolution for only 10k iterations at the end of the\ntraining is almost as good and only requiring a fraction of the compute. As a consequence, we include this\nstep at the end of the training rather than training at a high resolution from scratch.\n7 Results\nIn this section, we present the empirical evaluation of our models on many image understanding tasks. We\nevaluate both global and local image representations, on category and instance-level recognition, semantic\nsegmentation, monocular depth prediction, and action recognition. We detail the list of benchmarks in\nAppendix C. The goal of this evaluation is twofold. First, we show that our self-supervised features outper-\n9', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': '224 336 512 640 768\nresolution78798081828384Accuracy\nImageNet-1k\n224 336 512 640 768\nresolution3941434547mIoU\nADE-20K\n224\n416\n224416\nFigure 6: Role of resolution. Performance of ViT-L/16 trained on ImageNet-1k at ﬁxed resolution (“224”\nand “416”) or trained at 224 then 416 for a short duration (“224 →416”). We train linear classiﬁers on top of\nfrozen features at diﬀerent resolutions and report Top-1 accuracy on ImageNet and mIoU on ADE-20k. We\nobserve that performing SSL training at high resolution for a short duration achieve behavior and results\nclose to training at the same high resolution for the full training, at a fraction of the cost.\nform the current state of the art by a very large margin. Second, we show that they match, or surpass the\nperformance of weakly-supervised ones on a substantial number of tasks.\nBaselines. In our comparisons, we use two kinds of models as baselines. We compare to the best performing\nself-supervised models that are openly available. First, we run our evaluations for MAE (He et al., 2021),\nDINO (Caron et al., 2021), SEERv2 (Goyal et al., 2022a), MSN (Assran et al., 2022), EsViT (Li et al.,\n2022a), Mugs (Zhou et al., 2022) and iBOT (Zhou et al., 2021). When several architectural variants were\nproposed for a given method, we report results for the one that leads to best top-1 accuracy on ImageNet-1k.\nSecond, we report performance of open-source weakly-supervised models such as CLIP (Radford et al., 2021),\nOpenCLIP (Ilharco et al., 2021), and SWAG (Singh et al., 2022). When evaluating models on ImageNet-1k,\nwe report the performance for each of the aforementioned methods. For all other evaluations, we report\nthe four best-performing models amongst SSL ones. Also, for reference, we report the best performing\nOpenCLIP-G for weakly-supervised ones.\n7.1 ImageNet Classiﬁcation\nAs a ﬁrst evaluation, we probe the quality of the holistic image representation produced by the model on the\nImageNet-1k classiﬁcation dataset. We evaluate the quality of features by training a simple classiﬁer over a\nfrozen backbone, and do not perform ﬁnetuning of the backbone weights. Following previous work, we use\na linear model for simplicity, ensuring a reproducible evaluation, despite the fact that classes may not be\nlinearly separable. Because most SSL methods were developped using ImageNet-1k validation performance\nas a debugging signal, we also report the top-1 accuracy on ImageNet-ReaL and ImageNet-V2. In order\nto report this additional validation performance, for all models, we run the evaluation with our code. We\ncompare our frozen features to the best publicly available SSL features in Table 4, regardless of architecture\nor pretraining data. We see the components proposed in this work lead to a very signiﬁcant improvement\n(+4.2%) over the previous state of the art (iBOT ViT-L/16 trained on ImageNet-22k) on linear evaluation.\nAt the same time, we also see that the performance increase on the alternative test sets is larger for our\nmethod, indicating stronger generalization. We describe details of our linear evaluation in Appendix B.3.\nHow far are we from weakly-supervised models? We also want to validate that our features are com-\npetitive with state-of-the-art open-source weakly supervised models. To this end, we compare on ImageNet-\n1k, using the linear evaluation, to three oﬀ-the-shelf methods with several architectural variants. For all\nmodels, we run the linear evaluation using our code, after making sure that our numbers match those re-\nported in technical reports and papers. We show the result of this evaluation in Table 4. We see that our\nbackbone, surpases the performance of OpenCLIP with a ViT-G/14 architecture ( +0.3%) and EVA-CLIP\nwith a ViT-g/14 ( +0.1%). At the same time, we also observe that our performance on the ImageNet-V2 test\n10', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}]: %s
2023-12-07 11:13:44,538 - INFO - Check the data that is being passed [{'page_content': 'kNN linear\nMethod Arch. Data Text sup. val val ReaL V2\nWeakly supervised\nCLIP ViT-L/14 WIT-400M ✓ 79.8 84.3 88.1 75.3\nCLIP ViT-L/14 336WIT-400M ✓ 80.5 85.3 88.8 75.8\nSWAG ViT-H/14 IG3.6B ✓ 82.6 85.7 88.7 77.6\nOpenCLIP ViT-H/14 LAION ✓ 81.7 84.4 88.4 75.5\nOpenCLIP ViT-G/14 LAION ✓ 83.2 86.2 89.4 77.2\nEVA-CLIP ViT-g/14 custom∗✓ 83.5 86.4 89.3 77.4\nSelf-supervised\nMAE ViT-H/14 INet-1k \x15 49.4 76.6 83.3 64.8\nDINO ViT-S/8 INet-1k \x15 78.6 79.2 85.5 68.2\nSEERv2 RG10B IG2B \x15 – 79.8 – –\nMSN ViT-L/7 INet-1k \x15 79.2 80.7 86.0 69.7\nEsViT Swin-B/W=14 INet-1k \x15 79.4 81.3 87.0 70.4\nMugs ViT-L/16 INet-1k \x15 80.2 82.1 86.9 70.8\niBOT ViT-L/16 INet-22k \x15 72.9 82.3 87.5 72.4\nDINOv2ViT-S/14 LVD-142M \x15 79.0 81.1 86.6 70.9\nViT-B/14 LVD-142M \x15 82.1 84.5 88.3 75.1\nViT-L/14 LVD-142M \x15 83.5 86.3 89.5 78.0\nViT-g/14 LVD-142M \x15 83.5 86.5 89.6 78.4\nTable4:LinearevaluationonImageNet-1koffrozenpretrainedfeatures. WereportTop-1accuracy\non the validation set for publicly available models trained on public or private data, and with or without\ntext supervision (text sup.). For reference, we also report the kNN performance on the validation set. We\ncompare across any possible architectures (Arch.), at resolution 224×224unless stated otherwise. The\ndataset used for training EVA-CLIP is a custom mixture, see paper for details (Fang et al., 2023).\nset is signiﬁcantly better ( +1.1%versus EVA-CLIP), indicating better generalization. For the remainder of\nthis section, we report OpenCLIP-G as a reference for weakly-supervised models.\nCan we ﬁnetune the encoders? We question if the ability of our models to produce high quality frozen\nfeatures impact their performance when ﬁnetuned with supervision on a speciﬁc dataset. While this is not\ncore to this paper, this experiment is indicative of whether we have involuntarily specialized our models\nto the setting of linear evaluations of frozen features. To run this sanity check, we apply the ﬁnetuning\npipeline from Touvron et al. (2022), without tweaking hyper-parameters. In Table 5, we show that the\nTop-1 accuracy on the validation set of ImageNet-1k improves by more than +2%when the backbone is\nﬁnetuned. This is true both when using models at resolution 224and448. Further gains can be obtained by\ntuning the hyper-parameters of the ﬁnetuning, but this is beyond the goal of this sanity check. Nonetheless,\nour best ﬁnetuned performance ( 88.9%) is only a couple of percent below ( −2.2%) the absolute state of the\narts ( 91.1%), obtained by Chen et al. (2023). As DINOv2 leads to features that are strong in both the linear\nand ﬁnetuning settings, a strong property of our approach is that ﬁnetuning is optional .\nRobustness analysis. To complement our study, and probe the generalization of our features, we evaluate\nour ImageNet-1k models trained with linear classiﬁcation heads on domain generalization benchmarks. We\nuse the best performing linear classiﬁer as described above and simply run inference on those benchmarks.\nPlease note that most results in the litterature are obtained with models that are ﬁnetuned end-to-end on\nImageNet-1k. We show the result of this experiment in Table 6. When comparing with state-of-the-art SSL\nmethods, our models shows drastically better robustness ( +29.6%on A, +22.1%on R and +23.0%on Sketch\n11', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Arch. Res. Linear Finetuned ∆\nViT-g/14224 86.5 88.5 +2.0\n448 86.7 88.9 +2.2\nTable 5: Supervised ﬁnetuning on ImageNet-1k. We use the pipeline of Touvron et al. (2022) to\nﬁnetune our encoders on ImageNet-1k at resolutions 224×224or448×448. We compare with the accuracy\nobtained with linear probing and observe only modest improvements with ﬁne-tuning: this suggests that\nDINOv2 features already perform well out-of-the-box.\nMethod Arch Data Im-A Im-R Im-C ↓Sketch\nOpenCLIP ViT-G/14 LAION 63.8 87.8 45.366.4\nMAE ViT-H/14 INet-1k 10.2 34.4 61.4 21.9\nDINO ViT-B/8 INet-1k 23.9 37.0 56.6 25.5\niBOT ViT-L/16 INet-22k 41.5 51.0 43.9 38.5\nDINOv2ViT-S/14 LVD-142M 33.5 53.7 54.4 41.2\nViT-B/14 LVD-142M 55.1 63.3 42.7 50.6\nViT-L/14 LVD-142M 71.3 74.4 31.5 59.3\nViT-g/14 LVD-142M 75.978.828.2 62.5\nTable 6:Domain Generalization with a linear probe on top of frozen features at a resolution of 224.\nHigher numbers are better for all benchmarks except Im-C.\ncompared to iBOT). Our model also improves upon the best weakly-supervised model on ImageNet-A while\nlagging behind on R and Sketch.\n7.2 Additional Image and Video classiﬁcation Benchmarks\nIn this section we study the generalization of our features on downstream classiﬁcation benchmarks. We\nconsider two sets of evaluations in that context. On one hand, we use large and ﬁnegrained datasets such\nas iNaturalist and Places205. On the other, we use the 12 image classiﬁcation tasks originally proposed\nin SimCLR (Chen et al., 2020). For iNaturalist 2018, iNaturalist 2021, and Places205, we train a linear\nclassiﬁer with data augmentations as in Sec. 7.1 We report top-1 accuracy for those three datasets in Table 7.\nInterestingly, our model signiﬁcantly outperforms OpenCLIP ViT-G/14 on both variants of iNaturalist\n(+8.6%and+9.7%for 2018 and 2021 respectively), and lags slightly behind on Places 205 ( −2.3%).\nIn a second set of evaluations, we measure the performance of our model on video action recognition even\nthough our features were not trained on videos.. We evaluated features on three datasets, namely UCF-\n101 (Soomro et al., 2012), Kinetics-400 (Kay et al., 2017) and Something-Something v2 (Goyal et al., 2017).\nFor this evaluation, we pick 8evenly spaced frames in the video and train a linear classiﬁer on the average\nof the features for UCF and K-400. For SSv2, we opt for concatenation to retain more temporal information\nthan with feature averaging. For each dataset, we measure average accuracy and report the results in\nTable 7. We see that amongst self-supervised approaches, our model clearly sets a new state of the art.\nMoreover, our model matches the accuracy of the OpenCLIP features on UCF and Kinetics ( +0.1%and\n+0.5%respectively) and clearly outperforms them on SSv2 ( +2.5%). This is particularly interesting, as\nSSv2 requires a much richer understanding of the video frames.\nFinally, in Table 8, we compare selected frozen features on 12 transfer classiﬁcation benchmarks initially\nproposed by Chen et al. (2020). This benchmark covers scenes, objects (food, cars, planes), and textures.\nWe replace the Birdsnap dataset with CUB because the former was not publicly available in its entirety. We\nfollow the experimental protocol as outlined by Chen et al. (2020), namely training a logistic regression on\nprecomputed features. Our model signiﬁcantly outperforms state-of-the-art SSL models, with most notable\ndiﬀerences on Stanford Cars ( +14.8%versus DINO ViT-B/8) and FGVC Aircraft ( +14.8%versus iBOT\n12', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Image classiﬁcation Video classiﬁcation\nFeature Arch iNat2018 iNat2021 Places205 K400 UCF-101 SSv2\nOpenCLIP ViT-G/14 73.0 76.0 69.8 78.3 90.7 35.8\nMAE ViT-H/14 31.0 32.3 52.4 54.2 70.6 29.2\nDINO ViT-B/8 59.6 68.3 60.4 64.5 85.0 32.6\niBOT ViT-L/16 66.3 74.6 64.4 72.6 88.6 38.7\nDINOv2ViT-S/14 69 74.2 62.9 67.8 87 33.1\nViT-B/14 76.4 81.1 66.2 73.2 89.1 34.4\nViT-L/14 80.4 85.1 67.3 76.3 90.5 35.6\nViT-g/14 81.6 85.7 67.5 78.4 91.2 38.3\nTable 7:Linear evaluation on other image and video classiﬁcation. The image benchmarks contain\na large quantity of ﬁne-grained examples about objects or scenes. The video benchmarks cover action\nclassiﬁcation and human-object interaction. All the features are frozen with a linear probe on top.\nFeature Arch Food C10 C100 SUN Cars Aircr VOC DTD Pets Cal101 Flowers CUB Avg\nOpenCLIP ViT-G/14 94.5 98.7 91.0 84.0 96.1 80.289.3 86.0 95.798.199.5 89.9 91.9\nMAE ViT-H/14 78.4 96.1 83.9 63.9 56.1 63.4 84.3 75.4 89.4 95.9 92.3 57.2 78.0\nDINO ViT-B/8 85.1 97.2 86.9 70.3 76.6 70.6 86.7 79.6 93.2 95.4 97.6 81.7 85.1\niBOT ViT-L/16 91.0 99.0 92.8 75.6 71.8 72.4 89.0 80.7 87.7 97.5 99.6 82.1 86.6\nDINOv2ViT-S/14 89.1 97.7 87.5 74.4 81.6 74.0 87.8 80.6 95.1 97.0 99.6 88.1 87.7\nViT-B/14 92.8 98.7 91.3 77.3 88.2 79.4 88.2 83.3 96.2 96.1 99.6 89.6 90.1\nViT-L/14 94.3 99.3 93.4 78.7 90.1 81.5 88.3 84.0 96.6 97.5 99.7 90.5 91.2\nViT-g/14 94.7 99.5 94.4 78.7 91.4 87.289.0 84.5 96.797.699.7 91.6 92.1\nTable 8:Linear evaluation of frozen features on ﬁne-grained benchmarks. Accuracy on 12 bench-\nmarks covering objects, scenes and textures following the evaluation protocol proposed in Chen et al. (2020).\nViT-L/16). Even though these benchmarks favor text-guided pretraining, our features are still competitive\nwith OpenCLIP on most classiﬁcation benchmarks, with the exception of a few datasets, especially SUN\n(−5.3%) and Cars (−4.7%).\n7.3 Instance Recognition\nIn this experiment, we probe our model on the task of instance-level recognition using a non-parametric\napproach. Images from a database are ranked according to their cosine similarity with a query image. We\nevaluated our model and compare to baselines on Paris and Oxford, that are landmark recognition bench-\nmarks. We also evaluated on Met, a dataset of artworks from the Metropolitan museum, and AmsterTime,\ncontaining street view images matched to archival images of Amsterdam. We measure performance by com-\nputing the mean average precision and report our results in Table 9. We see that our features signiﬁcantly\noutperform both SSL ( +41%mAP on Oxford-Hard), and weakly-supervised ( +34%mAP on Oxford-Hard)\nones. Itisinterestingtoseethatourfeaturesperformwellacrosstaskgranularities, bothatthecategory-level\nand instance-level. This is a desirable property for strong oﬀ-the-shelf computer vision features.\n7.4 Dense Recognition Tasks\nWe probe the quality of patch-level features extracted from our network on several dense downstream tasks.\nWeconsidersemanticimagesegmentationandmonoculardepthestimationinseveralsettingsandweconduct\nevaluations on multiple datasets for each.\n13', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Oxford Paris Met AmsterTime\nFeature Arch M H M H GAP GAP- ACC mAP\nOpenCLIP ViT-G/14 50.7 19.7 79.2 60.2 6.5 23.9 34.4 24.6\nMAE ViT-H/14 11.7 2.2 19.9 4.7 7.5 23.5 30.5 4.2\nDINO ViT-B/8 40.1 13.7 65.3 35.3 17.1 37.7 43.9 24.6\niBOT ViT-L/16 39.0 12.7 70.7 47.0 25.1 54.8 58.2 26.7\nDINOv2ViT-S/14 68.8 43.2 84.6 68.5 29.4 54.3 57.7 43.5\nViT-B/14 72.9 49.5 90.3 78.6 36.7 63.5 66.1 45.6\nViT-L/14 75.1 54.0 92.7 83.5 40.0 68.9 71.6 50.0\nViT-g/14 73.6 52.3 92.1 82.6 36.8 73.6 76.5 46.7\nTable 9:Evaluation of frozen features on instance-level recognition. We consider 4 diﬀerent bench-\nmarks and report their main metrics.\nADE20k CityScapes Pascal VOC\n(62.9) (86.9) (89.0)\nMethod Arch. lin. +ms lin. +ms lin. +ms\nOpenCLIP ViT-G/14 39.3 46.0 60.3 70.3 71.4 79.2\nMAE ViT-H/14 33.3 30.7 58.4 61.0 67.6 63.3\nDINO ViT-B/8 31.8 35.2 56.9 66.2 66.4 75.6\niBOT ViT-L/16 44.6 47.5 64.8 74.5 82.3 84.3\nDINOv2ViT-S/14 44.3 47.2 66.6 77.1 81.1 82.6\nViT-B/14 47.3 51.3 69.4 80.0 82.5 84.9\nViT-L/14 47.7 53.1 70.3 80.9 82.1 86.0\nViT-g/14 49.053.071.3 81.0 83.0 86.2\nTable 10: Semantic segmentation on ADE20K, CityScapes and Pascal VOC with frozen features\nand a linear classiﬁer (lin.) and with multiscale (+ms). The absolute state of the art – from Wang et al.\n(2022), Liu et al. (2021) and Chen et al. (2018) respectively – are mentioned at the top of the Table. For\nreference, using the Mask2Former pipeline (Steiner et al., 2021) with a ViT-Adapter (Chen et al., 2022) on\ntop of our frozen ViT-g/14 backbone gives 60.2 mIoU on ADE-20k.\nSemantic segmentation. For our semantic segmentation evaluation, we consider two diﬀerent setups.\nLinear: a linear layer is trained to predict class logits from a patch tokens. It is used to produce a low-\nresolution logit map (eg 32x32 for a model with patch size 16), which is then upsampled to full resolution\n(512x512) to obtain a segmentation map. This procedure is extremely simple but cannot easily produce\nhigh-resolution segmentations. +ms: a boosted version of the linear setup. We concatenate the patch\ntokens of the 4 last layers, use a larger image resolution of 640, and use multiscale test-time augmentations\nto improve the predictions. We report the performance of our model variants as well as the baselines on\nthree datasets under the two setups in Table 10.\nOur models show very good performance on all datasets and for all setups. Interestingly, our evaluation\nusing+msis on par with fully ﬁnetuning MAE with an Upernet decoder ( 53.0versus 53.6mIoU). This is\nsurprising because we use a signiﬁcantly simpler predictor. Also, our best model, when evaluated using the\nboosted recipe, almost matches the state of the art on Pascal VOC ( 86.2versus 89.0mIoU).\nIn a ﬁnal experiment, we freeze our backbone, and plug it into a ViT-Adapter Chen et al. (2022) with a\nMask2former head (Cheng et al., 2022). We tune the weights of the adapter and head, but keep the backbone\nfrozen: only a fraction of the weights are tuned, keeping the training procedure lightweight. We reach 60.2\nmIoU on ADE20k, close to the competitive state of the art, standing at 62.9 mIoU (Wang et al., 2022).\n14', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'NYUd KITTI NYUd →SUN RGB-D\n(0.330) (2.10) (0.421)\nMethod Arch. lin. 1 lin. 4 DPT lin. 1 lin. 4 DPT lin. 1 lin. 4 DPT\nOpenCLIP ViT-G/14 0.541 0.510 0.414 3.57 3.21 2.56 0.537 0.476 0.408\nMAE ViT-H/14 0.517 0.483 0.415 3.66 3.26 2.59 0.545 0.523 0.506\nDINO ViT-B/8 0.555 0.539 0.492 3.81 3.56 2.74 0.553 0.541 0.520\niBOT ViT-L/16 0.417 0.387 0.358 3.31 3.07 2.55 0.447 0.435 0.426\nDINOv2ViT-S/14 0.449 0.417 0.356 3.10 2.86 2.34 0.477 0.431 0.409\nViT-B/14 0.399 0.362 0.317 2.90 2.59 2.23 0.448 0.400 0.377\nViT-L/14 0.384 0.333 0.293 2.78 2.50 2.14 0.429 0.396 0.360\nViT-g/14 0.344 0.298 0.279 2.62 2.35 2.11 0.402 0.362 0.338\nTable 11: Depth estimation with frozen features . We report performance when training a linear\nclassiﬁer on top of one (lin. 1) or four (lin. 4) transformer layers, as well, as the DPT decoder (DPT) of\nRanftl et al. (2021). We report the RMSE metric on the 3 datasets. Lower is better. For reference, we\nreport state-of-the-art results taken from Li et al. (2022b) on each benchmark on top of the Table.\nDepth estimation. In this experiment, we evaluate our patch-level features on three monocular depth\nestimation benchmarks: NYUd, KITTI and zero-shot transfer from NYUd to SUN3d. We follow the evalu-\nation protocol of Li et al. (2022b). We consider three diﬀerent setups for this evaluation. lin. 1: we extract\nthe last layer of the frozen transformer and concatenate the [CLS]token to each patch token. Then we\nbi-linearly upsample the tokens by a factor of 4 to increase the resolution. Finally we train a simple linear\nlayer using a classiﬁcation loss by dividing the depth prediction range in 256 uniformly distributed bins and\nuse a linear normalization following Bhat et al. (2021). lin. 4: we use the same protocol that we use with\none layer, but concatenate the tokens from layers l={3,6,9,12}for ViT-S/B, l={5,12,18,24}for ViT-L,\nandl={10,20,30,40}for ViT-g. DPT: we use the DPT decoder (Ranftl et al., 2021) on top of our frozen\nmodels and setup a regression task. We scale the size of the head following the dimension of the features for\neach architecture. We show results for all baselines, all datasets and all setups in Table 11.\nFrom this table, we see that our features clearly surpass the best SSL and WSL features available. It\nis interesting to see that iBOT features extracted from a ViT-L outperform the ones of OpenCLIP with\na ViT-G. This observation supports the intuition that caption-based feature learning fails to learn subtle\npatterns like this one. Also, our model, with the DPT decoder and frozen backbone, matches or exceeds\nthe performance of the recent work of Li et al. (2022b). Finally, the out-of-domain generalization result on\nSUN-RGBd shows that our features allow very good transfer between domains. A depth prediction module\ntrained on indoor scenes from NYUd generalizes pretty well to the outdoor examples of SUN-RGBd.\n7.5 Qualitative Results\nIn this ﬁnal section of the empirical evaluation of our features, we propose a few qualitative analyses.\nSemantic Segmentation and Depth Estimation. We show some qualitative results for our dense\nprediction evaluations: segmentation on ADE20K in Fig. 7 and depth estimation on NYUd, KITTI and\nSUN RGB-D in Fig. 7. We compare DINOv2 with OpenCLIP with a linear classiﬁer on each dataset. While\nnot perfect, the linear segmentation model using our DINOv2 backbone produces good results and behaves\nmuch better than the OpenCLIP one under this evaluation setup. Indeed, the segmentation mask produced\nby OpenCLIP-G shows many artifacts and disconnected components. The qualitative results on depth\nestimation clearly illustrate the quantitative gap between OpenCLIP and DINOv2. These results highlight\nthat our features, as well as the features extracted from OpenCLIP, are able to linearly separate complex\ninformation such as depth, even though neither was trained with this type of information. However, our\nfeatures lead to a much smoother depth estimation, with less artifacts. Some objects such as the chair on\nthe SUN RGB-D image are completely ignored by OpenCLIP and correctly positioned using our features.\n15', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 7: Segmentation and depth estimation with linear classiﬁers. Examples from ADE20K,\nNYUd, SUN RGB-D and KITTI with a linear probe on frozen OpenCLIP-G and DINOv2-g features.\nFigure 8:Examples of out-of-distribution examples with frozen DINOv2-g features and a linear probe.\nOut-of-distribution generalization. We show a few examples of applying the depth prediction and\nsegmentation linear classiﬁers to out-of-distribution examples in Fig. 8. The qualitative results support our\nclaim that our features transfer between domains. The quality of the depth and segmentation predicted for\npictures of animals, or paintings is very good, even though the domains are very diﬀerent.\nPCA of patch features. We show the results of the principal component analysis (PCA) performed on\nthe patch features extracted by our model. We keep only patches with a positive value after we threshold\nthe ﬁrst component. This procedure turns out to separate the image’s main object from the background. We\ncompute a second PCA on the remaining patches across three images depicting the same category. We color\nthe three ﬁrst components with three diﬀerent colors and present the results in Fig. 1 and 9. There are two\ninteresting observations: ﬁrst, our unsupervised foreground / background detector, based on detecting the\n16', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 9:Morevisualization oftheﬁrst PCAcomponents. We compute the PCA between the patches\nfrom all of the images and show their ﬁrst 3 components. Each component corresponds to a speciﬁc color\nchannel. Same parts are matched between related images depsite changes of pose, style or even objects.\nBackground is removed by removing patches with a negative score of the ﬁrst PCA component.\nhighest variance direction, performs very well and is capable of delineating the boundary of the main object\nin the picture. Second, the other components correspond to "parts" of objects and match well for images of\nthe same category. This is an emerging property – our model was not trained to parse parts of objects.\nPatch matching. Finally, we explore what type of information our patch-level features contain by match-\ning them across images. We start by detecting the foreground object using the procedure described above.\nThen, we compute the euclidean distance between patch features extracted from two images and map them\nby solving an assignment problem. In order to reduce the number of matches, we then apply a non-maximum\nsuppression to keep only the salient ones. In Fig. 10, we show some examples of such matchings.\nWe observe that the features seem to capture information about semantic regions that serve similar purpose\nin diﬀerent objects or animals. For instance, the wing of a plane matches the wing of a bird. We also observe\nthat the model is robust to style (image versus drawing), and to large variation of poses (see the elephant).\n8 Fairness and Bias Analysis\nWe conduct two fairness evaluations of our models. We probe for geographical fairness and potential harmful\nlabel associations. For both evaluations, we experiment with our largest ViT-g model.\n8.1 Geographical Fairness\nWe evaluate geographical fairness on the Dollar Street dataset introduced in De Vries et al. (2019) using\nthe evaluation protocol of Goyal et al. (2022b). This benchmark compares performance across countries and\nincome levels. It contains 16,073 images from 289 households across 54 countries. The task is to recognize\n94 concepts that vary visually between households based on income or location. In Table 12, we compare\nour model with SEERv2 (Goyal et al., 2022a), a model trained on a geographically diverse set of images.\nOur model is slightly fairer across regions and incomes than the SEERv2 model and signiﬁcantly better than\n17', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 10: Matching across images. We match patch-level features between images from diﬀerent do-\nmains, poses and even objects that share similar semantic information. This exhibits the ability of our model\nto transfer across domains and understand relations between similar parts of diﬀerent objects.\nIncome buckets Regions\nMethod Arch. Data low medium high Africa Asia Americas Europe\nSEERv2 RG-10B IG-1B 59.7 78.5 86.6 65.9 76.3 81.1 85.6\nDINOv2 ViT-g/14 LVD-142M 67.4 83.3 90.5 74.0 81.6 86.2 89.7\nTable 12: Geographical fairness and diversity analysis across income buckets and regions.\nthe supervised baseline reported by Goyal et al. (2022a). However, we still observe a signiﬁcant diﬀerence\nbetween regions, particularly in Africa, where our model performance drops by 25.7% compared to Europe.\nThisshowthatourmodelisstillbiasedtowardWesterncountries. Similarly, ourmodelperformssigniﬁcantly\nbetter on high-income households than low-income ones, with a diﬀerence of 31.7%. Despite improvements,\nwe observe signiﬁcant biases in our models toward wealthy households from Western countries.\n8.2 Gender, Skintones and Age\nIn a second set of evaluations, we question how our model classiﬁes images of people of diﬀerent gender, skin\ntone, and age (all self-reported). We follow the protocol of Goyal et al. (2022b), where we train a multiclass\nclassiﬁer on a subset of 619 classes of ImageNet-22k. We group the 619 classes into four broader categories:\nHuman, Possibly Human, Non-Human, or Crime. Non-Human and Crime are considered harmful. Using\nthis classiﬁer, we run inference on 2955 images from the Casual Conversations dataset (Hazirbas et al., 2021)\nand keep all labels in the top-5 that are assigned a probability of 0.1 or more. Because of that, we can assign\n18', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Gender Skintone Age Groups\nModel Assoc.female\ndarkerfemale\nlightermale\ndarkermale\nlighter18-30 30-45 45-70 70+\nSEER Non-Human 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nRG-10B Crime 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nHuman 94.9 95.8 86.6 79.0 90.5 88.3 91.9 82.3\nPossibly-Human 13.6 6.7 65.0 60.2 32.8 37.2 29.4 6.5\nDINOv2 Non-Human 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nViT-g/14 Crime 0.0 0.0 0.2 0.0 0.0 0.1 0.0 0.0\nHuman 97.3 97.7 86.1 84.0 91.2 90.2 93.2 88.7\nPossibly-Human 15.8 17.2 52.2 48.1 35.3 37.3 23.0 9.7\nTable 13: Label association fairness evaluation across gender, skintones and age groups. We\nfollow the protocol proposed by Goyal et al. (2022b) with a slight modiﬁcation. Instead of ﬁnetuning the\nbackbone, we simply learn a linear classiﬁer on the subset of 619 classes of ImageNet-22k.\nModel toGPU TypeGPU PowerGPU-hours PUETotal power Carbon emitted\nReproduce consumption consumption (tCO 2eq)\nDINOv2-g A100-40GB 400W 22,016 1.1 9.7 MWh 3.7\nTable 14: Carbon footprint of reproducing DINOv2. We report the potential carbon emission of\nreproducing DINOv2-g when assuming a power consumption for the A100-40GB of 400W, a PUE of 1.1 and\ncarbon intensity factor of 0.385 kg CO 2e per KWh.\nmultiple classes to each image. We make one modiﬁcation to the original evaluation protocol: we do not\nbackpropagate gradients to the backbone and keep it frozen. We compare our model to SEERv2 in Table 13.\nOur model often classiﬁes images of all groups as Human without large deviations across skin tones. Neither\nSEERv2 nor DINOv2 predict harmful labels from the Non-Human or Crime meta-categories (except for two\ninstances where the background contains bars visually similar to prison bars). We see that our model triggers\nthe Possibly-Human classes often. This class is constructed from objects in ImageNet-22k that are often\nrelated to Humans, such as Scarf, Glasses, or Beard. Our model often predicts the Possibly-Human class\nfor men because of the prevalence of the Beard class. No clear pattern indicates a bias against a particular\ngroup in this study. While this is encouraging, we also acknowledge that a more thorough evaluation of\nbiases may reveal ﬂaws in our model.\n9 Estimating the Environmental Impact of Training our Models\nTraining foundation models consumes a signiﬁcant amount of energy, resulting in carbon dioxide emissions.\nPatterson et al. (2021) propose a methodology to report an estimation of the carbon emitted during the\ntraining of a model based on the speciﬁcs of the data center and its power grid. This computation informs\nthe design of the data center used for the training of models and the choice of location for data centers.\nThis methodology requires to know the speciﬁcs of the data center used for training, which can be complex\nwhen multiple data centers are involved over time. Additionally, these speciﬁcs are most often not in the\ncontrol of the AI practitioner, and hence, this methodology is less helpful when practioners make technical\ndecisions about future trainings. Instead, in this section, we follow an alternative that reports the potential\ncarbon emission of retraining a similar model in an average data center located in the US. This methodology\nwas used in previous work in natural language processing (Strubell et al., 2019; Touvron et al., 2023) to\nestablish an apple-to-apple comparison between pretraining schemes. More precisely, we ﬁx the value of all\nexogenous variables, i.e., the Power Usage Eﬀectiveness (PUE) and carbon intensity factor of a power grid\nto the same values as in Touvron et al. (2023), that is, a PUE of 1.1 and the carbon intensity factor to the\n19', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'US average of 0.385 kg CO 2eq/KWh. We use the same formula as in Patterson et al. (2021) to estimate the\npotential energy consumption and the carbon emission. For the power consumption of an A100-80GB, we\ntake the thermal design power for NVLink systems, which is 400W. We report the potential carbon emission\nof retraining a DINOv2 ViT-g in Table 14. For comparison, retraining an OpenCLIP ViT-L or OpenCLIP\nViT-G would require 22.4 MWh and 118.9 MWh, respectively, if run in the same data center. This is 10 ×\nmore carbon emission. Note that this comparison is not fair to them, since they also train a text encoder in\nparallel, and we thus do not report them in the table. However, it gives a reasonable guidelines for those who\nare interested in training only visual features: in this context, training a self-supervised model is preferable\nin terms of carbon emission. Training a text-guided model still makes sense when planning to reuse the text\nencoder.\nCarbon footprint of the whole project. Additionally, we estimate the footprint of the whole project\nto be between 0.5k and 1k tCO 2eq using the same grid as presented above. This carbon footprint represents\nin the order of 200k GPU-days. The primary sources of emissions are the self-supervised pre-trainings of\nthe models. For example, a single pre-training of a ViT-g model (22k GPU-hours) emits 3.7 tons of CO 2eq,\nwhile a ﬁnetuning on ImageNet-1k (1k GPU-hours) emits 0.2 tons. This estimate only considers the GPUs’\nelectricity consumption and ignores other emissions, such as their manufacturing and disposal.\n10 Future work and Discussion\nIn this work, we present DINOv2, a new series of image encoders pretrained on large curated data with no\nsupervision. This is the ﬁrst SSL work on image data that leads to visual features that close the performance\ngap with (weakly) supervised alternatives across a wide range of benchmarks and without the need for\nﬁnetuning. A few properties emerge from these models, such as an understanding of object parts and scene\ngeometry regardless of the image domains. We expect that more of these properties will emerge at larger\nscales of models and data, akin to instruction emergence in large language models, and plan to continue\nscaling along these axes. This paper also demonstrates that these visual features are compatible with\nclassiﬁers as simple as linear layers - meaning the underlying information is readily available . In future work,\nwe plan to leverage this ability to train a a language-enabled AI system that can process visual features as\nif they were word tokens, and extract the required information to ground the system.\nAcknowledgments.\nWe thank Mathilde Caron for initial discussions that led to this work. We thank Olivia Joulin for the horse\ndrawing used in Fig. 10. We also thank the rest of FAIR and Meta AI for feedback on this work through\nthe entire project.\nReferences\nYuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous clustering\nand representation learning. In ICLR, 2020.\nMahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand\nJoulin, Michael Rabbat, and Nicolas Ballas. Masked siamese networks for label-eﬃcient learning. arXiv\npreprint arXiv:2204.07141 , 2022.\nMahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann\nLeCun, and Nicolas Ballas. Self-supervised learning from images with a joint-embedding predictive archi-\ntecture.arXiv preprint arXiv:2301.08243 , 2023.\nAlexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. Data2vec: A general\nframework for self-supervised learning in speech, vision and language. arXiv preprint arXiv:2202.03555 ,\n2022.\nHangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint\narXiv:2106.08254 , 2021.\n20', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}]: %s
2023-12-07 11:13:44,539 - INFO - Check the results [{'page_content': 'kNN linear\nMethod Arch. Data Text sup. val val ReaL V2\nWeakly supervised\nCLIP ViT-L/14 WIT-400M ✓ 79.8 84.3 88.1 75.3\nCLIP ViT-L/14 336WIT-400M ✓ 80.5 85.3 88.8 75.8\nSWAG ViT-H/14 IG3.6B ✓ 82.6 85.7 88.7 77.6\nOpenCLIP ViT-H/14 LAION ✓ 81.7 84.4 88.4 75.5\nOpenCLIP ViT-G/14 LAION ✓ 83.2 86.2 89.4 77.2\nEVA-CLIP ViT-g/14 custom∗✓ 83.5 86.4 89.3 77.4\nSelf-supervised\nMAE ViT-H/14 INet-1k \x15 49.4 76.6 83.3 64.8\nDINO ViT-S/8 INet-1k \x15 78.6 79.2 85.5 68.2\nSEERv2 RG10B IG2B \x15 – 79.8 – –\nMSN ViT-L/7 INet-1k \x15 79.2 80.7 86.0 69.7\nEsViT Swin-B/W=14 INet-1k \x15 79.4 81.3 87.0 70.4\nMugs ViT-L/16 INet-1k \x15 80.2 82.1 86.9 70.8\niBOT ViT-L/16 INet-22k \x15 72.9 82.3 87.5 72.4\nDINOv2ViT-S/14 LVD-142M \x15 79.0 81.1 86.6 70.9\nViT-B/14 LVD-142M \x15 82.1 84.5 88.3 75.1\nViT-L/14 LVD-142M \x15 83.5 86.3 89.5 78.0\nViT-g/14 LVD-142M \x15 83.5 86.5 89.6 78.4\nTable4:LinearevaluationonImageNet-1koffrozenpretrainedfeatures. WereportTop-1accuracy\non the validation set for publicly available models trained on public or private data, and with or without\ntext supervision (text sup.). For reference, we also report the kNN performance on the validation set. We\ncompare across any possible architectures (Arch.), at resolution 224×224unless stated otherwise. The\ndataset used for training EVA-CLIP is a custom mixture, see paper for details (Fang et al., 2023).\nset is signiﬁcantly better ( +1.1%versus EVA-CLIP), indicating better generalization. For the remainder of\nthis section, we report OpenCLIP-G as a reference for weakly-supervised models.\nCan we ﬁnetune the encoders? We question if the ability of our models to produce high quality frozen\nfeatures impact their performance when ﬁnetuned with supervision on a speciﬁc dataset. While this is not\ncore to this paper, this experiment is indicative of whether we have involuntarily specialized our models\nto the setting of linear evaluations of frozen features. To run this sanity check, we apply the ﬁnetuning\npipeline from Touvron et al. (2022), without tweaking hyper-parameters. In Table 5, we show that the\nTop-1 accuracy on the validation set of ImageNet-1k improves by more than +2%when the backbone is\nﬁnetuned. This is true both when using models at resolution 224and448. Further gains can be obtained by\ntuning the hyper-parameters of the ﬁnetuning, but this is beyond the goal of this sanity check. Nonetheless,\nour best ﬁnetuned performance ( 88.9%) is only a couple of percent below ( −2.2%) the absolute state of the\narts ( 91.1%), obtained by Chen et al. (2023). As DINOv2 leads to features that are strong in both the linear\nand ﬁnetuning settings, a strong property of our approach is that ﬁnetuning is optional .\nRobustness analysis. To complement our study, and probe the generalization of our features, we evaluate\nour ImageNet-1k models trained with linear classiﬁcation heads on domain generalization benchmarks. We\nuse the best performing linear classiﬁer as described above and simply run inference on those benchmarks.\nPlease note that most results in the litterature are obtained with models that are ﬁnetuned end-to-end on\nImageNet-1k. We show the result of this experiment in Table 6. When comparing with state-of-the-art SSL\nmethods, our models shows drastically better robustness ( +29.6%on A, +22.1%on R and +23.0%on Sketch\n11', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Arch. Res. Linear Finetuned ∆\nViT-g/14224 86.5 88.5 +2.0\n448 86.7 88.9 +2.2\nTable 5: Supervised ﬁnetuning on ImageNet-1k. We use the pipeline of Touvron et al. (2022) to\nﬁnetune our encoders on ImageNet-1k at resolutions 224×224or448×448. We compare with the accuracy\nobtained with linear probing and observe only modest improvements with ﬁne-tuning: this suggests that\nDINOv2 features already perform well out-of-the-box.\nMethod Arch Data Im-A Im-R Im-C ↓Sketch\nOpenCLIP ViT-G/14 LAION 63.8 87.8 45.366.4\nMAE ViT-H/14 INet-1k 10.2 34.4 61.4 21.9\nDINO ViT-B/8 INet-1k 23.9 37.0 56.6 25.5\niBOT ViT-L/16 INet-22k 41.5 51.0 43.9 38.5\nDINOv2ViT-S/14 LVD-142M 33.5 53.7 54.4 41.2\nViT-B/14 LVD-142M 55.1 63.3 42.7 50.6\nViT-L/14 LVD-142M 71.3 74.4 31.5 59.3\nViT-g/14 LVD-142M 75.978.828.2 62.5\nTable 6:Domain Generalization with a linear probe on top of frozen features at a resolution of 224.\nHigher numbers are better for all benchmarks except Im-C.\ncompared to iBOT). Our model also improves upon the best weakly-supervised model on ImageNet-A while\nlagging behind on R and Sketch.\n7.2 Additional Image and Video classiﬁcation Benchmarks\nIn this section we study the generalization of our features on downstream classiﬁcation benchmarks. We\nconsider two sets of evaluations in that context. On one hand, we use large and ﬁnegrained datasets such\nas iNaturalist and Places205. On the other, we use the 12 image classiﬁcation tasks originally proposed\nin SimCLR (Chen et al., 2020). For iNaturalist 2018, iNaturalist 2021, and Places205, we train a linear\nclassiﬁer with data augmentations as in Sec. 7.1 We report top-1 accuracy for those three datasets in Table 7.\nInterestingly, our model signiﬁcantly outperforms OpenCLIP ViT-G/14 on both variants of iNaturalist\n(+8.6%and+9.7%for 2018 and 2021 respectively), and lags slightly behind on Places 205 ( −2.3%).\nIn a second set of evaluations, we measure the performance of our model on video action recognition even\nthough our features were not trained on videos.. We evaluated features on three datasets, namely UCF-\n101 (Soomro et al., 2012), Kinetics-400 (Kay et al., 2017) and Something-Something v2 (Goyal et al., 2017).\nFor this evaluation, we pick 8evenly spaced frames in the video and train a linear classiﬁer on the average\nof the features for UCF and K-400. For SSv2, we opt for concatenation to retain more temporal information\nthan with feature averaging. For each dataset, we measure average accuracy and report the results in\nTable 7. We see that amongst self-supervised approaches, our model clearly sets a new state of the art.\nMoreover, our model matches the accuracy of the OpenCLIP features on UCF and Kinetics ( +0.1%and\n+0.5%respectively) and clearly outperforms them on SSv2 ( +2.5%). This is particularly interesting, as\nSSv2 requires a much richer understanding of the video frames.\nFinally, in Table 8, we compare selected frozen features on 12 transfer classiﬁcation benchmarks initially\nproposed by Chen et al. (2020). This benchmark covers scenes, objects (food, cars, planes), and textures.\nWe replace the Birdsnap dataset with CUB because the former was not publicly available in its entirety. We\nfollow the experimental protocol as outlined by Chen et al. (2020), namely training a logistic regression on\nprecomputed features. Our model signiﬁcantly outperforms state-of-the-art SSL models, with most notable\ndiﬀerences on Stanford Cars ( +14.8%versus DINO ViT-B/8) and FGVC Aircraft ( +14.8%versus iBOT\n12', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Image classiﬁcation Video classiﬁcation\nFeature Arch iNat2018 iNat2021 Places205 K400 UCF-101 SSv2\nOpenCLIP ViT-G/14 73.0 76.0 69.8 78.3 90.7 35.8\nMAE ViT-H/14 31.0 32.3 52.4 54.2 70.6 29.2\nDINO ViT-B/8 59.6 68.3 60.4 64.5 85.0 32.6\niBOT ViT-L/16 66.3 74.6 64.4 72.6 88.6 38.7\nDINOv2ViT-S/14 69 74.2 62.9 67.8 87 33.1\nViT-B/14 76.4 81.1 66.2 73.2 89.1 34.4\nViT-L/14 80.4 85.1 67.3 76.3 90.5 35.6\nViT-g/14 81.6 85.7 67.5 78.4 91.2 38.3\nTable 7:Linear evaluation on other image and video classiﬁcation. The image benchmarks contain\na large quantity of ﬁne-grained examples about objects or scenes. The video benchmarks cover action\nclassiﬁcation and human-object interaction. All the features are frozen with a linear probe on top.\nFeature Arch Food C10 C100 SUN Cars Aircr VOC DTD Pets Cal101 Flowers CUB Avg\nOpenCLIP ViT-G/14 94.5 98.7 91.0 84.0 96.1 80.289.3 86.0 95.798.199.5 89.9 91.9\nMAE ViT-H/14 78.4 96.1 83.9 63.9 56.1 63.4 84.3 75.4 89.4 95.9 92.3 57.2 78.0\nDINO ViT-B/8 85.1 97.2 86.9 70.3 76.6 70.6 86.7 79.6 93.2 95.4 97.6 81.7 85.1\niBOT ViT-L/16 91.0 99.0 92.8 75.6 71.8 72.4 89.0 80.7 87.7 97.5 99.6 82.1 86.6\nDINOv2ViT-S/14 89.1 97.7 87.5 74.4 81.6 74.0 87.8 80.6 95.1 97.0 99.6 88.1 87.7\nViT-B/14 92.8 98.7 91.3 77.3 88.2 79.4 88.2 83.3 96.2 96.1 99.6 89.6 90.1\nViT-L/14 94.3 99.3 93.4 78.7 90.1 81.5 88.3 84.0 96.6 97.5 99.7 90.5 91.2\nViT-g/14 94.7 99.5 94.4 78.7 91.4 87.289.0 84.5 96.797.699.7 91.6 92.1\nTable 8:Linear evaluation of frozen features on ﬁne-grained benchmarks. Accuracy on 12 bench-\nmarks covering objects, scenes and textures following the evaluation protocol proposed in Chen et al. (2020).\nViT-L/16). Even though these benchmarks favor text-guided pretraining, our features are still competitive\nwith OpenCLIP on most classiﬁcation benchmarks, with the exception of a few datasets, especially SUN\n(−5.3%) and Cars (−4.7%).\n7.3 Instance Recognition\nIn this experiment, we probe our model on the task of instance-level recognition using a non-parametric\napproach. Images from a database are ranked according to their cosine similarity with a query image. We\nevaluated our model and compare to baselines on Paris and Oxford, that are landmark recognition bench-\nmarks. We also evaluated on Met, a dataset of artworks from the Metropolitan museum, and AmsterTime,\ncontaining street view images matched to archival images of Amsterdam. We measure performance by com-\nputing the mean average precision and report our results in Table 9. We see that our features signiﬁcantly\noutperform both SSL ( +41%mAP on Oxford-Hard), and weakly-supervised ( +34%mAP on Oxford-Hard)\nones. Itisinterestingtoseethatourfeaturesperformwellacrosstaskgranularities, bothatthecategory-level\nand instance-level. This is a desirable property for strong oﬀ-the-shelf computer vision features.\n7.4 Dense Recognition Tasks\nWe probe the quality of patch-level features extracted from our network on several dense downstream tasks.\nWeconsidersemanticimagesegmentationandmonoculardepthestimationinseveralsettingsandweconduct\nevaluations on multiple datasets for each.\n13', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Oxford Paris Met AmsterTime\nFeature Arch M H M H GAP GAP- ACC mAP\nOpenCLIP ViT-G/14 50.7 19.7 79.2 60.2 6.5 23.9 34.4 24.6\nMAE ViT-H/14 11.7 2.2 19.9 4.7 7.5 23.5 30.5 4.2\nDINO ViT-B/8 40.1 13.7 65.3 35.3 17.1 37.7 43.9 24.6\niBOT ViT-L/16 39.0 12.7 70.7 47.0 25.1 54.8 58.2 26.7\nDINOv2ViT-S/14 68.8 43.2 84.6 68.5 29.4 54.3 57.7 43.5\nViT-B/14 72.9 49.5 90.3 78.6 36.7 63.5 66.1 45.6\nViT-L/14 75.1 54.0 92.7 83.5 40.0 68.9 71.6 50.0\nViT-g/14 73.6 52.3 92.1 82.6 36.8 73.6 76.5 46.7\nTable 9:Evaluation of frozen features on instance-level recognition. We consider 4 diﬀerent bench-\nmarks and report their main metrics.\nADE20k CityScapes Pascal VOC\n(62.9) (86.9) (89.0)\nMethod Arch. lin. +ms lin. +ms lin. +ms\nOpenCLIP ViT-G/14 39.3 46.0 60.3 70.3 71.4 79.2\nMAE ViT-H/14 33.3 30.7 58.4 61.0 67.6 63.3\nDINO ViT-B/8 31.8 35.2 56.9 66.2 66.4 75.6\niBOT ViT-L/16 44.6 47.5 64.8 74.5 82.3 84.3\nDINOv2ViT-S/14 44.3 47.2 66.6 77.1 81.1 82.6\nViT-B/14 47.3 51.3 69.4 80.0 82.5 84.9\nViT-L/14 47.7 53.1 70.3 80.9 82.1 86.0\nViT-g/14 49.053.071.3 81.0 83.0 86.2\nTable 10: Semantic segmentation on ADE20K, CityScapes and Pascal VOC with frozen features\nand a linear classiﬁer (lin.) and with multiscale (+ms). The absolute state of the art – from Wang et al.\n(2022), Liu et al. (2021) and Chen et al. (2018) respectively – are mentioned at the top of the Table. For\nreference, using the Mask2Former pipeline (Steiner et al., 2021) with a ViT-Adapter (Chen et al., 2022) on\ntop of our frozen ViT-g/14 backbone gives 60.2 mIoU on ADE-20k.\nSemantic segmentation. For our semantic segmentation evaluation, we consider two diﬀerent setups.\nLinear: a linear layer is trained to predict class logits from a patch tokens. It is used to produce a low-\nresolution logit map (eg 32x32 for a model with patch size 16), which is then upsampled to full resolution\n(512x512) to obtain a segmentation map. This procedure is extremely simple but cannot easily produce\nhigh-resolution segmentations. +ms: a boosted version of the linear setup. We concatenate the patch\ntokens of the 4 last layers, use a larger image resolution of 640, and use multiscale test-time augmentations\nto improve the predictions. We report the performance of our model variants as well as the baselines on\nthree datasets under the two setups in Table 10.\nOur models show very good performance on all datasets and for all setups. Interestingly, our evaluation\nusing+msis on par with fully ﬁnetuning MAE with an Upernet decoder ( 53.0versus 53.6mIoU). This is\nsurprising because we use a signiﬁcantly simpler predictor. Also, our best model, when evaluated using the\nboosted recipe, almost matches the state of the art on Pascal VOC ( 86.2versus 89.0mIoU).\nIn a ﬁnal experiment, we freeze our backbone, and plug it into a ViT-Adapter Chen et al. (2022) with a\nMask2former head (Cheng et al., 2022). We tune the weights of the adapter and head, but keep the backbone\nfrozen: only a fraction of the weights are tuned, keeping the training procedure lightweight. We reach 60.2\nmIoU on ADE20k, close to the competitive state of the art, standing at 62.9 mIoU (Wang et al., 2022).\n14', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'NYUd KITTI NYUd →SUN RGB-D\n(0.330) (2.10) (0.421)\nMethod Arch. lin. 1 lin. 4 DPT lin. 1 lin. 4 DPT lin. 1 lin. 4 DPT\nOpenCLIP ViT-G/14 0.541 0.510 0.414 3.57 3.21 2.56 0.537 0.476 0.408\nMAE ViT-H/14 0.517 0.483 0.415 3.66 3.26 2.59 0.545 0.523 0.506\nDINO ViT-B/8 0.555 0.539 0.492 3.81 3.56 2.74 0.553 0.541 0.520\niBOT ViT-L/16 0.417 0.387 0.358 3.31 3.07 2.55 0.447 0.435 0.426\nDINOv2ViT-S/14 0.449 0.417 0.356 3.10 2.86 2.34 0.477 0.431 0.409\nViT-B/14 0.399 0.362 0.317 2.90 2.59 2.23 0.448 0.400 0.377\nViT-L/14 0.384 0.333 0.293 2.78 2.50 2.14 0.429 0.396 0.360\nViT-g/14 0.344 0.298 0.279 2.62 2.35 2.11 0.402 0.362 0.338\nTable 11: Depth estimation with frozen features . We report performance when training a linear\nclassiﬁer on top of one (lin. 1) or four (lin. 4) transformer layers, as well, as the DPT decoder (DPT) of\nRanftl et al. (2021). We report the RMSE metric on the 3 datasets. Lower is better. For reference, we\nreport state-of-the-art results taken from Li et al. (2022b) on each benchmark on top of the Table.\nDepth estimation. In this experiment, we evaluate our patch-level features on three monocular depth\nestimation benchmarks: NYUd, KITTI and zero-shot transfer from NYUd to SUN3d. We follow the evalu-\nation protocol of Li et al. (2022b). We consider three diﬀerent setups for this evaluation. lin. 1: we extract\nthe last layer of the frozen transformer and concatenate the [CLS]token to each patch token. Then we\nbi-linearly upsample the tokens by a factor of 4 to increase the resolution. Finally we train a simple linear\nlayer using a classiﬁcation loss by dividing the depth prediction range in 256 uniformly distributed bins and\nuse a linear normalization following Bhat et al. (2021). lin. 4: we use the same protocol that we use with\none layer, but concatenate the tokens from layers l={3,6,9,12}for ViT-S/B, l={5,12,18,24}for ViT-L,\nandl={10,20,30,40}for ViT-g. DPT: we use the DPT decoder (Ranftl et al., 2021) on top of our frozen\nmodels and setup a regression task. We scale the size of the head following the dimension of the features for\neach architecture. We show results for all baselines, all datasets and all setups in Table 11.\nFrom this table, we see that our features clearly surpass the best SSL and WSL features available. It\nis interesting to see that iBOT features extracted from a ViT-L outperform the ones of OpenCLIP with\na ViT-G. This observation supports the intuition that caption-based feature learning fails to learn subtle\npatterns like this one. Also, our model, with the DPT decoder and frozen backbone, matches or exceeds\nthe performance of the recent work of Li et al. (2022b). Finally, the out-of-domain generalization result on\nSUN-RGBd shows that our features allow very good transfer between domains. A depth prediction module\ntrained on indoor scenes from NYUd generalizes pretty well to the outdoor examples of SUN-RGBd.\n7.5 Qualitative Results\nIn this ﬁnal section of the empirical evaluation of our features, we propose a few qualitative analyses.\nSemantic Segmentation and Depth Estimation. We show some qualitative results for our dense\nprediction evaluations: segmentation on ADE20K in Fig. 7 and depth estimation on NYUd, KITTI and\nSUN RGB-D in Fig. 7. We compare DINOv2 with OpenCLIP with a linear classiﬁer on each dataset. While\nnot perfect, the linear segmentation model using our DINOv2 backbone produces good results and behaves\nmuch better than the OpenCLIP one under this evaluation setup. Indeed, the segmentation mask produced\nby OpenCLIP-G shows many artifacts and disconnected components. The qualitative results on depth\nestimation clearly illustrate the quantitative gap between OpenCLIP and DINOv2. These results highlight\nthat our features, as well as the features extracted from OpenCLIP, are able to linearly separate complex\ninformation such as depth, even though neither was trained with this type of information. However, our\nfeatures lead to a much smoother depth estimation, with less artifacts. Some objects such as the chair on\nthe SUN RGB-D image are completely ignored by OpenCLIP and correctly positioned using our features.\n15', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 7: Segmentation and depth estimation with linear classiﬁers. Examples from ADE20K,\nNYUd, SUN RGB-D and KITTI with a linear probe on frozen OpenCLIP-G and DINOv2-g features.\nFigure 8:Examples of out-of-distribution examples with frozen DINOv2-g features and a linear probe.\nOut-of-distribution generalization. We show a few examples of applying the depth prediction and\nsegmentation linear classiﬁers to out-of-distribution examples in Fig. 8. The qualitative results support our\nclaim that our features transfer between domains. The quality of the depth and segmentation predicted for\npictures of animals, or paintings is very good, even though the domains are very diﬀerent.\nPCA of patch features. We show the results of the principal component analysis (PCA) performed on\nthe patch features extracted by our model. We keep only patches with a positive value after we threshold\nthe ﬁrst component. This procedure turns out to separate the image’s main object from the background. We\ncompute a second PCA on the remaining patches across three images depicting the same category. We color\nthe three ﬁrst components with three diﬀerent colors and present the results in Fig. 1 and 9. There are two\ninteresting observations: ﬁrst, our unsupervised foreground / background detector, based on detecting the\n16', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 9:Morevisualization oftheﬁrst PCAcomponents. We compute the PCA between the patches\nfrom all of the images and show their ﬁrst 3 components. Each component corresponds to a speciﬁc color\nchannel. Same parts are matched between related images depsite changes of pose, style or even objects.\nBackground is removed by removing patches with a negative score of the ﬁrst PCA component.\nhighest variance direction, performs very well and is capable of delineating the boundary of the main object\nin the picture. Second, the other components correspond to "parts" of objects and match well for images of\nthe same category. This is an emerging property – our model was not trained to parse parts of objects.\nPatch matching. Finally, we explore what type of information our patch-level features contain by match-\ning them across images. We start by detecting the foreground object using the procedure described above.\nThen, we compute the euclidean distance between patch features extracted from two images and map them\nby solving an assignment problem. In order to reduce the number of matches, we then apply a non-maximum\nsuppression to keep only the salient ones. In Fig. 10, we show some examples of such matchings.\nWe observe that the features seem to capture information about semantic regions that serve similar purpose\nin diﬀerent objects or animals. For instance, the wing of a plane matches the wing of a bird. We also observe\nthat the model is robust to style (image versus drawing), and to large variation of poses (see the elephant).\n8 Fairness and Bias Analysis\nWe conduct two fairness evaluations of our models. We probe for geographical fairness and potential harmful\nlabel associations. For both evaluations, we experiment with our largest ViT-g model.\n8.1 Geographical Fairness\nWe evaluate geographical fairness on the Dollar Street dataset introduced in De Vries et al. (2019) using\nthe evaluation protocol of Goyal et al. (2022b). This benchmark compares performance across countries and\nincome levels. It contains 16,073 images from 289 households across 54 countries. The task is to recognize\n94 concepts that vary visually between households based on income or location. In Table 12, we compare\nour model with SEERv2 (Goyal et al., 2022a), a model trained on a geographically diverse set of images.\nOur model is slightly fairer across regions and incomes than the SEERv2 model and signiﬁcantly better than\n17', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Figure 10: Matching across images. We match patch-level features between images from diﬀerent do-\nmains, poses and even objects that share similar semantic information. This exhibits the ability of our model\nto transfer across domains and understand relations between similar parts of diﬀerent objects.\nIncome buckets Regions\nMethod Arch. Data low medium high Africa Asia Americas Europe\nSEERv2 RG-10B IG-1B 59.7 78.5 86.6 65.9 76.3 81.1 85.6\nDINOv2 ViT-g/14 LVD-142M 67.4 83.3 90.5 74.0 81.6 86.2 89.7\nTable 12: Geographical fairness and diversity analysis across income buckets and regions.\nthe supervised baseline reported by Goyal et al. (2022a). However, we still observe a signiﬁcant diﬀerence\nbetween regions, particularly in Africa, where our model performance drops by 25.7% compared to Europe.\nThisshowthatourmodelisstillbiasedtowardWesterncountries. Similarly, ourmodelperformssigniﬁcantly\nbetter on high-income households than low-income ones, with a diﬀerence of 31.7%. Despite improvements,\nwe observe signiﬁcant biases in our models toward wealthy households from Western countries.\n8.2 Gender, Skintones and Age\nIn a second set of evaluations, we question how our model classiﬁes images of people of diﬀerent gender, skin\ntone, and age (all self-reported). We follow the protocol of Goyal et al. (2022b), where we train a multiclass\nclassiﬁer on a subset of 619 classes of ImageNet-22k. We group the 619 classes into four broader categories:\nHuman, Possibly Human, Non-Human, or Crime. Non-Human and Crime are considered harmful. Using\nthis classiﬁer, we run inference on 2955 images from the Casual Conversations dataset (Hazirbas et al., 2021)\nand keep all labels in the top-5 that are assigned a probability of 0.1 or more. Because of that, we can assign\n18', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Gender Skintone Age Groups\nModel Assoc.female\ndarkerfemale\nlightermale\ndarkermale\nlighter18-30 30-45 45-70 70+\nSEER Non-Human 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nRG-10B Crime 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nHuman 94.9 95.8 86.6 79.0 90.5 88.3 91.9 82.3\nPossibly-Human 13.6 6.7 65.0 60.2 32.8 37.2 29.4 6.5\nDINOv2 Non-Human 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nViT-g/14 Crime 0.0 0.0 0.2 0.0 0.0 0.1 0.0 0.0\nHuman 97.3 97.7 86.1 84.0 91.2 90.2 93.2 88.7\nPossibly-Human 15.8 17.2 52.2 48.1 35.3 37.3 23.0 9.7\nTable 13: Label association fairness evaluation across gender, skintones and age groups. We\nfollow the protocol proposed by Goyal et al. (2022b) with a slight modiﬁcation. Instead of ﬁnetuning the\nbackbone, we simply learn a linear classiﬁer on the subset of 619 classes of ImageNet-22k.\nModel toGPU TypeGPU PowerGPU-hours PUETotal power Carbon emitted\nReproduce consumption consumption (tCO 2eq)\nDINOv2-g A100-40GB 400W 22,016 1.1 9.7 MWh 3.7\nTable 14: Carbon footprint of reproducing DINOv2. We report the potential carbon emission of\nreproducing DINOv2-g when assuming a power consumption for the A100-40GB of 400W, a PUE of 1.1 and\ncarbon intensity factor of 0.385 kg CO 2e per KWh.\nmultiple classes to each image. We make one modiﬁcation to the original evaluation protocol: we do not\nbackpropagate gradients to the backbone and keep it frozen. We compare our model to SEERv2 in Table 13.\nOur model often classiﬁes images of all groups as Human without large deviations across skin tones. Neither\nSEERv2 nor DINOv2 predict harmful labels from the Non-Human or Crime meta-categories (except for two\ninstances where the background contains bars visually similar to prison bars). We see that our model triggers\nthe Possibly-Human classes often. This class is constructed from objects in ImageNet-22k that are often\nrelated to Humans, such as Scarf, Glasses, or Beard. Our model often predicts the Possibly-Human class\nfor men because of the prevalence of the Beard class. No clear pattern indicates a bias against a particular\ngroup in this study. While this is encouraging, we also acknowledge that a more thorough evaluation of\nbiases may reveal ﬂaws in our model.\n9 Estimating the Environmental Impact of Training our Models\nTraining foundation models consumes a signiﬁcant amount of energy, resulting in carbon dioxide emissions.\nPatterson et al. (2021) propose a methodology to report an estimation of the carbon emitted during the\ntraining of a model based on the speciﬁcs of the data center and its power grid. This computation informs\nthe design of the data center used for the training of models and the choice of location for data centers.\nThis methodology requires to know the speciﬁcs of the data center used for training, which can be complex\nwhen multiple data centers are involved over time. Additionally, these speciﬁcs are most often not in the\ncontrol of the AI practitioner, and hence, this methodology is less helpful when practioners make technical\ndecisions about future trainings. Instead, in this section, we follow an alternative that reports the potential\ncarbon emission of retraining a similar model in an average data center located in the US. This methodology\nwas used in previous work in natural language processing (Strubell et al., 2019; Touvron et al., 2023) to\nestablish an apple-to-apple comparison between pretraining schemes. More precisely, we ﬁx the value of all\nexogenous variables, i.e., the Power Usage Eﬀectiveness (PUE) and carbon intensity factor of a power grid\nto the same values as in Touvron et al. (2023), that is, a PUE of 1.1 and the carbon intensity factor to the\n19', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'US average of 0.385 kg CO 2eq/KWh. We use the same formula as in Patterson et al. (2021) to estimate the\npotential energy consumption and the carbon emission. For the power consumption of an A100-80GB, we\ntake the thermal design power for NVLink systems, which is 400W. We report the potential carbon emission\nof retraining a DINOv2 ViT-g in Table 14. For comparison, retraining an OpenCLIP ViT-L or OpenCLIP\nViT-G would require 22.4 MWh and 118.9 MWh, respectively, if run in the same data center. This is 10 ×\nmore carbon emission. Note that this comparison is not fair to them, since they also train a text encoder in\nparallel, and we thus do not report them in the table. However, it gives a reasonable guidelines for those who\nare interested in training only visual features: in this context, training a self-supervised model is preferable\nin terms of carbon emission. Training a text-guided model still makes sense when planning to reuse the text\nencoder.\nCarbon footprint of the whole project. Additionally, we estimate the footprint of the whole project\nto be between 0.5k and 1k tCO 2eq using the same grid as presented above. This carbon footprint represents\nin the order of 200k GPU-days. The primary sources of emissions are the self-supervised pre-trainings of\nthe models. For example, a single pre-training of a ViT-g model (22k GPU-hours) emits 3.7 tons of CO 2eq,\nwhile a ﬁnetuning on ImageNet-1k (1k GPU-hours) emits 0.2 tons. This estimate only considers the GPUs’\nelectricity consumption and ignores other emissions, such as their manufacturing and disposal.\n10 Future work and Discussion\nIn this work, we present DINOv2, a new series of image encoders pretrained on large curated data with no\nsupervision. This is the ﬁrst SSL work on image data that leads to visual features that close the performance\ngap with (weakly) supervised alternatives across a wide range of benchmarks and without the need for\nﬁnetuning. A few properties emerge from these models, such as an understanding of object parts and scene\ngeometry regardless of the image domains. We expect that more of these properties will emerge at larger\nscales of models and data, akin to instruction emergence in large language models, and plan to continue\nscaling along these axes. This paper also demonstrates that these visual features are compatible with\nclassiﬁers as simple as linear layers - meaning the underlying information is readily available . In future work,\nwe plan to leverage this ability to train a a language-enabled AI system that can process visual features as\nif they were word tokens, and extract the required information to ground the system.\nAcknowledgments.\nWe thank Mathilde Caron for initial discussions that led to this work. We thank Olivia Joulin for the horse\ndrawing used in Fig. 10. We also thank the rest of FAIR and Meta AI for feedback on this work through\nthe entire project.\nReferences\nYuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous clustering\nand representation learning. In ICLR, 2020.\nMahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand\nJoulin, Michael Rabbat, and Nicolas Ballas. Masked siamese networks for label-eﬃcient learning. arXiv\npreprint arXiv:2204.07141 , 2022.\nMahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann\nLeCun, and Nicolas Ballas. Self-supervised learning from images with a joint-embedding predictive archi-\ntecture.arXiv preprint arXiv:2301.08243 , 2023.\nAlexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. Data2vec: A general\nframework for self-supervised learning in speech, vision and language. arXiv preprint arXiv:2202.03555 ,\n2022.\nHangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint\narXiv:2106.08254 , 2021.\n20', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}]: %s
2023-12-07 11:13:45,844 - INFO - Check the data that is being passed [{'page_content': 'Jan Beirlant, Edward J Dudewicz, László Györﬁ, Edward C Van der Meulen, et al. Nonparametric entropy\nestimation: An overview. International Journal of Mathematical and Statistical Sciences , 6(1):17–39, 1997.\nMaxim Berman, Hervé Jégou, Vedaldi Andrea, Iasonas Kokkinos, and Matthijs Douze. MultiGrain: a uniﬁed\nimage embedding for classes and instances. arXiv preprint arXiv:1902.05509 , 2019.\nLucas Beyer, Olivier J Hénaﬀ, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord. Are we done\nwith imagenet? arXiv preprint arXiv:2006.07159 , 2020.\nShariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. AdaBins: Depth estimation using adaptive\nbins. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . IEEE, jun\n2021. doi: 10.1109/cvpr46437.2021.00400. URL https://doi.org/10.1109%2Fcvpr46437.2021.00400 .\nPiotr Bojanowski and Armand Joulin. Unsupervised learning by predicting noise. In ICML, 2017.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of\nfoundation models. arXiv preprint arXiv:2108.07258 , 2021.\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative components\nwith random forests. In European Conference on Computer Vision , 2014.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\npreprint arXiv:2005.14165 , 2020.\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised\nlearning of visual features. In ECCV, 2018.\nMathilde Caron, Piotr Bojanowski, Julien Mairal, and Armand Joulin. Unsupervised pre-training of image\nfeatures on non-curated data. In ICCV, 2019.\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsuper-\nvised learning of visual features by contrasting cluster assignments. In NeurIPS , 2020.\nMathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand\nJoulin. Emerging properties in self-supervised vision transformers. arXiv preprint arXiv:2104.14294 , 2021.\nLiang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroﬀ, and Hartwig Adam. Encoder-decoder\nwith atrous separable convolution for semantic image segmentation. In Proceedings of the European con-\nference on computer vision (ECCV) , pp. 801–818, 2018.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoﬀrey Hinton. A simple framework for contrastive\nlearning of visual representations. preprint arXiv:2002.05709 , 2020.\nXiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi\nDong, Thang Luong, Cho-Jui Hsieh, et al. Symbolic discovery of optimization algorithms. arXiv preprint\narXiv:2302.06675 , 2023.\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. preprint arXiv:2011.10566 ,\n2020.\nXinleiChen, SainingXie, andKaimingHe. Anempiricalstudyoftrainingself-supervisedvisiontransformers.\narXiv preprint arXiv:2104.02057 , 2021.\nZhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer\nadapter for dense predictions. arXiv preprint arXiv:2205.08534 , 2022.\nBowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention\nmask transformer for universal image segmentation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pp. 1290–1299, 2022.\n21', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language\nmodeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.\nM. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In\nProceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , 2014.\nMarius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,\nUwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understand-\ning. InProceedings of the IEEE conference on computer vision and pattern recognition , pp. 3213–3223,\n2016.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-\neﬃcient exact attention with io-awareness. arXiv preprint arXiv:2205.14135 , 2022.\nTerrance De Vries, Ishan Misra, Changhan Wang, and Laurens Van der Maaten. Does object recognition\nworkforeveryone? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition\nworkshops , pp. 52–59, 2019.\nSylvain Delattre and Nicolas Fournier. On the kozachenko–leonenko entropy estimator. Journal of Statistical\nPlanning and Inference , 185:69–93, 2017.\nJacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert: Pre-trainingofdeepbidirectional\ntransformers for language understanding. preprint arXiv:1810.04805 , 2018.\nJosip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander Kolesnikov,\nJoan Puigcerver, Matthias Minderer, Alexander D’Amour, Dan Moldovan, et al. On robustness and\ntransferabilityofconvolutionalneuralnetworks. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pp. 16458–16468, 2021.\nCarl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context\nprediction. In ICCV, 2015.\nAlexey Dosovitskiy, Jost Tobias Springenberg, Martin A. Riedmiller, and Thomas Brox. Discriminative\nunsupervised feature learning with convolutional neural networks. CoRR, abs/1406.6909, 2014. URL\nhttp://arxiv.org/abs/1406.6909 .\nAlexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Dis-\ncriminative unsupervised feature learning with exemplar convolutional neural networks. TPAMI, 2016.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-\nterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. preprint arXiv:2010.11929 , 2020.\nMatthijs Douze, Hervé Jégou, Harsimrat Sandhawalia, Laurent Amsaleg, and Cordelia Schmid. Evaluation\nof gist descriptors for web-scale image search. In CIVR, 2009.\nQuentin Duval, Ishan Misra, and Nicolas Ballas. A simple recipe for competitive low-compute self supervised\nvision models. arXiv preprint arXiv:2301.09451 , 2023.\nAlaaeldin El-Nouby, Gautier Izacard, Hugo Touvron, Ivan Laptev, Hervé Jegou, and Edouard Grave. Are\nlarge-scale datasets necessary for self-supervised pre-training? arXiv preprint arXiv:2112.10740 , 2021.\nM. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal\nvisual object classes challenge: A retrospective. International Journal of Computer Vision , 111(1):98–136,\nJanuary 2015.\nYuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang,\nand Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. 2023.\n22', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples:\nAn incremental bayesian approach tested on 101 object categories. In 2004 conference on computer vision\nand pattern recognition workshop , pp. 178–178. IEEE, 2004.\nAndreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti\ndataset. The International Journal of Robotics Research , 32(11):1231–1237, 2013.\nSpyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting\nimage rotations, 2018.\nRohit Girdhar, Alaaeldin El-Nouby, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Is-\nhan Misra. Omnimae: Single model masked pretraining on images and videos. arXiv preprint\narXiv:2206.08356 , 2022.\nPriya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. Scaling and benchmarking self-supervised\nvisual representation learning. In ICCV, 2019.\nPriya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat Singh,\nVitaliy Liptchinsky, Ishan Misra, Armand Joulin, et al. Self-supervised pretraining of visual features in\nthe wild. preprint arXiv:2103.01988 , 2021.\nPriya Goyal, Quentin Duval, Isaac Seessel, Mathilde Caron, Mannat Singh, Ishan Misra, Levent Sagun,\nArmand Joulin, and Piotr Bojanowski. Vision models are more robust and fair when pretrained on\nuncurated images without supervision. arXiv preprint arXiv:2202.08360 , 2022a.\nPriya Goyal, Adriana Romero Soriano, Caner Hazirbas, Levent Sagun, and Nicolas Usunier. Fairness in-\ndicators for systematic assessments of visual feature extractors. In 2022 ACM Conference on Fairness,\nAccountability, and Transparency , pp. 70–88, 2022b.\nRaghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, He-\nuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The" something\nsomething" video database for learning and evaluating visual common sense. In Proceedings of the IEEE\ninternational conference on computer vision , pp. 5842–5850, 2017.\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya,\nCarl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray\nKavukcuoglu, Rémi Munos, and Michal Valko. Bootstrap your own latent: A new approach to self-\nsupervised learning. In NeurIPS , 2020.\nRaia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping.\nInCVPR, 2006.\nCanerHazirbas, JoannaBitton, BrianDolhansky, JacquelinePan, AlbertGordo, andCristianCantonFerrer.\nTowards measuring fairness in ai: the casual conversations dataset. IEEE Transactions on Biometrics,\nBehavior, and Identity Science , 4(3):324–332, 2021.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised\nvisual representation learning. In CVPR, 2020.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders\nare scalable vision learners. arXiv preprint arXiv:2111.06377 , 2021.\nOlivierJHénaﬀ, AravindSrinivas, JeﬀreyDeFauw, AliRazavi, CarlDoersch, SMEslami, andAaronvanden\nOord. Data-eﬃcientimagerecognitionwithcontrastivepredictivecoding. preprint arXiv:1905.09272 ,2019.\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions\nand perturbations. In International Conference on Learning Representations , 2019.\n23', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai,\nTyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-\nof-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pp. 8340–8349, 2021.\nGeoﬀrey Hinton, Oriol Vinyals, and Jeﬀ Dean. Distilling the knowledge in a neural network. preprint\narXiv:1503.02531 , 2015.\nJordan Hoﬀmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal\nlarge language models. arXiv preprint arXiv:2203.15556 , 2022.\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochas-\ntic depth. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands,\nOctober 11–14, 2016, Proceedings, Part IV 14 , pp. 646–661. Springer, 2016.\nGabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal\nDave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig\nSchmidt. Openclip. 2021.\nHerve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. IEEE\ntransactions on pattern analysis and machine intelligence , 33(1), 2010.\nJeﬀ Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with GPUs. IEEE Transac-\ntions on Big Data , 7(3):535–547, 2019.\nArmand Joulin, Laurens Van Der Maaten, Allan Jabri, and Nicolas Vasilache. Learning visual features from\nlarge weakly supervised data. In ECCV, 2016.\nWill Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio\nViola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv\npreprint arXiv:1705.06950 , 2017.\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for ﬁne-grained catego-\nrization. In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13) , Sydney,\nAustralia, 2013.\nAlex Krizhevsky, Geoﬀrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\nBenjaminLefaudeux, FranciscoMassa, DianaLiskovich, WenhanXiong, VittorioCaggiano, SeanNaren, Min\nXu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza. xformers: A modular\nand hackable transformer modelling library. https://github.com/facebookresearch/xformers , 2022.\nChunyuan Li, Jianwei Yang, Pengchuan Zhang, Mei Gao, Bin Xiao, Xiyang Dai, Lu Yuan, and Jianfeng\nGao. Eﬃcient self-supervised vision transformers for representation learning. In ICLR, 2022a.\nZhenyu Li, Xuyang Wang, Xianming Liu, and Junjun Jiang. Binsformer: Revisiting adaptive bins for\nmonocular depth estimation. arXiv preprint arXiv:2204.00987 , 2022b.\nHuajun Liu, Fuqiang Liu, Xinyi Fan, and Dong Huang. Polarized self-attention: towards high-quality pixel-\nwise regression. arXiv preprint arXiv:2107.00782 , 2021.\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin\nBharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In ECCV,\n2018.\nS. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classiﬁcation of aircraft.\nTechnical report, 2013.\n24', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. In\nCVPR, 2020.\nMaria-Elena Nilsback and Andrew Zisserman. Automated ﬂower classiﬁcation over a large number of classes.\nInIndian Conference on Computer Vision, Graphics and Image Processing , Dec 2008.\nMehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles.\nIn Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (eds.), Computer Vision – ECCV 2016 , pp.\n69–84, Cham, 2016. Springer International Publishing. ISBN 978-3-319-46466-4.\nOmkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In IEEE\nConference on Computer Vision and Pattern Recognition , 2012.\nDeepak Pathak, Philipp Krähenbühl, Jeﬀ Donahue, Trevor Darrell, and Alexei Efros. Context encoders:\nFeature learning by inpainting. In CVPR, 2016.\nDavid Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David\nSo, Maud Texier, and Jeﬀ Dean. Carbon emissions and large neural network training. arXiv preprint\narXiv:2104.10350 , 2021.\nEd Pizzi, Sreya Dutta Roy, Sugosh Nagavara Ravindra, Priya Goyal, and Matthijs Douze. A self-supervised\ndescriptor for image copy detection. arXiv preprint arXiv:2202.10261 , 2022.\nFilip Radenović, Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondřej Chum. Revisiting oxford and\nparis: Large-scale image retrieval benchmarking. In CVPR, 2018a.\nFilip Radenović, Giorgos Tolias, and Ondřej Chum. Fine-tuning cnn image retrieval with no human anno-\ntation.IEEE transactions on pattern analysis and machine intelligence , 2018b.\nAlec Radford, Jeﬀrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models\nare unsupervised multitask learners.\nAlec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to generate reviews and discovering sentiment.\narXiv preprint arXiv:1704.01444 , 2017.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In International Conference on Machine Learning , pp. 8748–8763. PMLR,\n2021.\nColin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer.\nJ. Mach. Learn. Res. , 21(140):1–67, 2020.\nRené Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Pro-\nceedings of the IEEE/CVF International Conference on Computer Vision , pp. 12179–12188, 2021.\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classiﬁers generalize\nto imagenet? In International Conference on Machine Learning , pp. 5389–5400. PMLR, 2019.\nJerome Revaud, Jon Almazán, Rafael S Rezende, and Cesar Roberto de Souza. Learning with average\nprecision: Training image retrieval with a listwise loss. In ICCV, 2019.\nYangjun Ruan, Saurabh Singh, Warren Morningstar, Alexander A Alemi, Sergey Ioﬀe, Ian Fischer, and\nJoshua V Dillon. Weighted ensemble self-supervised learning. arXiv preprint arXiv:2211.09981 , 2022.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej\nKarpathy, Aditya Khosla, Michael Bernstein, Alexander C Berg, and Li Fei-Fei. Imagenet large scale\nvisual recognition challenge. IJCV, 2015.\n25', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, and Hervé Jégou. Spreading vectors for similarity\nsearch.arXiv preprint arXiv:1806.03198 , 2018.\nNoam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202 , 2020.\nNathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support\ninference from rgbd images. In European conference on computer vision , pp. 746–760. Springer, 2012.\nMannat Singh, Laura Gustafson, Aaron Adcock, Vinicius de Freitas Reis, Bugra Gedik, Raj Prateek\nKosaraju, Dhruv Mahajan, Ross Girshick, Piotr Dollár, and Laurens van der Maaten. Revisiting Weakly\nSupervised Pre-Training of Visual Perception Models. In CVPR, 2022.\nShuranSong, SamuelPLichtenberg, andJianxiongXiao. Sunrgb-d: Argb-dsceneunderstandingbenchmark\nsuite. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 567–576,\n2015.\nKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes\nfrom videos in the wild. arXiv preprint arXiv:1212.0402 , 2012.\nAndreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer.\nHow to train your vit? data, augmentation, and regularization in vision transformers. arXiv preprint\narXiv:2106.10270 , 2021.\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning\nin nlp.arXiv preprint arXiv:1906.02243 , 2019.\nYonglong Tian, Olivier J Henaﬀ, and Aäron van den Oord. Divide and contrast: Self-supervised learning\nfrom uncurated data. In Proceedings of the IEEE/CVF International Conference on Computer Vision ,\npp. 10063–10074, 2021.\nGiorgos Tolias, Ronan Sicre, and Hervé Jégou. Particular object retrieval with integral max-pooling of cnn\nactivations. arXiv preprint arXiv:1511.05879 , 2015.\nZhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-eﬃcient\nlearners for self-supervised video pre-training. arXiv preprint arXiv:2203.12602 , 2022.\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Hervé Jégou. Fixing the train-test resolution discrep-\nancy. In NeurIPS , 2019.\nHugo Touvron, Matthieu Cord, and Hervé Jégou. Deit iii: Revenge of the vit. arXiv preprint\narXiv:2204.07118 , 2022.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. Llama: Open and eﬃcient foundation language models. arXiv preprint\narXiv:2302.13971 , 2023.\nGrant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro\nPerona, and Serge Belongie. The inaturalist species classiﬁcation and detection dataset. In CVPR, 2018.\nGrant Van Horn, Elijah Cole, Sara Beery, Kimberly Wilber, Serge Belongie, and Oisin Mac Aodha. Bench-\nmarking representation learning for natural world image collections. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition , pp. 12884–12893, 2021.\nWenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei\nLu, Hongsheng Li, et al. Internimage: Exploring large-scale vision foundation models with deformable\nconvolutions. arXiv preprint arXiv:2211.05778 , 2022.\nXiaolong Wang, Allan Jabri, and Alexei A Efros. Learning correspondence from the cycle-consistency of\ntime. In CVPR, 2019.\n26', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Philippe Weinzaepfel, Thomas Lucas, Diane Larlus, and Yannis Kalantidis. Learning super-features for\nimage retrieval. In International Conference on Learning Representations , 2021.\nP. Welinder, S. Branson, T. Mita, C. Wah, F. Schroﬀ, S. Belongie, and P. Perona. Caltech-UCSD Birds 200.\nTechnical Report CNS-TR-2010-001, California Institute of Technology, 2010.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand\nJoulin, and Edouard Grave. Ccnet: Extracting high quality monolingual datasets from web crawl data.\narXiv preprint arXiv:1911.00359 , 2019.\nZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric\ninstance discrimination. In CVPR, 2018.\nJ. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recognition from\nabbey to zoo. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition ,\npp. 3485–3492, June 2010. doi: 10.1109/CVPR.2010.5539970.\nHuXu,JunchengLi,AlexeiBaevski,MichaelAuli,WojciechGaluba,FlorianMetze,ChristophFeichtenhofer,\net al. Masked autoencoders that listen. arXiv preprint arXiv:2207.06405 , 2022.\nI Zeki Yalniz, Hervé Jégou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. Billion-scale semi-supervised\nlearning for image classiﬁcation. arXiv preprint arXiv:1905.00546 , 2019.\nBurak Yildiz, Seyran Khademi, Ronald Maria Siebes, and Jan van Gemert. Amstertime: A visual place\nrecognition benchmark dataset for severe domain shift. arXiv preprint arXiv:2203.16291 , 2022.\nNikolaos-Antonios Ypsilantis, Noa Garcia, Guangxing Han, Sarah Ibrahimi, Nanne Van Noord, and Giorgos\nTolias. The met dataset: Instance-level recognition for artworks. In Thirty-ﬁfth Conference on Neural\nInformation Processing Systems Datasets and Benchmarks Track (Round 2) , 2021.\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 12104–12113,\n2022.\nRichard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016.\nBolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep features\nfor scene recognition using places database. In NeurIPS , 2014.\nBolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing\nthroughade20kdataset. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npp. 633–641, 2017.\nJinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert\npre-training with online tokenizer. arXiv preprint arXiv:2111.07832 , 2021.\nPan Zhou, Yichen Zhou, Chenyang Si, Weihao Yu, Teck Khim Ng, and Shuicheng Yan. Mugs: A multi-\ngranular self-supervised learning framework. arXiv preprint arXiv:2203.14415 , 2022.\n27', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'A Data Processing\nA.1 Data selection\nOur selection of datasets for building LVD-142M is detailed in Tab. 15. This collection is intended to provide\nimages covering well various downstream vision tasks both for image-level and dense recognition.\nA.2 Image similarity\nWe employ cosine similarity to compare image features (whether ours or feature generated for deduplication)\nwith the following similarity function m:\nm(s, r) =cosine-similarity (f(s), f(r)) =f(s).f(r)\n∥f(s)∥2∥f(r)∥2\nwhere sandrare a pair of images to compare and fis the model generating features.\nA.3 Deduplication\nSelf-deduplication. To deduplicate our uncurated data source of 1.3B images, we compute and use the\nembeddings generated by Pizzi et al. (2022) and retrieve the k= 64nearest neighbors of each image (using\ncosine similarity). Considering only neighbors with a similarity >0.6, we extract the connected components\nof the associated k-NN graph thanks to a scalable disjoint set data structure implementation. We then only\nkeep one representative for each component of duplicate images. This results in a self-deduplicated data\nsource of 1.1B images.\nRelativededuplication Toreduceredundancyandalsoproperlyevaluatetheperformanceofourfeatures,\nwe discard remaining images of our self-deduplicated data source that are too similar to train and test splits\nof our evaluation datasets. To achieve this, we apply a similar procedure as for self-deduplication, with a\nstricter similarity >0.45, this time identifying the duplicate components (if any) to which each reference\nimage belong and discarding it entirely. This results in a self- and relatively-deduplicated data source of\n744M images.\nA.4 Retrieval\nWe employ two approaches to augment dataset via retrieval: sample-based and cluster-based. The ﬁrst one,\nsample-based, applies to datasets larger than 1M images and consists in collecting a ﬁxed number kof nearest\nimages for each sample image of the dataset to retrieve, eﬀectively trying to multiply by kthe size of the\ndataset. We use k= 4for Google Landmarks v2 and ImageNet-22k but a larger k= 32to make this speciﬁc\nretrieval a core part of our LVD-142M dataset. For smaller datasets, the second approach, cluster-based,\nconsists in ﬁrst clustering our uncurated data source into 100,000separate clusters thanks to a distributed\nk-means implementation. Each cluster should capture diﬀerent types of image concept and contents. We\nthen pick 10,000images from each cluster associated with more than 3images of the retrieved dataset. As\nthis can result in a very large number of retrieved images for some dataset, we restrict such retrievals to a\nmaximum of 1M images to maintain the balance between the diﬀerent datasets within LVD-142M.\nB Implementation Details\nB.1 Unsupervised pre-training\nFor unsupervised pre-training we build on the DINO and iBOT codebases. We use hyperparameters shown\nin Table 16, ViT architectures described in Table 17.\nKoLeo regularization. We apply the KoLeo regularizer with a weight of 0.1 between the class tokens of\nthe ﬁrst global crop, for all samples within a GPU without cross-communication for this step.\n28', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Task Dataset / Split Images Retrieval Retrieved Final\nclassiﬁcation ImageNet-22k / – 14,197,086 as is – 14,197,086\nclassiﬁcation ImageNet-22k / – 14,197,086 sample 56,788,344 56,788,344\nclassiﬁcation ImageNet-1k / train 1,281,167 sample 40,997,344 40,997,344\nﬁne-grained classif. Caltech 101 / train 3,030 cluster 2,630,000 1,000,000\nﬁne-grained classif. CUB-200-2011 / train 5,994 cluster 1,300,000 1,000,000\nﬁne-grained classif. DTD / train1 1,880 cluster 1,580,000 1,000,000\nﬁne-grained classif. FGVC-Aircraft / train 3,334 cluster 1,170,000 1,000,000\nﬁne-grained classif. Flowers-102 / train 1,020 cluster 1,060,000 1,000,000\nﬁne-grained classif. Food-101 / train 75,750 cluster 21,670,000 1,000,000\nﬁne-grained classif. Oxford-IIIT Pet / trainval 3,680 cluster 2,750,000 1,000,000\nﬁne-grained classif. Stanford Cars / train 8,144 cluster 7,220,000 1,000,000\nﬁne-grained classif. SUN397 / train1 19,850 cluster 18,950,000 1,000,000\nﬁne-grained classif. Pascal VOC 2007 / train 2,501 cluster 1,010,000 1,000,000\nsegmentation ADE20K / train 20,210 cluster 20,720,000 1,000,000\nsegmentation Cityscapes / train 2,975 cluster 1,390,000 1,000,000\nsegmentation Pascal VOC 2012 (seg.) / trainaug 1,464 cluster 10,140,000 1,000,000\ndepth estimation Mapillary SLS / train 1,434,262 as is – 1,434,262\ndepth estimation KITTI / train (Eigen) 23,158 cluster 3,700,000 1,000,000\ndepth estimation NYU Depth V2 / train 24,231 cluster 10,850,000 1,000,000\ndepth estimation SUN RGB-D / train 4,829 cluster 4,870,000 1,000,000\nretrieval Google Landmarks v2 / train (clean) 1,580,470 as is – 1,580,470\nretrieval Google Landmarks v2 / train (clean) 1,580,470 sample 6,321,880 6,321,880\nretrieval AmsterTime / new 1,231 cluster 960,000 960,000\nretrieval AmsterTime / old 1,231 cluster 830,000 830,000\nretrieval Met / train 397,121 cluster 62,860,000 1,000,000\nretrieval Revisiting Oxford / base 4,993 cluster 3,680,000 1,000,000\nretrieval Revisiting Paris / base 6,322 cluster 3,660,000 1,000,000\n142,109,386\nTable 15: Composition of our LVD-142M dataset. We report the list of datasets and associated splits\nused to build the dataset, how they were included (as is without retrieval or via sample-based or cluster-based\nretrieval). For retrievals, we indicate the actual number of retrieved images and the ﬁnal number included\nin the dataset.\n29', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Arch. Drop-rate LR Batch size\nDINOv2-S (distilled) ViT-S/14 0 1e-3 2048\nDINOv2-B (distilled) ViT-B/14 0 1e-3 2048\nDINOv2-L (distilled) ViT-L/14 0 1e-3 2048\nDINOv2-L (from scratch) ViT-L/14 0.4 3.5e-4 3072\nDINOv2-g (from scratch) ViT-g/14 0.4 3.5e-4 3072\nTable 16: Training hyperparameters for DINOv2-S, DINOv2-B, DINOv2-L and DINOv2-g. All\nmodels run for 625k iterations with optimizer AdamW, an initial LayerScale value of 1e-5, a weight decay\ncosine schedule from 0.04 to 0.2, a learning rate warmup of 100k iterations, a teacher momentum cosine\nschedule from 0.994 to 1, and we train in ﬂoat16 precision in all cases (except for the DINO heads where we\nreduce the gradients in ﬂoat32).\nArch. Embed dim Heads Blocks FFN layer\nViT-S/14 (distilled) 384 6 12 MLP\nViT-B/14 (distilled) 768 12 18 MLP\nViT-L/14 (distilled) 1024 16 24 MLP\nViT-L/14 (from scratch) 1024 16 24 SwiGLU\nViT-g/14 (from scratch) 1536 24 40 SwiGLU\nTable 17: Architecture details of the ViT-S/B/L/g networks used in this work. We use MLP\nfeed-forward networks for distilled models, and SwiGLU (Shazeer, 2020) when training from scratch.\nEMA update for the teacher. The teacher is initialized with the same state as the student, and is an\nexponential moving average of the student network, with a momentum value in [0.994, 1.0] following a cosine\nschedule. It is updated at the end of every training step.\nB.2 High-Resolution adaptation\nWe initialise the model with the pretrained weights then train it for 10k iterations with the same procedure\nas the original pretraining. All the schedules are kept the same as in the original training, but compressed\nto ﬁt in 10k iterations. All the hyperparameters are kept the same as in the ﬁrst pretraining, except the\nbase learning rate which is reduced.\nB.3 Linear probing evaluation\nFor linear probing we deﬁne 3 evaluation parameters: the learning rate, how many output layers we use,\nwhether we concatenate the average-pooled patch token features with the class token (or use only the\nclass token). We train our linear layer with SGD for 12500 iterations, using random-resized-crop data\naugmentation, and perform the following grid search:\n•learning rate in{0.0001,0.0002,0.0005,0.001,0.002,0.005,0.01,0.02,0.05,0.1,0.2,0.3,0.5}\n•output layers in{1,4}\n•concatenate average-pooled tokens in {yes, no}\nWe then report the highest accuracy value obtained on the validation set as is common practice. Note that\nthis grid search is not expensive, because at each iteration we perform inference on the backbone only once,\nthen feed the output to all linear classiﬁers (each performing a single matrix multiplication).\nC List of benchmarks used for evaluations\nWe show in Table 18 the list of benchmarks and datasets used for evaluation.\n30', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Dataset name Task Citation\nImageNet-1k Image Classiﬁcation (Russakovsky et al., 2015)\nImageNet-V2 Image Classiﬁcation (Recht et al., 2019)\nImageNet-ReaL Image Classiﬁcation (Beyer et al., 2020)\nImageNet-A Image Classiﬁcation (Djolonga et al., 2021)\nImageNet-C Image Classiﬁcation (Hendrycks & Dietterich, 2019)\nImageNet-Rendition Image Classiﬁcation (Hendrycks et al., 2021)\nImageNet-Sketch Image Classiﬁcation (Wang et al., 2019)\nFood-101 Image Classiﬁcation (Bossard et al., 2014)\nCIFAR-10 Image Classiﬁcation (Krizhevsky et al., 2009)\nCIFAR-100 Image Classiﬁcation (Krizhevsky et al., 2009)\nSUN397 Image Classiﬁcation (Xiao et al., 2010)\nStanfordCars Image Classiﬁcation (Krause et al., 2013)\nFGVC-Aircraft Image Classiﬁcation (Maji et al., 2013)\nPascal VOC 2007 Image Classiﬁcation (Everingham et al., 2015)\nDescribable Textures Image Classiﬁcation (Cimpoi et al., 2014)\nOxford Pets Image Classiﬁcation (Parkhi et al., 2012)\nCaltech101 Image Classiﬁcation (Fei-Fei et al., 2004)\nOxford Flowers Image Classiﬁcation (Nilsback & Zisserman, 2008)\nCUB200 Image Classiﬁcation (Welinder et al., 2010)\niNaturalist 2018 Image Classiﬁcation (Van Horn et al., 2018)\niNaturalist 2021 Image Classiﬁcation (Van Horn et al., 2021)\nPlaces-205 Image Classiﬁcation (Zhou et al., 2014)\nUCF101 Video Classiﬁcation (Soomro et al., 2012)\nKinetics-400 Video Classiﬁcation (Kay et al., 2017)\nSomething-Something-V2 Video Classiﬁcation (Goyal et al., 2017)\nRevisiting-Paris Image Retrieval (Radenović et al., 2018a)\nRevisiting-Oxford Image Retrieval (Radenović et al., 2018a)\nMet Image Retrieval (Ypsilantis et al., 2021)\nAmstertime Image Retrieval (Yildiz et al., 2022)\nADE20k Image Segmentation (Zhou et al., 2017)\nCityscapes Image Segmentation (Cordts et al., 2016)\nPascal VOC 2012 Image Segmentation (Everingham et al., 2015)\nNYU-Depth V2 Monocular Depth Estimation (Silberman et al., 2012)\nKITTI Monocular Depth Estimation (Geiger et al., 2013)\nSUN-RGBD Monocular Depth Estimation (Song et al., 2015)\nDollarStreet Fairness Analysis (De Vries et al., 2019)\nCasual Conversations Fairness Analysis (Hazirbas et al., 2021)\nTable 18: List of datasets used for evaluation.\n31', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}]: %s
2023-12-07 11:13:45,845 - INFO - Check the results [{'page_content': 'Jan Beirlant, Edward J Dudewicz, László Györﬁ, Edward C Van der Meulen, et al. Nonparametric entropy\nestimation: An overview. International Journal of Mathematical and Statistical Sciences , 6(1):17–39, 1997.\nMaxim Berman, Hervé Jégou, Vedaldi Andrea, Iasonas Kokkinos, and Matthijs Douze. MultiGrain: a uniﬁed\nimage embedding for classes and instances. arXiv preprint arXiv:1902.05509 , 2019.\nLucas Beyer, Olivier J Hénaﬀ, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord. Are we done\nwith imagenet? arXiv preprint arXiv:2006.07159 , 2020.\nShariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. AdaBins: Depth estimation using adaptive\nbins. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . IEEE, jun\n2021. doi: 10.1109/cvpr46437.2021.00400. URL https://doi.org/10.1109%2Fcvpr46437.2021.00400 .\nPiotr Bojanowski and Armand Joulin. Unsupervised learning by predicting noise. In ICML, 2017.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of\nfoundation models. arXiv preprint arXiv:2108.07258 , 2021.\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative components\nwith random forests. In European Conference on Computer Vision , 2014.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\npreprint arXiv:2005.14165 , 2020.\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised\nlearning of visual features. In ECCV, 2018.\nMathilde Caron, Piotr Bojanowski, Julien Mairal, and Armand Joulin. Unsupervised pre-training of image\nfeatures on non-curated data. In ICCV, 2019.\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsuper-\nvised learning of visual features by contrasting cluster assignments. In NeurIPS , 2020.\nMathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand\nJoulin. Emerging properties in self-supervised vision transformers. arXiv preprint arXiv:2104.14294 , 2021.\nLiang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroﬀ, and Hartwig Adam. Encoder-decoder\nwith atrous separable convolution for semantic image segmentation. In Proceedings of the European con-\nference on computer vision (ECCV) , pp. 801–818, 2018.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoﬀrey Hinton. A simple framework for contrastive\nlearning of visual representations. preprint arXiv:2002.05709 , 2020.\nXiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi\nDong, Thang Luong, Cho-Jui Hsieh, et al. Symbolic discovery of optimization algorithms. arXiv preprint\narXiv:2302.06675 , 2023.\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. preprint arXiv:2011.10566 ,\n2020.\nXinleiChen, SainingXie, andKaimingHe. Anempiricalstudyoftrainingself-supervisedvisiontransformers.\narXiv preprint arXiv:2104.02057 , 2021.\nZhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer\nadapter for dense predictions. arXiv preprint arXiv:2205.08534 , 2022.\nBowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention\nmask transformer for universal image segmentation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pp. 1290–1299, 2022.\n21', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language\nmodeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.\nM. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In\nProceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , 2014.\nMarius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,\nUwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understand-\ning. InProceedings of the IEEE conference on computer vision and pattern recognition , pp. 3213–3223,\n2016.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-\neﬃcient exact attention with io-awareness. arXiv preprint arXiv:2205.14135 , 2022.\nTerrance De Vries, Ishan Misra, Changhan Wang, and Laurens Van der Maaten. Does object recognition\nworkforeveryone? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition\nworkshops , pp. 52–59, 2019.\nSylvain Delattre and Nicolas Fournier. On the kozachenko–leonenko entropy estimator. Journal of Statistical\nPlanning and Inference , 185:69–93, 2017.\nJacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert: Pre-trainingofdeepbidirectional\ntransformers for language understanding. preprint arXiv:1810.04805 , 2018.\nJosip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander Kolesnikov,\nJoan Puigcerver, Matthias Minderer, Alexander D’Amour, Dan Moldovan, et al. On robustness and\ntransferabilityofconvolutionalneuralnetworks. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pp. 16458–16468, 2021.\nCarl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context\nprediction. In ICCV, 2015.\nAlexey Dosovitskiy, Jost Tobias Springenberg, Martin A. Riedmiller, and Thomas Brox. Discriminative\nunsupervised feature learning with convolutional neural networks. CoRR, abs/1406.6909, 2014. URL\nhttp://arxiv.org/abs/1406.6909 .\nAlexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Dis-\ncriminative unsupervised feature learning with exemplar convolutional neural networks. TPAMI, 2016.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-\nterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. preprint arXiv:2010.11929 , 2020.\nMatthijs Douze, Hervé Jégou, Harsimrat Sandhawalia, Laurent Amsaleg, and Cordelia Schmid. Evaluation\nof gist descriptors for web-scale image search. In CIVR, 2009.\nQuentin Duval, Ishan Misra, and Nicolas Ballas. A simple recipe for competitive low-compute self supervised\nvision models. arXiv preprint arXiv:2301.09451 , 2023.\nAlaaeldin El-Nouby, Gautier Izacard, Hugo Touvron, Ivan Laptev, Hervé Jegou, and Edouard Grave. Are\nlarge-scale datasets necessary for self-supervised pre-training? arXiv preprint arXiv:2112.10740 , 2021.\nM. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal\nvisual object classes challenge: A retrospective. International Journal of Computer Vision , 111(1):98–136,\nJanuary 2015.\nYuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang,\nand Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. 2023.\n22', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples:\nAn incremental bayesian approach tested on 101 object categories. In 2004 conference on computer vision\nand pattern recognition workshop , pp. 178–178. IEEE, 2004.\nAndreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti\ndataset. The International Journal of Robotics Research , 32(11):1231–1237, 2013.\nSpyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting\nimage rotations, 2018.\nRohit Girdhar, Alaaeldin El-Nouby, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Is-\nhan Misra. Omnimae: Single model masked pretraining on images and videos. arXiv preprint\narXiv:2206.08356 , 2022.\nPriya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. Scaling and benchmarking self-supervised\nvisual representation learning. In ICCV, 2019.\nPriya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat Singh,\nVitaliy Liptchinsky, Ishan Misra, Armand Joulin, et al. Self-supervised pretraining of visual features in\nthe wild. preprint arXiv:2103.01988 , 2021.\nPriya Goyal, Quentin Duval, Isaac Seessel, Mathilde Caron, Mannat Singh, Ishan Misra, Levent Sagun,\nArmand Joulin, and Piotr Bojanowski. Vision models are more robust and fair when pretrained on\nuncurated images without supervision. arXiv preprint arXiv:2202.08360 , 2022a.\nPriya Goyal, Adriana Romero Soriano, Caner Hazirbas, Levent Sagun, and Nicolas Usunier. Fairness in-\ndicators for systematic assessments of visual feature extractors. In 2022 ACM Conference on Fairness,\nAccountability, and Transparency , pp. 70–88, 2022b.\nRaghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, He-\nuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The" something\nsomething" video database for learning and evaluating visual common sense. In Proceedings of the IEEE\ninternational conference on computer vision , pp. 5842–5850, 2017.\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya,\nCarl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray\nKavukcuoglu, Rémi Munos, and Michal Valko. Bootstrap your own latent: A new approach to self-\nsupervised learning. In NeurIPS , 2020.\nRaia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping.\nInCVPR, 2006.\nCanerHazirbas, JoannaBitton, BrianDolhansky, JacquelinePan, AlbertGordo, andCristianCantonFerrer.\nTowards measuring fairness in ai: the casual conversations dataset. IEEE Transactions on Biometrics,\nBehavior, and Identity Science , 4(3):324–332, 2021.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised\nvisual representation learning. In CVPR, 2020.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders\nare scalable vision learners. arXiv preprint arXiv:2111.06377 , 2021.\nOlivierJHénaﬀ, AravindSrinivas, JeﬀreyDeFauw, AliRazavi, CarlDoersch, SMEslami, andAaronvanden\nOord. Data-eﬃcientimagerecognitionwithcontrastivepredictivecoding. preprint arXiv:1905.09272 ,2019.\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions\nand perturbations. In International Conference on Learning Representations , 2019.\n23', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai,\nTyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-\nof-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pp. 8340–8349, 2021.\nGeoﬀrey Hinton, Oriol Vinyals, and Jeﬀ Dean. Distilling the knowledge in a neural network. preprint\narXiv:1503.02531 , 2015.\nJordan Hoﬀmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal\nlarge language models. arXiv preprint arXiv:2203.15556 , 2022.\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochas-\ntic depth. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands,\nOctober 11–14, 2016, Proceedings, Part IV 14 , pp. 646–661. Springer, 2016.\nGabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal\nDave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig\nSchmidt. Openclip. 2021.\nHerve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. IEEE\ntransactions on pattern analysis and machine intelligence , 33(1), 2010.\nJeﬀ Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with GPUs. IEEE Transac-\ntions on Big Data , 7(3):535–547, 2019.\nArmand Joulin, Laurens Van Der Maaten, Allan Jabri, and Nicolas Vasilache. Learning visual features from\nlarge weakly supervised data. In ECCV, 2016.\nWill Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio\nViola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv\npreprint arXiv:1705.06950 , 2017.\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for ﬁne-grained catego-\nrization. In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13) , Sydney,\nAustralia, 2013.\nAlex Krizhevsky, Geoﬀrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\nBenjaminLefaudeux, FranciscoMassa, DianaLiskovich, WenhanXiong, VittorioCaggiano, SeanNaren, Min\nXu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza. xformers: A modular\nand hackable transformer modelling library. https://github.com/facebookresearch/xformers , 2022.\nChunyuan Li, Jianwei Yang, Pengchuan Zhang, Mei Gao, Bin Xiao, Xiyang Dai, Lu Yuan, and Jianfeng\nGao. Eﬃcient self-supervised vision transformers for representation learning. In ICLR, 2022a.\nZhenyu Li, Xuyang Wang, Xianming Liu, and Junjun Jiang. Binsformer: Revisiting adaptive bins for\nmonocular depth estimation. arXiv preprint arXiv:2204.00987 , 2022b.\nHuajun Liu, Fuqiang Liu, Xinyi Fan, and Dong Huang. Polarized self-attention: towards high-quality pixel-\nwise regression. arXiv preprint arXiv:2107.00782 , 2021.\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin\nBharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In ECCV,\n2018.\nS. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classiﬁcation of aircraft.\nTechnical report, 2013.\n24', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. In\nCVPR, 2020.\nMaria-Elena Nilsback and Andrew Zisserman. Automated ﬂower classiﬁcation over a large number of classes.\nInIndian Conference on Computer Vision, Graphics and Image Processing , Dec 2008.\nMehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles.\nIn Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (eds.), Computer Vision – ECCV 2016 , pp.\n69–84, Cham, 2016. Springer International Publishing. ISBN 978-3-319-46466-4.\nOmkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In IEEE\nConference on Computer Vision and Pattern Recognition , 2012.\nDeepak Pathak, Philipp Krähenbühl, Jeﬀ Donahue, Trevor Darrell, and Alexei Efros. Context encoders:\nFeature learning by inpainting. In CVPR, 2016.\nDavid Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David\nSo, Maud Texier, and Jeﬀ Dean. Carbon emissions and large neural network training. arXiv preprint\narXiv:2104.10350 , 2021.\nEd Pizzi, Sreya Dutta Roy, Sugosh Nagavara Ravindra, Priya Goyal, and Matthijs Douze. A self-supervised\ndescriptor for image copy detection. arXiv preprint arXiv:2202.10261 , 2022.\nFilip Radenović, Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondřej Chum. Revisiting oxford and\nparis: Large-scale image retrieval benchmarking. In CVPR, 2018a.\nFilip Radenović, Giorgos Tolias, and Ondřej Chum. Fine-tuning cnn image retrieval with no human anno-\ntation.IEEE transactions on pattern analysis and machine intelligence , 2018b.\nAlec Radford, Jeﬀrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models\nare unsupervised multitask learners.\nAlec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to generate reviews and discovering sentiment.\narXiv preprint arXiv:1704.01444 , 2017.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In International Conference on Machine Learning , pp. 8748–8763. PMLR,\n2021.\nColin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer.\nJ. Mach. Learn. Res. , 21(140):1–67, 2020.\nRené Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Pro-\nceedings of the IEEE/CVF International Conference on Computer Vision , pp. 12179–12188, 2021.\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classiﬁers generalize\nto imagenet? In International Conference on Machine Learning , pp. 5389–5400. PMLR, 2019.\nJerome Revaud, Jon Almazán, Rafael S Rezende, and Cesar Roberto de Souza. Learning with average\nprecision: Training image retrieval with a listwise loss. In ICCV, 2019.\nYangjun Ruan, Saurabh Singh, Warren Morningstar, Alexander A Alemi, Sergey Ioﬀe, Ian Fischer, and\nJoshua V Dillon. Weighted ensemble self-supervised learning. arXiv preprint arXiv:2211.09981 , 2022.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej\nKarpathy, Aditya Khosla, Michael Bernstein, Alexander C Berg, and Li Fei-Fei. Imagenet large scale\nvisual recognition challenge. IJCV, 2015.\n25', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, and Hervé Jégou. Spreading vectors for similarity\nsearch.arXiv preprint arXiv:1806.03198 , 2018.\nNoam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202 , 2020.\nNathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support\ninference from rgbd images. In European conference on computer vision , pp. 746–760. Springer, 2012.\nMannat Singh, Laura Gustafson, Aaron Adcock, Vinicius de Freitas Reis, Bugra Gedik, Raj Prateek\nKosaraju, Dhruv Mahajan, Ross Girshick, Piotr Dollár, and Laurens van der Maaten. Revisiting Weakly\nSupervised Pre-Training of Visual Perception Models. In CVPR, 2022.\nShuranSong, SamuelPLichtenberg, andJianxiongXiao. Sunrgb-d: Argb-dsceneunderstandingbenchmark\nsuite. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 567–576,\n2015.\nKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes\nfrom videos in the wild. arXiv preprint arXiv:1212.0402 , 2012.\nAndreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer.\nHow to train your vit? data, augmentation, and regularization in vision transformers. arXiv preprint\narXiv:2106.10270 , 2021.\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning\nin nlp.arXiv preprint arXiv:1906.02243 , 2019.\nYonglong Tian, Olivier J Henaﬀ, and Aäron van den Oord. Divide and contrast: Self-supervised learning\nfrom uncurated data. In Proceedings of the IEEE/CVF International Conference on Computer Vision ,\npp. 10063–10074, 2021.\nGiorgos Tolias, Ronan Sicre, and Hervé Jégou. Particular object retrieval with integral max-pooling of cnn\nactivations. arXiv preprint arXiv:1511.05879 , 2015.\nZhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-eﬃcient\nlearners for self-supervised video pre-training. arXiv preprint arXiv:2203.12602 , 2022.\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Hervé Jégou. Fixing the train-test resolution discrep-\nancy. In NeurIPS , 2019.\nHugo Touvron, Matthieu Cord, and Hervé Jégou. Deit iii: Revenge of the vit. arXiv preprint\narXiv:2204.07118 , 2022.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. Llama: Open and eﬃcient foundation language models. arXiv preprint\narXiv:2302.13971 , 2023.\nGrant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro\nPerona, and Serge Belongie. The inaturalist species classiﬁcation and detection dataset. In CVPR, 2018.\nGrant Van Horn, Elijah Cole, Sara Beery, Kimberly Wilber, Serge Belongie, and Oisin Mac Aodha. Bench-\nmarking representation learning for natural world image collections. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition , pp. 12884–12893, 2021.\nWenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei\nLu, Hongsheng Li, et al. Internimage: Exploring large-scale vision foundation models with deformable\nconvolutions. arXiv preprint arXiv:2211.05778 , 2022.\nXiaolong Wang, Allan Jabri, and Alexei A Efros. Learning correspondence from the cycle-consistency of\ntime. In CVPR, 2019.\n26', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Philippe Weinzaepfel, Thomas Lucas, Diane Larlus, and Yannis Kalantidis. Learning super-features for\nimage retrieval. In International Conference on Learning Representations , 2021.\nP. Welinder, S. Branson, T. Mita, C. Wah, F. Schroﬀ, S. Belongie, and P. Perona. Caltech-UCSD Birds 200.\nTechnical Report CNS-TR-2010-001, California Institute of Technology, 2010.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand\nJoulin, and Edouard Grave. Ccnet: Extracting high quality monolingual datasets from web crawl data.\narXiv preprint arXiv:1911.00359 , 2019.\nZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric\ninstance discrimination. In CVPR, 2018.\nJ. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recognition from\nabbey to zoo. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition ,\npp. 3485–3492, June 2010. doi: 10.1109/CVPR.2010.5539970.\nHuXu,JunchengLi,AlexeiBaevski,MichaelAuli,WojciechGaluba,FlorianMetze,ChristophFeichtenhofer,\net al. Masked autoencoders that listen. arXiv preprint arXiv:2207.06405 , 2022.\nI Zeki Yalniz, Hervé Jégou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. Billion-scale semi-supervised\nlearning for image classiﬁcation. arXiv preprint arXiv:1905.00546 , 2019.\nBurak Yildiz, Seyran Khademi, Ronald Maria Siebes, and Jan van Gemert. Amstertime: A visual place\nrecognition benchmark dataset for severe domain shift. arXiv preprint arXiv:2203.16291 , 2022.\nNikolaos-Antonios Ypsilantis, Noa Garcia, Guangxing Han, Sarah Ibrahimi, Nanne Van Noord, and Giorgos\nTolias. The met dataset: Instance-level recognition for artworks. In Thirty-ﬁfth Conference on Neural\nInformation Processing Systems Datasets and Benchmarks Track (Round 2) , 2021.\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 12104–12113,\n2022.\nRichard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016.\nBolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep features\nfor scene recognition using places database. In NeurIPS , 2014.\nBolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing\nthroughade20kdataset. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npp. 633–641, 2017.\nJinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert\npre-training with online tokenizer. arXiv preprint arXiv:2111.07832 , 2021.\nPan Zhou, Yichen Zhou, Chenyang Si, Weihao Yu, Teck Khim Ng, and Shuicheng Yan. Mugs: A multi-\ngranular self-supervised learning framework. arXiv preprint arXiv:2203.14415 , 2022.\n27', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'A Data Processing\nA.1 Data selection\nOur selection of datasets for building LVD-142M is detailed in Tab. 15. This collection is intended to provide\nimages covering well various downstream vision tasks both for image-level and dense recognition.\nA.2 Image similarity\nWe employ cosine similarity to compare image features (whether ours or feature generated for deduplication)\nwith the following similarity function m:\nm(s, r) =cosine-similarity (f(s), f(r)) =f(s).f(r)\n∥f(s)∥2∥f(r)∥2\nwhere sandrare a pair of images to compare and fis the model generating features.\nA.3 Deduplication\nSelf-deduplication. To deduplicate our uncurated data source of 1.3B images, we compute and use the\nembeddings generated by Pizzi et al. (2022) and retrieve the k= 64nearest neighbors of each image (using\ncosine similarity). Considering only neighbors with a similarity >0.6, we extract the connected components\nof the associated k-NN graph thanks to a scalable disjoint set data structure implementation. We then only\nkeep one representative for each component of duplicate images. This results in a self-deduplicated data\nsource of 1.1B images.\nRelativededuplication Toreduceredundancyandalsoproperlyevaluatetheperformanceofourfeatures,\nwe discard remaining images of our self-deduplicated data source that are too similar to train and test splits\nof our evaluation datasets. To achieve this, we apply a similar procedure as for self-deduplication, with a\nstricter similarity >0.45, this time identifying the duplicate components (if any) to which each reference\nimage belong and discarding it entirely. This results in a self- and relatively-deduplicated data source of\n744M images.\nA.4 Retrieval\nWe employ two approaches to augment dataset via retrieval: sample-based and cluster-based. The ﬁrst one,\nsample-based, applies to datasets larger than 1M images and consists in collecting a ﬁxed number kof nearest\nimages for each sample image of the dataset to retrieve, eﬀectively trying to multiply by kthe size of the\ndataset. We use k= 4for Google Landmarks v2 and ImageNet-22k but a larger k= 32to make this speciﬁc\nretrieval a core part of our LVD-142M dataset. For smaller datasets, the second approach, cluster-based,\nconsists in ﬁrst clustering our uncurated data source into 100,000separate clusters thanks to a distributed\nk-means implementation. Each cluster should capture diﬀerent types of image concept and contents. We\nthen pick 10,000images from each cluster associated with more than 3images of the retrieved dataset. As\nthis can result in a very large number of retrieved images for some dataset, we restrict such retrievals to a\nmaximum of 1M images to maintain the balance between the diﬀerent datasets within LVD-142M.\nB Implementation Details\nB.1 Unsupervised pre-training\nFor unsupervised pre-training we build on the DINO and iBOT codebases. We use hyperparameters shown\nin Table 16, ViT architectures described in Table 17.\nKoLeo regularization. We apply the KoLeo regularizer with a weight of 0.1 between the class tokens of\nthe ﬁrst global crop, for all samples within a GPU without cross-communication for this step.\n28', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Task Dataset / Split Images Retrieval Retrieved Final\nclassiﬁcation ImageNet-22k / – 14,197,086 as is – 14,197,086\nclassiﬁcation ImageNet-22k / – 14,197,086 sample 56,788,344 56,788,344\nclassiﬁcation ImageNet-1k / train 1,281,167 sample 40,997,344 40,997,344\nﬁne-grained classif. Caltech 101 / train 3,030 cluster 2,630,000 1,000,000\nﬁne-grained classif. CUB-200-2011 / train 5,994 cluster 1,300,000 1,000,000\nﬁne-grained classif. DTD / train1 1,880 cluster 1,580,000 1,000,000\nﬁne-grained classif. FGVC-Aircraft / train 3,334 cluster 1,170,000 1,000,000\nﬁne-grained classif. Flowers-102 / train 1,020 cluster 1,060,000 1,000,000\nﬁne-grained classif. Food-101 / train 75,750 cluster 21,670,000 1,000,000\nﬁne-grained classif. Oxford-IIIT Pet / trainval 3,680 cluster 2,750,000 1,000,000\nﬁne-grained classif. Stanford Cars / train 8,144 cluster 7,220,000 1,000,000\nﬁne-grained classif. SUN397 / train1 19,850 cluster 18,950,000 1,000,000\nﬁne-grained classif. Pascal VOC 2007 / train 2,501 cluster 1,010,000 1,000,000\nsegmentation ADE20K / train 20,210 cluster 20,720,000 1,000,000\nsegmentation Cityscapes / train 2,975 cluster 1,390,000 1,000,000\nsegmentation Pascal VOC 2012 (seg.) / trainaug 1,464 cluster 10,140,000 1,000,000\ndepth estimation Mapillary SLS / train 1,434,262 as is – 1,434,262\ndepth estimation KITTI / train (Eigen) 23,158 cluster 3,700,000 1,000,000\ndepth estimation NYU Depth V2 / train 24,231 cluster 10,850,000 1,000,000\ndepth estimation SUN RGB-D / train 4,829 cluster 4,870,000 1,000,000\nretrieval Google Landmarks v2 / train (clean) 1,580,470 as is – 1,580,470\nretrieval Google Landmarks v2 / train (clean) 1,580,470 sample 6,321,880 6,321,880\nretrieval AmsterTime / new 1,231 cluster 960,000 960,000\nretrieval AmsterTime / old 1,231 cluster 830,000 830,000\nretrieval Met / train 397,121 cluster 62,860,000 1,000,000\nretrieval Revisiting Oxford / base 4,993 cluster 3,680,000 1,000,000\nretrieval Revisiting Paris / base 6,322 cluster 3,660,000 1,000,000\n142,109,386\nTable 15: Composition of our LVD-142M dataset. We report the list of datasets and associated splits\nused to build the dataset, how they were included (as is without retrieval or via sample-based or cluster-based\nretrieval). For retrievals, we indicate the actual number of retrieved images and the ﬁnal number included\nin the dataset.\n29', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Arch. Drop-rate LR Batch size\nDINOv2-S (distilled) ViT-S/14 0 1e-3 2048\nDINOv2-B (distilled) ViT-B/14 0 1e-3 2048\nDINOv2-L (distilled) ViT-L/14 0 1e-3 2048\nDINOv2-L (from scratch) ViT-L/14 0.4 3.5e-4 3072\nDINOv2-g (from scratch) ViT-g/14 0.4 3.5e-4 3072\nTable 16: Training hyperparameters for DINOv2-S, DINOv2-B, DINOv2-L and DINOv2-g. All\nmodels run for 625k iterations with optimizer AdamW, an initial LayerScale value of 1e-5, a weight decay\ncosine schedule from 0.04 to 0.2, a learning rate warmup of 100k iterations, a teacher momentum cosine\nschedule from 0.994 to 1, and we train in ﬂoat16 precision in all cases (except for the DINO heads where we\nreduce the gradients in ﬂoat32).\nArch. Embed dim Heads Blocks FFN layer\nViT-S/14 (distilled) 384 6 12 MLP\nViT-B/14 (distilled) 768 12 18 MLP\nViT-L/14 (distilled) 1024 16 24 MLP\nViT-L/14 (from scratch) 1024 16 24 SwiGLU\nViT-g/14 (from scratch) 1536 24 40 SwiGLU\nTable 17: Architecture details of the ViT-S/B/L/g networks used in this work. We use MLP\nfeed-forward networks for distilled models, and SwiGLU (Shazeer, 2020) when training from scratch.\nEMA update for the teacher. The teacher is initialized with the same state as the student, and is an\nexponential moving average of the student network, with a momentum value in [0.994, 1.0] following a cosine\nschedule. It is updated at the end of every training step.\nB.2 High-Resolution adaptation\nWe initialise the model with the pretrained weights then train it for 10k iterations with the same procedure\nas the original pretraining. All the schedules are kept the same as in the original training, but compressed\nto ﬁt in 10k iterations. All the hyperparameters are kept the same as in the ﬁrst pretraining, except the\nbase learning rate which is reduced.\nB.3 Linear probing evaluation\nFor linear probing we deﬁne 3 evaluation parameters: the learning rate, how many output layers we use,\nwhether we concatenate the average-pooled patch token features with the class token (or use only the\nclass token). We train our linear layer with SGD for 12500 iterations, using random-resized-crop data\naugmentation, and perform the following grid search:\n•learning rate in{0.0001,0.0002,0.0005,0.001,0.002,0.005,0.01,0.02,0.05,0.1,0.2,0.3,0.5}\n•output layers in{1,4}\n•concatenate average-pooled tokens in {yes, no}\nWe then report the highest accuracy value obtained on the validation set as is common practice. Note that\nthis grid search is not expensive, because at each iteration we perform inference on the backbone only once,\nthen feed the output to all linear classiﬁers (each performing a single matrix multiplication).\nC List of benchmarks used for evaluations\nWe show in Table 18 the list of benchmarks and datasets used for evaluation.\n30', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}, {'page_content': 'Dataset name Task Citation\nImageNet-1k Image Classiﬁcation (Russakovsky et al., 2015)\nImageNet-V2 Image Classiﬁcation (Recht et al., 2019)\nImageNet-ReaL Image Classiﬁcation (Beyer et al., 2020)\nImageNet-A Image Classiﬁcation (Djolonga et al., 2021)\nImageNet-C Image Classiﬁcation (Hendrycks & Dietterich, 2019)\nImageNet-Rendition Image Classiﬁcation (Hendrycks et al., 2021)\nImageNet-Sketch Image Classiﬁcation (Wang et al., 2019)\nFood-101 Image Classiﬁcation (Bossard et al., 2014)\nCIFAR-10 Image Classiﬁcation (Krizhevsky et al., 2009)\nCIFAR-100 Image Classiﬁcation (Krizhevsky et al., 2009)\nSUN397 Image Classiﬁcation (Xiao et al., 2010)\nStanfordCars Image Classiﬁcation (Krause et al., 2013)\nFGVC-Aircraft Image Classiﬁcation (Maji et al., 2013)\nPascal VOC 2007 Image Classiﬁcation (Everingham et al., 2015)\nDescribable Textures Image Classiﬁcation (Cimpoi et al., 2014)\nOxford Pets Image Classiﬁcation (Parkhi et al., 2012)\nCaltech101 Image Classiﬁcation (Fei-Fei et al., 2004)\nOxford Flowers Image Classiﬁcation (Nilsback & Zisserman, 2008)\nCUB200 Image Classiﬁcation (Welinder et al., 2010)\niNaturalist 2018 Image Classiﬁcation (Van Horn et al., 2018)\niNaturalist 2021 Image Classiﬁcation (Van Horn et al., 2021)\nPlaces-205 Image Classiﬁcation (Zhou et al., 2014)\nUCF101 Video Classiﬁcation (Soomro et al., 2012)\nKinetics-400 Video Classiﬁcation (Kay et al., 2017)\nSomething-Something-V2 Video Classiﬁcation (Goyal et al., 2017)\nRevisiting-Paris Image Retrieval (Radenović et al., 2018a)\nRevisiting-Oxford Image Retrieval (Radenović et al., 2018a)\nMet Image Retrieval (Ypsilantis et al., 2021)\nAmstertime Image Retrieval (Yildiz et al., 2022)\nADE20k Image Segmentation (Zhou et al., 2017)\nCityscapes Image Segmentation (Cordts et al., 2016)\nPascal VOC 2012 Image Segmentation (Everingham et al., 2015)\nNYU-Depth V2 Monocular Depth Estimation (Silberman et al., 2012)\nKITTI Monocular Depth Estimation (Geiger et al., 2013)\nSUN-RGBD Monocular Depth Estimation (Song et al., 2015)\nDollarStreet Fairness Analysis (De Vries et al., 2019)\nCasual Conversations Fairness Analysis (Hazirbas et al., 2021)\nTable 18: List of datasets used for evaluation.\n31', 'document_title': 'DINOv2- Learning Robust Visual Features without Supervision.pdf'}]: %s
2023-12-07 11:22:45,359 - INFO - Received requests to /inference endpoint
2023-12-07 11:22:45,461 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 11:22:45,461 - INFO - Received request: {'username': 'amin', 'prompt': 'Give me a summary of the paper', 'memory': False, 'conversation_number': 0, 'AI_assistance': False, 'collection_name': 'paper', 'llm_model': 'Llama_13b'}
2023-12-07 11:22:51,417 - INFO - Processed the request successfully
2023-12-07 11:23:06,416 - INFO - Received requests to /inference endpoint
2023-12-07 11:23:06,517 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 11:23:06,517 - INFO - Received request: {'username': 'amin', 'prompt': 'Give me a summary of the paper', 'memory': True, 'conversation_number': 1, 'AI_assistance': False, 'collection_name': 'paper', 'llm_model': 'Llama_13b'}
2023-12-07 11:23:12,431 - INFO - Processed the request successfully
2023-12-07 12:19:06,327 - INFO - Created a temporary directory at /tmp/tmp1c0q8e14
2023-12-07 12:19:06,327 - INFO - Writing /tmp/tmp1c0q8e14/_remote_module_non_scriptable.py
2023-12-07 12:19:08,184 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-07 12:19:12,799 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-07 12:20:57,233 - INFO - classes: ['paper']: %s
2023-12-07 12:21:04,759 - INFO - classes: ['paper']: %s
2023-12-07 12:21:04,885 - INFO - classes: ['paper']: %s
2023-12-07 12:21:13,335 - INFO - classes: ['paper']: %s
2023-12-07 12:21:23,093 - INFO - classes: ['paper']: %s
2023-12-07 12:21:23,115 - INFO - checking the request/ username='amin' class_name='docs' mode='create_collection' vectorDB_type='Weaviate' file_path=None: %s
2023-12-07 12:21:23,175 - INFO - checkpoint 1
2023-12-07 12:21:23,175 - INFO - checkpoint 2 amin: %s
2023-12-07 12:21:23,175 - INFO - checkpoint 2 amin_docs: %s
2023-12-07 12:21:23,203 - INFO - class name added successfully to database
2023-12-07 12:21:23,204 - INFO - success: class docs created for user amin
2023-12-07 12:21:23,299 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:21:29,368 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:21:33,695 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:21:35,442 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:21:35,490 - INFO - request received username='amin' class_name='docs' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/a29c93bee1380fec': %s
2023-12-07 12:21:36,447 - INFO - actors creation successful [Actor(WeaviateEmbedder, 53ad824eee264f4556b1947601000000), Actor(WeaviateEmbedder, 82e41e0a29c2431dd66c1c7001000000), Actor(WeaviateEmbedder, 316f98277fc6c910aef05c3501000000)]: %s
2023-12-07 12:21:36,447 - INFO - check 1st step of ray was successful
2023-12-07 12:21:36,448 - INFO - check if ray was successful:
2023-12-07 12:21:36,448 - INFO - check weaviate add data, 
2023-12-07 12:21:36,448 - INFO - response: {'status': 'success', 'message': 'Processed 19 documents in batches for class amin_docs.'}: %s
2023-12-07 12:21:36,448 - INFO - request processed successfully username='amin' class_name='docs' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/received_files/a29c93bee1380fec': %s
2023-12-07 12:21:38,096 - INFO - Check the data that is being passed [{'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nTable 4. Main results (%). Size = backbone model size. Question classes: NAT = natural science, SOC = social science, LAN = language\nscience, TXT = text context, IMG = image context, NO = no context, G1-6 = grades 1-6, G7-12 = grades 7-12. Results except ours are\ntaken from Lu et al. (2022a). Segment 1: Human performance; Segment 2: VQA baselines; Segment 3: UniﬁedQA baselines; Segment 4:\nGPT-3.5 baselines; Segment 5: Our Multimodal-CoT results. Results in bold are the best performance.\nModel Size NAT SOC LAN TXT IMG NO G1-6 G7-12 Avg\nHuman -90.23 84.97 87.48 89.60 87.50 88.10 91.59 82.42 88.40\nMCAN (Yu et al., 2019) 95M 56.08 46.23 58.09 59.43 51.17 55.40 51.65 59.72 54.54\nTop-Down (Anderson et al., 2018) 70M 59.50 54.33 61.82 62.90 54.88 59.79 57.27 62.16 59.02\nBAN (Kim et al., 2018) 112M 60.88 46.57 66.64 62.61 52.60 65.51 56.83 63.94 59.37\nDFAF (Gao et al., 2019) 74M 64.03 48.82 63.55 65.88 54.49 64.11 57.12 67.17 60.72\nViLT (Kim et al., 2021) 113M 60.48 63.89 60.27 63.20 61.38 57.00 60.72 61.90 61.14\nPatch-TRM (Lu et al., 2021) 90M 65.19 46.79 65.55 66.96 55.28 64.95 58.04 67.50 61.42\nVisualBERT (Li et al., 2019) 111M 59.33 69.18 61.18 62.71 62.17 58.54 62.96 59.92 61.87\nUniﬁedQA Base (Khashabi et al., 2020) 223M 68.16 69.18 74.91 63.78 61.38 77.84 72.98 65.00 70.12\nUniﬁedQA Base w/ CoT (Lu et al., 2022a) 223M 71.00 76.04 78.91 66.42 66.53 81.81 77.06 68.82 74.11\nGPT-3.5 (Chen et al., 2020) 175B 74.64 69.74 76.00 74.44 67.28 77.42 76.80 68.89 73.97\nGPT-3.5 w/ CoT (Lu et al., 2022a) 175B 75.44 70.87 78.09 74.68 67.43 79.93 78.23 69.68 75.17\nMutimodal-CoT Base 223M 87.52 77.17 85.82 87.88 82.90 86.83 84.65 85.37 84.91\nMutimodal-CoT Large 738M 95.91 82.00 90.82 95.26 88.80 92.89 92.44 90.31 91.68\nTable 5. Ablation results of Multimodal-CoT.\nModel NAT SOC LAN TXT IMG NO G1-6 G7-12 Avg\nMultimodal-CoT 87.52 77.17 85.82 87.88 82.90 86.83 84.65 85.37 84.91\nw/o Two-Stage Framework 80.99 87.40 81.91 80.25 78.83 83.62 82.78 82.20 82.57\nw/o Vision Features 71.09 70.75 69.18 71.16 65.84 71.57 71.00 69.68 70.53\nmaximum input sequence length is 512. The batch sizes for\nthe base and large models are 16 and 8, respectively. Our\nexperiments are run on 4 NVIDIA Tesla V100 32G GPUs.\nBaseline Models Following Lu et al. (2022a), our base-\nlines include (i) Visual question answering (VQA) models\n(Anderson et al., 2018; Kim et al., 2018; Yu et al., 2019;\nGao et al., 2019; Kim et al., 2021; Lu et al., 2021; Li et al.,\n2019); (ii) Text-to-text LM models. (Khashabi et al., 2020);\n(iii) GPT-3.5 models (Chen et al., 2020). More details are\npresented in Appendix B.1.\n5.3. Main Results\nTable 4 shows the main results. Mutimodal-CoT Large out-\nperforms GPT-3.5 by 16.51% (75.17% →91.68%) and sur-\npasses human performance. Speciﬁcally, among the 8\nquestion classes, Mutimodal-CoT Large achieves a 21.37%\n(67.43%→88.80%) performance gain for the questions with\npaired images (IMG). Compared with existing UniﬁedQA\nand GPT-3.5 methods that leverage image captions in the\ncontext to provide vision semantics, the results indicate that\nusing image features is more effective. In addition, our\ntwo-stage framework contributes to the superior results ac-\ncording to our ablation study results in Table 5. Overall,\nthe results verify the effectiveness of multimodality and the\npotential of achieving CoT reasoning with 1B-models via\nour two-stage framework.12345678910405060708090\nEpochAccuracyOne-stage Baseline One-stage Multimodal\nTwo-Stage Baseline Two-Stage Multimodal\nFigure 5. Accuracy curve of the No-CoT baseline and Multimodal-\nCoT variants across epochs.\n6. Analysis\nThe following analysis will investigate how Multimodal-\nCoT works and discuss contribution factors and limitations.\nWe use models under the base size for analysis unless\notherwise stated.\n6.1. Multimodality Boosts Convergence\nFigure 5 shows the evaluation accuracy curve of the baseline\nand Multimodal-CoT in different training epochs. “One-\nstage” is based on the QCM →A input-output format as it', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nachieves the best performance in Table 2 and “Two-stage”\nis our two-stage framework. We ﬁnd that the two-stage\nmethods achieve relatively higher accuracy at the beginning\nthan the one-stage baselines that generate the answer directly\nwithout CoT. However, without the vision features, the two-\nstage baseline could not yield better results as the training\ngoes on due to the low-quality rationales (as observed in\nSection 3). In contrast, using vision features helps generate\nmore effective rationales that contribute to better answer\naccuracy in our two-stage multimodal variant.\n6.2. Using Different Vision Features\nDifferent vision features may affect the model performance.\nWe compare three widely-used types of vision features,\nCLIP (Radford et al., 2021), DETR (Carion et al., 2020),\nand ResNet (He et al., 2016). CLIP and DETR are patch-like\nfeatures where DETR is based on object detection. For the\nResNet features, we repeat the pooled features of ResNet-\n50 to the same length with the text sequence to imitate the\npatch-like features, where each patch is the same as the\npooled image features. More details of the vision features\nare presented in Appendix B.2.\nTable 6. Accuracy (%) of using different vision features.\nMethod One-stage Two-Stage\nw/ CLIP 81.21 84.81\nw/ DETR 82.57 84.91\nw/ ResNet 80.97 84.77\nTable 6 shows the comparative results of vision features. We\nobserve that using vision features generally achieves better\nperformance than the language only baseline. Speciﬁcally,\nDETR achieves relatively better performance in general.\nTherefore, we use DETR by default in Multimodal-CoT.\n6.3. General Effectiveness Across Backbone Models\nTo test the generality of the beneﬁts of our approach to\nother backbone models, we alter the underlying LMs to\nother variants in different sizes or types. As shown in Table\n7, our approach is generally effective for the widely-used\nbackbone models.\nTable 7. Accuracy (%) with different backbone language models.\nMethod Size Language Only Mutimodal-CoT\nUniﬁedQA Base 223M 80.40 84.91\nUniﬁedQA Large 738M 83.60 91.68\nFLAN-T5 Base 248M 83.42 85.85\nFLAN-T5 Large 783M 85.19 93.02\n6.4. Error Analysis\nTo better understand the behavior of Multimodal-CoT and\nfacilitate future studies, we manually investigate randomly\nselected examples generated by our approach. Table 8 sum-marizes the categorization results generated by Multimodal-\nCoT. We randomly picked up 50 samples whose answers\nwere correct and 50 samples whose answers were incor-\nrect. The corresponding examples from each category are\npresented in Appendix C.\nTable 8. Categorization analysis of Multimodal-CoT.\nAnswer CoT Category Percentage (%)\nCorrectCoT is correct 90\nCoT is incorrect 10\nIncorrectCommonsense Mistake 82\nLogical Mistake 12\nCoT is correct 6\nWe ﬁnd that the correct samples (i.e., whose answers are cor-\nrect) contain a certain amount of incorrect chain-of-thought\n(10%). The results indicate that CoT may not always beneﬁt\nthe answer inference, and the model is robust to some extent\n— it can predict the correct answer by ignoring incorrect\nrationales. For incorrect samples (i.e., whose answers are\nincorrect), commonsense mistake in the CoT is the most\nfrequent error type (88%). The model often makes com-\nmonsense mistakes when answering the questions requires\ncommonsense knowledge, e.g., understand maps and count-\ning numbers in the images (Figure 9), and utilizing the\nalphabet (Figure 10). The other type of mistake is a logical\nmistake (12%), with contradictions in the reasoning chains\n(Figure 11). In addition, there are cases with incorrect an-\nswers while their CoT are correct (6%) but might not be\nnecessarily related to answer options (Figure 12).\nThe analysis indicates that there are prospective directions\nfor future studies. It is possible to improve Multimodal-\nCoT by (i) incorporating more informative vision features\nand improving language-vision interaction to be capable of\nunderstanding maps and counting numbers; (ii) injecting\ncommonsense knowledge; (iii) applying a ﬁltering mecha-\nnism, e.g., using only the effective CoT to infer the answer\nand get rid of irrelevant CoT.\n7. Conclusion\nWe formally study the problem of multimodal CoT. We pro-\npose Multimodal-CoT that incorporates language and vision\nmodalities into a two-stage framework that separates ratio-\nnale generation and answer inference, so answer inference\ncan leverage better generated rationales from multimodal in-\nformation. With Multimodal-CoT, we show that our method\nsurpasses GPT-3.5 by 16 percentage points in accuracy on\nthe ScienceQA benchmark. Our error analysis shows that\nit is the potential to leverage more effective vision features,\ninject commonsense knowledge, and apply ﬁltering mecha-\nnisms to improve CoT reasoning in future studies.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nReferences\nAnderson, P., He, X., Buehler, C., Teney, D., Johnson, M.,\nGould, S., and Zhang, L. Bottom-up and top-down atten-\ntion for image captioning and visual question answering.\nIn2018 IEEE Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2018, Salt Lake City, UT, USA,\nJune 18-22, 2018 , pp. 6077–6086. IEEE Computer Soci-\nety, 2018. doi: 10.1109/CVPR.2018.00636.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,\nHenighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,\nJ., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,\nGray, S., Chess, B., Clark, J., Berner, C., McCandlish,\nS., Radford, A., Sutskever, I., and Amodei, D. Language\nmodels are few-shot learners. In Larochelle, H., Ranzato,\nM., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances\nin Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020.\nCarion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov,\nA., and Zagoruyko, S. End-to-end object detection with\ntransformers. In Computer Vision–ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23–28, 2020,\nProceedings, Part I , pp. 213–229, 2020.\nChen, T., Kornblith, S., Swersky, K., Norouzi, M., and\nHinton, G. E. Big self-supervised models are strong\nsemi-supervised learners. In Larochelle, H., Ranzato, M.,\nHadsell, R., Balcan, M., and Lin, H. (eds.), Advances\nin Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020.\nChen, W., Ma, X., Wang, X., and Cohen, W. W. Program\nof thoughts prompting: Disentangling computation from\nreasoning for numerical reasoning tasks. ArXiv preprint ,\nabs/2211.12588, 2022.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton,\nC., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,\nS., Maynez, J., Rao, A., Barnes, P., Tay, Y ., Shazeer,\nN., Prabhakaran, V ., Reif, E., Du, N., Hutchinson, B.,\nPope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G.,\nYin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S.,\nMichalewski, H., Garcia, X., Misra, V ., Robinson, K., Fe-\ndus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph,\nB., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal,\nS., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M.,\nLewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee,\nK., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O.,\nCatasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean,J., Petrov, S., and Fiedel, N. Palm: Scaling language mod-\neling with pathways. ArXiv preprint , abs/2204.02311,\n2022.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y .,\nFedus, W., Li, E., Wang, X., Dehghani, M., Brahma,\nS., et al. Scaling instruction-ﬁnetuned language models.\narXiv preprint arXiv:2210.11416 , 2022.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\nHeigold, G., Gelly, S., et al. An image is worth 16x16\nwords: Transformers for image recognition at scale. In\nThe International Conference on Learning Representa-\ntions (ICLR) , 2021.\nFu, Y ., Peng, H., Sabharwal, A., Clark, P., and Khot, T.\nComplexity-based prompting for multi-step reasoning.\nArXiv preprint , abs/2210.00720, 2022.\nGao, P., Jiang, Z., You, H., Lu, P., Hoi, S. C. H., Wang, X.,\nand Li, H. Dynamic fusion with intra- and inter-modality\nattention ﬂow for visual question answering. In IEEE\nConference on Computer Vision and Pattern Recognition,\nCVPR 2019, Long Beach, CA, USA, June 16-20, 2019 , pp.\n6639–6648. Computer Vision Foundation / IEEE, 2019.\ndoi: 10.1109/CVPR.2019.00680.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\nlearning for image recognition. In 2016 IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2016,\nLas Vegas, NV, USA, June 27-30, 2016 , pp. 770–778.\nIEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.\n90.\nHo, N., Schmid, L., and Yun, S.-Y . Large language models\nare reasoning teachers. arXiv preprint arXiv:2212.10071 ,\n2022.\nJi, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y ., Ishii, E.,\nBang, Y ., Madotto, A., and Fung, P. Survey of halluci-\nnation in natural language generation. ACM Computing\nSurveys , 2022.\nKhashabi, D., Min, S., Khot, T., Sabharwal, A., Tafjord,\nO., Clark, P., and Hajishirzi, H. UNIFIEDQA: Crossing\nformat boundaries with a single QA system. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2020 , pp. 1896–1907, Online, 2020. Association\nfor Computational Linguistics. doi: 10.18653/v1/2020.\nﬁndings-emnlp.171.\nKhot, T., Trivedi, H., Finlayson, M., Fu, Y ., Richardson, K.,\nClark, P., and Sabharwal, A. Decomposed prompting:\nA modular approach for solving complex tasks. ArXiv\npreprint , abs/2210.02406, 2022.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nKim, J., Jun, J., and Zhang, B. Bilinear attention networks.\nIn Bengio, S., Wallach, H. M., Larochelle, H., Grauman,\nK., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances\nin Neural Information Processing Systems 31: Annual\nConference on Neural Information Processing Systems\n2018, NeurIPS 2018, December 3-8, 2018, Montr ´eal,\nCanada , pp. 1571–1581, 2018.\nKim, W., Son, B., and Kim, I. Vilt: Vision-and-language\ntransformer without convolution or region supervision.\nInProceedings of the 38th International Conference on\nMachine Learning (ICML) , pp. 5583–5594, 2021.\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y ., and Iwasawa,\nY . Large language models are zero-shot reasoners. ArXiv\npreprint , abs/2205.11916, 2022.\nLi, B., Lv, C., Zhou, Z., Zhou, T., Xiao, T., Ma, A., and Zhu,\nJ. On vision features in multimodal machine translation.\nInProceedings of the 60th Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: Long\nPapers) , pp. 6327–6337, 2022a.\nLi, L. H., Yatskar, M., Yin, D., Hsieh, C.-J., and Chang,\nK.-W. Visualbert: A simple and performant baseline for\nvision and language. ArXiv preprint , abs/1908.03557,\n2019.\nLi, Y ., Lin, Z., Zhang, S., Fu, Q., Chen, B., Lou, J.-G., and\nChen, W. On the advance of making language models\nbetter reasoners. ArXiv preprint , abs/2206.02336, 2022b.\nLu, P., Qiu, L., Chen, J., Xia, T., Zhao, Y ., Zhang, W., Yu,\nZ., Liang, X., and Zhu, S.-C. Iconqa: A new benchmark\nfor abstract diagram understanding and visual language\nreasoning. In The 35th Conference on Neural Information\nProcessing Systems (NeurIPS) Track on Datasets and\nBenchmarks , 2021.\nLu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu,\nS.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to\nexplain: Multimodal reasoning via thought chains for sci-\nence question answering. ArXiv preprint , abs/2209.09513,\n2022a.\nLu, P., Qiu, L., Chang, K.-W., Wu, Y . N., Zhu, S.-C., Ra-\njpurohit, T., Clark, P., and Kalyan, A. Dynamic prompt\nlearning via policy gradient for semi-structured mathemat-\nical reasoning. ArXiv preprint , abs/2209.14610, 2022b.\nMagister, L. C., Mallinson, J., Adamek, J., Malmi, E., and\nSeveryn, A. Teaching small language models to reason.\nArXiv preprint , abs/2212.08410, 2022.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\net al. Learning transferable visual models from natural\nlanguage supervision. In International Conference on\nMachine Learning , pp. 8748–8763. PMLR, 2021.Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J.,\nSong, F., Aslanides, J., Henderson, S., Ring, R., Young,\nS., Rutherford, E., Hennigan, T., Menick, J., Cassirer, A.,\nPowell, R., Driessche, G. v. d., Hendricks, L. A., Rauh,\nM., Huang, P.-S., Glaese, A., Welbl, J., Dathathri, S.,\nHuang, S., Uesato, J., Mellor, J., Higgins, I., Creswell,\nA., McAleese, N., Wu, A., Elsen, E., Jayakumar, S.,\nBuchatskaya, E., Budden, D., Sutherland, E., Simonyan,\nK., Paganini, M., Sifre, L., Martens, L., Li, X. L., Kun-\ncoro, A., Nematzadeh, A., Gribovskaya, E., Donato, D.,\nLazaridou, A., Mensch, A., Lespiau, J.-B., Tsimpoukelli,\nM., Grigorev, N., Fritz, D., Sottiaux, T., Pajarskas, M.,\nPohlen, T., Gong, Z., Toyama, D., d’Autume, C. d. M.,\nLi, Y ., Terzi, T., Mikulik, V ., Babuschkin, I., Clark, A.,\nCasas, D. d. L., Guy, A., Jones, C., Bradbury, J., Johnson,\nM., Hechtman, B., Weidinger, L., Gabriel, I., Isaac, W.,\nLockhart, E., Osindero, S., Rimell, L., Dyer, C., Vinyals,\nO., Ayoub, K., Stanway, J., Bennett, L., Hassabis, D.,\nKavukcuoglu, K., and Irving, G. Scaling language mod-\nels: Methods, analysis & insights from training gopher.\nArXiv preprint , abs/2112.11446, 2021.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring the\nlimits of transfer learning with a uniﬁed text-to-text trans-\nformer. Journal of Machine Learning Research (JMLR) ,\n21:1–67, 2020.\nRubin, O., Herzig, J., and Berant, J. Learning to re-\ntrieve prompts for in-context learning. In Proceedings\nof the 2022 Conference of the North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies , pp. 2655–2671, 2022. doi:\n10.18653/v1/2022.naacl-main.191.\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kul-\nshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L.,\nDu, Y ., Li, Y ., Lee, H., Zheng, H. S., Ghafouri, A., Mene-\ngali, M., Huang, Y ., Krikun, M., Lepikhin, D., Qin, J.,\nChen, D., Xu, Y ., Chen, Z., Roberts, A., Bosma, M.,\nZhao, V ., Zhou, Y ., Chang, C.-C., Krivokon, I., Rusch,\nW., Pickett, M., Srinivasan, P., Man, L., Meier-Hellstern,\nK., Morris, M. R., Doshi, T., Santos, R. D., Duke, T.,\nSoraker, J., Zevenbergen, B., Prabhakaran, V ., Diaz, M.,\nHutchinson, B., Olson, K., Molina, A., Hoffman-John, E.,\nLee, J., Aroyo, L., Rajakumar, R., Butryna, A., Lamm,\nM., Kuzmina, V ., Fenton, J., Cohen, A., Bernstein, R.,\nKurzweil, R., Aguera-Arcas, B., Cui, C., Croak, M., Chi,\nE., and Le, Q. Lamda: Language models for dialog\napplications. ArXiv preprint , abs/2201.08239, 2022.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need. In Guyon, I., von Luxburg, U., Bengio,\nS., Wallach, H. M., Fergus, R., Vishwanathan, S. V . N.,\nand Garnett, R. (eds.), Advances in Neural Information', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nProcessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA , pp. 5998–6008, 2017.\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E.,\nand Zhou, D. Self-consistency improves chain of\nthought reasoning in language models. ArXiv preprint ,\nabs/2203.11171, 2022a.\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and\nZhou, D. Rationale-augmented ensembles in language\nmodels. ArXiv preprint , abs/2207.00747, 2022b.\nWei, J., Tay, Y ., Bommasani, R., Raffel, C., Zoph, B.,\nBorgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Met-\nzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang,\nP., Dean, J., and Fedus, W. Emergent abilities of large\nlanguage models. Transactions on Machine Learning\nResearch , 2022a. Survey Certiﬁcation.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E.,\nLe, Q., and Zhou, D. Chain of thought prompting elic-\nits reasoning in large language models. ArXiv preprint ,\nabs/2201.11903, 2022b.\nWu, Z., Kong, L., Bi, W., Li, X., and Kao, B. Good for\nmisconceived reasons: An empirical revisiting on the\nneed for visual context in multimodal machine transla-\ntion. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers) , pp. 6153–6166, Online,\n2021. Association for Computational Linguistics. doi:\n10.18653/v1/2021.acl-long.480.\nYu, Z., Yu, J., Cui, Y ., Tao, D., and Tian, Q. Deep modu-\nlar co-attention networks for visual question answering.\nInIEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2019, Long Beach, CA, USA, June\n16-20, 2019 , pp. 6281–6290. Computer Vision Founda-\ntion / IEEE, 2019. doi: 10.1109/CVPR.2019.00644.\nZhang, Z., Chen, K., Wang, R., Utiyama, M., Sumita, E., Li,\nZ., and Zhao, H. Neural machine translation with univer-\nsal visual representation. In 8th International Conference\non Learning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020 . OpenReview.net, 2020.\nZhang, Z., Zhang, A., Li, M., and Smola, A. Automatic\nchain of thought prompting in large language models.\nArXiv preprint , abs/2210.03493, 2022.\nZhang, Z., Chen, K., Wang, R., Utiyama, M., Sumita, E., Li,\nZ., and Zhao, H. Universal multimodal representation for\nlanguage understanding. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence , pp. 1–18, 2023. doi:\n10.1109/TPAMI.2023.3234170.Zhou, D., Sch ¨arli, N., Hou, L., Wei, J., Scales, N., Wang,\nX., Schuurmans, D., Bousquet, O., Le, Q., and Chi, E.\nLeast-to-most prompting enables complex reasoning in\nlarge language models. ArXiv preprint , abs/2205.10625,\n2022.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nA. Extended analysis for the challenge of Multimodal-CoT\nA.1. More Examples of Misleading by Hallucinated Rationales\nAccording to our case studies (Section 3.2), we ﬁnd that the baseline tends to generate hallucinated rationales. We provide\nfurther examples as shown in Figure 6.\nOptions:(B) lichen(A) mushroomProblem\nBaselineQuestion:Basedonthearrows,whichofthefollowingorganismsisaconsumer?Context:BelowisafoodwebfromatundraecosysteminNunavut,aterritoryinNorthernCanada.Afoodwebmodelshowthemattereatenbyorganismsmovesthroughanecosystem.Thearrowsinafoodwebrepresenthowmattermovesbetweenorganismsinanecosystem.GoldRationale:Afoodwebisamodel…Anorganisminafoodwebcanhavemorethanonearrowpointingfromit.Thisshowsthattheorganismiseatenbymorethanoneotherorganisminthefoodweb.Anorganisminafoodwebcanalsohavemorethanonearrowpointingtoit.Thisshowsthattheorganismeatsmorethanoneotherorganisminthefoodweb.Consumerseatotherorganisms.So,therearearrowsinafoodwebthatpointfromotherorganismstoconsumers.Themushroomhasarrowspointingtoitfromthebarren-groundcaribouandthegrizzlybear.So,themushroomisaconsumer.Thelichendoesnothaveanyarrowspointingtoit.So,thelichenisaproducer,notaconsumer.Answer:Theansweris(A).Rationale:Afoodwebisamodel…Thisshowsthattheorganismiseatenbymorethanoneotherorganisminthefoodweb.Anorganisminafoodwebcanalsohavemorethanonearrowpointingtoit.Thisshowsthattheorganismeatsmorethanoneotherorganisminthefoodweb.Consumerseatproducers.So,inafoodweb,consumershavearrowspointingtothemfromproducers.Themushroomhasanarrowpointingtoitfromthemushroom.Themushroomisnotaconsumer,sothemushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Answer:Theansweris(B).Rationale:Afoodwebisamodel…Anorganisminafoodwebcanhavemorethanonearrowpointingfromit.Thisshowsthattheorganismiseatenbymorethanoneotherorganisminthefoodweb.Anorganisminafoodwebcanalsohavemorethanonearrowpointingtoit.Thisshowsthattheorganismeatsmorethanoneotherorganisminthefoodweb.Consumerseatproducersorganisms.So,inarearrowsafoodwebthatpointfromotherorganismstoconsumers.Themushroomhasarrowspointingtoitfromthen-browmushroomibou.thelichenzzlybear.So,themushroomisaconsumer.Thelichenhasnothaveanyarrowspointingtoit.So,themushroomisaconsumer.Lichenisnotaconsumer.Answer:Theansweris(B).Vision\n+ Vision Features\nOptions:(B) sample A(A) neither; the samples have the same temperatureProblem\nBaselineQuestion:Comparetheaveragekineticenergiesoftheparticlesineachsample.Whichsamplehasthehighertemperature?Context:Thediagramsbelowshowtwopuresamplesofgasinidenticalclosed,rigidcontainers.Eachcoloredballrepresentsonegasparticle.Bothsampleshavethesamenumberofparticles.GoldRationale:Thetemperatureofasubstancedependsontheaveragekineticenergyoftheparticlesinthesubstance.Thehighertheaveragekineticenergyoftheparticles,thehigherthetemperatureofthesubstance.Thekineticenergyofaparticleisdeterminedbyitsmassandspeed.Forapuresubstance,thegreaterthemassofeachparticleinthesubstanceandthehighertheaveragespeedoftheparticles,thehighertheiraveragekineticenergy.EachparticleinsampleAhasmoremassthaneachparticleinsampleB.TheparticlesinsampleAalso+ Vision FeaturesVision(C) sample B\nhaveahigheraveragespeedthantheparticlesinsampleB.So,theparticlesinsampleAhaveahigheraveragekineticenergythantheparticlesinsampleB.BecausetheparticlesinsampleAhavethehigheraveragekineticenergy,sampleAmusthavethehighertemperature.Answer:Theansweris(B).Rationale:Thetemperatureofasubstancedependsontheaveragekineticenergyoftheparticlesinthesubstance.Thehighertheaveragekineticenergyoftheparticles,thehigherthetemperatureofthesubstance.Thekineticenergyofaparticleisdeterminedbyitsmassandspeed.Forapuresubstance,thegreaterthemassofeachparticleinthesubstanceandthehighertheaveragespeedoftheparticles,thehighertheiraveragekineticenergy.EachparticleinsampleBhasmoremassthaneachparticleinsampleA.TheparticlesinsampleBalsohaveahigheraveragespeedthantheparticlesinsampleA.So,theparticlesinsampleBhaveahigheraveragekineticenergythantheparticlesinsampleA.BecausetheparticlesinsampleBhavethehigheraveragekineticenergy,sampleBmusthavethehighertemperature.Answer:Theansweris(C).Rationale:Thetemperatureofasubstancedependsontheaveragekineticenergyoftheparticlesinthesubstance.Thehighertheaveragekineticenergyoftheparticles,thehigherthetemperatureofthesubstance.Thekineticenergyofaparticleisdeterminedbyitsmassandspeed.Forapuresubstance,thegreaterthemassofeachparticleinthesubstanceandthehighertheaveragespeedoftheparticles,thehighertheiraveragekineticenergy.EachparticleinsampleAhasmoremassthaneachparticleinsampleB.TheparticlesinsampleAalsohaveahigheraveragespeedthantheparticlesinsampleB.So,theparticlesinsampleAhaveahigheraveragekineticenergythantheparticlesinsampleB.BecausetheparticlesinsampleAhavethehigheraveragekineticenergy,sampleAmusthavethehighertemperature.Answer:Theansweris(B).\nFigure 6. Examples of the two-stage framework without vision features (baseline) and with vision features (ours) for generating rationales\nand predicting answers. The upper part presents the problem details, and the lower part shows the outputs of the baseline and our method.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}]: %s
2023-12-07 12:21:38,096 - INFO - Check the results [{'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nTable 4. Main results (%). Size = backbone model size. Question classes: NAT = natural science, SOC = social science, LAN = language\nscience, TXT = text context, IMG = image context, NO = no context, G1-6 = grades 1-6, G7-12 = grades 7-12. Results except ours are\ntaken from Lu et al. (2022a). Segment 1: Human performance; Segment 2: VQA baselines; Segment 3: UniﬁedQA baselines; Segment 4:\nGPT-3.5 baselines; Segment 5: Our Multimodal-CoT results. Results in bold are the best performance.\nModel Size NAT SOC LAN TXT IMG NO G1-6 G7-12 Avg\nHuman -90.23 84.97 87.48 89.60 87.50 88.10 91.59 82.42 88.40\nMCAN (Yu et al., 2019) 95M 56.08 46.23 58.09 59.43 51.17 55.40 51.65 59.72 54.54\nTop-Down (Anderson et al., 2018) 70M 59.50 54.33 61.82 62.90 54.88 59.79 57.27 62.16 59.02\nBAN (Kim et al., 2018) 112M 60.88 46.57 66.64 62.61 52.60 65.51 56.83 63.94 59.37\nDFAF (Gao et al., 2019) 74M 64.03 48.82 63.55 65.88 54.49 64.11 57.12 67.17 60.72\nViLT (Kim et al., 2021) 113M 60.48 63.89 60.27 63.20 61.38 57.00 60.72 61.90 61.14\nPatch-TRM (Lu et al., 2021) 90M 65.19 46.79 65.55 66.96 55.28 64.95 58.04 67.50 61.42\nVisualBERT (Li et al., 2019) 111M 59.33 69.18 61.18 62.71 62.17 58.54 62.96 59.92 61.87\nUniﬁedQA Base (Khashabi et al., 2020) 223M 68.16 69.18 74.91 63.78 61.38 77.84 72.98 65.00 70.12\nUniﬁedQA Base w/ CoT (Lu et al., 2022a) 223M 71.00 76.04 78.91 66.42 66.53 81.81 77.06 68.82 74.11\nGPT-3.5 (Chen et al., 2020) 175B 74.64 69.74 76.00 74.44 67.28 77.42 76.80 68.89 73.97\nGPT-3.5 w/ CoT (Lu et al., 2022a) 175B 75.44 70.87 78.09 74.68 67.43 79.93 78.23 69.68 75.17\nMutimodal-CoT Base 223M 87.52 77.17 85.82 87.88 82.90 86.83 84.65 85.37 84.91\nMutimodal-CoT Large 738M 95.91 82.00 90.82 95.26 88.80 92.89 92.44 90.31 91.68\nTable 5. Ablation results of Multimodal-CoT.\nModel NAT SOC LAN TXT IMG NO G1-6 G7-12 Avg\nMultimodal-CoT 87.52 77.17 85.82 87.88 82.90 86.83 84.65 85.37 84.91\nw/o Two-Stage Framework 80.99 87.40 81.91 80.25 78.83 83.62 82.78 82.20 82.57\nw/o Vision Features 71.09 70.75 69.18 71.16 65.84 71.57 71.00 69.68 70.53\nmaximum input sequence length is 512. The batch sizes for\nthe base and large models are 16 and 8, respectively. Our\nexperiments are run on 4 NVIDIA Tesla V100 32G GPUs.\nBaseline Models Following Lu et al. (2022a), our base-\nlines include (i) Visual question answering (VQA) models\n(Anderson et al., 2018; Kim et al., 2018; Yu et al., 2019;\nGao et al., 2019; Kim et al., 2021; Lu et al., 2021; Li et al.,\n2019); (ii) Text-to-text LM models. (Khashabi et al., 2020);\n(iii) GPT-3.5 models (Chen et al., 2020). More details are\npresented in Appendix B.1.\n5.3. Main Results\nTable 4 shows the main results. Mutimodal-CoT Large out-\nperforms GPT-3.5 by 16.51% (75.17% →91.68%) and sur-\npasses human performance. Speciﬁcally, among the 8\nquestion classes, Mutimodal-CoT Large achieves a 21.37%\n(67.43%→88.80%) performance gain for the questions with\npaired images (IMG). Compared with existing UniﬁedQA\nand GPT-3.5 methods that leverage image captions in the\ncontext to provide vision semantics, the results indicate that\nusing image features is more effective. In addition, our\ntwo-stage framework contributes to the superior results ac-\ncording to our ablation study results in Table 5. Overall,\nthe results verify the effectiveness of multimodality and the\npotential of achieving CoT reasoning with 1B-models via\nour two-stage framework.12345678910405060708090\nEpochAccuracyOne-stage Baseline One-stage Multimodal\nTwo-Stage Baseline Two-Stage Multimodal\nFigure 5. Accuracy curve of the No-CoT baseline and Multimodal-\nCoT variants across epochs.\n6. Analysis\nThe following analysis will investigate how Multimodal-\nCoT works and discuss contribution factors and limitations.\nWe use models under the base size for analysis unless\notherwise stated.\n6.1. Multimodality Boosts Convergence\nFigure 5 shows the evaluation accuracy curve of the baseline\nand Multimodal-CoT in different training epochs. “One-\nstage” is based on the QCM →A input-output format as it', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nachieves the best performance in Table 2 and “Two-stage”\nis our two-stage framework. We ﬁnd that the two-stage\nmethods achieve relatively higher accuracy at the beginning\nthan the one-stage baselines that generate the answer directly\nwithout CoT. However, without the vision features, the two-\nstage baseline could not yield better results as the training\ngoes on due to the low-quality rationales (as observed in\nSection 3). In contrast, using vision features helps generate\nmore effective rationales that contribute to better answer\naccuracy in our two-stage multimodal variant.\n6.2. Using Different Vision Features\nDifferent vision features may affect the model performance.\nWe compare three widely-used types of vision features,\nCLIP (Radford et al., 2021), DETR (Carion et al., 2020),\nand ResNet (He et al., 2016). CLIP and DETR are patch-like\nfeatures where DETR is based on object detection. For the\nResNet features, we repeat the pooled features of ResNet-\n50 to the same length with the text sequence to imitate the\npatch-like features, where each patch is the same as the\npooled image features. More details of the vision features\nare presented in Appendix B.2.\nTable 6. Accuracy (%) of using different vision features.\nMethod One-stage Two-Stage\nw/ CLIP 81.21 84.81\nw/ DETR 82.57 84.91\nw/ ResNet 80.97 84.77\nTable 6 shows the comparative results of vision features. We\nobserve that using vision features generally achieves better\nperformance than the language only baseline. Speciﬁcally,\nDETR achieves relatively better performance in general.\nTherefore, we use DETR by default in Multimodal-CoT.\n6.3. General Effectiveness Across Backbone Models\nTo test the generality of the beneﬁts of our approach to\nother backbone models, we alter the underlying LMs to\nother variants in different sizes or types. As shown in Table\n7, our approach is generally effective for the widely-used\nbackbone models.\nTable 7. Accuracy (%) with different backbone language models.\nMethod Size Language Only Mutimodal-CoT\nUniﬁedQA Base 223M 80.40 84.91\nUniﬁedQA Large 738M 83.60 91.68\nFLAN-T5 Base 248M 83.42 85.85\nFLAN-T5 Large 783M 85.19 93.02\n6.4. Error Analysis\nTo better understand the behavior of Multimodal-CoT and\nfacilitate future studies, we manually investigate randomly\nselected examples generated by our approach. Table 8 sum-marizes the categorization results generated by Multimodal-\nCoT. We randomly picked up 50 samples whose answers\nwere correct and 50 samples whose answers were incor-\nrect. The corresponding examples from each category are\npresented in Appendix C.\nTable 8. Categorization analysis of Multimodal-CoT.\nAnswer CoT Category Percentage (%)\nCorrectCoT is correct 90\nCoT is incorrect 10\nIncorrectCommonsense Mistake 82\nLogical Mistake 12\nCoT is correct 6\nWe ﬁnd that the correct samples (i.e., whose answers are cor-\nrect) contain a certain amount of incorrect chain-of-thought\n(10%). The results indicate that CoT may not always beneﬁt\nthe answer inference, and the model is robust to some extent\n— it can predict the correct answer by ignoring incorrect\nrationales. For incorrect samples (i.e., whose answers are\nincorrect), commonsense mistake in the CoT is the most\nfrequent error type (88%). The model often makes com-\nmonsense mistakes when answering the questions requires\ncommonsense knowledge, e.g., understand maps and count-\ning numbers in the images (Figure 9), and utilizing the\nalphabet (Figure 10). The other type of mistake is a logical\nmistake (12%), with contradictions in the reasoning chains\n(Figure 11). In addition, there are cases with incorrect an-\nswers while their CoT are correct (6%) but might not be\nnecessarily related to answer options (Figure 12).\nThe analysis indicates that there are prospective directions\nfor future studies. It is possible to improve Multimodal-\nCoT by (i) incorporating more informative vision features\nand improving language-vision interaction to be capable of\nunderstanding maps and counting numbers; (ii) injecting\ncommonsense knowledge; (iii) applying a ﬁltering mecha-\nnism, e.g., using only the effective CoT to infer the answer\nand get rid of irrelevant CoT.\n7. Conclusion\nWe formally study the problem of multimodal CoT. We pro-\npose Multimodal-CoT that incorporates language and vision\nmodalities into a two-stage framework that separates ratio-\nnale generation and answer inference, so answer inference\ncan leverage better generated rationales from multimodal in-\nformation. With Multimodal-CoT, we show that our method\nsurpasses GPT-3.5 by 16 percentage points in accuracy on\nthe ScienceQA benchmark. Our error analysis shows that\nit is the potential to leverage more effective vision features,\ninject commonsense knowledge, and apply ﬁltering mecha-\nnisms to improve CoT reasoning in future studies.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nReferences\nAnderson, P., He, X., Buehler, C., Teney, D., Johnson, M.,\nGould, S., and Zhang, L. Bottom-up and top-down atten-\ntion for image captioning and visual question answering.\nIn2018 IEEE Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2018, Salt Lake City, UT, USA,\nJune 18-22, 2018 , pp. 6077–6086. IEEE Computer Soci-\nety, 2018. doi: 10.1109/CVPR.2018.00636.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,\nHenighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,\nJ., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,\nGray, S., Chess, B., Clark, J., Berner, C., McCandlish,\nS., Radford, A., Sutskever, I., and Amodei, D. Language\nmodels are few-shot learners. In Larochelle, H., Ranzato,\nM., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances\nin Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020.\nCarion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov,\nA., and Zagoruyko, S. End-to-end object detection with\ntransformers. In Computer Vision–ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23–28, 2020,\nProceedings, Part I , pp. 213–229, 2020.\nChen, T., Kornblith, S., Swersky, K., Norouzi, M., and\nHinton, G. E. Big self-supervised models are strong\nsemi-supervised learners. In Larochelle, H., Ranzato, M.,\nHadsell, R., Balcan, M., and Lin, H. (eds.), Advances\nin Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020.\nChen, W., Ma, X., Wang, X., and Cohen, W. W. Program\nof thoughts prompting: Disentangling computation from\nreasoning for numerical reasoning tasks. ArXiv preprint ,\nabs/2211.12588, 2022.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton,\nC., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,\nS., Maynez, J., Rao, A., Barnes, P., Tay, Y ., Shazeer,\nN., Prabhakaran, V ., Reif, E., Du, N., Hutchinson, B.,\nPope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G.,\nYin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S.,\nMichalewski, H., Garcia, X., Misra, V ., Robinson, K., Fe-\ndus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph,\nB., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal,\nS., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M.,\nLewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee,\nK., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O.,\nCatasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean,J., Petrov, S., and Fiedel, N. Palm: Scaling language mod-\neling with pathways. ArXiv preprint , abs/2204.02311,\n2022.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y .,\nFedus, W., Li, E., Wang, X., Dehghani, M., Brahma,\nS., et al. Scaling instruction-ﬁnetuned language models.\narXiv preprint arXiv:2210.11416 , 2022.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\nHeigold, G., Gelly, S., et al. An image is worth 16x16\nwords: Transformers for image recognition at scale. In\nThe International Conference on Learning Representa-\ntions (ICLR) , 2021.\nFu, Y ., Peng, H., Sabharwal, A., Clark, P., and Khot, T.\nComplexity-based prompting for multi-step reasoning.\nArXiv preprint , abs/2210.00720, 2022.\nGao, P., Jiang, Z., You, H., Lu, P., Hoi, S. C. H., Wang, X.,\nand Li, H. Dynamic fusion with intra- and inter-modality\nattention ﬂow for visual question answering. In IEEE\nConference on Computer Vision and Pattern Recognition,\nCVPR 2019, Long Beach, CA, USA, June 16-20, 2019 , pp.\n6639–6648. Computer Vision Foundation / IEEE, 2019.\ndoi: 10.1109/CVPR.2019.00680.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\nlearning for image recognition. In 2016 IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2016,\nLas Vegas, NV, USA, June 27-30, 2016 , pp. 770–778.\nIEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.\n90.\nHo, N., Schmid, L., and Yun, S.-Y . Large language models\nare reasoning teachers. arXiv preprint arXiv:2212.10071 ,\n2022.\nJi, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y ., Ishii, E.,\nBang, Y ., Madotto, A., and Fung, P. Survey of halluci-\nnation in natural language generation. ACM Computing\nSurveys , 2022.\nKhashabi, D., Min, S., Khot, T., Sabharwal, A., Tafjord,\nO., Clark, P., and Hajishirzi, H. UNIFIEDQA: Crossing\nformat boundaries with a single QA system. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2020 , pp. 1896–1907, Online, 2020. Association\nfor Computational Linguistics. doi: 10.18653/v1/2020.\nﬁndings-emnlp.171.\nKhot, T., Trivedi, H., Finlayson, M., Fu, Y ., Richardson, K.,\nClark, P., and Sabharwal, A. Decomposed prompting:\nA modular approach for solving complex tasks. ArXiv\npreprint , abs/2210.02406, 2022.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nKim, J., Jun, J., and Zhang, B. Bilinear attention networks.\nIn Bengio, S., Wallach, H. M., Larochelle, H., Grauman,\nK., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances\nin Neural Information Processing Systems 31: Annual\nConference on Neural Information Processing Systems\n2018, NeurIPS 2018, December 3-8, 2018, Montr ´eal,\nCanada , pp. 1571–1581, 2018.\nKim, W., Son, B., and Kim, I. Vilt: Vision-and-language\ntransformer without convolution or region supervision.\nInProceedings of the 38th International Conference on\nMachine Learning (ICML) , pp. 5583–5594, 2021.\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y ., and Iwasawa,\nY . Large language models are zero-shot reasoners. ArXiv\npreprint , abs/2205.11916, 2022.\nLi, B., Lv, C., Zhou, Z., Zhou, T., Xiao, T., Ma, A., and Zhu,\nJ. On vision features in multimodal machine translation.\nInProceedings of the 60th Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: Long\nPapers) , pp. 6327–6337, 2022a.\nLi, L. H., Yatskar, M., Yin, D., Hsieh, C.-J., and Chang,\nK.-W. Visualbert: A simple and performant baseline for\nvision and language. ArXiv preprint , abs/1908.03557,\n2019.\nLi, Y ., Lin, Z., Zhang, S., Fu, Q., Chen, B., Lou, J.-G., and\nChen, W. On the advance of making language models\nbetter reasoners. ArXiv preprint , abs/2206.02336, 2022b.\nLu, P., Qiu, L., Chen, J., Xia, T., Zhao, Y ., Zhang, W., Yu,\nZ., Liang, X., and Zhu, S.-C. Iconqa: A new benchmark\nfor abstract diagram understanding and visual language\nreasoning. In The 35th Conference on Neural Information\nProcessing Systems (NeurIPS) Track on Datasets and\nBenchmarks , 2021.\nLu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu,\nS.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to\nexplain: Multimodal reasoning via thought chains for sci-\nence question answering. ArXiv preprint , abs/2209.09513,\n2022a.\nLu, P., Qiu, L., Chang, K.-W., Wu, Y . N., Zhu, S.-C., Ra-\njpurohit, T., Clark, P., and Kalyan, A. Dynamic prompt\nlearning via policy gradient for semi-structured mathemat-\nical reasoning. ArXiv preprint , abs/2209.14610, 2022b.\nMagister, L. C., Mallinson, J., Adamek, J., Malmi, E., and\nSeveryn, A. Teaching small language models to reason.\nArXiv preprint , abs/2212.08410, 2022.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\net al. Learning transferable visual models from natural\nlanguage supervision. In International Conference on\nMachine Learning , pp. 8748–8763. PMLR, 2021.Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J.,\nSong, F., Aslanides, J., Henderson, S., Ring, R., Young,\nS., Rutherford, E., Hennigan, T., Menick, J., Cassirer, A.,\nPowell, R., Driessche, G. v. d., Hendricks, L. A., Rauh,\nM., Huang, P.-S., Glaese, A., Welbl, J., Dathathri, S.,\nHuang, S., Uesato, J., Mellor, J., Higgins, I., Creswell,\nA., McAleese, N., Wu, A., Elsen, E., Jayakumar, S.,\nBuchatskaya, E., Budden, D., Sutherland, E., Simonyan,\nK., Paganini, M., Sifre, L., Martens, L., Li, X. L., Kun-\ncoro, A., Nematzadeh, A., Gribovskaya, E., Donato, D.,\nLazaridou, A., Mensch, A., Lespiau, J.-B., Tsimpoukelli,\nM., Grigorev, N., Fritz, D., Sottiaux, T., Pajarskas, M.,\nPohlen, T., Gong, Z., Toyama, D., d’Autume, C. d. M.,\nLi, Y ., Terzi, T., Mikulik, V ., Babuschkin, I., Clark, A.,\nCasas, D. d. L., Guy, A., Jones, C., Bradbury, J., Johnson,\nM., Hechtman, B., Weidinger, L., Gabriel, I., Isaac, W.,\nLockhart, E., Osindero, S., Rimell, L., Dyer, C., Vinyals,\nO., Ayoub, K., Stanway, J., Bennett, L., Hassabis, D.,\nKavukcuoglu, K., and Irving, G. Scaling language mod-\nels: Methods, analysis & insights from training gopher.\nArXiv preprint , abs/2112.11446, 2021.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring the\nlimits of transfer learning with a uniﬁed text-to-text trans-\nformer. Journal of Machine Learning Research (JMLR) ,\n21:1–67, 2020.\nRubin, O., Herzig, J., and Berant, J. Learning to re-\ntrieve prompts for in-context learning. In Proceedings\nof the 2022 Conference of the North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies , pp. 2655–2671, 2022. doi:\n10.18653/v1/2022.naacl-main.191.\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kul-\nshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L.,\nDu, Y ., Li, Y ., Lee, H., Zheng, H. S., Ghafouri, A., Mene-\ngali, M., Huang, Y ., Krikun, M., Lepikhin, D., Qin, J.,\nChen, D., Xu, Y ., Chen, Z., Roberts, A., Bosma, M.,\nZhao, V ., Zhou, Y ., Chang, C.-C., Krivokon, I., Rusch,\nW., Pickett, M., Srinivasan, P., Man, L., Meier-Hellstern,\nK., Morris, M. R., Doshi, T., Santos, R. D., Duke, T.,\nSoraker, J., Zevenbergen, B., Prabhakaran, V ., Diaz, M.,\nHutchinson, B., Olson, K., Molina, A., Hoffman-John, E.,\nLee, J., Aroyo, L., Rajakumar, R., Butryna, A., Lamm,\nM., Kuzmina, V ., Fenton, J., Cohen, A., Bernstein, R.,\nKurzweil, R., Aguera-Arcas, B., Cui, C., Croak, M., Chi,\nE., and Le, Q. Lamda: Language models for dialog\napplications. ArXiv preprint , abs/2201.08239, 2022.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need. In Guyon, I., von Luxburg, U., Bengio,\nS., Wallach, H. M., Fergus, R., Vishwanathan, S. V . N.,\nand Garnett, R. (eds.), Advances in Neural Information', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nProcessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA , pp. 5998–6008, 2017.\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E.,\nand Zhou, D. Self-consistency improves chain of\nthought reasoning in language models. ArXiv preprint ,\nabs/2203.11171, 2022a.\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and\nZhou, D. Rationale-augmented ensembles in language\nmodels. ArXiv preprint , abs/2207.00747, 2022b.\nWei, J., Tay, Y ., Bommasani, R., Raffel, C., Zoph, B.,\nBorgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Met-\nzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang,\nP., Dean, J., and Fedus, W. Emergent abilities of large\nlanguage models. Transactions on Machine Learning\nResearch , 2022a. Survey Certiﬁcation.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E.,\nLe, Q., and Zhou, D. Chain of thought prompting elic-\nits reasoning in large language models. ArXiv preprint ,\nabs/2201.11903, 2022b.\nWu, Z., Kong, L., Bi, W., Li, X., and Kao, B. Good for\nmisconceived reasons: An empirical revisiting on the\nneed for visual context in multimodal machine transla-\ntion. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers) , pp. 6153–6166, Online,\n2021. Association for Computational Linguistics. doi:\n10.18653/v1/2021.acl-long.480.\nYu, Z., Yu, J., Cui, Y ., Tao, D., and Tian, Q. Deep modu-\nlar co-attention networks for visual question answering.\nInIEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2019, Long Beach, CA, USA, June\n16-20, 2019 , pp. 6281–6290. Computer Vision Founda-\ntion / IEEE, 2019. doi: 10.1109/CVPR.2019.00644.\nZhang, Z., Chen, K., Wang, R., Utiyama, M., Sumita, E., Li,\nZ., and Zhao, H. Neural machine translation with univer-\nsal visual representation. In 8th International Conference\non Learning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020 . OpenReview.net, 2020.\nZhang, Z., Zhang, A., Li, M., and Smola, A. Automatic\nchain of thought prompting in large language models.\nArXiv preprint , abs/2210.03493, 2022.\nZhang, Z., Chen, K., Wang, R., Utiyama, M., Sumita, E., Li,\nZ., and Zhao, H. Universal multimodal representation for\nlanguage understanding. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence , pp. 1–18, 2023. doi:\n10.1109/TPAMI.2023.3234170.Zhou, D., Sch ¨arli, N., Hou, L., Wei, J., Scales, N., Wang,\nX., Schuurmans, D., Bousquet, O., Le, Q., and Chi, E.\nLeast-to-most prompting enables complex reasoning in\nlarge language models. ArXiv preprint , abs/2205.10625,\n2022.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nA. Extended analysis for the challenge of Multimodal-CoT\nA.1. More Examples of Misleading by Hallucinated Rationales\nAccording to our case studies (Section 3.2), we ﬁnd that the baseline tends to generate hallucinated rationales. We provide\nfurther examples as shown in Figure 6.\nOptions:(B) lichen(A) mushroomProblem\nBaselineQuestion:Basedonthearrows,whichofthefollowingorganismsisaconsumer?Context:BelowisafoodwebfromatundraecosysteminNunavut,aterritoryinNorthernCanada.Afoodwebmodelshowthemattereatenbyorganismsmovesthroughanecosystem.Thearrowsinafoodwebrepresenthowmattermovesbetweenorganismsinanecosystem.GoldRationale:Afoodwebisamodel…Anorganisminafoodwebcanhavemorethanonearrowpointingfromit.Thisshowsthattheorganismiseatenbymorethanoneotherorganisminthefoodweb.Anorganisminafoodwebcanalsohavemorethanonearrowpointingtoit.Thisshowsthattheorganismeatsmorethanoneotherorganisminthefoodweb.Consumerseatotherorganisms.So,therearearrowsinafoodwebthatpointfromotherorganismstoconsumers.Themushroomhasarrowspointingtoitfromthebarren-groundcaribouandthegrizzlybear.So,themushroomisaconsumer.Thelichendoesnothaveanyarrowspointingtoit.So,thelichenisaproducer,notaconsumer.Answer:Theansweris(A).Rationale:Afoodwebisamodel…Thisshowsthattheorganismiseatenbymorethanoneotherorganisminthefoodweb.Anorganisminafoodwebcanalsohavemorethanonearrowpointingtoit.Thisshowsthattheorganismeatsmorethanoneotherorganisminthefoodweb.Consumerseatproducers.So,inafoodweb,consumershavearrowspointingtothemfromproducers.Themushroomhasanarrowpointingtoitfromthemushroom.Themushroomisnotaconsumer,sothemushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Themushroomdoesnothaveanyarrowspointingtoit.So,themushroomisnotaconsumer.Answer:Theansweris(B).Rationale:Afoodwebisamodel…Anorganisminafoodwebcanhavemorethanonearrowpointingfromit.Thisshowsthattheorganismiseatenbymorethanoneotherorganisminthefoodweb.Anorganisminafoodwebcanalsohavemorethanonearrowpointingtoit.Thisshowsthattheorganismeatsmorethanoneotherorganisminthefoodweb.Consumerseatproducersorganisms.So,inarearrowsafoodwebthatpointfromotherorganismstoconsumers.Themushroomhasarrowspointingtoitfromthen-browmushroomibou.thelichenzzlybear.So,themushroomisaconsumer.Thelichenhasnothaveanyarrowspointingtoit.So,themushroomisaconsumer.Lichenisnotaconsumer.Answer:Theansweris(B).Vision\n+ Vision Features\nOptions:(B) sample A(A) neither; the samples have the same temperatureProblem\nBaselineQuestion:Comparetheaveragekineticenergiesoftheparticlesineachsample.Whichsamplehasthehighertemperature?Context:Thediagramsbelowshowtwopuresamplesofgasinidenticalclosed,rigidcontainers.Eachcoloredballrepresentsonegasparticle.Bothsampleshavethesamenumberofparticles.GoldRationale:Thetemperatureofasubstancedependsontheaveragekineticenergyoftheparticlesinthesubstance.Thehighertheaveragekineticenergyoftheparticles,thehigherthetemperatureofthesubstance.Thekineticenergyofaparticleisdeterminedbyitsmassandspeed.Forapuresubstance,thegreaterthemassofeachparticleinthesubstanceandthehighertheaveragespeedoftheparticles,thehighertheiraveragekineticenergy.EachparticleinsampleAhasmoremassthaneachparticleinsampleB.TheparticlesinsampleAalso+ Vision FeaturesVision(C) sample B\nhaveahigheraveragespeedthantheparticlesinsampleB.So,theparticlesinsampleAhaveahigheraveragekineticenergythantheparticlesinsampleB.BecausetheparticlesinsampleAhavethehigheraveragekineticenergy,sampleAmusthavethehighertemperature.Answer:Theansweris(B).Rationale:Thetemperatureofasubstancedependsontheaveragekineticenergyoftheparticlesinthesubstance.Thehighertheaveragekineticenergyoftheparticles,thehigherthetemperatureofthesubstance.Thekineticenergyofaparticleisdeterminedbyitsmassandspeed.Forapuresubstance,thegreaterthemassofeachparticleinthesubstanceandthehighertheaveragespeedoftheparticles,thehighertheiraveragekineticenergy.EachparticleinsampleBhasmoremassthaneachparticleinsampleA.TheparticlesinsampleBalsohaveahigheraveragespeedthantheparticlesinsampleA.So,theparticlesinsampleBhaveahigheraveragekineticenergythantheparticlesinsampleA.BecausetheparticlesinsampleBhavethehigheraveragekineticenergy,sampleBmusthavethehighertemperature.Answer:Theansweris(C).Rationale:Thetemperatureofasubstancedependsontheaveragekineticenergyoftheparticlesinthesubstance.Thehighertheaveragekineticenergyoftheparticles,thehigherthetemperatureofthesubstance.Thekineticenergyofaparticleisdeterminedbyitsmassandspeed.Forapuresubstance,thegreaterthemassofeachparticleinthesubstanceandthehighertheaveragespeedoftheparticles,thehighertheiraveragekineticenergy.EachparticleinsampleAhasmoremassthaneachparticleinsampleB.TheparticlesinsampleAalsohaveahigheraveragespeedthantheparticlesinsampleB.So,theparticlesinsampleAhaveahigheraveragekineticenergythantheparticlesinsampleB.BecausetheparticlesinsampleAhavethehigheraveragekineticenergy,sampleAmusthavethehighertemperature.Answer:Theansweris(B).\nFigure 6. Examples of the two-stage framework without vision features (baseline) and with vision features (ours) for generating rationales\nand predicting answers. The upper part presents the problem details, and the lower part shows the outputs of the baseline and our method.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}]: %s
2023-12-07 12:21:39,728 - INFO - Check the data that is being passed [{'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nZhuosheng Zhang1Aston Zhang2Mu Li2Hai Zhao1George Karypis2Alex Smola2\nAbstract\nLarge language models (LLMs) have shown im-\npressive performance on complex reasoning by\nleveraging chain-of-thought (CoT) prompting to\ngenerate intermediate reasoning chains as the ra-\ntionale to infer the answer. However, existing\nCoT studies have focused on the language modal-\nity. We propose Multimodal-CoT that incorpo-\nrates language (text) and vision (images) modal-\nities into a two-stage framework that separates\nrationale generation and answer inference. In this\nway, answer inference can leverage better gen-\nerated rationales that are based on multimodal\ninformation. With Multimodal-CoT, our model\nunder 1 billion parameters outperforms the previ-\nous state-of-the-art LLM (GPT-3.5) by 16 percent-\nage points (75.17% →91.68% accuracy) and even\nsurpasses human performance on the ScienceQA\nbenchmark. Code is publicly available.1\n1. Introduction\nImagine reading a textbook with no ﬁgures or tables. Our\nability to knowledge acquisition is greatly strengthened by\njointly modeling diverse data modalities, such as vision, lan-\nguage, and audio. Recently, large language models (LLMs)\n(Brown et al., 2020; Thoppilan et al., 2022; Rae et al., 2021;\nChowdhery et al., 2022) have shown impressive perfor-\nmance in complex reasoning by generating intermediate\nreasoning steps before inferring the answer. The intriguing\ntechnique is called chain-of-thought (CoT) reasoning (Wei\net al., 2022b; Kojima et al., 2022; Zhang et al., 2022).\nHowever, existing studies related to CoT reasoning are\nlargely isolated in the language modality (Wang et al.,\n2022b; Zhou et al., 2022; Lu et al., 2022b; Fu et al., 2022),\nwith little consideration of multimodal scenarios. To elicit\nCoT reasoning in multimodality, we advocate a Multimodal-\n1Shanghai Jiao Tong University2Amazon Web Services.\nCorrespondence to: Zhuosheng Zhang (work done at Ama-\nzon Web Services) <zhangzs@sjtu.edu.cn >, Aston Zhang\n<az@astonzhang.com >.\n1https://github.com/amazon-science/mm-cot\nOptions:(B) salty(A) softOutputQuestion:Whichpropertydothesetwoobjectshaveincommon?Context: Select the better answer.\nRationale:Lookateachobject.Foreachobject,decideifithasthatproperty.Potatochipshaveasaltytaste.Bothobjectsaresalty.Asoftobjectchangesshapewhenyousqueezeit.Thefriesaresoft,butthecrackerisnot.Thepropertythatbothobjectshaveincommonissalty.Answer:Theansweris(B).VisionLanguageInputFigure 1. Example of the multimodal CoT task.\nCoT paradigm. Given the inputs in different modalities,\nMultimodal-CoT decomposes multi-step problems into in-\ntermediate reasoning steps (rationale) and then infers the\nanswer. Since vision and language are the most popular\nmodalities, we focus on those two modalities in this work.\nAn example is shown in Figure 1. In general, there are two\nways to elicit Multimodal-CoT reasoning as follows: (i)\nprompting LLMs and (ii) ﬁne-tuning small models.2\nThe most immediate way to perform Multimodal-CoT is to\ntransform the input of different modalities into one modality\nand prompt LLMs to perform CoT. For example, it is possi-\nble to extract the caption of an image by a captioning model\nand then concatenate the caption with the original language\ninput to be fed into LLMs (Lu et al., 2022a). However, there\nis severe information loss in the captioning process; thus,\nusing the captions (as opposed to vision features) may suffer\nfrom a lack of mutual synergy in the representation space\nof different modalities.\nTo facilitate the interaction between modalities, another\npotential solution is to ﬁne-tune smaller language models\n(LMs) by fusing multimodal features (Zhang et al., 2023).\nAs this approach allows the ﬂexibility of adjusting model\narchitectures to incorporate multimodal features, we study\nﬁne-tuning models in this work instead of prompting LLMs.\nThe key challenge is that language models under 100 billion\nparameters tend to generate hallucinated rationales that mis-\nlead the answer inference (Ho et al., 2022; Magister et al.,\n2In this work, we refer to small models as models with less\nthan 1 billion parameters (hereinafter dubbed as 1B-models).arXiv:2302.00923v4  [cs.CL]  17 Feb 2023', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nTable 1. Typical CoT techniques (FT: ﬁne-tuning; KD: knowledge distillation). Segment 1: in-context learning techniques; Segment 2:\nﬁne-tuning techniques. To the best of our knowledge, our work is the ﬁrst to study CoT reasoning in different modalities. Besides, we\nfocus on 1B-models, without relying on the outputs of LLMs.\nModels Mutimodal w/o LLM Model / Engine Training CoT Role CoT Source\nZero-Shot-CoT (Kojima et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Template\nFew-Shot-CoT (Wei et al., 2022b) \x17 \x17 PaLM (540B) ICL Reasoning Hand-crafted\nSelf-Consistency-CoT (Wang et al., 2022a) \x17 \x17 Codex (175B) ICL Reasoning Hand-crafted\nLeast-to-Most Prompting (Zhou et al., 2022) \x17 \x17 Codex (175B) ICL Reasoning Hand-crafted\nRetrieval-CoT (Zhang et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Auto-generated\nPromptPG-CoT (Lu et al., 2022b) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Hand-crafted\nAuto-CoT (Zhang et al., 2022) \x17 \x17 Codex (175B) ICL Reasoning Auto-generated\nComplexity-CoT (Fu et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Hand-crafted\nFew-Shot-PoT (Chen et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Hand-crafted\nUniﬁedQA (Lu et al., 2022a) \x17 ✓ T5 (770M) FT Explanation Crawled\nFine-Tuned T5 XXL (Magister et al., 2022) \x17 \x17 T5 (11B) KD Reasoning LLM-generated\nFine-Tune-CoT (Ho et al., 2022) \x17 \x17 GPT-3 (6.7B) KD Reasoning LLM-generated\nMultimodal-CoT (our work) ✓ ✓ T5 (770M) FT Reasoning Crawled\n2022; Ji et al., 2022).\nTo mitigate the challenge of hallucination, we propose\nMultimodal-CoT that incorporates language (text) and vi-\nsion (images) modalities into a two-stage framework that\nseparates rationale generation and answer inference. In\nthis way, answer inference can leverage better generated\nrationales that are based on multimodal information. Our\nexperiments are conducted on the ScienceQA benchmark\n(Lu et al., 2022a), which is the latest multimodal reasoning\nbenchmark with annotated reasoning chains. Experimental\nresults show that our method surpasses the previous state-of-\nthe-art GPT-3.5 model by +16% (75.17% →91.68%) on the\nbenchmark. Our contributions are summarized as follows:\n(i) To the best of our knowledge, this work is the ﬁrst to\nstudy CoT reasoning in different modalities.\n(ii) We propose a two-stage framework by ﬁne-tuning lan-\nguage models to fuse vision and language representations\nto perform Multimodal-CoT. The model is able to generate\ninformative rationales to facilitate inferring ﬁnal answers.\n(iii) Our method achieves new state-of-the-art performance\non the ScienceQA benchmark, outperforming accuracy of\nGPT-3.5 by 16% and even surpassing human performance.\n2. Background\nThis section reviews recent progress of eliciting CoT rea-\nsoning by prompting and ﬁne-tuning language models.\n2.1. CoT Reasoning with LLMs\nRecently, CoT has been widely used to elicit the multi-step\nreasoning abilities of LLMs (Wei et al., 2022b). Concretely,\nCoT techniques encourage the LLM to generate intermedi-\nate reasoning chains for solving a problem. Studies have\nshown that LLMs can perform CoT reasoning with two ma-\njor paradigms of techniques: Zero-Shot-CoT (Kojima et al.,2022) and Few-Shot-CoT (Wei et al., 2022b; Zhang et al.,\n2022). For Zero-Shot-CoT, Kojima et al. (2022) showed that\nLLMs are decent zero-shot reasoners by adding a prompt\nlike “Let’s think step by step” after the test question to in-\nvoke CoT reasoning. For Few-Shot-CoT, a few step-by-step\nreasoning demonstrations are used as conditions for infer-\nence. Each demonstration has a question and a reasoning\nchain that leads to the ﬁnal answer. The demonstrations are\ncommonly obtained by hand-crafting or automatic gener-\nation. The corresponding techniques are thus referred to\nas Manual-CoT (Wei et al., 2022b) and Auto-CoT (Zhang\net al., 2022).\nWith effective demonstrations, Few-Shot-CoT often\nachieves stronger performance than Zero-Shot-CoT and has\nattracted more research interest. Therefore, most recent\nstudies focused on how to improve Few-Shot-CoT. Those\nstudies are categorized into two major research lines: (i)\noptimizing the demonstrations; (ii) optimizing the reasoning\nchains. Table 1 compares typical CoT techniques.\nOptimizing Demonstrations The performance of Few-\nShot-CoT relies on the quality of demonstrations. As re-\nported in Wei et al. (2022b), using demonstrations written\nby different annotators results in dramatic accuracy dispar-\nity in a symbolic reasoning task. Beyond hand-crafting the\ndemonstrations, recent studies have investigated ways to op-\ntimize the demonstration selection process. Notably, Rubin\net al. (2022) retrieved the semantically similar demonstra-\ntions with the test instance. However, this approach shows\na degraded performance when there are mistakes in the rea-\nsoning chains (Zhang et al., 2022). To address the limitation,\nZhang et al. (2022) found that the key is the diversity of\ndemonstration questions and proposed Auto-CoT: (i) par-\ntition questions of a given dataset into a few clusters; (ii)\nsample a representative question from each cluster and gen-\nerate its reasoning chain using Zero-Shot-CoT with simple\nheuristics. In addition, reinforcement learning (RL) and', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\ncomplexity-based selection strategies were also proposed\nto obtain effective demonstrations. Fu et al. (2022) chose\nexamples with complex reasoning chains (i.e., with more\nreasoning steps) as the demonstrations. Lu et al. (2022b)\ntrained an agent to ﬁnd optimal in-context examples from\na candidate pool and maximize the prediction rewards on\ngiven training examples when interacting with GPT-3.5.\nOptimizing Reasoning Chains A notable way to opti-\nmize reasoning chains is problem decomposition. Zhou\net al. (2022) proposed least-to-most prompting to decom-\npose complex problems into sub-problems and then solve\nthese sub-problems sequentially. As a result, solving a\ngiven sub-problem is facilitated by the answers to previ-\nously solved sub-problems. Similarly, Khot et al. (2022)\nused diverse decomposition structures and designed differ-\nent prompts to answer each sub-question. In addition to\nprompting the reasoning chains as natural language texts,\nChen et al. (2022) proposed program-of-thoughts (PoT),\nwhich modeled the reasoning process as a program and\nprompted LLMs to derive the answer by executing the gen-\nerated programs. Another trend is to vote over multiple\nreasoning paths for a test question. Wang et al. (2022a)\nintroduced a self-consistency decoding strategy to sample\nmultiple outputs of LLMs and then took a majority over\nthe ﬁnal answers. Wang et al. (2022b) and Li et al. (2022b)\nintroduced randomness in the input space to produce more\ndiverse outputs for voting.\n2.2. Eliciting CoT Reasoning by Fine-Tuning Models\nA recent interest is eliciting CoT reasoning by ﬁne-tuning\nlanguage models. Lu et al. (2022a) ﬁne-tuned the encoder-\ndecoder T5 model on a large-scale dataset with CoT annota-\ntions. However, a dramatic performance decline is observed\nwhen using CoT to infer the answer, i.e., generating the rea-\nsoning chain before the answer (reasoning). Instead, CoT\nis only used as an explanation after the answer. Magister\net al. (2022) and Ho et al. (2022) employed knowledge\ndistillation by ﬁne-tuning a student model on the chain-of-\nthought outputs generated by a larger teacher model. The\nproposed methods showed performance gains in arithmetic,\ncommonsense, and symbolic reasoning tasks.\nThere is a key challenge in training 1B-models to be CoT\nreasoners. As observed by Wei et al. (2022b), models un-\nder 100 billion parameters tend to produce illogical CoT\nthat leads to wrong answers. In other words, it might be\nharder for 1B-models to generate effective CoT than directly\ngenerating the answer. It becomes even more challenging\nin a multimodal setting where answering the question also\nrequires understanding the multimodal inputs. In the follow-\ning part, we will explore the challenge of Multimodal-CoT\nand investigate how to perform effective multi-step reason-\ning.3. Challenge of Multimodal-CoT\nExisting studies have suggested that the CoT reasoning abil-\nity may emerge in language models at a certain scale, e.g.,\nover 100 billion parameters (Wei et al., 2022a). However,\nit remains an unresolved challenge to elicit such reasoning\nabilities in 1B-models, let alone in the multimodal scenario.\nThis work focuses on 1B-models as they can be ﬁne-tuned\nand deployed with consumer-grade GPUs (e.g., 32G mem-\nory). In this section, we will investigate why 1B-models\nfail at CoT reasoning and study how to design an effective\napproach to overcome the challenge.\n3.1. Towards the Role of CoT\nTo begin with, we ﬁne-tune a text-only baseline for CoT rea-\nsoning on the ScienceQA benchmark (Lu et al., 2022a).\nFollowing Lu et al. (2022a), we adopt UniﬁedQA Base\n(Khashabi et al., 2020) as the backbone language model.3\nOur task is modeled as a text generation problem, where the\nmodel takes the textual information as the input and gener-\nates the output sequence that consists of the rationale and\nthe answer. As an example shown in Figure 1, the model\ntakes the concatenation of tokens of the question text (Q),\nthe context text (C), and multiple options (M) as the input.\nTo study the effect of CoT, we compare the performance\nwith three variants: (i) No-CoT which predicts the answer\ndirectly (QCM→A); (ii) Reasoning where answer inference\nis conditioned to the rationale (QCM →RA); (iii) Explana-\ntion where the rationale is used for explaining the answer\ninference (QCM→AR).\nTable 2. Effects of CoT in the one-stage setting.\nMethod Format Accuracy\nNo-CoT QCM →A 80.40\nReasoning QCM →RA 67.86\nExplanation QCM →AR 69.77\nSurprisingly, we observe a ↓12.54% accuracy decrease\n(80.40%→67.86%) if the model predicts rationales before\nanswers (QCM→RA). The results imply that the rationales\nmight not necessarily contribute to predicting the right an-\nswer. A similar phenomenon was observed in Lu et al.\n(2022a), where the plausible reason might be that the model\nexceeds the maximum token limits before obtaining the\nrequired answer or stops generating the prediction early.\nHowever, we ﬁnd that the maximum length of the gener-\nated outputs (RA) is always less than 400 tokens, which\nis below the length limit of language models (i.e., 512 in\nUniﬁedQA Base). Therefore, it deserves a more in-depth\ninvestigation into why the rationales harm answer inference.\n3UniﬁedQA (Khashabi et al., 2020) is adopted as it is the best\nﬁne-tuning model in Lu et al. (2022a). Model information and\nimplementation details are presented in Appendix B.1.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nGeneratedRationale:Magnetscanpullorpushoneachotherwithouttouching.Whenmagnetsattract,theypulltogether.Whenmagnetsrepel,theypushapart.Whetheramagnetattractsorrepelsothermagnetsdependsonthepositionsofitspoles,orends.Everymagnethastwopoles,callednorthandsouth.Herearesomeexamplesofmagnets.ThenorthpoleofeachmagnetismarkedN,andthesouthpoleismarkedS.Ifdifferentpolesareclosesttoeachother,themagnetsattract.Themagnetsinthepairbelowattract.Ifthesamepolesareclosesttoeachother,themagnetsrepel.Themagnetsinbothpairsbelowrepel.Willthesemagnetsattractorrepel?Tofindout,lookatwhichpolesareclosesttoeachother.Thesouthpoleofonemagnetisclosesttothesouthpoleoftheothermagnet.Polesthatarethesamerepel.So,thesemagnetswillrepeleachother.Answer:Theansweris(B).Options:(B) repel(A) attractProblem\nBaselineQuestion:Willthesemagnetsattractorrepeleachother?Context:Twomagnetsareplacedasshown.Hint:Magnetsthatattractpulltogether.Magnetsthatrepelpushapart.GoldRationale:Magnetscanpullorpushoneachotherwithouttouching.Whenmagnetsattract,theypulltogether.Whenmagnetsrepel,theypushapart.Whetheramagnetattractsorrepelsothermagnetsdependsonthepositionsofitspoles,orends.Everymagnethastwopoles,callednorthandsouth.Herearesomeexamplesofmagnets.ThenorthpoleofeachmagnetismarkedN,andthesouthpoleismarkedS.Ifdifferentpolesareclosesttoeachother,themagnetsattract.Themagnetsinthepairbelowattract.Ifthesamepolesareclosesttoeachother,themagnetsrepel.Themagnetsinbothpairsbelowrepel.Willthesemagnetsattractorrepel?Tofindout,lookatwhichpolesareclosesttoeachother.Thenorthpoleofonemagnetisclosesttothesouthpoleoftheothermagnet.Polesthataredifferentattract.So,thesemagnetswillattracteachother.Answer:Theansweris(A).GeneratedRationale:Magnetscanpullorpushoneachotherwithouttouching.Whenmagnetsattract,theypulltogether.Whenmagnetsrepel,theypushapart.Whetheramagnetattractsorrepelsothermagnetsdependsonthepositionsofitspoles,orends.Everymagnethastwopoles,callednorthandsouth.Herearesomeexamplesofmagnets.ThenorthpoleofeachmagnetismarkedN,andthesouthpoleismarkedS.Ifdifferentpolesareclosesttoeachother,themagnetsattract.Themagnetsinthepairbelowattract.Ifthesamepolesareclosesttoeachother,themagnetsrepel.Themagnetsinbothpairsbelowrepel.Willthesemagnetsattractorrepel?Tofindout,lookatwhichpolesareclosesttoeachother.Thenorthpoleofonemagnetisclosesttothesouthpoleoftheothermagnet.Polesthataredifferentattract.So,thesemagnetswillattracteachother.Answer:Theansweris(A).+ Vision Features\nVision\nFigure 2. Example of the two-stage framework without vision features (baseline) and with vision features (ours) for generating rationales\nand predicting answers. The upper part presents the problem details with a gold rationale, and the lower part shows the outputs of the\nbaseline and our method incorporated with vision features. We observe that the baseline fails to predict the right answer due to the\nmisleading by hallucinated rationales. More examples are shown in Appendix A.1.\n3.2. Misleading by Hallucinated Rationales\nTo dive into how the rationales affect the answer prediction,\nwe separate the CoT problem into two stages, rationale\ngeneration andanswer inference . We report the RougeL\nscore and accuracy for the rationale generation and answer\ninference, respectively. Table 3 shows the results based\non the two-stage framework. Although the two-stage base-\nline model achieves a 91.76 RougeL score of the rationale\ngeneration, the answer inference accuracy is only 70.53%.\nCompared with the QCM →A variant (80.40%) in Table 2,\nthe result shows that the generated rationale in the two-stage\nframework does not improve answer accuracy.\nTable 3. Two-stage setting of (i) rationale generation (RougeL) and\n(ii) answer inference (Accuracy).\nMethod (i) QCM →R (ii) QCMR→A\nTwo-Stage Framework 91.76 70.53\nw/ Captions 91.85 71.12\nw/ Vision Features 96.97 84.91\nThen, we randomly sample 50 error cases and ﬁnd that the\nmodel tends to generate hallucinated rationales that mislead\nthe answer inference. As an example shown in Figure 2, the\nmodel (left part) hallucinates that, “ The south pole of one\nmagnet is closest to the south pole of the other magnet ”, due\nto the lack of reference to the vision content. We ﬁnd that\nsuch mistakes occur at a ratio of 64% among the error cases\nOthers(36%)Resolved (62.5%)Unresolved(37.5%)Hallucination(64%)(a) ratio of hallucination mistakes(b) correction rate w/ vision features  Figure 3. The ratio of hallucination mistakes (a) and correction\nrate w/ vision features (b).\n(Figure 3(a)).\n3.3. Multimodality Contributes to Effective Rationales\nWe speculate that such a phenomenon of hallucination is\ndue to a lack of necessary vision contexts for performing\neffective Multimodal-CoT. To inject vision information, a\nsimple way is to transform the paired image into a caption\n(Lu et al., 2022a) and then append the caption in the input of\nboth stages. However, as shown in Table 3, using captions\nonly yields marginal performance gains ( ↑0.59%). Then,\nwe explore an advanced technique by incorporating vision\nfeatures into the language model. Concretely, we feed the\npaired image to the DETR model (Carion et al., 2020) to\nextract vision features. Then we fuse the vision features', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nVisionLanguageRationale GenerationQuestion:Whichpropertydothesetwoobjectshaveincommon?Context: Select the better answer.Lookateachobject.Foreachobject,decideifithasthatproperty.Potatochipshaveasaltytaste.Bothobjectsaresalty.Asoftobjectchangesshapewhenyousqueezeit.Thefriesaresoft,butthecrackerisnot.Thepropertythatbothobjectshaveincommonissalty.\nOptions:(B) salty(A) softRationaleAnswer InferenceTheansweris(B).Answer\nFigure 4. Overview of our Multimodal-CoT framework. Multimodal-CoT consists of two stages: (i) rationale generation and (ii) answer\ninference. Both stages share the same model architecture but differ in the input and output. In the ﬁrst stage, we feed the model with\nlanguage and vision inputs to generate rationales. In the second stage, we append the original language input with the rationale generated\nfrom the ﬁrst stage. Then, we feed the updated language input with the original vision input to the model to infer the answer.\nwith the encoded language representations before feeding\nto the decoder (more details will be presented in Section\n4). Interestingly, with vision features, the RougeL score of\nthe rationale generation has boosted to 96.97% (QCM →R),\nwhich correspondingly contributes to better answer accuracy\nof 84.91% (QCMR →A). With those effective rationales,\nthe phenomenon of hallucination is mitigated — 62.5%\nhallucination mistakes in Section 3.2 have been corrected\n(Figure 3(b)), as an example shown in Figure 2 (right part).4\nThe analysis so far compellingly shows that vision features\nare indeed beneﬁcial for generating effective rationales and\ncontributing to accurate answer inference. As the two-stage\nmethod (QCMR→A) in Table 3 achieves better performance\nthan all the one-stage method in Table 2, we choose the two-\nstage method in our Multimodal-CoT framework.\n4. Multimodal-CoT\nBased on the observations and discussions in Section 3, we\npropose Multimodal-CoT to incorporate language (text) and\nvision (images) modalities into a two-stage framework. In\nthis section, we will ﬁrst overview the procedure of the\nframework and then elaborate on the technical design of the\nmodel architecture.\n4.1. Framework Overview\nMultimodal-CoT consists of two training stages: (i) ratio-\nnale generation and (ii) answer inference. Both stages share\nthe same model architecture but differ in the input Xand\noutputY. The overall procedure is illustrated in Figure 4.\nWe will take vision-language as an example to show how\nMultimodal-CoT works.\n4The left mistakes are mainly about map understanding, which\nrequires more advanced vision features. We will discuss them in\nSection 6.4.In the rationale generation stage, we feed the model with\nX={X1\nlanguage,Xvision}whereX1\nlanguage represents the lan-\nguage input in the ﬁrst stage and Xvision represents the vision\ninput, i.e., the image. For example, Xcan be instantiated as\na concatenation of question, context, and options of a multi-\nple choice reasoning problem (Lu et al., 2022a) as shown in\nFigure 4. The goal is to learn a rationale generation model\nR=F(X)whereRis the rationale.\nIn the answer inference stage, the rationale Ris appended\nto the original language input X1\nlanguage to construct the lan-\nguage input in the second stage, X2\nlanguage =X1\nlanguage◦R\nwhere◦denotes concatenation. Then, we feed the updated\ninputX′={X2\nlanguage,Xvision}to the answer inference\nmodel to infer the ﬁnal answer A=F(X′).\nIn both stages, we train two models with the same archi-\ntecture independently. They take the annotated elements\n(e.g.,X→R,XR→A, respectively) from the training\nset for supervised learning. During inference, given X, the\nrationales for the test sets are generated using the model\ntrained in the ﬁrst stage; they are used in the second stage\nfor answer inference.\n4.2. Model Architecture\nGiven the language input Xlanguage∈{X1\nlanguage,X2\nlanguage}\nand the vision input Xvision, we compute the probability of\ngenerating target text Y(either the rationale or the answer\nin Figure 4) of length Nby\np(Y|Xlanguage,Xvision ) =N∏\ni=1pθ(Yi|Xlanguage,Xvision,Y<i),\n(1)\nwherepθ(Yi|Xlanguage,Xvision,Y<i)is implemented with\na Transformer-based network (Vaswani et al., 2017). The\nnetwork has three major procedures: encoding, interaction,', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nAlgorithm 1 Multimodal-CoT\nInput: Language input X1\nlanguage , vision input Xvision\nOutput: Generated rationale R, inferred answer A\n1: Construct the input X={Xlanguage,X vision}\n2: Generate rationale R=F(X)using the model F(·)\n3:Append the rationale Rto the original language input\nX2\nlanguage =X1\nlanguage◦R.\n4: Construct new input X′={X2\nlanguage,X vision}\n5:Infer the answer Aby conditioning on the new input, A=\nF(X′).\n6:procedure F(X)\n7: Encode the language and vision inputs Hlanguage andHvision,\nrespectively\n8: Build the interaction between language and vision features\nby attention Hattn\nvision\n9: FuseHlanguage andHattn\nvision by a gated fusion mechanism to\nhaveHfuse\n10: FeedHfuseto the decoder to obtain the target prediction Y\n11: return Y\n12:end procedure\nand decoding. Speciﬁcally, we feed the language text into\na Transformer encoder to obtain a textual representation,\nwhich is then interacted and fused with the vision represen-\ntation before being fed into the Transformer decoder.\nEncoding The modelF(X)takes both the language and\nvision inputs and obtains the text representation Hlanguage\nand the image feature Hvision by the following functions:\nHlanguage =LanguageEncoder (Xlanguage ), (2)\nHvision =Wh·VisionExtractor (Xvision ),(3)\nwhere LanguageEncoder( ·) is implemented as a Trans-\nformer model. We use the hidden states of the last layer\nin the Transformer encoder as the language representation\nHlanguage∈Rn×dwherendenotes the length of the lan-\nguage input, and dis the hidden dimension. Meanwhile,\nVisionExtractor(·) is used to vectorize the input image into\nvision features. Inspired by the recent success of Vision\nTransformers (Dosovitskiy et al., 2021), we fetch the patch-\nlevel features by off-the-shelf vision extraction models,5\nsuch as DETR (Carion et al., 2020). After obtaining the\npatch-level vision features, we apply a learnable projection\nmatrixWhto convert the shape of VisionExtractor (Xvision )\ninto that ofHlanguage ; thus we have Hvision∈Rm×dwhere\nmis the number of patches.\nInteraction After obtaining language and vision represen-\ntations, we use a single-head attention network to correlate\ntext tokens with image patches, where the query ( Q), key\n(K) and value (V) areHlanguage ,Hvision andHvision, respec-\n5The parameters of the vision extraction are frozen.tively. The attention output Hattn\nvision∈Rn×dis deﬁned as:\nHattn\nvision =Softmax (QK⊤\n√dk)V, (4)\nwheredkis the same as the dimension of Hlanguage because\na single head is used.\nThen, we apply the gated fusion mechanism (Zhang et al.,\n2020; Wu et al., 2021; Li et al., 2022a) to fuse Hlanguage and\nHvision. The fused output Hfuse∈Rn×dis obtained by:\nλ=Sigmoid (WlHlanguage +WvHattn\nvision ),(5)\nHfuse = (1−λ)·Hlanguage +λ·Hattn\nvision, (6)\nwhereWlandWvare learnable parameters.\nDecoding Finally, the fused output Hfuseis fed into the\nTransformer decoder to predict the target Y. The complete\nprocedure of Multimodal-CoT is shown in Algorithm 1.\n5. Experiments\nThis section will present the benchmark dataset, the imple-\nmentation of our technique, and the baselines for compar-\nisons. Then, we will report our main results and ﬁndings.\n5.1. Dataset\nOur method is evaluated on the ScienceQA benchmark (Lu\net al., 2022a). ScienceQA is the ﬁrst large-scale multimodal\nscience question dataset that annotates the answers with de-\ntailed lectures and explanations. It contains 21k multimodal\nmultiple choice questions with rich domain diversity across\n3 subjects, 26 topics, 127 categories, and 379 skills. The\nbenchmark dataset is split into training, validation, and test\nsplits with 12726, 4241, and 4241 examples, respectively.\n5.2. Implementation\nThe following part presents the experimental settings of\nMultimodal-CoT and the baseline methods.\nExperimental Settings As the Multimodal-CoT task re-\nquires generating the reasoning chains and leveraging the\nvision features, we use the T5 encoder-decoder architec-\nture (Raffel et al., 2020). Speciﬁcally, we adopt UniﬁedQA\n(Khashabi et al., 2020) to initialize our models in the two\nstages because it achieves the best ﬁne-tuning results in\nLu et al. (2022a). To verify the generality of our approach\nacross different LMs, we also employ FLAN-T5 (Chung\net al., 2022) as the backbone in Section 6.3. As using im-\nage captions does not yield signiﬁcant performance gains in\nSection 3.3, we did not use the captions. We ﬁne-tune the\nmodels up to 20 epochs, with a learning rate of 5e-5. The', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}]: %s
2023-12-07 12:21:39,728 - INFO - Check the results [{'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nZhuosheng Zhang1Aston Zhang2Mu Li2Hai Zhao1George Karypis2Alex Smola2\nAbstract\nLarge language models (LLMs) have shown im-\npressive performance on complex reasoning by\nleveraging chain-of-thought (CoT) prompting to\ngenerate intermediate reasoning chains as the ra-\ntionale to infer the answer. However, existing\nCoT studies have focused on the language modal-\nity. We propose Multimodal-CoT that incorpo-\nrates language (text) and vision (images) modal-\nities into a two-stage framework that separates\nrationale generation and answer inference. In this\nway, answer inference can leverage better gen-\nerated rationales that are based on multimodal\ninformation. With Multimodal-CoT, our model\nunder 1 billion parameters outperforms the previ-\nous state-of-the-art LLM (GPT-3.5) by 16 percent-\nage points (75.17% →91.68% accuracy) and even\nsurpasses human performance on the ScienceQA\nbenchmark. Code is publicly available.1\n1. Introduction\nImagine reading a textbook with no ﬁgures or tables. Our\nability to knowledge acquisition is greatly strengthened by\njointly modeling diverse data modalities, such as vision, lan-\nguage, and audio. Recently, large language models (LLMs)\n(Brown et al., 2020; Thoppilan et al., 2022; Rae et al., 2021;\nChowdhery et al., 2022) have shown impressive perfor-\nmance in complex reasoning by generating intermediate\nreasoning steps before inferring the answer. The intriguing\ntechnique is called chain-of-thought (CoT) reasoning (Wei\net al., 2022b; Kojima et al., 2022; Zhang et al., 2022).\nHowever, existing studies related to CoT reasoning are\nlargely isolated in the language modality (Wang et al.,\n2022b; Zhou et al., 2022; Lu et al., 2022b; Fu et al., 2022),\nwith little consideration of multimodal scenarios. To elicit\nCoT reasoning in multimodality, we advocate a Multimodal-\n1Shanghai Jiao Tong University2Amazon Web Services.\nCorrespondence to: Zhuosheng Zhang (work done at Ama-\nzon Web Services) <zhangzs@sjtu.edu.cn >, Aston Zhang\n<az@astonzhang.com >.\n1https://github.com/amazon-science/mm-cot\nOptions:(B) salty(A) softOutputQuestion:Whichpropertydothesetwoobjectshaveincommon?Context: Select the better answer.\nRationale:Lookateachobject.Foreachobject,decideifithasthatproperty.Potatochipshaveasaltytaste.Bothobjectsaresalty.Asoftobjectchangesshapewhenyousqueezeit.Thefriesaresoft,butthecrackerisnot.Thepropertythatbothobjectshaveincommonissalty.Answer:Theansweris(B).VisionLanguageInputFigure 1. Example of the multimodal CoT task.\nCoT paradigm. Given the inputs in different modalities,\nMultimodal-CoT decomposes multi-step problems into in-\ntermediate reasoning steps (rationale) and then infers the\nanswer. Since vision and language are the most popular\nmodalities, we focus on those two modalities in this work.\nAn example is shown in Figure 1. In general, there are two\nways to elicit Multimodal-CoT reasoning as follows: (i)\nprompting LLMs and (ii) ﬁne-tuning small models.2\nThe most immediate way to perform Multimodal-CoT is to\ntransform the input of different modalities into one modality\nand prompt LLMs to perform CoT. For example, it is possi-\nble to extract the caption of an image by a captioning model\nand then concatenate the caption with the original language\ninput to be fed into LLMs (Lu et al., 2022a). However, there\nis severe information loss in the captioning process; thus,\nusing the captions (as opposed to vision features) may suffer\nfrom a lack of mutual synergy in the representation space\nof different modalities.\nTo facilitate the interaction between modalities, another\npotential solution is to ﬁne-tune smaller language models\n(LMs) by fusing multimodal features (Zhang et al., 2023).\nAs this approach allows the ﬂexibility of adjusting model\narchitectures to incorporate multimodal features, we study\nﬁne-tuning models in this work instead of prompting LLMs.\nThe key challenge is that language models under 100 billion\nparameters tend to generate hallucinated rationales that mis-\nlead the answer inference (Ho et al., 2022; Magister et al.,\n2In this work, we refer to small models as models with less\nthan 1 billion parameters (hereinafter dubbed as 1B-models).arXiv:2302.00923v4  [cs.CL]  17 Feb 2023', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nTable 1. Typical CoT techniques (FT: ﬁne-tuning; KD: knowledge distillation). Segment 1: in-context learning techniques; Segment 2:\nﬁne-tuning techniques. To the best of our knowledge, our work is the ﬁrst to study CoT reasoning in different modalities. Besides, we\nfocus on 1B-models, without relying on the outputs of LLMs.\nModels Mutimodal w/o LLM Model / Engine Training CoT Role CoT Source\nZero-Shot-CoT (Kojima et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Template\nFew-Shot-CoT (Wei et al., 2022b) \x17 \x17 PaLM (540B) ICL Reasoning Hand-crafted\nSelf-Consistency-CoT (Wang et al., 2022a) \x17 \x17 Codex (175B) ICL Reasoning Hand-crafted\nLeast-to-Most Prompting (Zhou et al., 2022) \x17 \x17 Codex (175B) ICL Reasoning Hand-crafted\nRetrieval-CoT (Zhang et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Auto-generated\nPromptPG-CoT (Lu et al., 2022b) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Hand-crafted\nAuto-CoT (Zhang et al., 2022) \x17 \x17 Codex (175B) ICL Reasoning Auto-generated\nComplexity-CoT (Fu et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Hand-crafted\nFew-Shot-PoT (Chen et al., 2022) \x17 \x17 GPT-3.5 (175B) ICL Reasoning Hand-crafted\nUniﬁedQA (Lu et al., 2022a) \x17 ✓ T5 (770M) FT Explanation Crawled\nFine-Tuned T5 XXL (Magister et al., 2022) \x17 \x17 T5 (11B) KD Reasoning LLM-generated\nFine-Tune-CoT (Ho et al., 2022) \x17 \x17 GPT-3 (6.7B) KD Reasoning LLM-generated\nMultimodal-CoT (our work) ✓ ✓ T5 (770M) FT Reasoning Crawled\n2022; Ji et al., 2022).\nTo mitigate the challenge of hallucination, we propose\nMultimodal-CoT that incorporates language (text) and vi-\nsion (images) modalities into a two-stage framework that\nseparates rationale generation and answer inference. In\nthis way, answer inference can leverage better generated\nrationales that are based on multimodal information. Our\nexperiments are conducted on the ScienceQA benchmark\n(Lu et al., 2022a), which is the latest multimodal reasoning\nbenchmark with annotated reasoning chains. Experimental\nresults show that our method surpasses the previous state-of-\nthe-art GPT-3.5 model by +16% (75.17% →91.68%) on the\nbenchmark. Our contributions are summarized as follows:\n(i) To the best of our knowledge, this work is the ﬁrst to\nstudy CoT reasoning in different modalities.\n(ii) We propose a two-stage framework by ﬁne-tuning lan-\nguage models to fuse vision and language representations\nto perform Multimodal-CoT. The model is able to generate\ninformative rationales to facilitate inferring ﬁnal answers.\n(iii) Our method achieves new state-of-the-art performance\non the ScienceQA benchmark, outperforming accuracy of\nGPT-3.5 by 16% and even surpassing human performance.\n2. Background\nThis section reviews recent progress of eliciting CoT rea-\nsoning by prompting and ﬁne-tuning language models.\n2.1. CoT Reasoning with LLMs\nRecently, CoT has been widely used to elicit the multi-step\nreasoning abilities of LLMs (Wei et al., 2022b). Concretely,\nCoT techniques encourage the LLM to generate intermedi-\nate reasoning chains for solving a problem. Studies have\nshown that LLMs can perform CoT reasoning with two ma-\njor paradigms of techniques: Zero-Shot-CoT (Kojima et al.,2022) and Few-Shot-CoT (Wei et al., 2022b; Zhang et al.,\n2022). For Zero-Shot-CoT, Kojima et al. (2022) showed that\nLLMs are decent zero-shot reasoners by adding a prompt\nlike “Let’s think step by step” after the test question to in-\nvoke CoT reasoning. For Few-Shot-CoT, a few step-by-step\nreasoning demonstrations are used as conditions for infer-\nence. Each demonstration has a question and a reasoning\nchain that leads to the ﬁnal answer. The demonstrations are\ncommonly obtained by hand-crafting or automatic gener-\nation. The corresponding techniques are thus referred to\nas Manual-CoT (Wei et al., 2022b) and Auto-CoT (Zhang\net al., 2022).\nWith effective demonstrations, Few-Shot-CoT often\nachieves stronger performance than Zero-Shot-CoT and has\nattracted more research interest. Therefore, most recent\nstudies focused on how to improve Few-Shot-CoT. Those\nstudies are categorized into two major research lines: (i)\noptimizing the demonstrations; (ii) optimizing the reasoning\nchains. Table 1 compares typical CoT techniques.\nOptimizing Demonstrations The performance of Few-\nShot-CoT relies on the quality of demonstrations. As re-\nported in Wei et al. (2022b), using demonstrations written\nby different annotators results in dramatic accuracy dispar-\nity in a symbolic reasoning task. Beyond hand-crafting the\ndemonstrations, recent studies have investigated ways to op-\ntimize the demonstration selection process. Notably, Rubin\net al. (2022) retrieved the semantically similar demonstra-\ntions with the test instance. However, this approach shows\na degraded performance when there are mistakes in the rea-\nsoning chains (Zhang et al., 2022). To address the limitation,\nZhang et al. (2022) found that the key is the diversity of\ndemonstration questions and proposed Auto-CoT: (i) par-\ntition questions of a given dataset into a few clusters; (ii)\nsample a representative question from each cluster and gen-\nerate its reasoning chain using Zero-Shot-CoT with simple\nheuristics. In addition, reinforcement learning (RL) and', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\ncomplexity-based selection strategies were also proposed\nto obtain effective demonstrations. Fu et al. (2022) chose\nexamples with complex reasoning chains (i.e., with more\nreasoning steps) as the demonstrations. Lu et al. (2022b)\ntrained an agent to ﬁnd optimal in-context examples from\na candidate pool and maximize the prediction rewards on\ngiven training examples when interacting with GPT-3.5.\nOptimizing Reasoning Chains A notable way to opti-\nmize reasoning chains is problem decomposition. Zhou\net al. (2022) proposed least-to-most prompting to decom-\npose complex problems into sub-problems and then solve\nthese sub-problems sequentially. As a result, solving a\ngiven sub-problem is facilitated by the answers to previ-\nously solved sub-problems. Similarly, Khot et al. (2022)\nused diverse decomposition structures and designed differ-\nent prompts to answer each sub-question. In addition to\nprompting the reasoning chains as natural language texts,\nChen et al. (2022) proposed program-of-thoughts (PoT),\nwhich modeled the reasoning process as a program and\nprompted LLMs to derive the answer by executing the gen-\nerated programs. Another trend is to vote over multiple\nreasoning paths for a test question. Wang et al. (2022a)\nintroduced a self-consistency decoding strategy to sample\nmultiple outputs of LLMs and then took a majority over\nthe ﬁnal answers. Wang et al. (2022b) and Li et al. (2022b)\nintroduced randomness in the input space to produce more\ndiverse outputs for voting.\n2.2. Eliciting CoT Reasoning by Fine-Tuning Models\nA recent interest is eliciting CoT reasoning by ﬁne-tuning\nlanguage models. Lu et al. (2022a) ﬁne-tuned the encoder-\ndecoder T5 model on a large-scale dataset with CoT annota-\ntions. However, a dramatic performance decline is observed\nwhen using CoT to infer the answer, i.e., generating the rea-\nsoning chain before the answer (reasoning). Instead, CoT\nis only used as an explanation after the answer. Magister\net al. (2022) and Ho et al. (2022) employed knowledge\ndistillation by ﬁne-tuning a student model on the chain-of-\nthought outputs generated by a larger teacher model. The\nproposed methods showed performance gains in arithmetic,\ncommonsense, and symbolic reasoning tasks.\nThere is a key challenge in training 1B-models to be CoT\nreasoners. As observed by Wei et al. (2022b), models un-\nder 100 billion parameters tend to produce illogical CoT\nthat leads to wrong answers. In other words, it might be\nharder for 1B-models to generate effective CoT than directly\ngenerating the answer. It becomes even more challenging\nin a multimodal setting where answering the question also\nrequires understanding the multimodal inputs. In the follow-\ning part, we will explore the challenge of Multimodal-CoT\nand investigate how to perform effective multi-step reason-\ning.3. Challenge of Multimodal-CoT\nExisting studies have suggested that the CoT reasoning abil-\nity may emerge in language models at a certain scale, e.g.,\nover 100 billion parameters (Wei et al., 2022a). However,\nit remains an unresolved challenge to elicit such reasoning\nabilities in 1B-models, let alone in the multimodal scenario.\nThis work focuses on 1B-models as they can be ﬁne-tuned\nand deployed with consumer-grade GPUs (e.g., 32G mem-\nory). In this section, we will investigate why 1B-models\nfail at CoT reasoning and study how to design an effective\napproach to overcome the challenge.\n3.1. Towards the Role of CoT\nTo begin with, we ﬁne-tune a text-only baseline for CoT rea-\nsoning on the ScienceQA benchmark (Lu et al., 2022a).\nFollowing Lu et al. (2022a), we adopt UniﬁedQA Base\n(Khashabi et al., 2020) as the backbone language model.3\nOur task is modeled as a text generation problem, where the\nmodel takes the textual information as the input and gener-\nates the output sequence that consists of the rationale and\nthe answer. As an example shown in Figure 1, the model\ntakes the concatenation of tokens of the question text (Q),\nthe context text (C), and multiple options (M) as the input.\nTo study the effect of CoT, we compare the performance\nwith three variants: (i) No-CoT which predicts the answer\ndirectly (QCM→A); (ii) Reasoning where answer inference\nis conditioned to the rationale (QCM →RA); (iii) Explana-\ntion where the rationale is used for explaining the answer\ninference (QCM→AR).\nTable 2. Effects of CoT in the one-stage setting.\nMethod Format Accuracy\nNo-CoT QCM →A 80.40\nReasoning QCM →RA 67.86\nExplanation QCM →AR 69.77\nSurprisingly, we observe a ↓12.54% accuracy decrease\n(80.40%→67.86%) if the model predicts rationales before\nanswers (QCM→RA). The results imply that the rationales\nmight not necessarily contribute to predicting the right an-\nswer. A similar phenomenon was observed in Lu et al.\n(2022a), where the plausible reason might be that the model\nexceeds the maximum token limits before obtaining the\nrequired answer or stops generating the prediction early.\nHowever, we ﬁnd that the maximum length of the gener-\nated outputs (RA) is always less than 400 tokens, which\nis below the length limit of language models (i.e., 512 in\nUniﬁedQA Base). Therefore, it deserves a more in-depth\ninvestigation into why the rationales harm answer inference.\n3UniﬁedQA (Khashabi et al., 2020) is adopted as it is the best\nﬁne-tuning model in Lu et al. (2022a). Model information and\nimplementation details are presented in Appendix B.1.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nGeneratedRationale:Magnetscanpullorpushoneachotherwithouttouching.Whenmagnetsattract,theypulltogether.Whenmagnetsrepel,theypushapart.Whetheramagnetattractsorrepelsothermagnetsdependsonthepositionsofitspoles,orends.Everymagnethastwopoles,callednorthandsouth.Herearesomeexamplesofmagnets.ThenorthpoleofeachmagnetismarkedN,andthesouthpoleismarkedS.Ifdifferentpolesareclosesttoeachother,themagnetsattract.Themagnetsinthepairbelowattract.Ifthesamepolesareclosesttoeachother,themagnetsrepel.Themagnetsinbothpairsbelowrepel.Willthesemagnetsattractorrepel?Tofindout,lookatwhichpolesareclosesttoeachother.Thesouthpoleofonemagnetisclosesttothesouthpoleoftheothermagnet.Polesthatarethesamerepel.So,thesemagnetswillrepeleachother.Answer:Theansweris(B).Options:(B) repel(A) attractProblem\nBaselineQuestion:Willthesemagnetsattractorrepeleachother?Context:Twomagnetsareplacedasshown.Hint:Magnetsthatattractpulltogether.Magnetsthatrepelpushapart.GoldRationale:Magnetscanpullorpushoneachotherwithouttouching.Whenmagnetsattract,theypulltogether.Whenmagnetsrepel,theypushapart.Whetheramagnetattractsorrepelsothermagnetsdependsonthepositionsofitspoles,orends.Everymagnethastwopoles,callednorthandsouth.Herearesomeexamplesofmagnets.ThenorthpoleofeachmagnetismarkedN,andthesouthpoleismarkedS.Ifdifferentpolesareclosesttoeachother,themagnetsattract.Themagnetsinthepairbelowattract.Ifthesamepolesareclosesttoeachother,themagnetsrepel.Themagnetsinbothpairsbelowrepel.Willthesemagnetsattractorrepel?Tofindout,lookatwhichpolesareclosesttoeachother.Thenorthpoleofonemagnetisclosesttothesouthpoleoftheothermagnet.Polesthataredifferentattract.So,thesemagnetswillattracteachother.Answer:Theansweris(A).GeneratedRationale:Magnetscanpullorpushoneachotherwithouttouching.Whenmagnetsattract,theypulltogether.Whenmagnetsrepel,theypushapart.Whetheramagnetattractsorrepelsothermagnetsdependsonthepositionsofitspoles,orends.Everymagnethastwopoles,callednorthandsouth.Herearesomeexamplesofmagnets.ThenorthpoleofeachmagnetismarkedN,andthesouthpoleismarkedS.Ifdifferentpolesareclosesttoeachother,themagnetsattract.Themagnetsinthepairbelowattract.Ifthesamepolesareclosesttoeachother,themagnetsrepel.Themagnetsinbothpairsbelowrepel.Willthesemagnetsattractorrepel?Tofindout,lookatwhichpolesareclosesttoeachother.Thenorthpoleofonemagnetisclosesttothesouthpoleoftheothermagnet.Polesthataredifferentattract.So,thesemagnetswillattracteachother.Answer:Theansweris(A).+ Vision Features\nVision\nFigure 2. Example of the two-stage framework without vision features (baseline) and with vision features (ours) for generating rationales\nand predicting answers. The upper part presents the problem details with a gold rationale, and the lower part shows the outputs of the\nbaseline and our method incorporated with vision features. We observe that the baseline fails to predict the right answer due to the\nmisleading by hallucinated rationales. More examples are shown in Appendix A.1.\n3.2. Misleading by Hallucinated Rationales\nTo dive into how the rationales affect the answer prediction,\nwe separate the CoT problem into two stages, rationale\ngeneration andanswer inference . We report the RougeL\nscore and accuracy for the rationale generation and answer\ninference, respectively. Table 3 shows the results based\non the two-stage framework. Although the two-stage base-\nline model achieves a 91.76 RougeL score of the rationale\ngeneration, the answer inference accuracy is only 70.53%.\nCompared with the QCM →A variant (80.40%) in Table 2,\nthe result shows that the generated rationale in the two-stage\nframework does not improve answer accuracy.\nTable 3. Two-stage setting of (i) rationale generation (RougeL) and\n(ii) answer inference (Accuracy).\nMethod (i) QCM →R (ii) QCMR→A\nTwo-Stage Framework 91.76 70.53\nw/ Captions 91.85 71.12\nw/ Vision Features 96.97 84.91\nThen, we randomly sample 50 error cases and ﬁnd that the\nmodel tends to generate hallucinated rationales that mislead\nthe answer inference. As an example shown in Figure 2, the\nmodel (left part) hallucinates that, “ The south pole of one\nmagnet is closest to the south pole of the other magnet ”, due\nto the lack of reference to the vision content. We ﬁnd that\nsuch mistakes occur at a ratio of 64% among the error cases\nOthers(36%)Resolved (62.5%)Unresolved(37.5%)Hallucination(64%)(a) ratio of hallucination mistakes(b) correction rate w/ vision features  Figure 3. The ratio of hallucination mistakes (a) and correction\nrate w/ vision features (b).\n(Figure 3(a)).\n3.3. Multimodality Contributes to Effective Rationales\nWe speculate that such a phenomenon of hallucination is\ndue to a lack of necessary vision contexts for performing\neffective Multimodal-CoT. To inject vision information, a\nsimple way is to transform the paired image into a caption\n(Lu et al., 2022a) and then append the caption in the input of\nboth stages. However, as shown in Table 3, using captions\nonly yields marginal performance gains ( ↑0.59%). Then,\nwe explore an advanced technique by incorporating vision\nfeatures into the language model. Concretely, we feed the\npaired image to the DETR model (Carion et al., 2020) to\nextract vision features. Then we fuse the vision features', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nVisionLanguageRationale GenerationQuestion:Whichpropertydothesetwoobjectshaveincommon?Context: Select the better answer.Lookateachobject.Foreachobject,decideifithasthatproperty.Potatochipshaveasaltytaste.Bothobjectsaresalty.Asoftobjectchangesshapewhenyousqueezeit.Thefriesaresoft,butthecrackerisnot.Thepropertythatbothobjectshaveincommonissalty.\nOptions:(B) salty(A) softRationaleAnswer InferenceTheansweris(B).Answer\nFigure 4. Overview of our Multimodal-CoT framework. Multimodal-CoT consists of two stages: (i) rationale generation and (ii) answer\ninference. Both stages share the same model architecture but differ in the input and output. In the ﬁrst stage, we feed the model with\nlanguage and vision inputs to generate rationales. In the second stage, we append the original language input with the rationale generated\nfrom the ﬁrst stage. Then, we feed the updated language input with the original vision input to the model to infer the answer.\nwith the encoded language representations before feeding\nto the decoder (more details will be presented in Section\n4). Interestingly, with vision features, the RougeL score of\nthe rationale generation has boosted to 96.97% (QCM →R),\nwhich correspondingly contributes to better answer accuracy\nof 84.91% (QCMR →A). With those effective rationales,\nthe phenomenon of hallucination is mitigated — 62.5%\nhallucination mistakes in Section 3.2 have been corrected\n(Figure 3(b)), as an example shown in Figure 2 (right part).4\nThe analysis so far compellingly shows that vision features\nare indeed beneﬁcial for generating effective rationales and\ncontributing to accurate answer inference. As the two-stage\nmethod (QCMR→A) in Table 3 achieves better performance\nthan all the one-stage method in Table 2, we choose the two-\nstage method in our Multimodal-CoT framework.\n4. Multimodal-CoT\nBased on the observations and discussions in Section 3, we\npropose Multimodal-CoT to incorporate language (text) and\nvision (images) modalities into a two-stage framework. In\nthis section, we will ﬁrst overview the procedure of the\nframework and then elaborate on the technical design of the\nmodel architecture.\n4.1. Framework Overview\nMultimodal-CoT consists of two training stages: (i) ratio-\nnale generation and (ii) answer inference. Both stages share\nthe same model architecture but differ in the input Xand\noutputY. The overall procedure is illustrated in Figure 4.\nWe will take vision-language as an example to show how\nMultimodal-CoT works.\n4The left mistakes are mainly about map understanding, which\nrequires more advanced vision features. We will discuss them in\nSection 6.4.In the rationale generation stage, we feed the model with\nX={X1\nlanguage,Xvision}whereX1\nlanguage represents the lan-\nguage input in the ﬁrst stage and Xvision represents the vision\ninput, i.e., the image. For example, Xcan be instantiated as\na concatenation of question, context, and options of a multi-\nple choice reasoning problem (Lu et al., 2022a) as shown in\nFigure 4. The goal is to learn a rationale generation model\nR=F(X)whereRis the rationale.\nIn the answer inference stage, the rationale Ris appended\nto the original language input X1\nlanguage to construct the lan-\nguage input in the second stage, X2\nlanguage =X1\nlanguage◦R\nwhere◦denotes concatenation. Then, we feed the updated\ninputX′={X2\nlanguage,Xvision}to the answer inference\nmodel to infer the ﬁnal answer A=F(X′).\nIn both stages, we train two models with the same archi-\ntecture independently. They take the annotated elements\n(e.g.,X→R,XR→A, respectively) from the training\nset for supervised learning. During inference, given X, the\nrationales for the test sets are generated using the model\ntrained in the ﬁrst stage; they are used in the second stage\nfor answer inference.\n4.2. Model Architecture\nGiven the language input Xlanguage∈{X1\nlanguage,X2\nlanguage}\nand the vision input Xvision, we compute the probability of\ngenerating target text Y(either the rationale or the answer\nin Figure 4) of length Nby\np(Y|Xlanguage,Xvision ) =N∏\ni=1pθ(Yi|Xlanguage,Xvision,Y<i),\n(1)\nwherepθ(Yi|Xlanguage,Xvision,Y<i)is implemented with\na Transformer-based network (Vaswani et al., 2017). The\nnetwork has three major procedures: encoding, interaction,', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nAlgorithm 1 Multimodal-CoT\nInput: Language input X1\nlanguage , vision input Xvision\nOutput: Generated rationale R, inferred answer A\n1: Construct the input X={Xlanguage,X vision}\n2: Generate rationale R=F(X)using the model F(·)\n3:Append the rationale Rto the original language input\nX2\nlanguage =X1\nlanguage◦R.\n4: Construct new input X′={X2\nlanguage,X vision}\n5:Infer the answer Aby conditioning on the new input, A=\nF(X′).\n6:procedure F(X)\n7: Encode the language and vision inputs Hlanguage andHvision,\nrespectively\n8: Build the interaction between language and vision features\nby attention Hattn\nvision\n9: FuseHlanguage andHattn\nvision by a gated fusion mechanism to\nhaveHfuse\n10: FeedHfuseto the decoder to obtain the target prediction Y\n11: return Y\n12:end procedure\nand decoding. Speciﬁcally, we feed the language text into\na Transformer encoder to obtain a textual representation,\nwhich is then interacted and fused with the vision represen-\ntation before being fed into the Transformer decoder.\nEncoding The modelF(X)takes both the language and\nvision inputs and obtains the text representation Hlanguage\nand the image feature Hvision by the following functions:\nHlanguage =LanguageEncoder (Xlanguage ), (2)\nHvision =Wh·VisionExtractor (Xvision ),(3)\nwhere LanguageEncoder( ·) is implemented as a Trans-\nformer model. We use the hidden states of the last layer\nin the Transformer encoder as the language representation\nHlanguage∈Rn×dwherendenotes the length of the lan-\nguage input, and dis the hidden dimension. Meanwhile,\nVisionExtractor(·) is used to vectorize the input image into\nvision features. Inspired by the recent success of Vision\nTransformers (Dosovitskiy et al., 2021), we fetch the patch-\nlevel features by off-the-shelf vision extraction models,5\nsuch as DETR (Carion et al., 2020). After obtaining the\npatch-level vision features, we apply a learnable projection\nmatrixWhto convert the shape of VisionExtractor (Xvision )\ninto that ofHlanguage ; thus we have Hvision∈Rm×dwhere\nmis the number of patches.\nInteraction After obtaining language and vision represen-\ntations, we use a single-head attention network to correlate\ntext tokens with image patches, where the query ( Q), key\n(K) and value (V) areHlanguage ,Hvision andHvision, respec-\n5The parameters of the vision extraction are frozen.tively. The attention output Hattn\nvision∈Rn×dis deﬁned as:\nHattn\nvision =Softmax (QK⊤\n√dk)V, (4)\nwheredkis the same as the dimension of Hlanguage because\na single head is used.\nThen, we apply the gated fusion mechanism (Zhang et al.,\n2020; Wu et al., 2021; Li et al., 2022a) to fuse Hlanguage and\nHvision. The fused output Hfuse∈Rn×dis obtained by:\nλ=Sigmoid (WlHlanguage +WvHattn\nvision ),(5)\nHfuse = (1−λ)·Hlanguage +λ·Hattn\nvision, (6)\nwhereWlandWvare learnable parameters.\nDecoding Finally, the fused output Hfuseis fed into the\nTransformer decoder to predict the target Y. The complete\nprocedure of Multimodal-CoT is shown in Algorithm 1.\n5. Experiments\nThis section will present the benchmark dataset, the imple-\nmentation of our technique, and the baselines for compar-\nisons. Then, we will report our main results and ﬁndings.\n5.1. Dataset\nOur method is evaluated on the ScienceQA benchmark (Lu\net al., 2022a). ScienceQA is the ﬁrst large-scale multimodal\nscience question dataset that annotates the answers with de-\ntailed lectures and explanations. It contains 21k multimodal\nmultiple choice questions with rich domain diversity across\n3 subjects, 26 topics, 127 categories, and 379 skills. The\nbenchmark dataset is split into training, validation, and test\nsplits with 12726, 4241, and 4241 examples, respectively.\n5.2. Implementation\nThe following part presents the experimental settings of\nMultimodal-CoT and the baseline methods.\nExperimental Settings As the Multimodal-CoT task re-\nquires generating the reasoning chains and leveraging the\nvision features, we use the T5 encoder-decoder architec-\nture (Raffel et al., 2020). Speciﬁcally, we adopt UniﬁedQA\n(Khashabi et al., 2020) to initialize our models in the two\nstages because it achieves the best ﬁne-tuning results in\nLu et al. (2022a). To verify the generality of our approach\nacross different LMs, we also employ FLAN-T5 (Chung\net al., 2022) as the backbone in Section 6.3. As using im-\nage captions does not yield signiﬁcant performance gains in\nSection 3.3, we did not use the captions. We ﬁne-tune the\nmodels up to 20 epochs, with a learning rate of 5e-5. The', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}]: %s
2023-12-07 12:21:41,145 - INFO - Check the data that is being passed [{'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nA.2. Two-Stage Training Performance with Different Sizes of LMs.\nIn Section 3, we obverse that incorporating vision features helps generate more effective rationales, thus leading to improved\nanswer accuracy. Besides incorporating vision features, it is possible to scale the LM size to mitigate the issue of incorrect\nrationales. Figure 7 shows the answer accuracy with UniﬁedQA Base and UniﬁedQA Large . When using a larger LM, the\naccuracy of the baseline (w/o vision features) is boosted. The result indicates that scaling the LM is possible to mitigate the\nissue of incorrect rationales. However, the performance is still much inferior to using vision features. The result further\nveriﬁes the effectiveness of our Multimodal-CoT with different sizes of LMs.\nbase large6580100\n70.5384.9182.9791.68Accuracy (%)w/o Vision Modality w/ Vision Modality\nFigure 7. Answer accuracy with different sizes of LMs.\nB. Experimental Details\nB.1. Baseline Methods\nFollowing Lu et al. (2022a), our baselines include three types of methods:\n(i) Visual question answering (VQA) models (Yu et al., 2019; Anderson et al., 2018; Kim et al., 2018; Gao et al., 2019; Lu\net al., 2021; Li et al., 2019). The VQA baselines take the question, the context, and choices as the textual input, take the\nimage as the vision input, and predict the score distribution over choice candidates via a linear classiﬁer.\n(ii) Text-to-text LM models. UniﬁedQA (Khashabi et al., 2020) is adopted as it is the best ﬁne-tuning model in Lu et al.\n(2022a). UniﬁedQA takes the textual information as the input and outputs the answer option. The image is converted into a\ncaption extracted by an image captioning model based on ViT and GPT-2.6UniﬁedQA treats our task as a text generation\nproblem. In Lu et al. (2022a), it is trained to generate a target answer text, i.e., one of the candidate options. Then, the most\nsimilar option is selected as the ﬁnal prediction to evaluate the question answering accuracy.\n(iii) GPT-3.5 models (Chen et al., 2020) based on the text-davinci-002 engine. The inference is based on the few-shot\nprompting, where two in-context examples from the training set are concatenated before the test instance.\nFor UniﬁedQA and GPT-3.5, CoT is applied after the answer (Lu et al., 2022a). Besides the above baselines, we develop a\nstronger baseline with a slight modiﬁcation of the output format of UniﬁedQA. Instead of predicting the answer texts, our\nbaseline directly predicts the choice, e.g., the answer is B . This setting helps our baseline achieve better results than the\nexisting UniﬁedQA. Therefore, we use the stronger method as the language only baseline for analysis.\nB.2. Details of Vision Features\nIn Section 6.2, we compared four types of vision features, CLIP (Radford et al., 2021), DETR (Carion et al., 2020), and\nResNet (He et al., 2016). The speciﬁc models are: (i) CLIP: RN101;7(ii) DETR: detr resnet101 dc5;8(iii) ResNet: we use\n6https://huggingface.co/nlpconnect/vit-gpt2-image-captioning .\n7https://github.com/jianjieluo/OpenAI-CLIP-Feature .\n8https://github.com/facebookresearch/detr .', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nthe averaged pooled features of a pre-trained ResNet50 CNN. Table 9 presents the dimension of the vision features (after the\nfunction VisionExtractor( ·) in Eq. 3). For ResNet-50, we repeat the pooled features of ResNet-50 to the same length as the\ntext sequence to imitate the patch-like features, where each patch is the same as the pooled image features.\nTable 9. Dimension of vision features\nMethod Dimension\nCLIP (49, 2048)\nDETR (100, 256)\nResNet (512, 2048)\nC. Examples of Case Studies\nTo better understand the behavior of Multimodal-CoT, we manually investigate randomly selected examples generated by\nour approach. Table 8 summarizes the categorization results generated by Multimodal-CoT. We randomly picked up 50\nsamples whose prediction results were correct and 50 samples whose prediction results were incorrect.\nWe ﬁnd that the correct samples contain a certain amount of incorrect chain-of-thought. As shown in Figure 8(b), the model\ngenerates the incorrect rationale, “ Animals cannot their food by digesting other organisms ” but the predicted answer is\ncorrect. The result indicates that CoT may not always beneﬁt the answer inference, and the model is robust to some extent —\nit can predict the correct answer by ignoring incorrect rationales.\nFor incorrect samples, commonsense mistake is the most frequent error type. The model also makes commonsense mistakes\nwhen answering the questions requires commonsense knowledge, e.g., understand maps and counting numbers in the\nimages (Figure 9), and utilizing the alphabet (Figure 10). The other type of mistake is the logical mistake, where there are\ncontradictions in the reasoning chains (Figure 11). In addition, there are cases that the CoT is correct but might not be\nnecessarily related to answer options; thus the model chooses the incorrect answer.\nThe analysis indicates that there are prospective directions for future studies. On the one hand, it is possible to improve the\nquality of CoT by (i) using more ﬁne-grained interaction of language and vision features; and (ii) injecting commonsense\nknowledge. On the other hand, applying a ﬁltering mechanism to using only the effective CoT to infer the answer and\neliminate irrelevant CoT.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Birds,mammals,fish,reptiles,andamphibiansaregroupsofanimals.Theanimalsineachgrouphavetraitsincommon.Scientistssortanimalsintogroupsbasedontraitstheyhaveincommon.Thisprocessiscalledclassification.Atigersharkisafish.Itlivesunderwater.Ithasfins,notlimbs.Tigersharkscannocturnal.Theymeansthattheyareactivemostlyatnight.Ared-tailedhawkisabird.Ithasfeathers,twowings,andabeak.Red-tailedhawkslivetheirbeteethakstopushforpre.birds,and,repamptiles…Answer:Theansweris(B).Options:(A) red-tailed hawkProblem\nPredictionQuestion:Selectthebirdbelow.Context:Birdshavefeathers,twowings,andabeak.Aperegrinefalconisanexampleofabird.Rationale:Birds,mammals,fish,reptiles,andamphibiansaregroupsofanimals.Theanimalsineachgrouphavetraitsincommon.Scientistssortanimalsintogroupsbasedontraitstheyhaveincommon.Thisprocessiscalledclassification.Atigersharkisafish.Itlivesunderwater.Ithasfins,notlimbs.Tigersharkscannocturnal.Theymeansthattheyareactivemostlyatnight.Ared-tailedhawkisabird.Ithasfeathers,twowings,andabeak.Red-tailedhawkslivetheirbeteethakstopushforpre.birds,and,repamptiles…Answer:Theansweris(A).Vision(B) tiger shark\n(a)CoTiscorrect\nRationale:Leopardthepast,scientistsclassifiedlivingorganismsintotwogroups:plantsandanimals.Overthepast300years,scientistshavediscoveredmanymoretypesoforganisms.Today,manyscientistsclassifyorganismsintosixbroadgroups,calledkingdoms.Organismsineachkingdomhavespecifictraits.Thetablebelowshowssometraitsusedtodescribeeachkingdom.|Bacteria|Archaea|Protists|Fungi|Animals|Plants.Howmanycellsdotheyhave?|one|one|oneormany|oneormany|many|many.Dotheircellshaveanucleus?|no|no|yes|yes|yes|yesnCantheircellsmakefood?|somespeciescan|somespeciescan|somespeciescan|no|no|yesLeoparduswiediiisananimal.Animalcellscannotmaketheirownfood.Animalscannottheirfoodbydigestingotherorganisms.Answer:Theansweris(B).Options:(A) noProblem\nPredictionQuestion:CanLeoparduswiediicellsmaketheirownfood?Context:ThisorganismisLeoparduswiedii.Itisamemberoftheanimalkingdom.Leoparduswiediiiscommonlycalledamargay.MargaysliveintheforestinCentralandSouthAmerica.Margaysarenocturnal.Anocturnalanimalsleepsmostofthedayandisawakeatnight.Rationale:Inthepast,scientistsclassifiedlivingorganismsintotwogroups:plantsandanimals.Overthepast300years,scientistshavediscoveredmanymoretypesoforganisms.Today,manyscientistsclassifyorganismsintosixbroadgroups,calledkingdoms.Organismsineachkingdomhavespecifictraits.Thetablebelowshowssometraitsusedtodescribeeachkingdom.|Bacteria|Archaea|Protists|Fungi|Animals|Plants.Howmanycellsdotheyhave?|one|one|oneormany|oneormany|many|manyDotheircellshaveanucleus?|no|no|yes|yes|yes|yesCantheircellsmakefood?|somespeciescan|somespeciescan|somespeciescan|no|no|yesLeoparduswiediiisananimal.Animalcellscannotmaketheirownfood.Animalsgettheirfoodbydigestingotherorganisms.Answer:Theansweris(A).Vision(B) yes\n(b)CoTisincorrect\nFigure 8. Examples of answers are correct while the CoT is correct (a) or incorrect (b).', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Oceansarehugebodiesofsaltwater.Theworldhasfiveoceans.Alloftheoceansareconnected,makingoneworldocean.ThisistheIndianOcean.Answer:Theansweris(B).Options:(A) the Atlantic OceanProblem\nPredictionQuestion:Whichoceanishighlighted?Context:N/ARationale:Oceansarehugebodiesofsaltwater.Theworldhasfiveoceans.Alloftheoceansareconnected,makingoneworldocean.ThisisthePacificOcean.Answer:Theansweris(C).Vision(B) the Indian Ocean\n(a)Mistakesaboutmaps.\nRationale:Inasolution,soluteparticlesmoveandspreadthroughoutthesolvent.Thediagrambelowshowshowasolutioncanchangeovertime.Soluteparticlesmovefromtheareawheretheyareatahigherconcentrationtotheareawheretheyareatalowerconcentration…Lookatthediagramagain.Itshowsyouhowthesolutionchangedduringtheprocessofdiffusion.Beforethesoluteparticlesreachedequilibrium,therewere5soluteparticlesontheleftsideofthemembraneand4soluteparticlesontherightsideofthemembrane.Whenthesoluteparticlesreachedequilibrium,therewere5soluteparticlesoneachsideofthemembrane.Therewas1moresoluteparticleontherightsideofthemembranethanbefore.So,forthesoluteparticlestoreachequilibrium,moresoluteparticlesmusthavemovedacrossthemembranetotherightthantotheright.Answer:Theansweris(B).Options:(A) to the left than to the rightProblem\nPredictionQuestion:Completethetexttodescribethediagram.Soluteparticlesmovedinbothdirectionsacrossthepermeablemembrane.Butmoresoluteparticlesmovedacrossthemembrane().Whentherewasanequalconcentrationonbothsides,theparticlesreachedequilibrium.Context:Thediagrambelowshowsasolutionwithonesolute.Eachsoluteparticleisrepresentedbyagreenball.Thesolutionfillsaclosedcontainerthatisdividedinhalfbyamembrane.Themembrane,representedbyadottedline,ispermeabletothesoluteparticles.Thediagramshowshowthesolutioncanchangeovertimeduringtheprocessofdiffusion.Rationale:Inasolution,soluteparticlesmoveandspreadthroughoutthesolvent.Thediagrambelowshowshowasolutioncanchangeovertime.Soluteparticlesmovefromtheareawheretheyareatahigherconcentrationtotheareawheretheyareatalowerconcentration…Lookatthediagramagain.Itshowsyouhowthesolutionchangedduringtheprocessofdiffusion.Beforethesoluteparticlesreachedequilibrium,therewere3soluteparticlesontheleftsideofthemembraneand5soluteparticlesontherightsideofthemembrane.Whenthesoluteparticlesreachedequilibrium,therewere4soluteparticlesoneachsideofthemembrane.Therewas1moresoluteparticleontheleftsideofthemembranethanbefore.So,forthesoluteparticlestoreachequilibrium,moresoluteparticlesmusthavemovedacrossthemembranetotheleftthantotheright.Answer:Theansweris(A).Vision\n(b)Mistakesaboutcountingnumbersintheimage.(C) the Pacific Ocean(D) the Southern Ocean\n(B) to the right than to the left\nFigure 9. Examples of commonsense mistakes about understanding maps and counting numbers.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Guidewordsappearoneachpageofadictionary.Theytellyouthefirstwordandlastwordonthepage.Theotherwordsonthepagecomebetweentheguidewordsinalphabeticalorder.Toputwordsinalphabeticalorder,puttheminorderbytheirfirstletters.Ifthefirstlettersarethesame,lookatthesecondletters.Ifthesecondlettersarethesame,lookatthethirdletters,andsoon.Ifonewordisshorter,andtherearenomoreletterstocompare,thentheshorterwordcomesfirstinalphabeticalorder.Forexample,becomesbeforebed.Putthewordsinalphabeticalorder.Sincehardshipisbetweentheguidewordshelping-hunter,itwouldbefoundonthatpage.Answer:Theansweris(B).Options:(A) hospitalProblem\nPredictionQuestion:Whichwordwouldyoufindonadictionarypagewiththefollowingguidewords?helping-hunterContext:Thediagrambelowshowsasolutionwithonesolute.Eachsoluteparticleisrepresentedbyagreenball.Thesolutionfillsaclosedcontainerthatisdividedinhalfbyamembrane.Themembrane,representedbyadottedline,ispermeabletothesoluteparticles.Thediagramshowshowthesolutioncanchangeovertimeduringtheprocessofdiffusion.Rationale:Guidewordsappearoneachpageofadictionary.Theytellyouthefirstwordandlastwordonthepage.Theotherwordsonthepagecomebetweentheguidewordsinalphabeticalorder.Toputwordsinalphabeticalorder,puttheminorderbytheirfirstletters.Ifthefirstlettersarethesame,lookatthesecondletters.Ifthesecondlettersarethesame,lookatthethirdletters,andsoon.Ifonewordisshorter,andtherearenomoreletterstocompare,thentheshorterwordcomesfirstinalphabeticalorder.Forexample,becomesbeforebed.Putthewordsinalphabeticalorder.Sincehospitalisbetweentheguidewordshelping-hunter,itwouldbefoundonthatpage.Answer:Theansweris(A).(B) hardshipVisionN/A\nFigure 10. Examples of commonsense mistakes about utilizing alphabet.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': "Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Measurementsarewrittenwithbothanumberandaunit.Theunitcomesafterthenumber.Theunitshowswhatthenumbermeans.TemperaturecanbewrittenwithunitsofdegreesFahrenheit(°F)orCelsius(°C).Usethelistbelowtocomparethetwounits.212°F|Waterboils|100°C98.6°F|Bodytemperature|37°C68°F|Roomtemperature|20°C32°F|Waterfreezes|0°C.Thebetterestimateforthetemperatureoftheaironahotdayis36°F.36°Fistoohot.Answer:Theansweris(A).Options:(A) 36°FProblem\nPredictionQuestion:Whatisthetemperatureoftheaironahotday?Context:Selectthebetterestimate.Rationale:Measurementsarewrittenwithbothanumberandaunit.Theunitcomesafterthenumber.Theunitshowswhatthenumbermeans.TemperaturecanbewrittenwithunitsofdegreesFahrenheit(°F)orCelsius(°C).Usethelistbelowtocomparethetwounits.212°F|Waterboils|100°C98.6°F|Bodytemperature|37°C68°F|Roomtemperature|20°C32°F|Waterfreezes|0°C.Thebetterestimateforthetemperatureoftheaironahotdayis36°C.36°Fistoocold.Answer:Theansweris(B).(B) 36°CVisionN/A\nRationale::Asentenceisagroupofwordsthatexpressesacompletethought.ThebandI'minhasbeenrehearsingdailybecausewehaveaconcertintwoweeks.Arun-onsentenceisformedwhentwosentencesareruntogether,joinedbyjustacommaorbynopunctuationatall.Ifonlyacommaisused,therun-oniscalledacommasplice…ThebandI'minhasbeenrehearsingdailysincewehaveaconcertintwoweeks.Thisisnotarun-onsentence.Itisformedacommaspliceformedfromtwosentencesruntogether,joinedwithoutjustacomma.Right-handednessiscontrolledbygenetics,interestinglyenough,thesamegenesarealsoresponsibleforthebrainbecomingmorespecializedatcertaintasks.Answer:Theansweris(B).Options:(A) yesProblem\nPredictionQuestion:Isthisarun-onsentence?Right-handednessiscontrolledbygenetics,interestinglyenough,thesamegenesarealsoresponsibleforthebrainbecomingmorespecializedatcertaintasks.Context:N/ARationale:Asentenceisagroupofwordsthatexpressesacompletethought.ThebandI'minhasbeenrehearsingdailybecausewehaveaconcertintwoweeks.Arun-onsentenceisformedwhentwosentencesareruntogether,joinedbyjustacommaorbynopunctuationatall.Ifonlyacommaisused,therun-oniscalledacommasplice…ThebandI'minhasbeenrehearsingdailysincewehaveaconcertintwoweeks.Thisisarun-onsentence.Itisacommaspliceformedfromtwosentencesruntogether,joinedbyjustacomma.Right-handednessiscontrolledbygenetics,interestinglyenough,thesamegenesarealsoresponsibleforthebrainbecomingmorespecializedatcertaintasks.Answer:Theansweris(A).(B) noVisionN/A(a)Logicalmistakewherethemodelfailsatcomparisons.\n(b)Logicalmistakewherethethereisacontradictionintherationale.\nFigure 11. Examples of logical mistakes.", 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': "Multimodal Chain-of-Thought Reasoning in Language Models\nOptions:(A) black stripes on its skinProblem\nPredictionRationale:Thewayanorganismlooksoractsiscalledatrait.Scientistsusefossilstolearnmoreaboutthetraitsofancientorganisms.Fossilscanpreservetheremainsofbodypartsandactivities.Afossilofabodypart,suchasatailorawing,cantellyouwhatanorganismlookedlike.Afossilofanorganism'sactivities,suchasaburroworafootprint,cantellyouabouttheorganism'sbehavior.Herearethreeexamplesoffossilsandthetraitsthatyoucanobservefromthem:Thisisafossilofananimal.Thisfossiltellsyouthattheanimalhadaspiral-shapedshell.Thisisafossilofaplant.Thisfossiltellsyouthattheplanthadsmallleavesarrangedinabranchedpattern.Thisisafossilofananimal'sfootprint.Thisfossiltellsyouthattheanimalcouldwalkonland.Anorganism'sfossilmaynotshowalloftheorganism'straits.Thisisbecausemostbodypartsaredestroyedduringfossilformation.Whenanorganism'sbodyturnsintoafossil,onlyafewbodypartsareusuallypreserved.Answer:Theansweris(C).(B) large fins on its bodyVision(C) a long, thin bodyQuestion:WhichtraitdidPalaeopythonhave?Selectthetraityoucanobserveonthefossil.Context:ThispictureshowsafossilofanancientanimalcalledPalaeopython.Palaeopythonlivedintreesandcouldgrowmorethansixfeetlong.\nRationale:Thewayanorganismlooksoractsiscalledatrait.Scientistsusefossilstolearnmoreaboutthetraitsofancientorganisms.Fossilscanpreservetheremainsofbodypartsandactivities.Afossilofabodypart,suchasatailorawing,cantellyouwhatanorganismlookedlike.Afossilofanorganism'sactivities,suchasaburroworafootprint,cantellyouabouttheorganism'sbehavior.Herearethreeexamplesoffossilsandthetraitsthatyoucanobservefromthem:Thisisafossilofananimal.Thisfossiltellsyouthattheanimalhadaspiral-shapedshell.Thisisafossilofaplant.Thisfossiltellsyouthattheplanthadsmallleavesarrangedinabranchedpattern.Thisisafossilofananimal'sfootprint.Thisfossiltellsyouthattheanimalcouldwalkonland.Anorganism'sfossilmaynotshowalloftheorganism'straits.Thisisbecausemostbodypartsaredestroyedduringfossilformation.Whenanorganism'sbodyturnsintoafossil,onlyafewbodypartsareusuallypreserved.Answer:Theansweris(B).\nFigure 12. Examples of answers are incorrect while the CoT is correct.", 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}]: %s
2023-12-07 12:21:41,145 - INFO - Check the results [{'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nA.2. Two-Stage Training Performance with Different Sizes of LMs.\nIn Section 3, we obverse that incorporating vision features helps generate more effective rationales, thus leading to improved\nanswer accuracy. Besides incorporating vision features, it is possible to scale the LM size to mitigate the issue of incorrect\nrationales. Figure 7 shows the answer accuracy with UniﬁedQA Base and UniﬁedQA Large . When using a larger LM, the\naccuracy of the baseline (w/o vision features) is boosted. The result indicates that scaling the LM is possible to mitigate the\nissue of incorrect rationales. However, the performance is still much inferior to using vision features. The result further\nveriﬁes the effectiveness of our Multimodal-CoT with different sizes of LMs.\nbase large6580100\n70.5384.9182.9791.68Accuracy (%)w/o Vision Modality w/ Vision Modality\nFigure 7. Answer accuracy with different sizes of LMs.\nB. Experimental Details\nB.1. Baseline Methods\nFollowing Lu et al. (2022a), our baselines include three types of methods:\n(i) Visual question answering (VQA) models (Yu et al., 2019; Anderson et al., 2018; Kim et al., 2018; Gao et al., 2019; Lu\net al., 2021; Li et al., 2019). The VQA baselines take the question, the context, and choices as the textual input, take the\nimage as the vision input, and predict the score distribution over choice candidates via a linear classiﬁer.\n(ii) Text-to-text LM models. UniﬁedQA (Khashabi et al., 2020) is adopted as it is the best ﬁne-tuning model in Lu et al.\n(2022a). UniﬁedQA takes the textual information as the input and outputs the answer option. The image is converted into a\ncaption extracted by an image captioning model based on ViT and GPT-2.6UniﬁedQA treats our task as a text generation\nproblem. In Lu et al. (2022a), it is trained to generate a target answer text, i.e., one of the candidate options. Then, the most\nsimilar option is selected as the ﬁnal prediction to evaluate the question answering accuracy.\n(iii) GPT-3.5 models (Chen et al., 2020) based on the text-davinci-002 engine. The inference is based on the few-shot\nprompting, where two in-context examples from the training set are concatenated before the test instance.\nFor UniﬁedQA and GPT-3.5, CoT is applied after the answer (Lu et al., 2022a). Besides the above baselines, we develop a\nstronger baseline with a slight modiﬁcation of the output format of UniﬁedQA. Instead of predicting the answer texts, our\nbaseline directly predicts the choice, e.g., the answer is B . This setting helps our baseline achieve better results than the\nexisting UniﬁedQA. Therefore, we use the stronger method as the language only baseline for analysis.\nB.2. Details of Vision Features\nIn Section 6.2, we compared four types of vision features, CLIP (Radford et al., 2021), DETR (Carion et al., 2020), and\nResNet (He et al., 2016). The speciﬁc models are: (i) CLIP: RN101;7(ii) DETR: detr resnet101 dc5;8(iii) ResNet: we use\n6https://huggingface.co/nlpconnect/vit-gpt2-image-captioning .\n7https://github.com/jianjieluo/OpenAI-CLIP-Feature .\n8https://github.com/facebookresearch/detr .', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nthe averaged pooled features of a pre-trained ResNet50 CNN. Table 9 presents the dimension of the vision features (after the\nfunction VisionExtractor( ·) in Eq. 3). For ResNet-50, we repeat the pooled features of ResNet-50 to the same length as the\ntext sequence to imitate the patch-like features, where each patch is the same as the pooled image features.\nTable 9. Dimension of vision features\nMethod Dimension\nCLIP (49, 2048)\nDETR (100, 256)\nResNet (512, 2048)\nC. Examples of Case Studies\nTo better understand the behavior of Multimodal-CoT, we manually investigate randomly selected examples generated by\nour approach. Table 8 summarizes the categorization results generated by Multimodal-CoT. We randomly picked up 50\nsamples whose prediction results were correct and 50 samples whose prediction results were incorrect.\nWe ﬁnd that the correct samples contain a certain amount of incorrect chain-of-thought. As shown in Figure 8(b), the model\ngenerates the incorrect rationale, “ Animals cannot their food by digesting other organisms ” but the predicted answer is\ncorrect. The result indicates that CoT may not always beneﬁt the answer inference, and the model is robust to some extent —\nit can predict the correct answer by ignoring incorrect rationales.\nFor incorrect samples, commonsense mistake is the most frequent error type. The model also makes commonsense mistakes\nwhen answering the questions requires commonsense knowledge, e.g., understand maps and counting numbers in the\nimages (Figure 9), and utilizing the alphabet (Figure 10). The other type of mistake is the logical mistake, where there are\ncontradictions in the reasoning chains (Figure 11). In addition, there are cases that the CoT is correct but might not be\nnecessarily related to answer options; thus the model chooses the incorrect answer.\nThe analysis indicates that there are prospective directions for future studies. On the one hand, it is possible to improve the\nquality of CoT by (i) using more ﬁne-grained interaction of language and vision features; and (ii) injecting commonsense\nknowledge. On the other hand, applying a ﬁltering mechanism to using only the effective CoT to infer the answer and\neliminate irrelevant CoT.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Birds,mammals,fish,reptiles,andamphibiansaregroupsofanimals.Theanimalsineachgrouphavetraitsincommon.Scientistssortanimalsintogroupsbasedontraitstheyhaveincommon.Thisprocessiscalledclassification.Atigersharkisafish.Itlivesunderwater.Ithasfins,notlimbs.Tigersharkscannocturnal.Theymeansthattheyareactivemostlyatnight.Ared-tailedhawkisabird.Ithasfeathers,twowings,andabeak.Red-tailedhawkslivetheirbeteethakstopushforpre.birds,and,repamptiles…Answer:Theansweris(B).Options:(A) red-tailed hawkProblem\nPredictionQuestion:Selectthebirdbelow.Context:Birdshavefeathers,twowings,andabeak.Aperegrinefalconisanexampleofabird.Rationale:Birds,mammals,fish,reptiles,andamphibiansaregroupsofanimals.Theanimalsineachgrouphavetraitsincommon.Scientistssortanimalsintogroupsbasedontraitstheyhaveincommon.Thisprocessiscalledclassification.Atigersharkisafish.Itlivesunderwater.Ithasfins,notlimbs.Tigersharkscannocturnal.Theymeansthattheyareactivemostlyatnight.Ared-tailedhawkisabird.Ithasfeathers,twowings,andabeak.Red-tailedhawkslivetheirbeteethakstopushforpre.birds,and,repamptiles…Answer:Theansweris(A).Vision(B) tiger shark\n(a)CoTiscorrect\nRationale:Leopardthepast,scientistsclassifiedlivingorganismsintotwogroups:plantsandanimals.Overthepast300years,scientistshavediscoveredmanymoretypesoforganisms.Today,manyscientistsclassifyorganismsintosixbroadgroups,calledkingdoms.Organismsineachkingdomhavespecifictraits.Thetablebelowshowssometraitsusedtodescribeeachkingdom.|Bacteria|Archaea|Protists|Fungi|Animals|Plants.Howmanycellsdotheyhave?|one|one|oneormany|oneormany|many|many.Dotheircellshaveanucleus?|no|no|yes|yes|yes|yesnCantheircellsmakefood?|somespeciescan|somespeciescan|somespeciescan|no|no|yesLeoparduswiediiisananimal.Animalcellscannotmaketheirownfood.Animalscannottheirfoodbydigestingotherorganisms.Answer:Theansweris(B).Options:(A) noProblem\nPredictionQuestion:CanLeoparduswiediicellsmaketheirownfood?Context:ThisorganismisLeoparduswiedii.Itisamemberoftheanimalkingdom.Leoparduswiediiiscommonlycalledamargay.MargaysliveintheforestinCentralandSouthAmerica.Margaysarenocturnal.Anocturnalanimalsleepsmostofthedayandisawakeatnight.Rationale:Inthepast,scientistsclassifiedlivingorganismsintotwogroups:plantsandanimals.Overthepast300years,scientistshavediscoveredmanymoretypesoforganisms.Today,manyscientistsclassifyorganismsintosixbroadgroups,calledkingdoms.Organismsineachkingdomhavespecifictraits.Thetablebelowshowssometraitsusedtodescribeeachkingdom.|Bacteria|Archaea|Protists|Fungi|Animals|Plants.Howmanycellsdotheyhave?|one|one|oneormany|oneormany|many|manyDotheircellshaveanucleus?|no|no|yes|yes|yes|yesCantheircellsmakefood?|somespeciescan|somespeciescan|somespeciescan|no|no|yesLeoparduswiediiisananimal.Animalcellscannotmaketheirownfood.Animalsgettheirfoodbydigestingotherorganisms.Answer:Theansweris(A).Vision(B) yes\n(b)CoTisincorrect\nFigure 8. Examples of answers are correct while the CoT is correct (a) or incorrect (b).', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Oceansarehugebodiesofsaltwater.Theworldhasfiveoceans.Alloftheoceansareconnected,makingoneworldocean.ThisistheIndianOcean.Answer:Theansweris(B).Options:(A) the Atlantic OceanProblem\nPredictionQuestion:Whichoceanishighlighted?Context:N/ARationale:Oceansarehugebodiesofsaltwater.Theworldhasfiveoceans.Alloftheoceansareconnected,makingoneworldocean.ThisisthePacificOcean.Answer:Theansweris(C).Vision(B) the Indian Ocean\n(a)Mistakesaboutmaps.\nRationale:Inasolution,soluteparticlesmoveandspreadthroughoutthesolvent.Thediagrambelowshowshowasolutioncanchangeovertime.Soluteparticlesmovefromtheareawheretheyareatahigherconcentrationtotheareawheretheyareatalowerconcentration…Lookatthediagramagain.Itshowsyouhowthesolutionchangedduringtheprocessofdiffusion.Beforethesoluteparticlesreachedequilibrium,therewere5soluteparticlesontheleftsideofthemembraneand4soluteparticlesontherightsideofthemembrane.Whenthesoluteparticlesreachedequilibrium,therewere5soluteparticlesoneachsideofthemembrane.Therewas1moresoluteparticleontherightsideofthemembranethanbefore.So,forthesoluteparticlestoreachequilibrium,moresoluteparticlesmusthavemovedacrossthemembranetotherightthantotheright.Answer:Theansweris(B).Options:(A) to the left than to the rightProblem\nPredictionQuestion:Completethetexttodescribethediagram.Soluteparticlesmovedinbothdirectionsacrossthepermeablemembrane.Butmoresoluteparticlesmovedacrossthemembrane().Whentherewasanequalconcentrationonbothsides,theparticlesreachedequilibrium.Context:Thediagrambelowshowsasolutionwithonesolute.Eachsoluteparticleisrepresentedbyagreenball.Thesolutionfillsaclosedcontainerthatisdividedinhalfbyamembrane.Themembrane,representedbyadottedline,ispermeabletothesoluteparticles.Thediagramshowshowthesolutioncanchangeovertimeduringtheprocessofdiffusion.Rationale:Inasolution,soluteparticlesmoveandspreadthroughoutthesolvent.Thediagrambelowshowshowasolutioncanchangeovertime.Soluteparticlesmovefromtheareawheretheyareatahigherconcentrationtotheareawheretheyareatalowerconcentration…Lookatthediagramagain.Itshowsyouhowthesolutionchangedduringtheprocessofdiffusion.Beforethesoluteparticlesreachedequilibrium,therewere3soluteparticlesontheleftsideofthemembraneand5soluteparticlesontherightsideofthemembrane.Whenthesoluteparticlesreachedequilibrium,therewere4soluteparticlesoneachsideofthemembrane.Therewas1moresoluteparticleontheleftsideofthemembranethanbefore.So,forthesoluteparticlestoreachequilibrium,moresoluteparticlesmusthavemovedacrossthemembranetotheleftthantotheright.Answer:Theansweris(A).Vision\n(b)Mistakesaboutcountingnumbersintheimage.(C) the Pacific Ocean(D) the Southern Ocean\n(B) to the right than to the left\nFigure 9. Examples of commonsense mistakes about understanding maps and counting numbers.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': 'Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Guidewordsappearoneachpageofadictionary.Theytellyouthefirstwordandlastwordonthepage.Theotherwordsonthepagecomebetweentheguidewordsinalphabeticalorder.Toputwordsinalphabeticalorder,puttheminorderbytheirfirstletters.Ifthefirstlettersarethesame,lookatthesecondletters.Ifthesecondlettersarethesame,lookatthethirdletters,andsoon.Ifonewordisshorter,andtherearenomoreletterstocompare,thentheshorterwordcomesfirstinalphabeticalorder.Forexample,becomesbeforebed.Putthewordsinalphabeticalorder.Sincehardshipisbetweentheguidewordshelping-hunter,itwouldbefoundonthatpage.Answer:Theansweris(B).Options:(A) hospitalProblem\nPredictionQuestion:Whichwordwouldyoufindonadictionarypagewiththefollowingguidewords?helping-hunterContext:Thediagrambelowshowsasolutionwithonesolute.Eachsoluteparticleisrepresentedbyagreenball.Thesolutionfillsaclosedcontainerthatisdividedinhalfbyamembrane.Themembrane,representedbyadottedline,ispermeabletothesoluteparticles.Thediagramshowshowthesolutioncanchangeovertimeduringtheprocessofdiffusion.Rationale:Guidewordsappearoneachpageofadictionary.Theytellyouthefirstwordandlastwordonthepage.Theotherwordsonthepagecomebetweentheguidewordsinalphabeticalorder.Toputwordsinalphabeticalorder,puttheminorderbytheirfirstletters.Ifthefirstlettersarethesame,lookatthesecondletters.Ifthesecondlettersarethesame,lookatthethirdletters,andsoon.Ifonewordisshorter,andtherearenomoreletterstocompare,thentheshorterwordcomesfirstinalphabeticalorder.Forexample,becomesbeforebed.Putthewordsinalphabeticalorder.Sincehospitalisbetweentheguidewordshelping-hunter,itwouldbefoundonthatpage.Answer:Theansweris(A).(B) hardshipVisionN/A\nFigure 10. Examples of commonsense mistakes about utilizing alphabet.', 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': "Multimodal Chain-of-Thought Reasoning in Language Models\nRationale:Measurementsarewrittenwithbothanumberandaunit.Theunitcomesafterthenumber.Theunitshowswhatthenumbermeans.TemperaturecanbewrittenwithunitsofdegreesFahrenheit(°F)orCelsius(°C).Usethelistbelowtocomparethetwounits.212°F|Waterboils|100°C98.6°F|Bodytemperature|37°C68°F|Roomtemperature|20°C32°F|Waterfreezes|0°C.Thebetterestimateforthetemperatureoftheaironahotdayis36°F.36°Fistoohot.Answer:Theansweris(A).Options:(A) 36°FProblem\nPredictionQuestion:Whatisthetemperatureoftheaironahotday?Context:Selectthebetterestimate.Rationale:Measurementsarewrittenwithbothanumberandaunit.Theunitcomesafterthenumber.Theunitshowswhatthenumbermeans.TemperaturecanbewrittenwithunitsofdegreesFahrenheit(°F)orCelsius(°C).Usethelistbelowtocomparethetwounits.212°F|Waterboils|100°C98.6°F|Bodytemperature|37°C68°F|Roomtemperature|20°C32°F|Waterfreezes|0°C.Thebetterestimateforthetemperatureoftheaironahotdayis36°C.36°Fistoocold.Answer:Theansweris(B).(B) 36°CVisionN/A\nRationale::Asentenceisagroupofwordsthatexpressesacompletethought.ThebandI'minhasbeenrehearsingdailybecausewehaveaconcertintwoweeks.Arun-onsentenceisformedwhentwosentencesareruntogether,joinedbyjustacommaorbynopunctuationatall.Ifonlyacommaisused,therun-oniscalledacommasplice…ThebandI'minhasbeenrehearsingdailysincewehaveaconcertintwoweeks.Thisisnotarun-onsentence.Itisformedacommaspliceformedfromtwosentencesruntogether,joinedwithoutjustacomma.Right-handednessiscontrolledbygenetics,interestinglyenough,thesamegenesarealsoresponsibleforthebrainbecomingmorespecializedatcertaintasks.Answer:Theansweris(B).Options:(A) yesProblem\nPredictionQuestion:Isthisarun-onsentence?Right-handednessiscontrolledbygenetics,interestinglyenough,thesamegenesarealsoresponsibleforthebrainbecomingmorespecializedatcertaintasks.Context:N/ARationale:Asentenceisagroupofwordsthatexpressesacompletethought.ThebandI'minhasbeenrehearsingdailybecausewehaveaconcertintwoweeks.Arun-onsentenceisformedwhentwosentencesareruntogether,joinedbyjustacommaorbynopunctuationatall.Ifonlyacommaisused,therun-oniscalledacommasplice…ThebandI'minhasbeenrehearsingdailysincewehaveaconcertintwoweeks.Thisisarun-onsentence.Itisacommaspliceformedfromtwosentencesruntogether,joinedbyjustacomma.Right-handednessiscontrolledbygenetics,interestinglyenough,thesamegenesarealsoresponsibleforthebrainbecomingmorespecializedatcertaintasks.Answer:Theansweris(A).(B) noVisionN/A(a)Logicalmistakewherethemodelfailsatcomparisons.\n(b)Logicalmistakewherethethereisacontradictionintherationale.\nFigure 11. Examples of logical mistakes.", 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}, {'page_content': "Multimodal Chain-of-Thought Reasoning in Language Models\nOptions:(A) black stripes on its skinProblem\nPredictionRationale:Thewayanorganismlooksoractsiscalledatrait.Scientistsusefossilstolearnmoreaboutthetraitsofancientorganisms.Fossilscanpreservetheremainsofbodypartsandactivities.Afossilofabodypart,suchasatailorawing,cantellyouwhatanorganismlookedlike.Afossilofanorganism'sactivities,suchasaburroworafootprint,cantellyouabouttheorganism'sbehavior.Herearethreeexamplesoffossilsandthetraitsthatyoucanobservefromthem:Thisisafossilofananimal.Thisfossiltellsyouthattheanimalhadaspiral-shapedshell.Thisisafossilofaplant.Thisfossiltellsyouthattheplanthadsmallleavesarrangedinabranchedpattern.Thisisafossilofananimal'sfootprint.Thisfossiltellsyouthattheanimalcouldwalkonland.Anorganism'sfossilmaynotshowalloftheorganism'straits.Thisisbecausemostbodypartsaredestroyedduringfossilformation.Whenanorganism'sbodyturnsintoafossil,onlyafewbodypartsareusuallypreserved.Answer:Theansweris(C).(B) large fins on its bodyVision(C) a long, thin bodyQuestion:WhichtraitdidPalaeopythonhave?Selectthetraityoucanobserveonthefossil.Context:ThispictureshowsafossilofanancientanimalcalledPalaeopython.Palaeopythonlivedintreesandcouldgrowmorethansixfeetlong.\nRationale:Thewayanorganismlooksoractsiscalledatrait.Scientistsusefossilstolearnmoreaboutthetraitsofancientorganisms.Fossilscanpreservetheremainsofbodypartsandactivities.Afossilofabodypart,suchasatailorawing,cantellyouwhatanorganismlookedlike.Afossilofanorganism'sactivities,suchasaburroworafootprint,cantellyouabouttheorganism'sbehavior.Herearethreeexamplesoffossilsandthetraitsthatyoucanobservefromthem:Thisisafossilofananimal.Thisfossiltellsyouthattheanimalhadaspiral-shapedshell.Thisisafossilofaplant.Thisfossiltellsyouthattheplanthadsmallleavesarrangedinabranchedpattern.Thisisafossilofananimal'sfootprint.Thisfossiltellsyouthattheanimalcouldwalkonland.Anorganism'sfossilmaynotshowalloftheorganism'straits.Thisisbecausemostbodypartsaredestroyedduringfossilformation.Whenanorganism'sbodyturnsintoafossil,onlyafewbodypartsareusuallypreserved.Answer:Theansweris(B).\nFigure 12. Examples of answers are incorrect while the CoT is correct.", 'document_title': 'Multimodal Chain-of-Thought Reasoning in Language ModelsMultimodal Chain-of-Thought Reasoning in Language Models.pdf'}]: %s
2023-12-07 12:23:43,332 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:23:45,731 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:23:54,039 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:23:54,141 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:23:54,217 - INFO - query success: 1 documents found
2023-12-07 12:23:59,543 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:24:02,303 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:24:02,443 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:24:05,162 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:24:05,313 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:24:08,344 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:24:08,468 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:24:24,259 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:24:24,398 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:24:24,453 - INFO - Received requests to /inference endpoint
2023-12-07 12:24:24,554 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 12:24:24,555 - INFO - Received request: {'username': 'amin', 'prompt': 'provide me with a summary of multimodal paper.', 'memory': True, 'conversation_number': 2, 'AI_assistance': False, 'collection_name': 'docs', 'llm_model': 'Llama_13b'}
2023-12-07 12:25:07,137 - INFO - Processed the request successfully
2023-12-07 12:26:17,461 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:26:17,619 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:26:17,646 - INFO - Received requests to /inference endpoint
2023-12-07 12:26:17,747 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 12:26:17,748 - INFO - Received request: {'username': 'amin', 'prompt': 'provide me with a summary of multimodal paper.', 'memory': True, 'conversation_number': 2, 'AI_assistance': False, 'collection_name': 'docs', 'llm_model': 'Llama_13b'}
2023-12-07 12:26:56,719 - INFO - Processed the request successfully
2023-12-07 12:27:08,893 - INFO - Received requests to /inference endpoint
2023-12-07 12:27:08,994 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 12:27:08,995 - INFO - Received request: {'username': 'amin', 'prompt': 'Give me a summary of the paper', 'memory': False, 'conversation_number': 1, 'AI_assistance': False, 'collection_name': 'docs', 'llm_model': 'Llama_13b'}
2023-12-07 12:27:15,031 - INFO - Processed the request successfully
2023-12-07 12:33:45,181 - INFO - Created a temporary directory at /tmp/tmpqpn275fr
2023-12-07 12:33:45,181 - INFO - Writing /tmp/tmpqpn275fr/_remote_module_non_scriptable.py
2023-12-07 12:33:46,997 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2023-12-07 12:33:51,307 - INFO - Load pretrained SentenceTransformer: hkunlp/instructor-xl
2023-12-07 12:34:03,515 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:34:24,108 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:34:24,230 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:34:26,064 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:34:26,175 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:34:28,947 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:34:29,055 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:34:36,639 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:34:36,733 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:34:36,773 - INFO - Received requests to /inference endpoint
2023-12-07 12:34:36,874 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 12:34:36,874 - INFO - Received request: {'username': 'amin', 'prompt': 'provide me with a summary of multimodal paper.', 'memory': True, 'conversation_number': 2, 'AI_assistance': False, 'collection_name': 'docs', 'llm_model': 'Llama_13b'}
2023-12-07 12:34:36,888 - INFO - Collection name: Amin_docs
2023-12-07 12:35:19,401 - INFO - Processed the request successfully
2023-12-07 12:35:38,104 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:35:40,740 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:35:40,816 - INFO - Received requests to /inference endpoint
2023-12-07 12:35:40,917 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 12:35:40,917 - INFO - Received request: {'username': 'amin', 'prompt': 'hi', 'memory': True, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-12-07 12:35:51,777 - INFO - Processed the request successfully
2023-12-07 12:36:34,921 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:36:34,995 - INFO - Received requests to /inference endpoint
2023-12-07 12:36:35,096 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 12:36:35,097 - INFO - Received request: {'username': 'amin', 'prompt': 'what is the title of the paper?', 'memory': True, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-12-07 12:36:37,889 - INFO - Processed the request successfully
2023-12-07 12:43:32,375 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:43:32,538 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:43:51,590 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:43:51,691 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:43:51,731 - INFO - Received requests to /inference endpoint
2023-12-07 12:43:51,832 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 12:43:51,832 - INFO - Received request: {'username': 'amin', 'prompt': 'what is the novility of the approach in the paper?', 'memory': True, 'conversation_number': 2, 'AI_assistance': False, 'collection_name': 'paper', 'llm_model': 'Llama_13b'}
2023-12-07 12:43:51,837 - INFO - Collection name: Amin_paper
2023-12-07 12:44:31,344 - INFO - Processed the request successfully
2023-12-07 12:44:39,443 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:44:49,068 - INFO - classes: ['paper', 'docs']: %s
2023-12-07 12:44:49,146 - INFO - Received requests to /inference endpoint
2023-12-07 12:44:49,247 - INFO - Received a batch of request with batch size of: 1 
2023-12-07 12:44:49,247 - INFO - Received request: {'username': 'amin', 'prompt': 'what is the novility of the approach in the paper?', 'memory': True, 'conversation_number': 2, 'AI_assistance': True, 'collection_name': None, 'llm_model': 'Llama_13b'}
2023-12-07 12:44:51,154 - INFO - Processed the request successfully
