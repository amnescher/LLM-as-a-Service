{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database file /home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/API/chat_bot_db.db found.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from ray import serve\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import json\n",
    "import re\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel\n",
    "import textwrap\n",
    "from fastapi import FastAPI\n",
    "from langchain.chains import RetrievalQA\n",
    "import logging\n",
    "\n",
    "from langchain.vectorstores import Weaviate\n",
    "import weaviate\n",
    "import wandb\n",
    "from backend_database import Database\n",
    "from langchain.schema import messages_from_dict, messages_to_dict\n",
    "from langchain.memory.chat_message_histories.in_memory import ChatMessageHistory\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from typing import List\n",
    "import json\n",
    "import os\n",
    "import yaml\n",
    "import time\n",
    "\n",
    "\n",
    "# ------------------- Configuration --------------------\n",
    "class Config:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "\n",
    "\n",
    "with open(\"cluster_conf.yaml\", 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    config = Config(**config)\n",
    "\n",
    "# ------------------- Initialize Ray Cluster --------------------\n",
    "class Input(BaseModel):\n",
    "    username: Optional[str]\n",
    "    prompt: Optional[str]\n",
    "    memory: Optional[bool]\n",
    "    conversation_number: Optional[int]\n",
    "    AI_assistance: Optional[bool]\n",
    "    collection_name: Optional[str]\n",
    "    llm_model: Optional[str]\n",
    "\n",
    "\n",
    "# ------------------------------ LLM Deployment -------------------------------\n",
    "\n",
    "\n",
    "class PredictDeployment:\n",
    "    def __init__(self, model_id,\n",
    "                 temperature =  0.01,\n",
    "                 max_new_tokens=  512,\n",
    "                 repetition_penalty= 1.1,\n",
    "                 batch_size= 2):\n",
    "        import os\n",
    "        from langchain.llms import HuggingFacePipeline\n",
    "        from torch import cuda, bfloat16\n",
    "        import transformers\n",
    "        from langchain.chains import RetrievalQA\n",
    "        from langchain import PromptTemplate\n",
    "        from langchain.vectorstores import Weaviate\n",
    "\n",
    "        # class initialization\n",
    "        try:\n",
    "            self.weaviate_client = weaviate.Client(\n",
    "                url=config.weaviate_client_url,   \n",
    "            )\n",
    "        except:\n",
    "            self.logger.error(\"Error in connecting to Weaviate\")\n",
    "        self.model_id = model_id\n",
    "        self.temperature = temperature\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.repetition_penalty = repetition_penalty\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        #setting up weight and bias logging\n",
    "        self.wandb_logging_enabled = config.WANDB_ENABLE\n",
    "        if self.wandb_logging_enabled:\n",
    "            try:\n",
    "                wandb.login(key = config.WANDB_KEY)\n",
    "                wandb.init(project=\"Service Metrics\", notes=\"custom step\")\n",
    "                # Define the custom x axis metric\n",
    "                wandb.define_metric(\"The number of input tokens\")\n",
    "                wandb.define_metric(\"The number of generated tokens\")\n",
    "                wandb.define_metric(\"Inference Time\")\n",
    "                wandb.define_metric(\"token/second\")\n",
    "            except:\n",
    "                self.wandb_logging_enabled = False\n",
    "                pass\n",
    "        # load config\n",
    "        with open(\"cluster_conf.yaml\", \"r\") as self.file:\n",
    "            self.config = yaml.safe_load(self.file)\n",
    "            self.config = Config(**self.config)\n",
    "\n",
    "        self.access_token = config.Hugging_ACCESS_TOKEN\n",
    "        self.device = f\"cuda:{cuda.current_device()}\" if cuda.is_available() else \"cpu\"\n",
    "        self.B_INST, self.E_INST = \"[INST]\", \"[/INST]\"\n",
    "        self.B_SYS, self.E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "        self.DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "        You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "        If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "        self.instruction = \"Chat History:\\n\\n{chat_history} \\n\\nUser: {user_input}\"\n",
    "        self.system_prompt = \"You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\"\n",
    "\n",
    "        self.prompt_template = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "        Current conversation:\n",
    "        {chat_history}\n",
    "        Human: {input}\n",
    "        AI:\"\"\"\n",
    "\n",
    "        # setting up logging for debugging\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "            filename=\"app.log\",  # specify the file name if you want logging to be stored in a file\n",
    "            filemode=\"a\",  # append to the log file if it exists\n",
    "        )\n",
    "\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.propagate = True\n",
    "        \n",
    "        # set quantization configuration to load large model \n",
    "        self.device = f\"cuda:{cuda.current_device()}\" if cuda.is_available() else \"cpu\"\n",
    "     \n",
    "        # this requires the `bitsandbytes` library\n",
    "        bnb_config = transformers.BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=bfloat16,\n",
    "        )\n",
    "\n",
    "        # begin initializing HF items, need auth token for these\n",
    "        model_config = transformers.AutoConfig.from_pretrained(self.model_id, use_auth_token = self.access_token)\n",
    "\n",
    "        self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_id,\n",
    "            trust_remote_code=True,\n",
    "            config=model_config,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            use_auth_token = self.access_token\n",
    "        )\n",
    "        self.model.eval()\n",
    "\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "            model_id, padding_side=\"left\"\n",
    "        )\n",
    "        self.generate_text = transformers.pipeline(\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            return_full_text=True,  # langchain expects the full text\n",
    "            task=\"text-generation\",\n",
    "            # we pass model parameters here too\n",
    "            # stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
    "            temperature=self.temperature,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "            max_new_tokens=self.max_new_tokens,  # mex number of tokens to generate in the output\n",
    "            repetition_penalty=self.repetition_penalty,  # without this output begins repeating\n",
    "            batch_size=self.batch_size,  # number of independent sequences to generate\n",
    "        )\n",
    "        self.llm = HuggingFacePipeline(pipeline=self.generate_text)\n",
    "        self.generate_text.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "        # embeddings = OpenAIEmbeddings()\n",
    "\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\", return_messages=True, output_key=\"output\"\n",
    "        )\n",
    "        self.template = self.get_prompt(self.instruction)\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"chat_history\", \"user_input\"], template=self.template\n",
    "        )\n",
    "\n",
    "        self.embeddings = HuggingFaceInstructEmbeddings(\n",
    "            model_name=\"hkunlp/instructor-xl\", model_kwargs={\"device\": \"cuda\"}\n",
    "        )\n",
    "\n",
    "        self.weaviate_vectorstore = Weaviate(\n",
    "            self.weaviate_client, \n",
    "            \"Admin_General_collection\",\n",
    "            'page_content', \n",
    "            attributes=['page_content']\n",
    "        )\n",
    "        # self.vectorstore_video = Chroma(\"YouTube_store\", persist_directory=video_persist_directory, embedding_function=self.embeddings)\n",
    "        self.QA_document = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=self.weaviate_vectorstore.as_retriever(),\n",
    "            memory=self.memory,\n",
    "            output_key=\"output\",\n",
    "        )\n",
    "\n",
    "        # self.QA_video = RetrievalQA.from_chain_type(llm=self.llm, chain_type=\"stuff\", retriever=self.vectorstore_video.as_retriever(),memory = self.memory,output_key= \"output\")\n",
    "        self.database = Database()\n",
    "\n",
    "    def get_collection_based_retriver(self, client, collection):\n",
    "        content = ['page_content']\n",
    "        weaviate_vectorstore = Weaviate(\n",
    "            client,\n",
    "            str(collection),\n",
    "            'page_content', \n",
    "            attributes=['page_content']\n",
    "        )\n",
    "        return weaviate_vectorstore\n",
    "    \n",
    "    \n",
    "    '''def get_collection_based_retriver(self, collection):\n",
    "        vectorstore_doc = Chroma(\n",
    "            str(collection),\n",
    "            persist_directory=self.Doc_persist_directory,\n",
    "            embedding_function=self.embeddings,\n",
    "        )\n",
    "\n",
    "        return vectorstore_doc'''\n",
    "\n",
    "    def get_prompt(self, instruction):\n",
    "        SYSTEM_PROMPT = self.B_SYS + self.DEFAULT_SYSTEM_PROMPT + self.E_SYS\n",
    "        prompt_template = self.B_INST + SYSTEM_PROMPT + instruction + self.E_INST\n",
    "        return prompt_template\n",
    "\n",
    "    def cut_off_text(self, text, prompt):\n",
    "        cutoff_phrase = prompt\n",
    "        index = text.find(cutoff_phrase)\n",
    "        if index != -1:\n",
    "            return text[:index]\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "    def remove_substring(self, string, substring):\n",
    "        return string.replace(substring, \"\")\n",
    "\n",
    "    def cleaning_memory(self):\n",
    "        print(self.memory.chat_memory.messages)\n",
    "        self.memory.clear()\n",
    "        print(\"Chat History Deleted\")\n",
    "\n",
    "    def generate(self, text):\n",
    "        prompt = self.get_prompt(text)\n",
    "        with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=512,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "            final_outputs = self.tokenizer.batch_decode(\n",
    "                outputs, skip_special_tokens=True\n",
    "            )[0]\n",
    "            final_outputs = self.cut_off_text(final_outputs, \"</s>\")\n",
    "            final_outputs = self.remove_substring(final_outputs, prompt)\n",
    "\n",
    "        return final_outputs  # , outputs\n",
    "\n",
    "    def parse_text(self, text):\n",
    "        pattern = r\"\\s*Assistant:\\s*\"\n",
    "        pattern2 = r\"\\s*AI:\\s*\"\n",
    "        cleaned_text = re.sub(pattern, \"\", text)\n",
    "        cleaned_text = re.sub(pattern2, \"\", cleaned_text)\n",
    "        wrapped_text = textwrap.fill(cleaned_text, width=100)\n",
    "        return wrapped_text\n",
    "\n",
    "    def AI_assistance(self, request):\n",
    "        try:\n",
    "            #self.logger.info(\"Received request: %s\", request.dict())\n",
    "            input_prompt = request[\"prompt\"]\n",
    "            AI_assistance = request[\"AI_assistance\"]\n",
    "            username = request[\"username\"]\n",
    "            memory = request[\"memory\"]\n",
    "            conversation_number = request[\"conversation_number\"]\n",
    "            collection_name = request[\"collection_name\"]\n",
    "            # Initialize memory based on whether it's a new chat or not\n",
    "            if not memory:\n",
    "                memory = ConversationBufferMemory(\n",
    "                    memory_key=\"chat_history\", return_messages=True, output_key=\"output\"\n",
    "                )\n",
    "            else:\n",
    "                # Retrieve the latest conversation from the database\n",
    "                if conversation_number <= 0 or conversation_number is None:\n",
    "                    latest_chat = self.database.retrieve_conversation(\n",
    "                        {\n",
    "                            \"username\": username,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    chat_history = latest_chat[\"content\"]\n",
    "                    conversation_number = latest_chat[\"conversation_number\"]\n",
    "                    print(\n",
    "                        f\" the latest conversation for {username} with the conversation_id of {conversation_number} retrieved from database\"\n",
    "                    )\n",
    "                else:\n",
    "                    chat_history = self.database.retrieve_conversation(\n",
    "                        {\n",
    "                            \"username\": username,\n",
    "                            \"conversation_number\": conversation_number,\n",
    "                        }\n",
    "                    )\n",
    "                    chat_history = chat_history[\"content\"]\n",
    "                    print(\n",
    "                        f\"chat histroy for {username} with the conversation_number of {conversation_number} retrieved from database\"\n",
    "                    )\n",
    "\n",
    "                # Initialize memory from the retrieved conversation\n",
    "                \n",
    "                retrieved_messages = messages_from_dict(json.loads(chat_history))\n",
    "                retrieved_chat_history = ChatMessageHistory(messages=retrieved_messages)\n",
    "                memory = ConversationBufferMemory(\n",
    "                    chat_memory=retrieved_chat_history, memory_key=\"chat_history\"\n",
    "                )\n",
    "\n",
    "            # Create an LLM chain with the appropriate mode\n",
    "\n",
    "            if AI_assistance:\n",
    "                llm_chain = LLMChain(\n",
    "                    llm=self.llm,\n",
    "                    prompt=self.prompt,\n",
    "                    verbose=False,\n",
    "                    memory=memory,\n",
    "                    output_key=\"output\",\n",
    "                )\n",
    "            else:\n",
    "                #check if collection exists\n",
    "                if self.database.check_collection_exists(request):\n",
    "                    if username[0].isalpha():\n",
    "                        new_username= username[0].upper() + username[1:]\n",
    "                    collection_name = f\"{new_username}_{collection_name}\"\n",
    "                    self.logger.info(f\"Collection name: {collection_name}\")\n",
    "                    retriever = self.get_collection_based_retriver(self.weaviate_client,collection_name)\n",
    "                    llm_chain = RetrievalQA.from_chain_type(\n",
    "                        llm=self.llm,\n",
    "                        chain_type=\"stuff\",\n",
    "                        retriever=retriever.as_retriever(),\n",
    "                        memory=memory,\n",
    "                        output_key=\"output\",\n",
    "                    )\n",
    "                else: \n",
    "                    return {\"output\": \"Error: Collection does not exist\"}\n",
    "            pre_inference_memo = llm_chain.memory.chat_memory.messages\n",
    "            pre_inference_memo = \" \".join(\n",
    "                message.content for message in pre_inference_memo\n",
    "            )\n",
    "            pre_inference_memo_token_len = len(\n",
    "                self.tokenizer.tokenize(pre_inference_memo)\n",
    "            )\n",
    "            # Generate a response based on the mode\n",
    "            inference_start_time = time.time()\n",
    "\n",
    "            # Generate a response based on the mode\n",
    "            if AI_assistance:\n",
    "                response = llm_chain.predict(user_input=input_prompt)\n",
    "            else:\n",
    "               \n",
    "                response = llm_chain.run(input_prompt)\n",
    "                \n",
    "\n",
    "            # End measuring time after inference\n",
    "            inference_end_time = time.time()\n",
    "\n",
    "            # Calculate and log the elapsed time\n",
    "            inference_elapsed_time = inference_end_time - inference_start_time\n",
    "\n",
    "            # Store the conversation\n",
    "            extracted_messages = llm_chain.memory.chat_memory.messages\n",
    "            ingest_to_db = messages_to_dict(extracted_messages)\n",
    "            input_token_number = (\n",
    "                len(self.tokenizer.tokenize(input_prompt))\n",
    "                + pre_inference_memo_token_len\n",
    "            )\n",
    "            gen_token_number = len(self.tokenizer.tokenize(response))\n",
    "            db_response = self.database.update_conversation(\n",
    "                {\n",
    "                    \"username\": username,\n",
    "                    \"content\": json.dumps(ingest_to_db),\n",
    "                    \"gen_token_number\": gen_token_number,\n",
    "                    \"prompt_token_number\": input_token_number,\n",
    "                    \"conversation_number\": conversation_number,\n",
    "                }\n",
    "            )\n",
    "            response = self.parse_text(response)\n",
    "\n",
    "            wandb_log = {\n",
    "                \"The number of input tokens\": input_token_number,\n",
    "                \"The number of generated tokens\": gen_token_number,\n",
    "                \"Inference Time\": inference_elapsed_time,\n",
    "                \"token/second\": gen_token_number / inference_elapsed_time,\n",
    "            }\n",
    "            if self.wandb_logging_enabled:   \n",
    "                wandb.log(wandb_log)\n",
    "            self.logger.info(\"Processed the request successfully\")\n",
    "            return {\"output\": response}\n",
    "\n",
    "        except ConnectionError as ce:\n",
    "            self.logger.error(\"Error processing the request: %s\", str(ce))\n",
    "            # Handle connection errors (for example, interacting with the database or calling APIs)\n",
    "            return {\"output\": \"ConnectionError: An error occurred while processing the request\"}\n",
    "\n",
    "        except KeyError as ke:\n",
    "            # Handle key errors (for example, accessing a key in a dictionary that doesn’t exist)\n",
    "            self.logger.error(\"Error processing the request: %s\", str(ke))\n",
    "            return {\"output\": \" KeyError: An error occurred while processing the request\"}\n",
    "\n",
    "        except Exception as e:\n",
    "            # General exception to catch any other unforeseen errors\n",
    "            self.logger.error(\"Error processing the request: %s\", str(e))\n",
    "            return {\"output\": \"Exception : An error occurred while processing the request\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maminnsabet\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ubuntu/.netrc\n",
      "/usr/lib/python3.10/configparser.py:793: ResourceWarning: unclosed <ssl.SSLSocket fd=73, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.0.224', 35216), raddr=('35.186.228.49', 443)>\n",
      "  raise NoOptionError(option, section)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/home/ubuntu/dev/lib/python3.10/site-packages/wandb/sdk/lib/ipython.py:77: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/falcon/gti_repo/LLM-as-a-Service/Ray_demo/llmAPI/wandb/run-20231207_133443-6sxygbca</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aminnsabet/Service%20Metrics/runs/6sxygbca' target=\"_blank\">genial-wind-431</a></strong> to <a href='https://wandb.ai/aminnsabet/Service%20Metrics' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aminnsabet/Service%20Metrics' target=\"_blank\">https://wandb.ai/aminnsabet/Service%20Metrics</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aminnsabet/Service%20Metrics/runs/6sxygbca' target=\"_blank\">https://wandb.ai/aminnsabet/Service%20Metrics/runs/6sxygbca</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/dev/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1020: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/dev/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1efcad1c94344bd38ebd8d78d3dad57c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/dev/lib/python3.10/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "my_calls = PredictDeployment(model_id = \"meta-llama/Llama-2-13b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "request = {\"username\":\"amin\",\"prompt\":\"Give me a summary of the paper\",\"memory\":False,\"conversation_number\":1,\"AI_assistance\":False,\"collection_name\":\"docs\",\"llm_model\":\"Llama-13B\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': '1,1,1,1,0000000000000001.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_calls.AI_assistance(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weaviate_client = weaviate.Client(\n",
    "            url=\"http://localhost:8080\",   \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classes': [{'class': 'Test_class_2',\n",
       "   'description': 'normal description',\n",
       "   'invertedIndexConfig': {'bm25': {'b': 0.75, 'k1': 1.2},\n",
       "    'cleanupIntervalSeconds': 60,\n",
       "    'stopwords': {'additions': None, 'preset': 'en', 'removals': None}},\n",
       "   'moduleConfig': {'text2vec-transformers': {'poolingStrategy': 'masked_mean',\n",
       "     'vectorizeClassName': True,\n",
       "     'vectorizerClassName': False}},\n",
       "   'multiTenancyConfig': {'enabled': False},\n",
       "   'properties': [{'dataType': ['text'],\n",
       "     'description': 'the text from the documents parsed',\n",
       "     'indexFilterable': True,\n",
       "     'indexSearchable': True,\n",
       "     'moduleConfig': {'text2vec-transformers': {'skip': False,\n",
       "       'vectorizePropertyName': False}},\n",
       "     'name': 'page_content',\n",
       "     'tokenization': 'word'},\n",
       "    {'dataType': ['text'],\n",
       "     'indexFilterable': True,\n",
       "     'indexSearchable': True,\n",
       "     'moduleConfig': {'text2vec-transformers': {'skip': False,\n",
       "       'vectorizePropertyName': False}},\n",
       "     'name': 'document_title',\n",
       "     'tokenization': 'word'}],\n",
       "   'replicationConfig': {'factor': 1},\n",
       "   'shardingConfig': {'virtualPerPhysical': 128,\n",
       "    'desiredCount': 1,\n",
       "    'actualCount': 1,\n",
       "    'desiredVirtualCount': 128,\n",
       "    'actualVirtualCount': 128,\n",
       "    'key': '_id',\n",
       "    'strategy': 'hash',\n",
       "    'function': 'murmur3'},\n",
       "   'vectorIndexConfig': {'skip': False,\n",
       "    'cleanupIntervalSeconds': 300,\n",
       "    'maxConnections': 64,\n",
       "    'efConstruction': 128,\n",
       "    'ef': -1,\n",
       "    'dynamicEfMin': 100,\n",
       "    'dynamicEfMax': 500,\n",
       "    'dynamicEfFactor': 8,\n",
       "    'vectorCacheMaxObjects': 1000000000000,\n",
       "    'flatSearchCutoff': 40000,\n",
       "    'distance': 'cosine',\n",
       "    'pq': {'enabled': False,\n",
       "     'bitCompression': False,\n",
       "     'segments': 0,\n",
       "     'centroids': 256,\n",
       "     'trainingLimit': 100000,\n",
       "     'encoder': {'type': 'kmeans', 'distribution': 'log-normal'}}},\n",
       "   'vectorIndexType': 'hnsw',\n",
       "   'vectorizer': 'text2vec-transformers'},\n",
       "  {'class': 'Admin_pdf',\n",
       "   'description': 'normal description',\n",
       "   'invertedIndexConfig': {'bm25': {'b': 0.75, 'k1': 1.2},\n",
       "    'cleanupIntervalSeconds': 60,\n",
       "    'stopwords': {'additions': None, 'preset': 'en', 'removals': None}},\n",
       "   'moduleConfig': {'text2vec-transformers': {'poolingStrategy': 'masked_mean',\n",
       "     'vectorizeClassName': True,\n",
       "     'vectorizerClassName': False}},\n",
       "   'multiTenancyConfig': {'enabled': False},\n",
       "   'properties': [{'dataType': ['text'],\n",
       "     'description': 'the text from the documents parsed',\n",
       "     'indexFilterable': True,\n",
       "     'indexSearchable': True,\n",
       "     'moduleConfig': {'text2vec-transformers': {'skip': False,\n",
       "       'vectorizePropertyName': False}},\n",
       "     'name': 'page_content',\n",
       "     'tokenization': 'word'},\n",
       "    {'dataType': ['text'],\n",
       "     'indexFilterable': True,\n",
       "     'indexSearchable': True,\n",
       "     'moduleConfig': {'text2vec-transformers': {'skip': False,\n",
       "       'vectorizePropertyName': False}},\n",
       "     'name': 'document_title',\n",
       "     'tokenization': 'word'}],\n",
       "   'replicationConfig': {'factor': 1},\n",
       "   'shardingConfig': {'virtualPerPhysical': 128,\n",
       "    'desiredCount': 1,\n",
       "    'actualCount': 1,\n",
       "    'desiredVirtualCount': 128,\n",
       "    'actualVirtualCount': 128,\n",
       "    'key': '_id',\n",
       "    'strategy': 'hash',\n",
       "    'function': 'murmur3'},\n",
       "   'vectorIndexConfig': {'skip': False,\n",
       "    'cleanupIntervalSeconds': 300,\n",
       "    'maxConnections': 64,\n",
       "    'efConstruction': 128,\n",
       "    'ef': -1,\n",
       "    'dynamicEfMin': 100,\n",
       "    'dynamicEfMax': 500,\n",
       "    'dynamicEfFactor': 8,\n",
       "    'vectorCacheMaxObjects': 1000000000000,\n",
       "    'flatSearchCutoff': 40000,\n",
       "    'distance': 'cosine',\n",
       "    'pq': {'enabled': False,\n",
       "     'bitCompression': False,\n",
       "     'segments': 0,\n",
       "     'centroids': 256,\n",
       "     'trainingLimit': 100000,\n",
       "     'encoder': {'type': 'kmeans', 'distribution': 'log-normal'}}},\n",
       "   'vectorIndexType': 'hnsw',\n",
       "   'vectorizer': 'text2vec-transformers'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weaviate_client.schema.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deprecations': [], 'objects': []}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weaviate_client.data_object.get(class_name=\"Admin_pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "def add_pdf_to_DB(pdf_path):\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            pages = loader.load_and_split()\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=1500, chunk_overlap=200\n",
    "            )\n",
    "            texts = text_splitter.split_documents(pages)\n",
    "            return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = add_pdf_to_DB(\"docs/paper.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Temporal Early Exits for Efﬁcient Video Object\\nDetection\\nAmin Sabet\\nSchool of Electronics and Computer Science\\nUniversity of Southampton, UK\\nms4r18@soton.ac.ukJonathon Hare\\nSchool of Electronics and Computer Science\\nUniversity of Southampton, UK\\njsh2@ecs.soton.ac.uk,\\nBashir Al-Hashimi\\nFaculty of Natural and Mathematical Sciences\\nKing’s College London, UK\\nbashir.al-hashimi@kcl.ac.ukGeoff V . Merrett\\nSchool of Electronics and Computer Science\\nUniversity of Southampton, UK\\ngvm@ecs.soton.ac.uk\\nAbstract\\nTransferring image-based object detectors to the domain of video remains chal-\\nlenging under resource constraints. Previous efforts utilised optical ﬂow to allow\\nunchanged features to be propagated, however, the overhead is considerable when\\nworking with very slowly changing scenes from applications such as surveillance.\\nIn this paper, we propose temporal early exits to reduce the computational complex-\\nity of per-frame video object detection. Multiple temporal early exit modules with\\nlow computational overhead are inserted at early layers of the backbone network to\\nidentify the semantic differences between consecutive frames. Full computation\\nis only required if the frame is identiﬁed as having a semantic change to previous\\nframes; otherwise, detection results from previous frames are reused. Experiments\\non CDnet show that our method signiﬁcantly reduces the computational complexity\\nand execution of per-frame video object detection up to 34×compared to existing', metadata={'source': 'docs/paper.pdf', 'page': 0}),\n",
       " Document(page_content='on CDnet show that our method signiﬁcantly reduces the computational complexity\\nand execution of per-frame video object detection up to 34×compared to existing\\nmethods with an acceptable reduction of 2.2% in mAP.\\n1 Introduction\\nObject detection is one of the fundamental tasks in computer vision and serves as a core approach\\nin many practical applications, such as robotics and video surveillance [ 26,2]. Object detection in\\nstatic images has achieved remarkable successes in recent years using CNNs [ 7]. However, video\\nobject detection has now emerged as a new challenge beyond image data. This is due to the high\\ncomputational cost introduced by applying existing image object detection networks on numerous\\nindividual video frames. Figure 1-(a) shows the overview of the per-frame video object detection\\napproach, where all video frames are processed by a similar CNN. Deploying per-frame video object\\ndetection becomes even more challenging for resource and energy-constrained applications.\\nDeep optical ﬂow approaches [ 29] tackle the computational complexity challenge of video object\\ndetection by taking advantage of temporal information in videos. They exploit feature similarity\\nbetween consecutive frames to reduce the expensive feature computation on most frames and improve\\nthe speed. Instead of extracting features of all frames by a deep CNN, deep optical ﬂow uses a lighter\\nnetwork to extract, propagate and aggregate features of video frames with similar features to previous', metadata={'source': 'docs/paper.pdf', 'page': 0}),\n",
       " Document(page_content='network to extract, propagate and aggregate features of video frames with similar features to previous\\nframes. Figure 1-(b) shows the overview of deep optical ﬂow approaches [24].\\nFeature similarity between successive video frames occurs often in applications such as facility\\nmonitoring and surveillance systems, where the camera is static and there are less frequently moving\\nobjects in the videos [ 11]. Whilst reducing computational complexity, optical ﬂow approaches require\\nPreprint. Under review.arXiv:2106.11208v1  [cs.CV]  21 Jun 2021', metadata={'source': 'docs/paper.pdf', 'page': 0}),\n",
       " Document(page_content='Figure 1: Comparison video object detection approaches. (a) Conventional approach: applying deep\\nCNNs on individual frames. (b) Deep ﬂow estimation: employing lighter ﬂow estimation network\\nto propagate features across frames. (c) Proposed pipeline: identifying semantic variation in early\\nstages of network and avoiding deep CNN computation for unchanged video frames\\nsubstantial computational effort to generate and aggregating feature maps even though the features of\\nsuccessive frames remain unchanged.\\nTo address the challenge of identifying and processing frames with unchanged features, we propose\\na computationally lightweight, Temporal Early Exit Module (TEEM), which identiﬁes semantic\\nvariations between consecutive frames. We show that the full computational effort of a network [ 17] is\\nnot required to distinguish and detect semantic changes between frames. We then use TEEM to build\\na per-frame video object detection pipeline, shown in ﬁgure 1-(C). In this pipeline, a TEEM identiﬁes\\nsemantic variation between features of consecutive frames in the very early stages of the feature\\nnetwork. Then, the TEEM conditionally activates the deeper layers of the network if a semantic\\ndifference is detected between frames. If a frame is identiﬁed to be semantically unchanged, then\\nthe object detection results from the previous frame are reused. By identifying unchanged frames at\\nearlier stages and thus, avoiding processing video frames, the proposed pipeline signiﬁcantly reduces', metadata={'source': 'docs/paper.pdf', 'page': 1}),\n",
       " Document(page_content='earlier stages and thus, avoiding processing video frames, the proposed pipeline signiﬁcantly reduces\\nthe computational complexity and speeds up the video object detection.\\nThe contributions of this paper are as follows.\\n•We propose Temporal Early Exit Modules which exploit the features of the early con-\\nvolutional layers to infer a 2D spatial attention map between video frames. The atten-\\ntion map encodes the semantic variations between consecutive frames. The attention map is\\nthen used to generate reﬁned feature maps, feeding into a classiﬁer to classify frames into\\nchanged and unchanged categories. The experiments on CDnet dataset [ 5] show that the\\nTEEM classiﬁes frames into changed and unchanged classes with 94% accuracy.\\n•We demonstrate a temporal early exit video object detection pipeline that uses TEEMs\\nto conditionally process video frames. Full computation effort is required only for pro-\\ncessing video frames with a semantic variation to previous frames. The evaluation of the\\nproposed pipline on CDnet dataset [ 5] shows up to 34×speed up of per-frame video object\\ndetection with less than 2.2% reduction in mAP.\\n2 Related Work\\nOptical Flow Estimation. Recent optical ﬂow estimation approaches divide all video frames into\\nkey frame and non-key frame sets [ 30] [24] [29]. While deep networks are only applied on the\\nkey-frames, a lighter ﬂow estimation network is exploited to obtain the features of non-key frames', metadata={'source': 'docs/paper.pdf', 'page': 1}),\n",
       " Document(page_content='key frame and non-key frame sets [ 30] [24] [29]. While deep networks are only applied on the\\nkey-frames, a lighter ﬂow estimation network is exploited to obtain the features of non-key frames\\n(shown in Figure 1), which results in speeding up the algorithm. However, the ﬂow estimation\\napproaches rely on selecting the best key-frame, where the features of non-key frames are estimated\\nfrom key-frame features. Deep feature ﬂow network (DFF) [ 30] adopts a simple ﬁxed key-frame\\nselection scheme and fails to take account of the quality of key-frame and the optical-ﬂow. The\\ndifferential network [ 24] proposes a binary differential classiﬁer network to detect the key-frames\\nwith classiﬁcation accuracy of up to 76%. In order to address the challenge of ﬁnding the optimal key\\nvideo frames, instead of replacing the temporal features with the features of key-frames, PSLA [ 6]\\nproposes to progressively update the temporal features through a recursive feature updating network.\\nTo summarise, while ﬂow estimation networks speed up video object detection, they require large\\nand redundant computations for feature propagation for the majority of unchanged video frames.\\n2', metadata={'source': 'docs/paper.pdf', 'page': 1}),\n",
       " Document(page_content='Figure 2: Overview of the proposed temporal early exits video object detection pipeline.TEEMs are\\nadded to feature network of a object detection network (main branch) to detect unchanged video\\nframes and avoid redundant computations.\\n(A)\\n(B)\\n(C)\\nfri objects of interest i fri+j objects of interest i+j MFI (fri,fri+j)\\nFigure 3: Scenery change examples, (A) shows a changed scenery with the IoU=0. (B) shows an\\nunchanged scenery with the IoU=1 (C) shows a changed scenery with the IoU=0.19\\nEarly Exit CNNs. In a similar line of work to ours, early exit classiﬁers promote faster inference\\nby allowing classiﬁcation for easy instances to exit the network early. This class of algorithms\\nare based on the observation that often the features learned at earlier stages of a deep network can\\ncorrectly infer a large subset of the data samples. For example, BranchyNet [ 25] is a neural network\\narchitecture which adds side branches to the main branch, the original backbone network, to allow\\ncertain test samples to exit early. BranchyNet is trained by the joint optimization of loss functions for\\nall exit points. In the follow-up work, conditional deep learning (CDL) [ 16] measures the efﬁciency\\nimprovement due to the addition of the linear classiﬁer at each convolutional layer to ﬁnd the optimal\\nlayer for early exit branches in the backbone network. Early exit approaches reduce the runtime and\\nenergy consumption of image classiﬁcation. However, they have been developed only for image', metadata={'source': 'docs/paper.pdf', 'page': 2}),\n",
       " Document(page_content='layer for early exit branches in the backbone network. Early exit approaches reduce the runtime and\\nenergy consumption of image classiﬁcation. However, they have been developed only for image\\nclassiﬁcation applications.\\nScene Change Detection. Scene change detection is a fundamental problem in the ﬁeld of computer\\nvision. The most classical approach for scene change detection is image differencing [ 21,22], which\\ngenerates a change map by determining the set of pixels that are ‘signiﬁcantly different’ across two\\nimages. Then, a binary mask is generated by thresholding the change map. Although, this method\\nhas low computational cost, the raw pixel features are incapable of effectively differentiating between\\nsemantic changes and noise. To obtain more discriminative features, image rationing [ 12], change\\nvector analysis [ 4,3], Markov random ﬁelds [ 14] and dictionary learning [ 12,10] have been proposed.\\nHowever, they are still sensitive to noise and illumination changes [19].\\n3', metadata={'source': 'docs/paper.pdf', 'page': 2}),\n",
       " Document(page_content='3 Temporal Early Exits Video Object Detection Pipeline\\nWe propose a novel video object detection pipeline to tackle the computational complexity of video\\nobject detection. An overview of the proposed pipeline is shown in ﬁgure 2. Given a sequence of\\nvideo frames, multiple Temporal Early Exit Modules (TEEM) are used to build the early exit branches.\\nEach early exit branch uses features extracted in the early stages of the feature network to identify\\nany semantic variation between the features of consecutive video frames. If any semantic variations\\nis distinguished by a TEEM, a full computation effort (main branch) will be needed to process the\\nvideo frame. On the other hand, the computation process of a video frame without any semantic\\nvariations will be terminated at an early exit branch and the detection results from the previous frame\\nwill be reused. We describe the implementation details of the proposed pipeline in this section.\\n3.1 Scenery Change\\nThe key challenge in identifying feature variations is qualifying semantic dissimilarities between\\nimages [ 1]. To address this challenge, we introduce a scenery change (SC)metric to quantify\\nsemantic variations between video frames. We deﬁne a semantic variation to be only a variation in\\nobjects of interest where noisy variations are considered to be nuisance. Given any pairs of video\\nframes, a TEEM aims to identify semantic differences between frames. A scenery changes between a', metadata={'source': 'docs/paper.pdf', 'page': 3}),\n",
       " Document(page_content='objects of interest where noisy variations are considered to be nuisance. Given any pairs of video\\nframes, a TEEM aims to identify semantic differences between frames. A scenery changes between a\\npair of video frames (fri,fri+j), if the maximum motion in the Moving Fields of Interest (MFI) is\\ngreater than the variation threshold value τvaras follows,\\nSC(fri,fri+j) ={\\n1 max( MFI(fri,fri+j))>τvar\\n0otherwise. (1)\\nWe compute MFI set by measuring intersection over union (IoU) of each object of interest, Ok, across\\nframe pairs as follows,\\nMFI(ok) ={\\n1−IoU(Ofri\\nk,Ofri+j\\nk )ifOk∈fri∧fri+j\\n1 Otherwise. (2)\\nEach element in MFI quantiﬁes the variation in an object of interest between a pair of video frames.\\nThe scenery change metric distinguishes noisy variations from semantic variations in video frame\\npairs. Figure 3 visualizes some examples of scenery change metric for video frame pair.\\n3.2 Temporal Early Exit Module\\nTo detect a scenery change between frame pairs (fri,fri+j), we propose TEEM which consists of\\nAttention andClassiﬁer components. This section describes these components in details. Figure 2\\nshows an overview of a TEEM.\\nTheAttention component is represented as a function parameterized by θAttenlin equation (3).FAttn\\ntakesZl\\niandZl\\ni+jas inputs, and outputs an attention map AttMaplwhich encodes the semantic\\nvariations happened between friandfri+j. In equation (3),Zl\\niandZl\\ni+jare image features of', metadata={'source': 'docs/paper.pdf', 'page': 3}),\n",
       " Document(page_content='takesZl\\niandZl\\ni+jas inputs, and outputs an attention map AttMaplwhich encodes the semantic\\nvariations happened between friandfri+j. In equation (3),Zl\\niandZl\\ni+jare image features of\\nthe previous friand currentfri+jvideo frames, respectively, encoded by convolutional layer lin\\nthe feature network. The attention function captures the semantic variations between frame pairs\\nin the representation space by subtracting Zl\\nifromZl\\ni+jin equation (4). Then, by concatenating\\nthe subtraction results with the feature representation of the current video frame, Zl\\ni+j, a 2D spatial\\nattention map associated with the video frame pair is generated in equation (6).\\nAttMapl=FAttn(Zl\\ni,Zl\\ni+j;θAttenl). (3)\\nZsub=Zi+j−Zj, (4)\\n¯Zc=Conct [Zi:Zsub], (5)\\nAttMapl=Sigmoid (Conv(¯Zc)), (6)\\n4', metadata={'source': 'docs/paper.pdf', 'page': 3}),\n",
       " Document(page_content='In equations (6)and(5),Conct [; ],Conv ,Sigmoid indicate concatenation, convolutional layer and\\nsigmoid function, respectively. Following [ 13], to avoid introducing any form of global normalization,\\nwe use sigmoid, instead of softmax, for computing the attention map. The attention map makes\\nTEEM learn and focus on the motions of objects of interest between frame pairs. Notably, a TEEM\\nrequires storing the latest feature maps Zl\\nito be used for successive frames.\\nThe generated attention map alongside the feature map of the current video frame Zl\\ni+jare fed to\\naclassifier to detect any scenery change between frame pairs. The classifier component is a\\nclassiﬁer with two outputs which represented as a function FClass, parameterized by θClassin equation\\n(7). It takes the attention map, AttMapl, and the feature map of the current video frame Zl\\ni+jand\\nproduces a class label. The possible labels are changed and unchanged frame. To classify the video\\nframe, the AttMaplis multiplied element-wise with the feature map of current video frame to get the\\nreﬁned feature maps ZAttin equation (8). The reﬁned feature maps are passed through a convolutional\\nlayer and a fully connected layer, equation (9), to produce a class label.\\nClsl\\ni=FClass(Zl\\ni+j,AttMapl;θClass). (7)\\nZAtt=AttMapl⊙Zl\\ni+j. (8)\\nClsl\\ni=FC(ReLU (Norm (Conv(Zatt)))). (9)\\nIn equation (9)FC, ReLU, Norm and Conv indicate fully connected, ReLU, batch normalization and\\nconvolutional layers, respectively. Clsl', metadata={'source': 'docs/paper.pdf', 'page': 4}),\n",
       " Document(page_content='i+j. (8)\\nClsl\\ni=FC(ReLU (Norm (Conv(Zatt)))). (9)\\nIn equation (9)FC, ReLU, Norm and Conv indicate fully connected, ReLU, batch normalization and\\nconvolutional layers, respectively. Clsl\\niis the output of the TEEM, located at the convolutional layer\\nlof feature network. Each video frame can be classiﬁed by a TEEM located in an early exit branch\\ninto a changed or unchanged pair.\\n3.3 Training Policy\\nAn early exit branch, including a TEEM, can be added after any convolutional layer in the feature\\nnetwork. However, the location of an early exit branch in the feature network determines its\\nperformance in identifying semantic variations. Therefore, a temporal early attention video object\\ndetection pipeline have multiple early exits to identify semantic variations. However, each TEEM in\\nan early exit branch is regarded as a stand-alone module, hence, trained separately from the backbone\\nnetwork. Let (fri,fri+j)be a sampled video frame pair and θbe all parameters in TEEM in early\\nexitl. For a target label, P, the objective is to minimize the cross-entropy loss as follows:\\nLTEEMk(P,Pθ;θ) =−∑\\nkPk·log(Pθk) (10)\\nwherekis classiﬁer outputs and,\\nPθ=Softmax (S), (11)\\nand\\nS=FTEEM l(fri,fri+j;θ). (12)\\nHere,FTEEM lis the the output of the TEEM in early exit l. In the forward pass of training, a video\\nframe pair is passed through the network, including both the main branch and early exits. The\\noutput of each early exit is recorded to calculate the error of TEEM in the early exit. In backward', metadata={'source': 'docs/paper.pdf', 'page': 4}),\n",
       " Document(page_content='frame pair is passed through the network, including both the main branch and early exits. The\\noutput of each early exit is recorded to calculate the error of TEEM in the early exit. In backward\\npropagation, the error of each exit is passed back only through the TEEMs and the weights of TEEM\\nare updated using gradient descent. Notably, the weights of the main branch are not updated during\\ntraining of the TEEMs.\\n3.4 Fast inference in a Temporal Early Exit Video Object Detection Pipeline\\nAlgorithm 1 summarizes the temporal early exit video object detection pipeline inference algorithm.\\nGiven a pre-trained object detection network as the main branch, Fmain, withjearly exit branches,\\nthe ﬁrst video frame is processed by the main branch. The inference for successive frames proceeds\\nby feeding frames through the feature network of the main branch until reaching the ﬁrst early exit\\nbranch. Then, the softmax and entropy of FTEEM in the early exit are calculated to evaluate the\\nconﬁdence of the classiﬁer in prediction. If the entropy is less than the threshold γand the class\\nlabel with the maximum score (probability) is unchanged, the inference procedure terminates and\\n5', metadata={'source': 'docs/paper.pdf', 'page': 4}),\n",
       " Document(page_content='object detection results from the previous frame are returned. If the class label with maximum score\\nis changed, then the frame is processed by the main branch. For the entropy greater than γ, the frame\\ncontinues to be processed by the feature network till the next early exit branch. If the softmax and\\nentropy of all early exits are greater than thresholds, the frame continues to be processed by the main\\nbranch.\\nAlgorithm 1 Inference algorithm for video object detection\\nRequire: Video frames{I}\\nEnsure: detection =None\\nfori←1to←length (I)do\\nifi== 1 then\\ndetection←Fmain(Ii)\\nreturn detection\\nelse\\nforj←1toj= #( early exits )do\\ny←FTEEM j(Ii,Ii−1)\\nS←softmax (y)\\ne←entropy (S)\\nife<τ and arg max (S) == Unchanged then\\nreturn detection\\nelse ife<γ and arg max (S) == Changed then\\ndetection←Fmain(Ii)\\nreturn detection\\nelse ife>γ then\\npass\\nend if\\nend for\\nend if\\nend for\\n4 Dataset\\nThe CDnet dataset [ 5] provides a realistic, camera-captured, diverse set of videos for change detection.\\nThe videos have been selected to cover a wide range of detection challenges and are representative\\nof typical indoor and outdoor visual data captured in surveillance, smart environment. This dataset\\nconsists of 33 camera-captured videos ( ∼70,000 frames) with boats, trucks, and pedestrians as objects\\nof interest. It contains a range of challenging factors, including turbulence, dynamic background,\\ncamera jitter challenging weather and pan-tilt-zooming (PTZ). All videos come with ground-truth', metadata={'source': 'docs/paper.pdf', 'page': 5}),\n",
       " Document(page_content='of interest. It contains a range of challenging factors, including turbulence, dynamic background,\\ncamera jitter challenging weather and pan-tilt-zooming (PTZ). All videos come with ground-truth\\nsegmentation of motion areas for each video frame. However, CDnet dataset doesn’t provide labels\\nfor the individual instances of moving objects. This makes measuring SC metric challenging and\\ninaccurate in scenarios with multiple moving objects. To evaluate those scenarios, we also build\\nthe scenery change dataset. Our dataset is based on VIRAT dataset [ 15] which is inspired and used\\nby many change detection applications [ 17]. We used Vatic engine [ 27] to label individual moving\\nobjects with bounding boxes. The dataset consists of 20 camera-captured videos with pedestrian, car\\nand cyclist as objects of interest. We believe our dataset can complement existing change detection\\ndatasets like CDnet[5].\\n4.1 Dataset Balancing\\nTo train TEEMs, video frame pairs need to be randomly sampled from videos. However, random\\nsampling of frame pairs causes the unbalanced class dataset (changed scenery and unchanged scenery\\nclasses). The balance of classes depends on the interval between sampled video frame pairs. For\\nlong sampling interval, the dataset skews to changed scenery frame pairs. On the other hand, a short\\nsampling interval leads to skewing to unchanged scenery. As a result of the unbalanced dataset,', metadata={'source': 'docs/paper.pdf', 'page': 5}),\n",
       " Document(page_content='long sampling interval, the dataset skews to changed scenery frame pairs. On the other hand, a short\\nsampling interval leads to skewing to unchanged scenery. As a result of the unbalanced dataset,\\nthe classiﬁer of TEEMs tends to ignore small classes, while concentrating on learning to classify\\nlarge ones accurately. We tackle the unbalanced dataset problem by proposing the dynamic sampling\\ninterval approach. To this end, we divide the semantic variation into ten classes. Each class represents\\n6', metadata={'source': 'docs/paper.pdf', 'page': 5}),\n",
       " Document(page_content='a variation threshold between frame pairs i.e., class one and two represent frame pairs with variation\\nthreshold of [0, 0.1) and [0.1, 0.2), respectively. Consequently, starting from a random sampling\\ninterval, we sample frame pairs, measure the minimum IoU of objects across frame pairs and classify\\nthem into one of the ten classes. As the number of pairs in one class exceeds a threshold, we reject\\nthe sampling that falls into the class and adjusts the sampling interval toward the direction which\\nincreases classes with a smaller number of samples until all classes are balanced. We also face another\\nclass unbalance issue due to videos with different lengths — i.e., some videos have thousands of\\nframes while some have a few dozens of frames. We address this problem by upsampling frame pairs\\nfrom shorter videos.\\n5 Experiments and Results\\nIn this section, we describe the experimental setup used to evaluate the performance of the temporal\\nearly exit video object detection pipeline. The experimental results are also presented in this section.\\n5.1 Implementation and Experimental Setup\\nFor the main branch of the video object detection pipeline , Fmain, we use Faster-RCNN [ 20] with\\nResnet50 and Resnet101 feature networks trained on MS COCO detection dataset [ 9]. We use\\nTEE-Faster-RCNN to refer to this pipeline. Following Residual attention network [ 28], we add early\\nexit branches after each Bottleneck and Basic blocks of the feature network. Therefore, TEE-Faster-', metadata={'source': 'docs/paper.pdf', 'page': 6}),\n",
       " Document(page_content='TEE-Faster-RCNN to refer to this pipeline. Following Residual attention network [ 28], we add early\\nexit branches after each Bottleneck and Basic blocks of the feature network. Therefore, TEE-Faster-\\nRCNN consists of four early exits. We train TEEMs for 40 epochs using the Adam Optimizer [ 8]\\nwith a learning rate of 0.001 and a batch size of 64. In both training and inference, the images have\\nshorter sides of 224 pixels. We use 25K and 5k video frame pairs for training and testing, respectively.\\nTraining was performed on 4 GPUs and testing run time is measured on a single GTX 1080 GPU. We\\nset the entropy to 0.97 at test time. Our model is implemented using PyTorch [ 18], and our code and\\ndataset will be made publicly available. We evaluate the accuracy, computational complexity, and run\\ntime of TEE-Faster-RCNN.\\n5.2 Classiﬁcation Accuracy of Early Exits:\\nWe evaluate the accuracy of early exits using classiﬁcation accuracy, precision, recall, and F1 metrics.\\nAccuracy metric enumerates the number of correct predicted changed and unchanged frames by\\nTEEMs in early exit branches. Precision quantiﬁes the number of frames which predicted as the\\nchanged frames and belong to the changed frame class. Recall quantiﬁes the number of changed\\nframe predictions made out of all changed frame examples.\\nTable 1 shows the measured accuracy, precision, recall and F1 for early exits of TEE-Faster-RCNN\\nwith Resnet50 and Resnet101 feature networks. TEE-Faster-RCNN is trained for the variation', metadata={'source': 'docs/paper.pdf', 'page': 6}),\n",
       " Document(page_content='Table 1 shows the measured accuracy, precision, recall and F1 for early exits of TEE-Faster-RCNN\\nwith Resnet50 and Resnet101 feature networks. TEE-Faster-RCNN is trained for the variation\\nthreshold of 0.4, equation (1). Exit1, Exit2, Exit3, and Exit4 are located after the ﬁrst, second, third,\\nand fourth Bottleneck modules in the Faster-RCNN feature network, respectively. The Exit1 (with\\nResnet50 feature network) classiﬁes frames into change and unchanged categories with 89% accuracy.\\nThe classiﬁcation accuracy increases up to 91% and 93% in Exit2 and Exit3. The observations show\\nlower accuracy for Exit4 82% compares to Exit3. We believe that lower accuracy of Exit4 is due to\\nthe low-resolution feature maps used by the TEEM in early exit 4 to identify variations.\\nExit4 uses 7×7low-resolution but semantically strong feature, encoding very high-level concepts,\\nto identify semantic variations across frames. Since the high-level features remain often unchanged\\nacross consecutive video frames, Exit4 achieves lower accuracy in classifying video frames with\\nsmall semantic variation i.e., when the location of a small object changes slightly between two frames.\\nHowever, Exit4 accurately detects unchanged frame pairs as well as frame pairs with signiﬁcant\\nvariations.\\nCompared to TEEMs, DFF [ 24] detects the key-frames (refers to changed frames) with 76% accuracy,', metadata={'source': 'docs/paper.pdf', 'page': 6}),\n",
       " Document(page_content='variations.\\nCompared to TEEMs, DFF [ 24] detects the key-frames (refers to changed frames) with 76% accuracy,\\nyet with high computational cost. DFF concatenates the input frame pairs and sends them as a\\nsix-channel depth input through a differential network to get the difference feature map. However,\\ninstead of processing input frame pairs together to identify variations, we proposed to process a video\\nframe once and store and reuse the intermediate feature maps to build an attention map that encodes\\nthe semantic differences between frame pairs with small computational overhead.\\n7', metadata={'source': 'docs/paper.pdf', 'page': 6}),\n",
       " Document(page_content='Table 1: Accuracy, precision, recall, and F1 scores measures for TEEMs in Faster-RCNN with\\nResnet50 and Resnet101 feature networks.\\nResnet50 Resnet101\\nAcc Pr Re F1 Acc Pr Re F1\\nExit1 0.88 0.89 0.88 0.89 0.89 0.89 0.89 0.89\\nExit2 0.93 0.93 0.93 0.93 0.91 0.92 0.92 0.92\\nExit3 0.91 0.91 0.92 0.91 0.94 0.92 0.92 0.93\\nExit4 0.86 0.82 0.86 0.84 0.85 0.83 0.87 0.85\\nfri fri+jTEEM 1TEEM 2TEEM 3TEEM 4\\nFigure 4: Class activation maps of video frame pairs show that TEEMs effectively learn to focus only\\non the motions of the objects of interest between video frames to identify scenery change.\\n5.3 Class Activation Map of TEEMs\\nTo visualize the performance of TEEMs, ﬁgure 4 shows class activation map of TEEMs. Each class\\nactivation map shows which parts of a video frame have contributed more to the ﬁnal output of\\nthe TEEMs. Figure 4 shows that TEEMs effectively learn to focus on the moving ﬁelds of interest\\nbetween frame pairs to identify variations. Futhermore, Figure 4 illustrates that TEEM1 and TEEM2\\nidentify moving objects in higher resolution because the input feature maps into TEEM1 and TEEM2\\nhave high resolution. The resolution of input feature maps into TEEM1 and TEEM2 are 56×56\\nand28×28resolution, respectively. However, the class activation map of TEEM3 and TEEM4 are\\ncoarse-grain because they use low-resolution feature maps of 14×14and7×7, respectively.\\n5.4 Computational Complexity\\nTable 2 compares computational cost (number of MAC operations and parameters) and run time', metadata={'source': 'docs/paper.pdf', 'page': 7}),\n",
       " Document(page_content='5.4 Computational Complexity\\nTable 2 compares computational cost (number of MAC operations and parameters) and run time\\n(frames per second) of processing video frames by the original Faster-RCNN and TEE-Faster-RCNN\\nbranches. The ﬁrst row of table 2 indicates the original Faster-RCNN network, including feature\\nnetwork, region proposal network, and region-based convolutional neural network.\\nExit 1, Exit2, Exit3, and Exit4 refer to computational paths of TEE-Faster-RCNN from the input of\\nFaster-RCNN to TEEM1, TEEM2, TEEM3, and TEEM4, respectively. The second column of table 2\\nshows the required MAC operations and parameters for each computation path of TEE-Faster-RCNN\\nwith Resnet50 feature network. The third columns shows the speed of each early exit. The fourth\\ncolumn shows the computational complexity the TEEM added in early exits. The second part of table\\n2 shows the same results for TEE-Faster-RCNN with resnet101 feature network.\\nThe original Faster-RCNN with Resnet51 feature network requires up to 134G MAC operations\\nand 41M parameters to process a video frame. The high computation requirement limits the video\\nobject detection speed to 18 fps, 14 fps for the Faster-RCNN with Resnet101 feature network. Using\\nthe same network architecture for processing all video frames regardless of the semantic variations\\nbetween neighbouring frames leads to the inferior use of limited energy and computational resources.', metadata={'source': 'docs/paper.pdf', 'page': 7}),\n",
       " Document(page_content='between neighbouring frames leads to the inferior use of limited energy and computational resources.\\nHowever, the TEE-Faster-RCNN uses computationally lightweight early exit branches to process\\nunchanged video frames. Table 2 shows that early exit branches have substantially less computations\\nand memory requirements which speeds up processing video frames. Exit1 branch requires only\\n8', metadata={'source': 'docs/paper.pdf', 'page': 7}),\n",
       " Document(page_content='Table 2: Computational complexity and speed of TEE-Faster-RCNN video object detection.\\nResnet51 Resnet101\\nbranch(Op, Par) Speed TEEM(Opr,Par) branch(Op, Par) Speed TEEM(Opr,Par)\\nFR (134G , 41M) 18fps - (181G , 60) 14fps -\\nExit1 (1.7G , 0.525M) 628fps (0.94G , 0.3M) (1.7G, 0.525M) 597fps (0.94G , 0.3M)\\nExit2 (2.7G , 2.615M) 390fps (0.93G , 1.17M) (3.7G, 2.910M) 400fps (0.93G , 1.17M)\\nExit3 (3.5G , 9.733M) 263fps (0.23G , 1.19M) (9.1G, 30.195M) 140fps (0.23G , 1.19M)\\nExit4 (5G , 23.808) 218fps (0.25G , 5.129M) (9.9G, 50.289M) 126fps (0.2G , 5.129M)\\nTable 3: Comparing the detection accuracy of TEE-Faster-RCNN with per-frame Faster-RCNN\\nUpdating ratio mAP mIoU\\nFaster-RCNN 1 0.231 0.8\\nFixed-Step 7 0.211 0.76\\nFixed-Step 10 0.183 0.75\\nFixed-Step 20 0.16 0.70\\nTEE-Faster-RCNN 20 0.209 0.75\\n1.7G MAC operations and uses 0.5 M parameters. Signiﬁcant reduction in computation complexity\\nis because of avoided parts of the feature network, region proposal network, and region-based\\nconvolutional neural network. This reduction in computations speeds up processing unchanged video\\nframes up to 628 fps. The required MAC operation for Exit2, Exit3 and Exit4 branches are 2.7G,\\n3.5G, and 5G, respectively. Exit2, Exit3 and Exit4 speed up processing unchanged frames to 390 fps,\\n263 fps, and 218 fps, respectively. The fourth column of table 2 shows the required MAC operation\\nand number of parameters for TEEMs.\\n5.5 Detection Accuracy', metadata={'source': 'docs/paper.pdf', 'page': 8}),\n",
       " Document(page_content='263 fps, and 218 fps, respectively. The fourth column of table 2 shows the required MAC operation\\nand number of parameters for TEEMs.\\n5.5 Detection Accuracy\\nHaving tested the classiﬁcation performance of TEEMs, we then evaluate the mean average precision\\n(mAP @0.35:0.05:0.75) and mean intersection over union (mIOU) of TEE-Faster-RCNN video object\\ndetection. Table 3 compares the accuracy of TEE-Faster-RCNN detection results with the per-frame\\noriginal Faster-RCNN object detection. TEE-Faster-RCNN updates the detection results at the\\naverage step of 20. Therefore, for a fair comparison we performed Faster-RCNN with different ﬁxed\\nupdating steps. The results reﬂect that the accuracy of TEE-Faster-RCNN cannot compete with the\\nper-frame video object detection approach. Whilst, TEE-Faster-RCNN achieves the same accuracy\\nof per-frame Faster-RCNN with the ﬁxed updating steps of 7, its updating step of TEE-Faster-RCNN\\nis 20 on average. Notably, TEE-Faster-RCNN does not aim to improve the accuracy of detection but\\nintroduces a simple yet effective approach to signiﬁcantly reduce the computational complexity of\\nvideo object detection for the video applications with less frequent moving objects e.g., the CDnet\\ndataset [ 5]. For applications with frequent moving objects such as the ImageNet-VID dataset [ 23],\\nmore complex methods like optical ﬂow approaches are needed to achieve better accuracy.\\n6 Conclusion', metadata={'source': 'docs/paper.pdf', 'page': 8}),\n",
       " Document(page_content='more complex methods like optical ﬂow approaches are needed to achieve better accuracy.\\n6 Conclusion\\nWe proposes a temporal early exit object detection pipeline to reduce the computational complexity\\nof per-frame video object detection. The proposed approach takes advantage of infrequent variation\\nbetween features of consecutive video frames to avoid redundant computation. Video frames with\\ninvariant features are identiﬁed in the early stages of the network with very low computation effort. For\\nthe unchanged video frames, detection results from previous frames are reused. A full computation\\neffort is only required if a video frame is identiﬁed with semantic variations compared to previous\\nframes. The proposed approach accelerates per-frame video object detection up to 34×with less than\\n2.2 % reduction in mAP.\\n9', metadata={'source': 'docs/paper.pdf', 'page': 8}),\n",
       " Document(page_content='References\\n[1]Pablo F Alcantarilla, Simon Stent, German Ros, Roberto Arroyo, and Riccardo Gherardi. Street-\\nview change detection with deconvolutional networks. Autonomous Robots , 42(7):1301–1322,\\n2018.\\n[2]Ali Borji, Ming-Ming Cheng, Huaizu Jiang, and Jia Li. Salient object detection: A benchmark.\\nIEEE transactions on image processing , 24(12):5706–5722, 2015.\\n[3]Francesca Bovolo and Lorenzo Bruzzone. A theoretical framework for unsupervised change\\ndetection based on change vector analysis in the polar domain. IEEE Transactions on Geoscience\\nand Remote Sensing , 45(1):218–236, 2006.\\n[4]Lorenzo Bruzzone and D Fernandez Prieto. An adaptive semiparametric and context-based\\napproach to unsupervised change detection in multitemporal remote-sensing images. IEEE\\nTransactions on image processing , 11(4):452–466, 2002.\\n[5]Nil Goyette, Pierre-Marc Jodoin, Fatih Porikli, Janusz Konrad, and Prakash Ishwar. Changede-\\ntection. net: A new change detection benchmark dataset. In 2012 IEEE computer society\\nconference on computer vision and pattern recognition workshops , pages 1–8. IEEE, 2012.\\n[6]Chaoxu Guo, Bin Fan, Jie Gu, Qian Zhang, Shiming Xiang, Veronique Prinet, and Chunhong\\nPan. Progressive sparse local attention for video object detection. In Proceedings of the\\nIEEE/CVF International Conference on Computer Vision , pages 3909–3918, 2019.\\n[7]Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi,', metadata={'source': 'docs/paper.pdf', 'page': 9}),\n",
       " Document(page_content='IEEE/CVF International Conference on Computer Vision , pages 3909–3918, 2019.\\n[7]Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi,\\nIan Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, et al. Speed/accuracy trade-offs\\nfor modern convolutional object detectors. In Proceedings of the IEEE conference on computer\\nvision and pattern recognition , pages 7310–7311, 2017.\\n[8]Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\\narXiv:1412.6980 , 2014.\\n[9]Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\\nDollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European\\nconference on computer vision , pages 740–755. Springer, 2014.\\n[10] Xiaoqiang Lu, Yuan Yuan, and Xiangtao Zheng. Joint dictionary learning for multispectral\\nchange detection. IEEE transactions on cybernetics , 47(4):884–897, 2016.\\n[11] Yuan Luo, Hanxing Zhou, Qin Tan, Xuefeng Chen, and Mingjing Yun. Key frame extraction of\\nsurveillance video based on moving object detection and image similarity. Pattern Recognition\\nand Image Analysis , 28(2):225–231, 2018.\\n[12] H MAHMOUDZADEH. Digital change detection using remotely sensed data for monitoring\\ngreen space destruction in tabriz. 2007.\\n[13] David Mascharka, Philip Tran, Ryan Soklaski, and Arjun Majumdar. Transparency by design:\\nClosing the gap between performance and interpretability in visual reasoning. In Proceedings', metadata={'source': 'docs/paper.pdf', 'page': 9}),\n",
       " Document(page_content='[13] David Mascharka, Philip Tran, Ryan Soklaski, and Arjun Majumdar. Transparency by design:\\nClosing the gap between performance and interpretability in visual reasoning. In Proceedings\\nof the IEEE conference on computer vision and pattern recognition , pages 4942–4950, 2018.\\n[14] Gabriele Moser, Elena Angiati, and Sebastiano B Serpico. Multiscale unsupervised change\\ndetection on optical images by markov random ﬁelds and wavelets. IEEE Geoscience and\\nRemote Sensing Letters , 8(4):725–729, 2011.\\n[15] Sangmin Oh, Anthony Hoogs, Amitha Perera, Naresh Cuntoor, Chia-Chih Chen, Jong Taek Lee,\\nSaurajit Mukherjee, JK Aggarwal, Hyungtae Lee, Larry Davis, et al. A large-scale benchmark\\ndataset for event recognition in surveillance video. In CVPR 2011 , pages 3153–3160. IEEE,\\n2011.\\n[16] Priyadarshini Panda, Abhronil Sengupta, and Kaushik Roy. Conditional deep learning for\\nenergy-efﬁcient and enhanced pattern recognition. In 2016 Design, Automation & Test in\\nEurope Conference & Exhibition (DATE) , pages 475–480. IEEE, 2016.\\n10', metadata={'source': 'docs/paper.pdf', 'page': 9}),\n",
       " Document(page_content='[17] Dong Huk Park, Trevor Darrell, and Anna Rohrbach. Robust change captioning. In Proceedings\\nof the IEEE/CVF International Conference on Computer Vision , pages 4624–4633, 2019.\\n[18] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative\\nstyle, high-performance deep learning library. arXiv preprint arXiv:1912.01703 , 2019.\\n[19] Richard J Radke, Srinivas Andra, Omar Al-Kofahi, and Badrinath Roysam. Image change\\ndetection algorithms: a systematic survey. IEEE transactions on image processing , 14(3):\\n294–307, 2005.\\n[20] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time\\nobject detection with region proposal networks. arXiv preprint arXiv:1506.01497 , 2015.\\n[21] Paul Rosin. Thresholding for change detection. In Sixth International Conference on Computer\\nVision (IEEE Cat. No. 98CH36271) , pages 274–279. IEEE, 1998.\\n[22] Paul L Rosin and Efstathios Ioannidis. Evaluation of global image thresholding for change\\ndetection. Pattern recognition letters , 24(14):2345–2356, 2003.\\n[23] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual\\nrecognition challenge. International journal of computer vision , 115(3):211–252, 2015.\\n[24] Jing Shi and Chenliang Xu. Differential network for video object detection.', metadata={'source': 'docs/paper.pdf', 'page': 10}),\n",
       " Document(page_content='recognition challenge. International journal of computer vision , 115(3):211–252, 2015.\\n[24] Jing Shi and Chenliang Xu. Differential network for video object detection.\\n[25] Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Branchynet: Fast inference\\nvia early exiting from deep neural networks. In 2016 23rd International Conference on Pattern\\nRecognition (ICPR) , pages 2464–2469. IEEE, 2016.\\n[26] Yonglong Tian, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning strong parts for\\npedestrian detection. In Proceedings of the IEEE international conference on computer vision ,\\npages 1904–1912, 2015.\\n[27] Carl V ondrick, Donald Patterson, and Deva Ramanan. Efﬁciently scaling up crowdsourced\\nvideo annotation. International Journal of Computer Vision , pages 1–21. ISSN 0920-5691.\\nURL http://dx.doi.org/10.1007/s11263-012-0564-1 . 10.1007/s11263-012-0564-1.\\n[28] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang\\nWang, and Xiaoou Tang. Residual attention network for image classiﬁcation. In Proceedings of\\nthe IEEE conference on computer vision and pattern recognition , pages 3156–3164, 2017.\\n[29] Philippe Weinzaepfel, Jerome Revaud, Zaid Harchaoui, and Cordelia Schmid. Deepﬂow: Large\\ndisplacement optical ﬂow with deep matching. In Proceedings of the IEEE international\\nconference on computer vision , pages 1385–1392, 2013.\\n[30] Xizhou Zhu, Yuwen Xiong, Jifeng Dai, Lu Yuan, and Yichen Wei. Deep feature ﬂow for video', metadata={'source': 'docs/paper.pdf', 'page': 10}),\n",
       " Document(page_content='conference on computer vision , pages 1385–1392, 2013.\\n[30] Xizhou Zhu, Yuwen Xiong, Jifeng Dai, Lu Yuan, and Yichen Wei. Deep feature ﬂow for video\\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\\npages 2349–2358, 2017.\\n11', metadata={'source': 'docs/paper.pdf', 'page': 10})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
