{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arxiv reference parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports to load model\n",
    "from random import randrange\n",
    "from transformers import AutoTokenizer, set_seed, pipeline,BitsAndBytesConfig, LlamaForCausalLM, LlamaTokenizer, GenerationConfig,LlamaConfig,LlamaModel,AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, BitsAndBytesConfig \n",
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "import os\n",
    "import json\n",
    "\n",
    "#imports for langchain functionalities\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import transformers\n",
    "\n",
    "from langchain.chains import ConversationChain, ConversationalRetrievalChain, SequentialChain\n",
    "from langchain.memory import ConversationBufferMemory, ReadOnlySharedMemory\n",
    "from langchain.agents import ZeroShotAgent, AgentExecutor\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading llama-2 7b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BitsAndBytes configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bnb_config():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    return bnb_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, bnb_config):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = f'{40960}MB' #TODO Change if necessary\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\", \n",
    "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model ids and bnb config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id_normal = 'meta-llama/Llama-2-7b'\n",
    "model_id_normal_hf = 'meta-llama/Llama-2-7b-hf'\n",
    "model_id_chat = 'meta-llama/Llama-2-7b-chat'\n",
    "model_id_chat_hf = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "bnb_config = create_bnb_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f92adaa53c0495eb87e4e92f1001d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:628: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(model_id_chat_hf, bnb_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing inference on the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    #stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
    "    temperature=0.01,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=1024,  # mex number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n Unterscheidung zwischen \"Spain\" und \"Spanish\"\\n\\nThe capital of Spain is Madrid.\\n\\nIt\\'s important to note that \"Spain\" refers to the country as a whole, while \"Spanish\" can refer to either the language or the people from Spain. So, for example:\\n\\n* \"Spain is a beautiful country with a rich culture.\" (Here, \"Spain\" refers to the country.)\\n* \"I love speaking Spanish with my friends.\" (Here, \"Spanish\" refers to the language.)\\n* \"My grandparents are from Spain, so I have Spanish ancestry.\" (Here, \"Spanish\" refers to the people from Spain.)\\n\\nSo, to answer your question, the capital of Spain is Madrid.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict('what is the capital of Spain?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference parsing using LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jsonformer (impose a strict json structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jsonformer\n",
      "  Downloading jsonformer-0.12.0-py3-none-any.whl (6.6 kB)\n",
      "Collecting termcolor<3.0.0,>=2.3.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Installing collected packages: termcolor, jsonformer\n",
      "Successfully installed jsonformer-0.12.0 termcolor-2.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install jsonformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jsonformer.format import highlight_values\n",
    "from jsonformer.main import Jsonformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"references\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"ref_id\": {\"type\": \"string\"},\n",
    "                \"title\": {\"type\": \"string\"},\n",
    "                \"author\": {\"type\": \"string\"},\n",
    "                \"year\": {\"type\": \"string\"},\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = Jsonformer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    json_schema=test,\n",
    "    prompt='''\n",
    "Can you parse all the title, author and year of publications of these references?\n",
    "\n",
    "Allen, T. D., & Rush, M. C. (2001). The influence of ratee gender \n",
    "on ratings of organizational citizenship behavior. Journal of 63\n",
    "HWI, Servant Leadership, OCB and CWB in Italy\n",
    "Applied Social Psychology, 31 (12), 2561-2587.  https://doi.\n",
    "org/10.1111/j.1559-1816.2001.tb00191.x  \n",
    "Andreassen, C. S., Nielsen, M. B., Pallesen, S., & Gjerstad, J. (2019). The \n",
    "relationship between psychosocial work variables and workaholism: \n",
    "Findings from a nationally representative survey. International Journal \n",
    "of Stress Management, 26 (1), 1-10. https://doi.org/10.1037/str0000073   \n",
    "Aziz, S., Pittman, C., & Wuensch, K. (2020). Workaholism and organizational \n",
    "citizenship behaviors: Exploring gender role beliefs.  International \n",
    "Journal of Workplace Health Management, 13 (4), 413-425. https://doi.\n",
    "org/10.1108/IJWHM-06-2019-0089  \n",
    "Bakker, A. B., & Bal, P. M. (2010). Weekly work engagement and performance: A \n",
    "study among starting teachers. Journal of Occupational and Organizational \n",
    "Psychology, 83 (1), 189-206. https://doi.org/10.1348/096317909X402596  \n",
    "Balducci, C., Cecchin, M., Fraccaroli, F., & Schaufeli, W. B. (2012). Exploring the \n",
    "relationship between workaholism and workplace aggressive behaviour: \n",
    "The role of job-related emotion.  Personality and Individual Differences, \n",
    "53(5), 629-634. https://doi.org/10.1016/j.paid.2012.05.004  \n",
    "Barbaranelli, C., Fida, R., & Gulandri, M. (2013). Assessing counterproductive \n",
    "work behavior: A study on the dimensionality of CWB-checklist . TMP-\n",
    "Testing, Psychometrics, Methodology in Applied Psychology, 20 (3), 235-\n",
    "248.  https://doi.org/10.4473/TPM20.3.3  \n",
    "Beauregard, T.  A. (2012). Perfectionism, self-efficacy and OCB: The \n",
    "moderating role of gender. Personnel Review , 41(5), 590-608.  https://\n",
    "doi.org/10.1108/00483481211249120  \n",
    "Bentler, P. M., & Wu, E. J. (2005). EQS 6.1 for Windows: Structural equations \n",
    "program manual . Multivariate Software.\n",
    "Birkeland, I. K., & Buch, R. (2015). The dualistic model of passion for work: \n",
    "Discriminative and predictive validity with work engagement and \n",
    "workaholism. Motivation and Emotion, 39 (3), 392-408. https://doi.\n",
    "org/10.1007/s11031-014-9462-x  \n",
    "Borman, W. C., & Motowidlo, S. J. (1993). Expanding the criterion domain to \n",
    "include elements of contextual performance. In N. Schmitt & W. C. Borman \n",
    "(Eds.),  Personnel selection in organizations (pp. 71–98). Jossey-Bass.\n",
    "Bowling, N. A., & Eschleman, K. J. (2010). Employee personality as a moderator \n",
    "of the relationships between work stressors and counterproductive work \n",
    "behavior. Journal of Occupational Health Psychology, 15 (1), 91-103. \n",
    "https://doi.org/10.1037/a0017326\n",
    "Bruk-Lee, V., & Spector, P. (2006). The social stressors-counterproductive work \n",
    "behaviors link: Are conflicts with supervisors and coworkers the same? \n",
    "Journal of Occupational Health Psychology, 11 (2), 145-156. https://doi.\n",
    "org/10.1037/1076- 8998.11.2.145  \n",
    "Byrne, B. M. (2010). Structural equation modeling with AMOS: Basic concepts, \n",
    "applications, and programming  (2nd ed.). Routledge.\n",
    "Chappell, D., & Di Martino, V. (2006).  Violence at work (3rd ed.). International \n",
    "Labour Organization.\n",
    "Choi, Y. (2013). The differences between work engagement and workaholism, \n",
    "and organizational outcomes: An integrative model. Social Behavior \n",
    "and Personality, 41 (10), 1655-1666. https://doi.org/10.2224/\n",
    "sbp.2013.41.10.1655\n",
    "Dalal, R. S. (2005). A meta-analysis of the relationship between organizational \n",
    "citizenship behavior and counterproductive work behavior. Journal of \n",
    "Applied Psychology, 90 (6), 1241-1255. https://doi.org/10.1037/0021-\n",
    "9010.90.6.1241  \n",
    "Eagly, A. H. (1987). Sex differences in social behavior: A social role \n",
    "interpretation . Erlbaum.\n",
    "Eagly, A., Karau, S. J., & Makajhani, M. G. (1995). Gender and the effectiveness \n",
    "of leaders: A meta-analysis. Psychological Bulletin, 117 (1), 125-145.  \n",
    "https://doi.org/10.1037/0033-2909.117.1.125  \n",
    "Ehrhart, M. G. (2004). Leadership and procedural justice climate as \n",
    "antecedents of unit-level organizational citizenship behavior.  Personnel \n",
    "Psychology, 57 (1), 61-94.  https://doi.org/10.1111/j.1744-6570.2004.\n",
    "tb02484.x  \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = builder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ref': {'title': 'Allen, T. D., & Rush',\n",
       "  'author': 'Allen, T. D., & Rush',\n",
       "  'year': '2001'}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidance (Library to guide LLM towards certain types of outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting guidance\n",
      "  Downloading guidance-0.0.64-py3-none-any.whl (100 kB)\n",
      "\u001b[K     |████████████████████████████████| 100 kB 5.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ubuntu/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages (from guidance) (1.24.4)\n",
      "Collecting pyparsing>=3.0.0\n",
      "  Using cached pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "Collecting tiktoken>=0.3\n",
      "  Downloading tiktoken-0.5.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 12.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting openai>=0.27.8\n",
      "  Downloading openai-1.2.4-py3-none-any.whl (220 kB)\n",
      "\u001b[K     |████████████████████████████████| 220 kB 129.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting diskcache\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 2.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ubuntu/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages (from guidance) (2.31.0)\n",
      "Collecting gptcache\n",
      "  Downloading gptcache-0.1.42-py3-none-any.whl (131 kB)\n",
      "\u001b[K     |████████████████████████████████| 131 kB 125.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: platformdirs in /home/ubuntu/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages (from guidance) (3.10.0)\n",
      "Collecting msal\n",
      "  Downloading msal-1.25.0-py2.py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 6.7 MB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting pygtrie\n",
      "  Downloading pygtrie-2.5.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages (from guidance) (3.8.5)\n",
      "Requirement already satisfied: nest-asyncio in /home/ubuntu/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages (from guidance) (1.5.7)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/ubuntu/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages (from tiktoken>=0.3->guidance) (2023.6.3)\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Downloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
      "\u001b[K     |████████████████████████████████| 75 kB 3.0 MB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /home/ubuntu/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages (from openai>=0.27.8->guidance) (3.7.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/ubuntu/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages (from openai>=0.27.8->guidance) (1.10.12)\n",
      "Requirement already satisfied: tqdm>4 in /home/ubuntu/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages (from openai>=0.27.8->guidance) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in /home/ubuntu/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages (from openai>=0.27.8->guidance) (4.7.1)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Downloading distro-1.8.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages (from requests->guidance) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages (from requests->guidance) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages (from requests->guidance) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages (from requests->guidance) (2.0.4)\n",
      "Requirement already satisfied: cachetools in /home/ubuntu/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages (from gptcache->guidance) (5.3.1)\n",
      "Collecting PyJWT[crypto]<3,>=1.0.0\n",
      "  Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
      "Collecting cryptography<44,>=0.6\n",
      "  Downloading cryptography-41.0.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 118.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages (from aiohttp->guidance) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages (from aiohttp->guidance) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages (from aiohttp->guidance) (23.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ubuntu/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages (from aiohttp->guidance) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages (from aiohttp->guidance) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ubuntu/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages (from aiohttp->guidance) (4.0.2)\n",
      "Requirement already satisfied: sniffio in /home/ubuntu/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages (from httpx<1,>=0.23.0->openai>=0.27.8->guidance) (1.3.0)\n",
      "Collecting httpcore\n",
      "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 5.5 MB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: exceptiongroup; python_version < \"3.11\" in /home/ubuntu/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages (from anyio<4,>=3.5.0->openai>=0.27.8->guidance) (1.1.2)\n",
      "Collecting cffi>=1.12\n",
      "  Using cached cffi-1.16.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (444 kB)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ubuntu/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages (from httpcore->httpx<1,>=0.23.0->openai>=0.27.8->guidance) (0.14.0)\n",
      "Collecting pycparser\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Installing collected packages: pyparsing, tiktoken, httpcore, httpx, distro, openai, diskcache, gptcache, pycparser, cffi, cryptography, PyJWT, msal, pygtrie, guidance\n",
      "Successfully installed PyJWT-2.8.0 cffi-1.16.0 cryptography-41.0.5 diskcache-5.6.3 distro-1.8.0 gptcache-0.1.42 guidance-0.0.64 httpcore-1.0.2 httpx-0.25.1 msal-1.25.0 openai-1.2.4 pycparser-2.21 pygtrie-2.5.0 pyparsing-3.1.1 tiktoken-0.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to install package: redis\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "successfully installed package: redis\n",
      "start to install package: redis-om\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "successfully installed package: redis-om\n"
     ]
    }
   ],
   "source": [
    "import guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance.llm = guidance.llms.Transformers(model,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the prompt\n",
    "program = guidance(\"\"\"Given a text containing many references, re-organise each of them in a json format that contains their reference id, paper title, authors and year. \n",
    "----\n",
    "```json\n",
    "{\n",
    "    \"ref_id\": \"{{reference id}}\"\n",
    "    \"title\": \"{{paper title}}\",\n",
    "    \"authors\": \"{{authors}}\",\n",
    "    \"year\": \"{{year}}\",\n",
    "```\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {'input': \"Balducci, C., Cecchin, M., Fraccaroli, F., & Schaufeli, W. B. (2012). Exploring the relationship between workaholism and workplace aggressive behaviour: The role of job-related emotion.  Personality and Individual Differences, 53(5), 629-634. https://doi.org/10.1016/j.paid.2012.05.004, Aziz, S., Pittman, C., & Wuensch, K. (2020). Workaholism and organizational citizenship behaviors: Exploring gender role beliefs.  International Journal of Workplace Health Management, 13 (4), 413-425. https://doi.org/10.1108/IJWHM-06-2019-0089  Bakker, A. B., & Bal, P. M. (2010). Weekly work engagement and performance: A study among starting teachers. Journal of Occupational and Organizational Psychology, 83 (1), 189-206. https://doi.org/10.1348/096317909X402596  Balducci, C., Cecchin, M., Fraccaroli, F., & Schaufeli, W. B. (2012). Exploring the relationship between workaholism and workplace aggressive behaviour: The role of job-related emotion.  Personality and Individual Differences, 53(5), 629-634. https://doi.org/10.1016/j.paid.2012.05.004  Barbaranelli, C., Fida, R., & Gulandri, M. (2013). Assessing counterproductive work behavior: A study on the dimensionality of CWB-checklist . TMP-Testing, Psychometrics, Methodology in Applied Psychology, 20 (3), 235-248.  https://doi.org/10.4473/TPM20.3.3\"},\n",
    "     {\n",
    "         \"ref_id\": 'id1',\n",
    "         \"title\": \"Exploring the relationship between workaholism and workplace aggressive behaviour: The role of job-related emotion.  Personality and Individual Differences\",\n",
    "         \"authors\": \"Balducci, C., Cecchin, M., Fraccaroli, F., & Schaufeli, W. B.\",\n",
    "         \"year\": \"2012\"\n",
    "     },\n",
    "     {\n",
    "         \"ref_id\": 'id2',\n",
    "         \"title\": \"Workaholism and organizational citizenship behaviors: Exploring gender role beliefs.\",\n",
    "         \"authors\": \"Aziz, S., Pittman, C., & Wuensch, K.\",\n",
    "         \"year\": \"2020\"\n",
    "     },\n",
    "     {\n",
    "         'more references ...'\n",
    "     }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"guidance-stop-button-8b4eb446-e8e5-485c-8820-bb4f238fdccc\" style=\"cursor: pointer; margin: 0px; display: none; float: right; padding: 3px; border-radius: 4px 4px 4px 4px; border: 0px solid rgba(127, 127, 127, 1); padding-left: 10px; padding-right: 10px; font-size: 13px; background-color: rgba(127, 127, 127, 0.25);\">Stop program</div><div id=\"guidance-content-8b4eb446-e8e5-485c-8820-bb4f238fdccc\"><pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>Given a text containing many references, re-organise each of them in a json format that contains their reference id, paper title, authors and year. \n",
       "----\n",
       "```json\n",
       "{\n",
       "    &quot;ref_id&quot;: &quot;<span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{reference id}}</span>&quot;\n",
       "    &quot;title&quot;: &quot;<span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{paper title}}</span>&quot;,\n",
       "    &quot;authors&quot;: &quot;<span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{authors}}</span>&quot;,\n",
       "    &quot;year&quot;: &quot;<span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{year}}</span>&quot;,\n",
       "```</pre></div>\n",
       "<script type=\"text/javascript\">(()=>{var t={296:(t,e,n)=>{var i=NaN,o=\"[object Symbol]\",r=/^\\s+|\\s+$/g,a=/^[-+]0x[0-9a-f]+$/i,s=/^0b[01]+$/i,c=/^0o[0-7]+$/i,d=parseInt,u=\"object\"==typeof n.g&&n.g&&n.g.Object===Object&&n.g,l=\"object\"==typeof self&&self&&self.Object===Object&&self,f=u||l||Function(\"return this\")(),h=Object.prototype.toString,p=Math.max,m=Math.min,g=function(){return f.Date.now()};function b(t){var e=typeof t;return!!t&&(\"object\"==e||\"function\"==e)}function y(t){if(\"number\"==typeof t)return t;if(function(t){return\"symbol\"==typeof t||function(t){return!!t&&\"object\"==typeof t}(t)&&h.call(t)==o}(t))return i;if(b(t)){var e=\"function\"==typeof t.valueOf?t.valueOf():t;t=b(e)?e+\"\":e}if(\"string\"!=typeof t)return 0===t?t:+t;t=t.replace(r,\"\");var n=s.test(t);return n||c.test(t)?d(t.slice(2),n?2:8):a.test(t)?i:+t}t.exports=function(t,e,n){var i,o,r,a,s,c,d=0,u=!1,l=!1,f=!0;if(\"function\"!=typeof t)throw new TypeError(\"Expected a function\");function h(e){var n=i,r=o;return i=o=void 0,d=e,a=t.apply(r,n)}function v(t){var n=t-c;return void 0===c||n>=e||n<0||l&&t-d>=r}function _(){var t=g();if(v(t))return w(t);s=setTimeout(_,function(t){var n=e-(t-c);return l?m(n,r-(t-d)):n}(t))}function w(t){return s=void 0,f&&i?h(t):(i=o=void 0,a)}function j(){var t=g(),n=v(t);if(i=arguments,o=this,c=t,n){if(void 0===s)return function(t){return d=t,s=setTimeout(_,e),u?h(t):a}(c);if(l)return s=setTimeout(_,e),h(c)}return void 0===s&&(s=setTimeout(_,e)),a}return e=y(e)||0,b(n)&&(u=!!n.leading,r=(l=\"maxWait\"in n)?p(y(n.maxWait)||0,e):r,f=\"trailing\"in n?!!n.trailing:f),j.cancel=function(){void 0!==s&&clearTimeout(s),d=0,i=c=o=s=void 0},j.flush=function(){return void 0===s?a:w(g())},j}},777:t=>{var e,n,i=Math.max,o=(e=function(t,e){return function(t,e,n){if(\"function\"!=typeof t)throw new TypeError(\"Expected a function\");return setTimeout((function(){t.apply(void 0,n)}),1)}(t,0,e)},n=i(void 0===n?e.length-1:n,0),function(){for(var t=arguments,o=-1,r=i(t.length-n,0),a=Array(r);++o<r;)a[o]=t[n+o];o=-1;for(var s=Array(n+1);++o<n;)s[o]=t[o];return s[n]=a,function(t,e,n){switch(n.length){case 0:return t.call(e);case 1:return t.call(e,n[0]);case 2:return t.call(e,n[0],n[1]);case 3:return t.call(e,n[0],n[1],n[2])}return t.apply(e,n)}(e,this,s)});t.exports=o}},e={};function n(i){var o=e[i];if(void 0!==o)return o.exports;var r=e[i]={exports:{}};return t[i](r,r.exports,n),r.exports}n.n=t=>{var e=t&&t.__esModule?()=>t.default:()=>t;return n.d(e,{a:e}),e},n.d=(t,e)=>{for(var i in e)n.o(e,i)&&!n.o(t,i)&&Object.defineProperty(t,i,{enumerable:!0,get:e[i]})},n.g=function(){if(\"object\"==typeof globalThis)return globalThis;try{return this||new Function(\"return this\")()}catch(t){if(\"object\"==typeof window)return window}}(),n.o=(t,e)=>Object.prototype.hasOwnProperty.call(t,e),(()=>{\"use strict\";const t=t=>{const e=new Set;do{for(const n of Reflect.ownKeys(t))e.add([t,n])}while((t=Reflect.getPrototypeOf(t))&&t!==Object.prototype);return e};function e(e,{include:n,exclude:i}={}){const o=t=>{const e=e=>\"string\"==typeof e?t===e:e.test(t);return n?n.some(e):!i||!i.some(e)};for(const[n,i]of t(e.constructor.prototype)){if(\"constructor\"===i||!o(i))continue;const t=Reflect.getOwnPropertyDescriptor(n,i);t&&\"function\"==typeof t.value&&(e[i]=e[i].bind(e))}return e}var i=n(777),o=n.n(i),r=n(296),a=n.n(r);class s{constructor(t,n){e(this),this.interfaceId=t,this.callbackMap={},this.data={},this.pendingData={},this.jcomm=new c(\"guidance_interface_target_\"+this.interfaceId,this.updateData,\"open\"),this.debouncedSendPendingData500=a()(this.sendPendingData,500),this.debouncedSendPendingData1000=a()(this.sendPendingData,1e3),n&&o()(n)}send(t,e){this.addPendingData(t,e),this.sendPendingData()}sendEvent(t){for(const e of Object.keys(t))this.addPendingData(e,t[e]);this.sendPendingData()}debouncedSendEvent500(t){for(const e of Object.keys(t))this.addPendingData(e,t[e]);this.debouncedSendPendingData500()}debouncedSend500(t,e){this.addPendingData(t,e),this.debouncedSendPendingData500()}debouncedSend1000(t,e){this.addPendingData(t,e),this.debouncedSendPendingData1000()}addPendingData(t,e){Array.isArray(t)||(t=[t]);for(const n in t)this.pendingData[t[n]]=e}updateData(t){t=JSON.parse(t.data);for(const e in t)this.data[e]=t[e];for(const e in t)e in this.callbackMap&&this.callbackMap[e](this.data[e])}subscribe(t,e){this.callbackMap[t]=e,o()((e=>this.callbackMap[t](this.data[t])))}sendPendingData(){this.jcomm.send_data(this.pendingData),this.pendingData={}}}class c{constructor(t,e,n=\"open\"){this._fire_callback=this._fire_callback.bind(this),this._register=this._register.bind(this),this.jcomm=void 0,this.callback=e,void 0!==window.Jupyter?\"register\"===n?Jupyter.notebook.kernel.comm_manager.register_target(t,this._register):(this.jcomm=Jupyter.notebook.kernel.comm_manager.new_comm(t),this.jcomm.on_msg(this._fire_callback)):void 0!==window._mgr&&(\"register\"===n?window._mgr.widgetManager.proxyKernel.registerCommTarget(t,this._register):(this.jcomm=window._mgr.widgetManager.proxyKernel.createComm(t),this.jcomm.open({},\"\"),this.jcomm.onMsg=this._fire_callback))}send_data(t){void 0!==this.jcomm?this.jcomm.send(t):console.error(\"Jupyter comm module not yet loaded! So we can't send the message.\")}_register(t,e){this.jcomm=t,this.jcomm.on_msg(this._fire_callback)}_fire_callback(t){this.callback(t.content.data)}}class d{constructor(t,n){e(this),this.id=t,this.comm=new s(t),this.comm.subscribe(\"append\",this.appendData),this.comm.subscribe(\"replace\",this.replaceData),this.comm.subscribe(\"event\",this.eventOccurred),this.element=document.getElementById(\"guidance-content-\"+t),this.stop_button=document.getElementById(\"guidance-stop-button-\"+t),this.stop_button.onclick=()=>this.comm.send(\"event\",\"stop\")}appendData(t){t&&(this.stop_button.style.display=\"inline-block\",this.element.innerHTML+=t)}replaceData(t){t&&(this.stop_button.style.display=\"inline-block\",this.element.innerHTML=t)}eventOccurred(t){\"complete\"===t&&(this.stop_button.style.display=\"none\")}}window._guidanceDisplay=function(t,e){return new d(t,e)}})()})();; window._guidanceDisplay(\"8b4eb446-e8e5-485c-8820-bb4f238fdccc\");</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# execute the prompt\n",
    "out = program(input='''Giannini, M., & Loscalzo, Y. (2016). Workaholism: Health risk and prevention \n",
    "in the organizations. In A. di Fabio (Ed.), Neuroticism: Characteristics, \n",
    "impact on job performance and health outcomes (pp. 49-60). Nova \n",
    "Science Publishers.\n",
    "Graham, J. W. (1991). Servant-leadership in organizations: Inspirational \n",
    "and moral. The Leadership Quarterly, 2 (2), 105-119.  https://doi.\n",
    "org/10.1016/1048-9843(91)90025-WGreenleaf, R. K. (1977).  Servant-leadership: A journey into the nature of \n",
    "legitimate power and greatness . Paulist Press.\n",
    "Gruys, M. L., & Sackett, P. R. (2003). Investigating the dimensionality of \n",
    "counterproductive work behavior.  International Journal of Selection \n",
    "and Assessment, 11 (1), 30-41. https://doi.org/10.1111/1468-2389.00224\n",
    "Heymans, M. W., & Eekhout, I. (2019). Applied missing data analysis with \n",
    "SPSS and ®Studio. https://bookdown.org/mwheymans/bookmi\n",
    "Hofstede, G. (1980). Culture’s consequences: International differences in \n",
    "work-related values.  SAGE.\n",
    "Hofstede, G. (1991). Cultures and organization: Software of the mind. \n",
    "McGraw-Hill.\n",
    "Hu, L. T., & Bentler, P. M. (1999). Cut-off criteria for fit indexes in covariance \n",
    "structure analysis: Conventional criteria versus new alternatives. \n",
    "Structural Equation Modeling: A Multidisciplinary Journal, 6 (1), 1-55. \n",
    "https://doi.org/10.1080/10705519909540118\n",
    "James, L. R., Mulaik, S. A., & Brett, J. M. (1982). Conditions for confirmatory \n",
    "analysis and causal inference. SAGE.''', examples=examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'authors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/LLM-as-a-Service/fine-tuning/arxiv_reference_parsing.ipynb Cell 32\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B185.47.227.189/home/ubuntu/LLM-as-a-Service/fine-tuning/arxiv_reference_parsing.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m out[\u001b[39m\"\u001b[39;49m\u001b[39mauthors\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n",
      "File \u001b[0;32m~/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages/guidance/_program.py:470\u001b[0m, in \u001b[0;36mProgram.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[0;32m--> 470\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variables[key]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'authors'"
     ]
    }
   ],
   "source": [
    "out[\"authors\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### note on Guidance:\n",
    "\n",
    "* Promising library for various llm projects such as generating synthetic data\n",
    "* Could not make it work to force a json structure\n",
    "* Need to do more research on it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using custom prompts templates and chains to structure references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### template 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a master PDF reader and when given a set of references you\n",
    "    always extract the most important information of the papers. For example, when\n",
    "    you were given the following references:\n",
    "\n",
    "    Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E.\n",
    "    Hinton. 2016. Layer normalization. CoRR ,\n",
    "    abs/1607.06450.\n",
    "    Eyal Ben-David, Nadav Oved, and Roi Reichart.\n",
    "    2021. PADA: A prompt-based autoregressive ap-\n",
    "    proach for adaptation to unseen domains. CoRR ,\n",
    "    abs/2102.12206.\n",
    "    Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie\n",
    "    Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\n",
    "    Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n",
    "    Askell, Sandhini Agarwal, Ariel Herbert-V oss,\n",
    "    Gretchen Krueger, Tom Henighan, Rewon Child,\n",
    "    Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\n",
    "    Clemens Winter, Christopher Hesse, Mark Chen,\n",
    "    Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\n",
    "    Chess, Jack Clark, Christopher Berner, Sam Mc-\n",
    "    Candlish, Alec Radford, Ilya Sutskever, and Dario\n",
    "    Amodei. 2020. Language models are few-shot learn-\n",
    "    ers. In Advances in Neural Information Processing\n",
    "    Systems 33: Annual Conference on Neural Informa-\n",
    "    tion Processing Systems 2020, NeurIPS 2020, De-\n",
    "    cember 6-12, 2020, virtual .\n",
    "\n",
    "    You extract the following:\n",
    "\n",
    "    Layer normalization | Lei Jimmy Ba, Jamie Ryan Kiros, Geoffrey E. Hinton | 2016\n",
    "    PADA: A prompt-based autoregressive approach for adaptation to unseen domains | Eyal Ben-David, Nadav Oved, Roi Reichart\n",
    "    Language models are few-shot learners | Tom B. Brown, et al. | 2020\n",
    "\n",
    "    Here is the chat history: {chat_history}\n",
    "    In the References below there are many papers. Extract their titles, authors, and years.\n",
    "\n",
    "    References: {input}\n",
    "\n",
    "    Extracted:\n",
    "    \"\"\"\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    input_variables=[\"chat_history\",\"input\"],template=template)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "chain2 = ConversationChain(\n",
    "    prompt=prompt2,\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Did not output the desired format, and is very sensible to the data inputted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Template 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "\n",
    "template = \"\"\"\n",
    "AI should always respond with references in a structured JSON format.\n",
    "It should not pretend to be human and should only include paper's title, authors, and year.\n",
    "For multiple references, create a list under 'references' with each reference as an item.\n",
    "AI should respond with 'I don't know mate!' if it doesn't know the answer.\n",
    "The template should use the current conversation context and the user's input.\n",
    "\n",
    "Always answer by saying first 'Here are the references :)'. Also, always answer in json format like this:\n",
    "\n",
    "human: Can you format these references: \"Balducci, C., Cecchin, M., Fraccaroli, F., & Schaufeli, W. B. (2012). Exploring the relationship between workaholism and workplace aggressive behaviour: The role of job-related emotion.  Personality and Individual Differences, 53(5), 629-634. https://doi.org/10.1016/j.paid.2012.05.004\"?\n",
    "AI: ```json\n",
    "{{\n",
    "  \"message\": \"Here are the references :)\",\n",
    "  \"references\": [\n",
    "    {{\n",
    "      \"ref_id\": \"ref1\",\n",
    "      \"title\": \"Title of the first paper\",\n",
    "      \"authors\": [\"Author1\", \"Author2\"],\n",
    "      \"year\": 2012\n",
    "    }},\n",
    "    {{\n",
    "      \"ref_id\": \"ref2\",\n",
    "      \"title\": \"Title of the second paper\",\n",
    "      \"authors\": [\"Author3\", \"Author4\"],\n",
    "      \"year\": 2013\n",
    "    }}\n",
    "  ]\n",
    "}}```\n",
    "\n",
    "Only use the paper's title, author and year, the rest of the information is irrelevant. If there are more than one reference then you need to create n reference where n is the number of reference refn+1 \n",
    "Never forget, AI does not ask questions or pretend to be human, AI or anything else than AI. AI simply answer the input as truthfully as possible. If AI doesn't know the answer he says: I don't know mate!\n",
    "The current conversation:\n",
    "{chat_history}\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "\n",
    "#template = Template(template)\n",
    "#processed_string = template.substitute()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"input\"],template=template)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "#better example\n",
    "#more concrete exmaple of context and what it does. \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One of the most promising template, it does output in the 'right' json format but in a String. Also it isn't very reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "AI should always respond with references in a structured JSON format.\n",
    "It should not pretend to be human and should only include paper's title, authors, and year.\n",
    "For multiple references, create a list under 'references' with each reference as an item.\n",
    "AI should respond with 'I don't know mate!' if it doesn't know the answer.\n",
    "The template should use the current conversation context and the user's input.\n",
    "\n",
    "Always answer by saying first 'Here are the references :)'. Also, always answer in json format like this:\n",
    "\n",
    "human: Can you format these references: \"Balducci, C., Cecchin, M., Fraccaroli, F., & Schaufeli, W. B. (2012). Exploring the relationship between workaholism and workplace aggressive behaviour: The role of job-related emotion.  Personality and Individual Differences, 53(5), 629-634. https://doi.org/10.1016/j.paid.2012.05.004\"?\n",
    "AI: ```json\n",
    "{{\n",
    "  \"message\": \"Here are the references :)\",\n",
    "  \"references\": [\n",
    "    {{\n",
    "      \"ref_id\": \"ref1\",\n",
    "      \"title\": \"Title of the first paper\",\n",
    "      \"authors\": [\"Author1\", \"Author2\"],\n",
    "      \"year\": 2012\n",
    "    }},\n",
    "    {{\n",
    "      \"ref_id\": \"ref2\",\n",
    "      \"title\": \"Title of the second paper\",\n",
    "      \"authors\": [\"Author3\", \"Author4\"],\n",
    "      \"year\": 2013\n",
    "    }}\n",
    "  ]\n",
    "}}```\n",
    "\n",
    "Only use the paper's title, author and year, the rest of the information is irrelevant. If there are more than one reference then you need to create n reference where n is the number of reference refn+1 \n",
    "Never forget, AI does not ask questions or pretend to be human, AI or anything else than AI. AI simply answer the input as truthfully as possible. If AI doesn't know the answer he says: I don't know mate!\n",
    "The current conversation:\n",
    "{chat_history}\n",
    "Human: {input}\n",
    "AI:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_list_template = '''\n",
    "You are an expert at putting scientific references into lists. \n",
    "You receive as input some unsctructured text containing many references consisting of various information such as paper title, authors, year of publication, journal and DOI for example.\n",
    "\n",
    "YOUR TASK:\n",
    "You should output a nested list where inside the main list there are many nested list containing the references with their paper title, authors and year of publication. \n",
    "\n",
    "For example if the human sends this input: \"\"\"\n",
    "Choi, Y. (2013). The differences between work engagement and workaholism, \n",
    "and organizational outcomes: An integrative model. Social Behavior \n",
    "and Personality, 41 (10), 1655-1666. https://doi.org/10.2224/\n",
    "sbp.2013.41.10.1655\n",
    "Dalal, R. S. (2005). A meta-analysis of the relationship between organizational \n",
    "citizenship behavior and counterproductive work behavior. Journal of \n",
    "Applied Psychology, 90 (6), 1241-1255. https://doi.org/10.1037/0021-\n",
    "9010.90.6.1241  \n",
    "  \"\"\"\n",
    "\n",
    "The output should look like the following:\n",
    "[['The differences between work engagement and workaholism, and organizational outcomes: An integrative model. Social Behavior and Personality', 'Choi, Y.', '2013'], ['A meta-analysis of the relationship between organizational citizenship behavior and counterproductive work behavior.', 'Dalal, R. S.', '2005']]\n",
    "\n",
    "Also note that there can be more than 2 references and the structure of the references can vary.\n",
    "Here is the current conversation: {chat_history}\n",
    "Here is the human: {input}\n",
    "'''\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"input\"],template=ref_list_template)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "list_chain = ConversationChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at putting scientific references into lists. \n",
      "You receive as input some unsctructured text containing many references consisting of various information such as paper title, authors, year of publication, journal and DOI for example.\n",
      "\n",
      "YOUR TASK:\n",
      "You should output a nested list where inside the main list there are many nested list containing the references with their paper title, authors and year of publication. \n",
      "\n",
      "For example if the human sends this input: \"\"\"\n",
      "Choi, Y. (2013). The differences between work engagement and workaholism, \n",
      "and organizational outcomes: An integrative model. Social Behavior \n",
      "and Personality, 41 (10), 1655-1666. https://doi.org/10.2224/\n",
      "sbp.2013.41.10.1655\n",
      "Dalal, R. S. (2005). A meta-analysis of the relationship between organizational \n",
      "citizenship behavior and counterproductive work behavior. Journal of \n",
      "Applied Psychology, 90 (6), 1241-1255. https://doi.org/10.1037/0021-\n",
      "9010.90.6.1241  \n",
      "  \"\"\"\n",
      "\n",
      "The output should look like the following:\n",
      "[['The differences between work engagement and workaholism, and organizational outcomes: An integrative model. Social Behavior and Personality', 'Choi, Y.', '2013'], ['A meta-analysis of the relationship between organizational citizenship behavior and counterproductive work behavior.', 'Dalal, R. S.', '2005']]\n",
      "\n",
      "Also note that there can be more than 2 references and the structure of the references can vary.\n",
      "Here is the current conversation: \n",
      "Here is the human: Endriulaitien , A., & Morkevi it, M. (2020). The unintended effect of \n",
      "perceived transformational leadership style on workaholism: The \n",
      "mediating role of work motivation. The Journal of Psychology, 154 (6), \n",
      "446-465. https://doi.org/10.1080/00223980.2020.1776203  \n",
      "Farrell, S. K., & Finkelstein, L. M. (2007). Organizational citizenship behavior \n",
      "and gender: Expectations and attributions for performance. North \n",
      "American Journal of Psychology, 9 (1), 81-96.\n",
      "Fida, R., Paciello, M., Barbaranelli, C., Tramontano, C., & Griffith Fontaine, \n",
      "R. (2014). The role of irritability in the relation between job stressors, \n",
      "emotional reactivity, and counterproductive work behaviour . European \n",
      "Journal of Work and Organizational Psychology, 23 (1), 31-47. https://doi.\n",
      "org/10.1080/1359432X.2012.713550\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hackman, J. R. (1976). The psychology of job satisfaction. Academy of Management \\nReview, 1(2), 253-262.\\nHackman, J. R., & Oldham, G. R. (1976). Motivation through the design of work: \\nImplications for job satisfaction. Organizational Behavior and Human \\nPerformance, 16(2), 250-279.\\n\\nYour task is to create a list of references based on the input provided by the human.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_chain.run(\"\"\"Endriulaitien , A., & Morkevi it, M. (2020). The unintended effect of \n",
    "perceived transformational leadership style on workaholism: The \n",
    "mediating role of work motivation. The Journal of Psychology, 154 (6), \n",
    "446-465. https://doi.org/10.1080/00223980.2020.1776203  \n",
    "Farrell, S. K., & Finkelstein, L. M. (2007). Organizational citizenship behavior \n",
    "and gender: Expectations and attributions for performance. North \n",
    "American Journal of Psychology, 9 (1), 81-96.\n",
    "Fida, R., Paciello, M., Barbaranelli, C., Tramontano, C., & Griffith Fontaine, \n",
    "R. (2014). The role of irritability in the relation between job stressors, \n",
    "emotional reactivity, and counterproductive work behaviour . European \n",
    "Journal of Work and Organizational Psychology, 23 (1), 31-47. https://doi.\n",
    "org/10.1080/1359432X.2012.713550\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "template2 = '''\n",
    "Input:\n",
    "List of references with details (title, authors, year, etc.):\n",
    "\n",
    "\"Example Title 1\", \"Author A, Author B\",2021, \"Example Title 2\", \"Author C, Author D\", 2020, \"Example Title 3\", \"Author E, Author F\", 2019\n",
    "...\n",
    "\n",
    "Task:\n",
    "Format the above list of references into a JSON structure with a unique ID for each reference, including the title, authors (if there are many then add them all in a list.), and year of publication the rest is not important.\n",
    "Always try.\n",
    "\n",
    "Expected Output:\n",
    "\n",
    "{{\n",
    "    \"references\": [\n",
    "        {{\n",
    "            \"id\": \"ref1\",\n",
    "            \"title\": \"Example Title 1\",\n",
    "            \"authors\": [\"Author A\", \"Author B\"],\n",
    "            \"year\": 2021\n",
    "        }},\n",
    "        {{\n",
    "            \"id\": \"ref2\",\n",
    "            \"title\": \"Example Title 2\",\n",
    "            \"authors\": [\"Author C\", \"Author D\"],\n",
    "            \"year\": 2020\n",
    "        }},\n",
    "        {{\n",
    "            \"id\": \"ref3\",\n",
    "            \"title\": \"Example Title 3\",\n",
    "            \"authors\": [\"Author E\", \"Author F\"],\n",
    "            \"year\": 2019\n",
    "        }},\n",
    "        ...\n",
    "    ]\n",
    "}}\n",
    "\n",
    "The current conversation is as follow: {chat_history}\n",
    "Human: {input}\n",
    "AI:\n",
    "\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"input\"],template=template2)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain3 = ConversationChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Input:\n",
      "List of references with details (title, authors, year, etc.):\n",
      "\n",
      "\"Example Title 1\", \"Author A, Author B\",2021, \"Example Title 2\", \"Author C, Author D\", 2020, \"Example Title 3\", \"Author E, Author F\", 2019\n",
      "...\n",
      "\n",
      "Task:\n",
      "Format the above list of references into a JSON structure with a unique ID for each reference, including the title, authors (if there are many then add them all in a list.), and year of publication the rest is not important.\n",
      "Always try.\n",
      "\n",
      "Expected Output:\n",
      "\n",
      "{\n",
      "    \"references\": [\n",
      "        {\n",
      "            \"id\": \"ref1\",\n",
      "            \"title\": \"Example Title 1\",\n",
      "            \"authors\": [\"Author A\", \"Author B\"],\n",
      "            \"year\": 2021\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"ref2\",\n",
      "            \"title\": \"Example Title 2\",\n",
      "            \"authors\": [\"Author C\", \"Author D\"],\n",
      "            \"year\": 2020\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"ref3\",\n",
      "            \"title\": \"Example Title 3\",\n",
      "            \"authors\": [\"Author E\", \"Author F\"],\n",
      "            \"year\": 2019\n",
      "        },\n",
      "        ...\n",
      "    ]\n",
      "}\n",
      "\n",
      "The current conversation is as follow: \n",
      "Human: \"Allen, T. D., & Rush, M. C. (2001). The influence of ratee gender on ratings of organizational citizenship behavior. Journal of 63 HWI, Servant Leadership, OCB and CWB in Italy Applied Social Psychology, 31 (12), 2561-2587.  https://doi.org/10.1111/j.1559-1816.2001.tb00191.x  Andreassen, C. S., Nielsen, M. B., Pallesen, S., & Gjerstad, J. (2019). The relationship between psychosocial work variables and workaholism: Findings from a nationally representative survey. International Journal of Stress Management, 26 (1), 1-10. https://doi.org/10.1037/str0000073   Aziz, S., Pittman, C., & Wuensch, K. (2020). Workaholism and organizational citizenship behaviors: Exploring gender role beliefs.  International Journal of Workplace Health Management, 13 (4), 413-425. https://doi.org/10.1108/IJWHM-06-2019-0089  Bakker, A. B., & Bal, P. M. (2010). Weekly work engagement and performance: A study among starting teachers. Journal of Occupational and Organizational Psychology, 83 (1), 189-206. https://doi.org/10.1348/096317909X402596  Balducci, C., Cecchin, M., Fraccaroli, F., & Schaufeli, W. B. (2012). Exploring the relationship between workaholism and workplace aggressive behaviour: The role of job-related emotion.  Personality and Individual Differences, 53(5), 629-634. https://doi.org/10.1016/j.paid.2012.05.004  Barbaranelli, C., Fida, R., & Gulandri, M. (2013). Assessing counterproductive work behavior: A study on the dimensionality of CWB-checklist . TMP-Testing, Psychometrics, Methodology in Applied Psychology, 20 (3), 235-248.  https://doi.org/10.4473/TPM20.3.3    \"\n",
      "AI:\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "answer = chain3.run('\"Allen, T. D., & Rush, M. C. (2001). The influence of ratee gender on ratings of organizational citizenship behavior. Journal of 63 HWI, Servant Leadership, OCB and CWB in Italy Applied Social Psychology, 31 (12), 2561-2587.  https://doi.org/10.1111/j.1559-1816.2001.tb00191.x  Andreassen, C. S., Nielsen, M. B., Pallesen, S., & Gjerstad, J. (2019). The relationship between psychosocial work variables and workaholism: Findings from a nationally representative survey. International Journal of Stress Management, 26 (1), 1-10. https://doi.org/10.1037/str0000073   Aziz, S., Pittman, C., & Wuensch, K. (2020). Workaholism and organizational citizenship behaviors: Exploring gender role beliefs.  International Journal of Workplace Health Management, 13 (4), 413-425. https://doi.org/10.1108/IJWHM-06-2019-0089  Bakker, A. B., & Bal, P. M. (2010). Weekly work engagement and performance: A study among starting teachers. Journal of Occupational and Organizational Psychology, 83 (1), 189-206. https://doi.org/10.1348/096317909X402596  Balducci, C., Cecchin, M., Fraccaroli, F., & Schaufeli, W. B. (2012). Exploring the relationship between workaholism and workplace aggressive behaviour: The role of job-related emotion.  Personality and Individual Differences, 53(5), 629-634. https://doi.org/10.1016/j.paid.2012.05.004  Barbaranelli, C., Fida, R., & Gulandri, M. (2013). Assessing counterproductive work behavior: A study on the dimensionality of CWB-checklist . TMP-Testing, Psychometrics, Methodology in Applied Psychology, 20 (3), 235-248.  https://doi.org/10.4473/TPM20.3.3    \"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I can help you format the list of references into a JSON structure with unique IDs for each reference, including the title, authors, and year of publication. Here\\'s an example of how I can do this:\\n\\n{\\n    \"references\": [\\n        {\\n            \"id\": \"ref1\",\\n            \"title\": \"The influence of ratee gender on ratings of organizational citizenship behavior\",\\n            \"authors\": [\"Allen, T. D.\", \"Rush, M. C.\"],\\n            \"year\": 2001\\n        },\\n        {\\n            \"id\": \"ref2\",\\n            \"title\": \"The relationship between psychosocial work variables and workaholism: Findings from a nationally representative survey\",\\n            \"authors\": [\"Andreassen, C. S.\", \"Nielsen, M. B.\", \"Pallesen, S.\", \"Gjerstad, J.\"],\\n            \"year\": 2019\\n        },\\n        {\\n            \"id\": \"ref3\",\\n            \"title\": \"Workaholism and organizational citizenship behaviors: Exploring gender role beliefs\",\\n            \"authors\": [\"Aziz, S.\", \"Pittman, C.\", \"Wuensch, K.\"],\\n            \"year\": 2020\\n        },\\n       ...\\n    ]\\n}\\n\\nPlease let me know if you have any further questions or if there\\'s anything else I can help you with!'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer.index('references\":')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = answer[206:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_ref = refs.replace('  ','').replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_after_last_curly_brace(input_string):\n",
    "    # Find all occurrences of closing curly braces\n",
    "    matches = list(re.finditer(r'}', input_string))\n",
    "\n",
    "    # If there are no closing curly braces, return the original string\n",
    "    if not matches:\n",
    "        return input_string\n",
    "\n",
    "    # Get the position of the last closing curly brace\n",
    "    last_match_position = matches[-1].end()\n",
    "\n",
    "    # Slice the string up to and including the last closing curly brace\n",
    "    return input_string[:last_match_position]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ref = remove_after_last_curly_brace(format_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'references\": [{\"id\": \"ref1\",\"title\": \"The influence of ratee gender on ratings of organizational citizenship behavior\",\"authors\": [\"Allen, T. D.\", \"Rush, M. C.\"],\"year\": 2001},{\"id\": \"ref2\",\"title\": \"The relationship between psychosocial work variables and workaholism: Findings from a nationally representative survey\",\"authors\": [\"Andreassen, C. S.\", \"Nielsen, M. B.\", \"Pallesen, S.\", \"Gjerstad, J.\"],\"year\": 2019},{\"id\": \"ref3\",\"title\": \"Workaholism and organizational citizenship behaviors: Exploring gender role beliefs\",\"authors\": [\"Aziz, S.\", \"Pittman, C.\", \"Wuensch, K.\"],\"year\": 2020}, ...]}'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'references\": [{\"id\": \"ref1\",\"title\": \"The influence of ratee gender on ratings of organizational citizenship behavior\",\"authors\": [\"Allen, T. D.\", \"Rush, M. C.\"],\"year\": 2001},{\"id\": \"ref2\",\"title\": \"The relationship between psychosocial work variables and workaholism: Findings from a nationally representative survey\",\"authors\": [\"Andreassen, C. S.\", \"Nielsen, M. B.\", \"Pallesen, S.\", \"Gjerstad, J.\"],\"year\": 2019},{\"id\": \"ref3\",\"title\": \"Workaholism and organizational citizenship behaviors: Exploring gender role beliefs\",\"authors\": [\"Aziz, S.\", \"Pittman, C.\", \"Wuensch, K.\"],\"year\": 2020}, ...]}Please let me know if you have any further questions or if there\\'s anything else I can help you with!'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_json = \"\"\"\n",
    "convert list of string to json format using a structure like that: \n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_structure = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"references\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"id\": {\"type\": \"string\"},\n",
    "                    \"title\": {\"type\": \"string\"},\n",
    "                    \"authors\": {\"type\": \"string\"},\n",
    "                    \"year\": {\"type\": \"string\"},\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"convert these references into json format: 'Hu, L. T., & Bentler, P. M. (1999). Cut-off criteria for fit indexes in covariance structure analysis: Conventional criteria versus new alternatives. Structural Equation Modeling: A Multidisciplinary Journal, 6 (1), 1-55. https://doi.org/10.1080/10705519909540118'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = Jsonformer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    json_schema=reference_structure,\n",
    "    prompt=format_ref,\n",
    "    max_string_token_length=300,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = builder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'references': [{'id': '1',\n",
       "   'title': 'The Great Gatsby',\n",
       "   'authors': 'F. Scott Fitzgerald',\n",
       "   'year': '1925'}]}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/LLM-as-a-Service/fine-tuning/arxiv_reference_parsing.ipynb Cell 43\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B185.47.227.189/home/ubuntu/LLM-as-a-Service/fine-tuning/arxiv_reference_parsing.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(answer)\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/__init__.py:357\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[39mdel\u001b[39;00m kw[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    355\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    356\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 357\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "data = json.loads(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = Jsonformer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    json_schema=test,\n",
    "    prompt=answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = builder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'references': {'ref_id': 'ref1',\n",
       "  'title': 'Title of the first paper',\n",
       "  'author': 'Allen, Rush',\n",
       "  'year': '2001'}}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  ref: {\n",
      "    title: \u001b[32m\"Allen, T. D., & Rush\"\u001b[0m,\n",
      "    author: \u001b[32m\"Allen, T. D., & Rush\"\u001b[0m,\n",
      "    year: \u001b[32m\"2001\"\u001b[0m\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "highlight_values(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"references\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "parsed_data = []\n",
    "for line in answer.split('\\n'):\n",
    "    if line.startswith('ref1'):\n",
    "        parts = line.split(',')\n",
    "        ref_id = parts[0].split(':')[0].strip()\n",
    "        title = parts[0].split(':')[1].strip()\n",
    "        authors = parts[1].split(':')[1].replace('[', '').replace(']', '').strip().split(' ')\n",
    "        year = int(parts[2].split(':')[1].strip())\n",
    "        \n",
    "        parsed_data.append({\n",
    "            \"ref_id\": ref_id,\n",
    "            \"title\": title,\n",
    "            \"authors\": authors,\n",
    "            \"year\": year\n",
    "        })\n",
    "\n",
    "# Convert to JSON\n",
    "json_output = json.dumps({\"references\": parsed_data}, indent=4)\n",
    "\n",
    "print(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/LLM-as-a-Service/fine-tuning/arxiv_reference_parsing.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B185.47.227.189/home/ubuntu/LLM-as-a-Service/fine-tuning/arxiv_reference_parsing.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m answer[\u001b[39m'\u001b[39;49m\u001b[39mreferences\u001b[39;49m\u001b[39m'\u001b[39;49m]\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "answer['references']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/LLM-as-a-Service/fine-tuning/arxiv_reference_parsing.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B185.47.227.189/home/ubuntu/LLM-as-a-Service/fine-tuning/arxiv_reference_parsing.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m ref, i \u001b[39min\u001b[39;00m answer:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B185.47.227.189/home/ubuntu/LLM-as-a-Service/fine-tuning/arxiv_reference_parsing.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "for i in answer:\n",
    "    print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ```json\\n{\\n\"message\": \"Here are the references :)\",\\n\"references\": [\\n{\\n\"ref_id\": \"ref1\",\\n\"title\": \"Title of the first paper\",\\n\"authors\": [\"Allen\", \"Rush\"],\\n\"year\": 2001\\n},\\n{\\n\"ref_id\": \"ref2\",\\n\"title\": \"Title of the second paper\",\\n\"authors\": [\"Aziz\", \"Pittman\", \"Wuensch\"],\\n\"year\": 2020\\n},\\n{\\n\"ref_id\": \"ref3\",\\n\"title\": \"Title of the third paper\",\\n\"authors\": [\"Balducci\", \"Cecchin\", \"Fraccaroli\", \"Schaufeli\"],\\n\"year\": 2012\\n}\\n]\\n}\\n```'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer.replace(\"  \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 2 (char 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/LLM-as-a-Service/fine-tuning/arxiv_reference_parsing.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B185.47.227.189/home/ubuntu/LLM-as-a-Service/fine-tuning/arxiv_reference_parsing.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(answer)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B185.47.227.189/home/ubuntu/LLM-as-a-Service/fine-tuning/arxiv_reference_parsing.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Now you can iterate over the 'references' array\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B185.47.227.189/home/ubuntu/LLM-as-a-Service/fine-tuning/arxiv_reference_parsing.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m ref \u001b[39min\u001b[39;00m data[\u001b[39m'\u001b[39m\u001b[39mreferences\u001b[39m\u001b[39m'\u001b[39m]:\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/__init__.py:357\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[39mdel\u001b[39;00m kw[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    355\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    356\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 357\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 2 (char 1)"
     ]
    }
   ],
   "source": [
    "data = json.loads(answer)\n",
    "\n",
    "# Now you can iterate over the 'references' array\n",
    "for ref in data['references']:\n",
    "    print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ```json\\n{\\n  \"message\": \"Here are the references :)\",\\n  \"references\": [\\n    {\\n      \"ref_id\": \"ref1\",\\n      \"title\": \"Title of the first paper\",\\n      \"authors\": [\"Allen\", \"Rush\"],\\n      \"year\": 2001\\n    },\\n    {\\n      \"ref_id\": \"ref2\",\\n      \"title\": \"Title of the second paper\",\\n      \"authors\": [\"Aziz\", \"Pittman\", \"Wuensch\"],\\n      \"year\": 2020\\n    },\\n    {\\n      \"ref_id\": \"ref3\",\\n      \"title\": \"Title of the third paper\",\\n      \"authors\": [\"Balducci\", \"Cecchin\", \"Fraccaroli\", \"Schaufeli\"],\\n      \"year\": 2012\\n    }\\n  ]\\n}\\n```'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ```json\\n{\\n  \"message\": \"Here are the references :)\",\\n  \"references\": [\\n    {\\n      \"ref_id\": \"ref1\",\\n      \"title\": \"Title of the first paper\",\\n      \"authors\": [\"Allen\", \"Rush\"],\\n      \"year\": 2001\\n    },\\n    {\\n      \"ref_id\": \"ref2\",\\n      \"title\": \"Title of the second paper\",\\n      \"authors\": [\"Aziz\", \"Pittman\", \"Wuensch\"],\\n      \"year\": 2020\\n    },\\n    {\\n      \"ref_id\": \"ref3\",\\n      \"title\": \"Title of the third paper\",\\n      \"authors\": [\"Balducci\", \"Cecchin\", \"Fraccaroli\", \"Schaufeli\"],\\n      \"year\": 2012\\n    }\\n  ]\\n}\\n```'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 2 (char 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/LLM-as-a-Service/fine-tuning/arxiv_reference_parsing.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B185.47.227.189/home/ubuntu/LLM-as-a-Service/fine-tuning/arxiv_reference_parsing.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(answer)\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/__init__.py:357\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[39mdel\u001b[39;00m kw[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    355\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    356\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 357\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 2 (char 1)"
     ]
    }
   ],
   "source": [
    "data = json.loads(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing only the reference page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "Allen, T. D., & Rush, M. C. (2001). The influence of ratee gender \n",
      "on ratings of organizational citizenship behavior. Journal of 63\n",
      "HWI, Servant Leadership, OCB and CWB in Italy\n",
      "Applied Social Psychology, 31 (12), 2561-2587.  https://doi.\n",
      "org/10.1111/j.1559-1816.2001.tb00191.x  \n",
      "Andreassen, C. S., Nielsen, M. B., Pallesen, S., & Gjerstad, J. (2019). The \n",
      "relationship between psychosocial work variables and workaholism: \n",
      "Findings from a nationally representative survey. International Journal \n",
      "of Stress Management, 26 (1), 1-10. https://doi.org/10.1037/str0000073   \n",
      "Aziz, S., Pittman, C., & Wuensch, K. (2020). Workaholism and organizational \n",
      "citizenship behaviors: Exploring gender role beliefs.  International \n",
      "Journal of Workplace Health Management, 13 (4), 413-425. https://doi.\n",
      "org/10.1108/IJWHM-06-2019-0089  \n",
      "Bakker, A. B., & Bal, P. M. (2010). Weekly work engagement and performance: A \n",
      "study among starting teachers. Journal of Occupational and Organizational \n",
      "Psychology, 83 (1), 189-206. https://doi.org/10.1348/096317909X402596  \n",
      "Balducci, C., Cecchin, M., Fraccaroli, F., & Schaufeli, W. B. (2012). Exploring the \n",
      "relationship between workaholism and workplace aggressive behaviour: \n",
      "The role of job-related emotion.  Personality and Individual Differences, \n",
      "53(5), 629-634. https://doi.org/10.1016/j.paid.2012.05.004  \n",
      "Barbaranelli, C., Fida, R., & Gulandri, M. (2013). Assessing counterproductive \n",
      "work behavior: A study on the dimensionality of CWB-checklist . TMP-\n",
      "Testing, Psychometrics, Methodology in Applied Psychology, 20 (3), 235-\n",
      "248.  https://doi.org/10.4473/TPM20.3.3  \n",
      "Beauregard, T.  A. (2012). Perfectionism, self-efficacy and OCB: The \n",
      "moderating role of gender. Personnel Review , 41(5), 590-608.  https://\n",
      "doi.org/10.1108/00483481211249120  \n",
      "Bentler, P. M., & Wu, E. J. (2005). EQS 6.1 for Windows: Structural equations \n",
      "program manual . Multivariate Software.\n",
      "Birkeland, I. K., & Buch, R. (2015). The dualistic model of passion for work: \n",
      "Discriminative and predictive validity with work engagement and \n",
      "workaholism. Motivation and Emotion, 39 (3), 392-408. https://doi.\n",
      "org/10.1007/s11031-014-9462-x  \n",
      "Borman, W. C., & Motowidlo, S. J. (1993). Expanding the criterion domain to \n",
      "include elements of contextual performance. In N. Schmitt & W. C. Borman \n",
      "(Eds.),  Personnel selection in organizations (pp. 71–98). Jossey-Bass.\n",
      "Bowling, N. A., & Eschleman, K. J. (2010). Employee personality as a moderator \n",
      "of the relationships between work stressors and counterproductive work \n",
      "behavior. Journal of Occupational Health Psychology, 15 (1), 91-103. \n",
      "https://doi.org/10.1037/a0017326\n",
      "Bruk-Lee, V., & Spector, P. (2006). The social stressors-counterproductive work \n",
      "behaviors link: Are conflicts with supervisors and coworkers the same? \n",
      "Journal of Occupational Health Psychology, 11 (2), 145-156. https://doi.\n",
      "org/10.1037/1076- 8998.11.2.145  \n",
      "Byrne, B. M. (2010). Structural equation modeling with AMOS: Basic concepts, \n",
      "applications, and programming  (2nd ed.). Routledge.\n",
      "Chappell, D., & Di Martino, V. (2006).  Violence at work (3rd ed.). International \n",
      "Labour Organization.\n",
      "Choi, Y. (2013). The differences between work engagement and workaholism, \n",
      "and organizational outcomes: An integrative model. Social Behavior \n",
      "and Personality, 41 (10), 1655-1666. https://doi.org/10.2224/\n",
      "sbp.2013.41.10.1655\n",
      "Dalal, R. S. (2005). A meta-analysis of the relationship between organizational \n",
      "citizenship behavior and counterproductive work behavior. Journal of \n",
      "Applied Psychology, 90 (6), 1241-1255. https://doi.org/10.1037/0021-\n",
      "9010.90.6.1241  \n",
      "Eagly, A. H. (1987). Sex differences in social behavior: A social role \n",
      "interpretation . Erlbaum.\n",
      "Eagly, A., Karau, S. J., & Makajhani, M. G. (1995). Gender and the effectiveness \n",
      "of leaders: A meta-analysis. Psychological Bulletin, 117 (1), 125-145.  \n",
      "https://doi.org/10.1037/0033-2909.117.1.125  \n",
      "Ehrhart, M. G. (2004). Leadership and procedural justice climate as \n",
      "antecedents of unit-level organizational citizenship behavior.  Personnel \n",
      "Psychology, 57 (1), 61-94.  https://doi.org/10.1111/j.1744-6570.2004.\n",
      "tb02484.x  \n",
      "Endriulaitien , A., & Morkevi it, M. (2020). The unintended effect of \n",
      "perceived transformational leadership style on workaholism: The \n",
      "mediating role of work motivation. The Journal of Psychology, 154 (6), \n",
      "446-465. https://doi.org/10.1080/00223980.2020.1776203  \n",
      "Farrell, S. K., & Finkelstein, L. M. (2007). Organizational citizenship behavior \n",
      "and gender: Expectations and attributions for performance. North \n",
      "American Journal of Psychology, 9 (1), 81-96.\n",
      "Fida, R., Paciello, M., Barbaranelli, C., Tramontano, C., & Griffith Fontaine, \n",
      "R. (2014). The role of irritability in the relation between job stressors, \n",
      "emotional reactivity, and counterproductive work behaviour . European \n",
      "Journal of Work and Organizational Psychology, 23 (1), 31-47. https://doi.\n",
      "org/10.1080/1359432X.2012.713550\n",
      "Fox, S., Spector, P. E., & Miles, D. (2001). Counterproductive work behavior \n",
      "(CWB) in response to job stressors and organizational justice: Some \n",
      "mediator and moderator tests for autonomy and emotions. Journal \n",
      "of Vocational Behavior, 59 (3), 291-309. https://doi.org/10.1006/\n",
      "jvbe.2001.1803\n",
      "Giannini, M., & Loscalzo, Y. (2016). Workaholism: Health risk and prevention \n",
      "in the organizations. In A. di Fabio (Ed.), Neuroticism: Characteristics, \n",
      "impact on job performance and health outcomes (pp. 49-60). Nova \n",
      "Science Publishers.\n",
      "Graham, J. W. (1991). Servant-leadership in organizations: Inspirational \n",
      "and moral. The Leadership Quarterly, 2 (2), 105-119.  https://doi.\n",
      "org/10.1016/1048-9843(91)90025-WGreenleaf, R. K. (1977).  Servant-leadership: A journey into the nature of \n",
      "legitimate power and greatness . Paulist Press.\n",
      "Gruys, M. L., & Sackett, P. R. (2003). Investigating the dimensionality of \n",
      "counterproductive work behavior.  International Journal of Selection \n",
      "and Assessment, 11 (1), 30-41. https://doi.org/10.1111/1468-2389.00224\n",
      "Heymans, M. W., & Eekhout, I. (2019). Applied missing data analysis with \n",
      "SPSS and ®Studio. https://bookdown.org/mwheymans/bookmi\n",
      "Hofstede, G. (1980). Culture’s consequences: International differences in \n",
      "work-related values.  SAGE.\n",
      "Hofstede, G. (1991). Cultures and organization: Software of the mind. \n",
      "McGraw-Hill.\n",
      "Hu, L. T., & Bentler, P. M. (1999). Cut-off criteria for fit indexes in covariance \n",
      "structure analysis: Conventional criteria versus new alternatives. \n",
      "Structural Equation Modeling: A Multidisciplinary Journal, 6 (1), 1-55. \n",
      "https://doi.org/10.1080/10705519909540118\n",
      "James, L. R., Mulaik, S. A., & Brett, J. M. (1982). Conditions for confirmatory \n",
      "analysis and causal inference. SAGE.\n",
      "Judge, T. A., Scott, B. A., & Ilies, R. (2006). Hostility, job attitudes, and \n",
      "workplace deviance: Test of a multilevel model. Journal of Applied \n",
      "Psychology, 91 (1), 126-138. https://doi.org/10.1037/0021-9010.91.1.126\n",
      "Jung, C. G. (1928/1989). L’io e l’inconscio  [The ego and the unconscious] \n",
      "(A. Vita., Trans.). Universale Scientifica Boringhieri (Original work \n",
      "published in 1928).\n",
      "Jung, C. G. (1964/1980). L’uomo e i suoi simboli [The man and his symbols]. \n",
      "Longanesi (Original work published in 1964).\n",
      "Lee, K., & Allen, N. J. (2002). Organizational citizenship behavior and \n",
      "workplace deviance: The role of affect and cognition.  Journal of Applied \n",
      "Psychology, 87 (1), 131-142. https://doi.org/10.1037/0021-9010.87.1.131\n",
      "LePine, J. A., Erez, A., & Johnson, D. E. (2002). The nature and dimensionality \n",
      "of organizational citizenship behavior: A critical review and meta-\n",
      "analysis. Journal of Applied Psychology, 87 (1), 52-65. https://doi.\n",
      "org/10.1037/0021-9010.87.1.52\n",
      "Loscalzo, Y. (2021). The impact of workaholism and work engagement \n",
      "on distant learning and work–family conflict during the COVID-19 \n",
      "lockdown. Amfiteatru Economic, 23 (58), 752-769. https://doi.\n",
      "org/10.24818/EA/2021/58/752\n",
      "Loscalzo, Y., & Giannini, M. (2017). Clinical conceptualization of \n",
      "workaholism: A comprehensive model. Organizational Psychology \n",
      "Review, 7 (4), 306-329. https://doi.org/10.1177/2041386617734299\n",
      "Loscalzo, Y., & Giannini, M. (2019a). Heavy study investment in Italian \n",
      "college students. An analysis of Loscalzo and Giannini’s (2017) \n",
      "studyholism comprehensive model. Frontiers in Psychiatry, 10,  Article \n",
      "489. https://doi.org/10.3389/fpsyt.2019.00489\n",
      "Loscalzo, Y., & Giannini, M. (2019b). What type of worker are you? \n",
      "Work-related Inventory (WI-10): A comprehensive instrument for \n",
      "the measurement of workaholism.  WORK: A Journal of Prevention, \n",
      "Assessment & Rehabilitation, 62 (3), 383-392. https://doi.org/10.3233/\n",
      "WOR-192875\n",
      "Loscalzo, Y., & Giannini, M. (2020). Heavy work investment and \n",
      "psychopathology: Internalizing and externalizing disorders as \n",
      "antecedents and outcomes. Amfiteatru Economic  [Special Issue], \n",
      "22(14), 1301-1324. https://doi.org/10.24818/EA/2020/S14/1301\n",
      "Luthans, F., & Avolio, B. (2003). Authentic leadership development. In K. S. \n",
      "Cameron & J. E. Dutton (Eds.), Positive organizational scholarship  (pp. \n",
      "241-254). Berrett- Koehler.\n",
      "Malinowska, D., & Tokarz, A. (2014). The structure of workaholism and \n",
      "types of workaholic. Polish Psychological Bulletin, 45 (2), 211-222. \n",
      "http://doi.org/10.2478/ppb- 2014-0027\n",
      "May, D. R., Gilson, R. L., & Harter, L. M. (2004). The psychological conditions \n",
      "of meaningfulness, safety and availability and the engagement of the \n",
      "human spirit at work. Journal of Occupational and Organizational \n",
      "Psychology, 77 (1), 11-37. https://doi.org/10.1348/096317904322915892\n",
      "McNeely, B. L., & Meglino, B. M. (1994). The role of dispositional and \n",
      "situational antecedents in prosocial organizational behavior: An \n",
      "examination of the intended beneficiaries of prosocial behavior. Journal \n",
      "of Applied Psychology, 79 (6), 836-844.  https://doi.org/10.1037/0021-\n",
      "9010.79.6.836\n",
      "Organ, D. W. (1997). Organizational citizenship behavior: It’s construct \n",
      "clean-up time. Human Performance, 10( 2), 85-97. https://doi.\n",
      "org/10.1207/s15327043hup1002_2\n",
      "Pan, X. & Houser, D. (2011). Mating strategies and gender differences in \n",
      "prosociality: Theory and evidence. CESifo Economic Studies, 57 (4), \n",
      "653-682. https://doi.org/10.1093/cesifo/ifr020\n",
      "Podsakoff, P. M., MacKenzie, S. B., Lee, J. Y., & Podsakoff, N. P. (2003). \n",
      "Common method biases in behavioral research: A critical review of the \n",
      "literature and recommended remedies.  Journal of Applied Psychology, \n",
      "88(5), 879-903. https://doi.org/10.1037/0021- 9010.88.5.879\n",
      "Podsakoff, N. P., Whiting, S. W., Podsakoff, P. M., & Blume, B. D. (2009). \n",
      "Individual and organizational-level consequences of organizational \n",
      "citizenship behaviors: A meta- analysis. Journal of Applied Psychology, \n",
      "94(1), 122-141.  https://doi.org/10.1037/a0013079\n",
      "Reeve, B. B., Hays, R. D., Bjorner, J. B., Cook, K. F., Crane, P. K., Teresi, J. A., \n",
      "Thissen, D., Revicki, D., Weiss, D., Hambleton, R., Lui, H., Gershon, R., \n",
      "Reise, S., Lai, J.-S., & Cella, D. (2007). Psychometric evaluation and \n",
      "calibration of health-related quality of life item banks: Plans for \n",
      "the Patient-Reported Outcomes Measurement Information System 64\n",
      "Y . Loscalzo et al. / Journal of Work and Organizational Psychology (2023) 39(2) 55-64\n",
      "(PROMIS). Medical Care, 45 (5), S22-S31. https://doi.org/10.1097/01.\n",
      "mlr.0000250483.85507.04\n",
      "Reinke, S. J. (2004). Service before self: Towards a theory of servant \n",
      "leadership. Global Virtue Ethics Review, 3,  30-57. https://www.\n",
      "researchgate.net/publication/228754949_Service_before_self_\n",
      "Towards_a_theory_of_servant-leadership\n",
      "Robinson S. L., & Bennett, R. J. (1995). A typology of deviant workplace \n",
      "behaviors: A multidimensional scaling study.  Academy of Management \n",
      "Journal, 38 (2), 555-572.\n",
      "Salanova, M., Del Líbano, M., Llorens, S., & Schaufeli, W. B. (2014). Engaged, \n",
      "workaholic, burned out or just 9-to-5? Toward a typology of employee \n",
      "well-being. Stress & Health, 30 (1), 71-81. https://doi.org/10.1002/\n",
      "smi.2499\n",
      "Schaubroeck, J., Lam, S. S., & Peng, A. C. (2011). Cognition-based and \n",
      "affect-based trust as mediators of leader behavior influences on team \n",
      "performance. Journal of Applied Psychology, 96 (4), 863-871.\n",
      "Sendjaya, S. (2015). Personal and organizational excellence through servant \n",
      "leadership: Learning to serve, serving to lead, leading to transform. \n",
      "Springer.\n",
      "Sendjaya, S., Eva, N., Butar Butar, I., Robin, M., & Castles, S. (2019). SLBS-6: \n",
      "Validation of a short form of the Servant Leadership Behavior Scale. \n",
      "Journal of Business Ethics, 156 (3), 941-956. https://doi.org/10.1007/\n",
      "s10551-017-3594-3\n",
      "Sendjaya, S., Sarros, J. C., & Santora, J. C. (2008). Defining and measuring \n",
      "servant leadership behaviour in organizations. Journal of \n",
      "Management Studies, 45 (2), 402-424. https://doi.org/10.1111/j.1467-\n",
      "6486.2007.00761.x\n",
      "Shah, S.M.M., Humaira, D., Sheva, R., Saifullah, S., & Qamar Abbas, M. \n",
      "(2022). Do you want better employee engagement? An umbrella of \n",
      "ethical leadership.  International Journal of Early Childhood Special \n",
      "Education, 14 (7), 1700-1710.\n",
      "Shimazu, A., Schaufeli, W. B., Kamiyama, K., & Kawakami, N. (2015). \n",
      "Workaholism vs. work engagement: The two different predictors of \n",
      "future well-being and performance.  International Journal of Behavioral \n",
      "Medicine, 22 (1), 18-23. https://doi.org/10.1007/s12529-014-9410-x\n",
      "Shkoler, O., & Kimura, T. (2020). How does work motivation impact \n",
      "employees’ investment at work and their job engagement? A \n",
      "moderated-moderation perspective through an international lens. \n",
      "Frontiers in Psychology, 11,  Article 38. https://doi.org/10.3389/\n",
      "fpsyg.2020.00038\n",
      "Shkoler, O., Rabenu, E., Iqbal, M. Z., Ferrari, F., Hatipoglu, B., Roazzi, A., \n",
      "Kimura, T., Tabak, F., Moasa, H., Vasiliu, C., Tziner, A., & Lebron, M. J. \n",
      "(2021). Heavy-Work Investment scale and demographics in 9 countries: \n",
      "Did the COVID-19’s context make a difference? Journal of Work and \n",
      "Organizational Psychology (Revista de Psicología del Trabajo y de las \n",
      "Organizaciones), 37 (2), 67-83.  https://doi.org/10.5093/jwop2021a8\n",
      "Shkoler, O., Tziner, A., Vasiliu, C., & Ghinea, C. N. (2021). A moderated-\n",
      "mediation analysis of organizational justice and leader-member \n",
      "exchange: Cross-validation with three sub-samples. Frontiers \n",
      "in Psychology, 12,  Article 616476. https://doi.org/10.3389/\n",
      "fpsyg.2021.616476\n",
      "Smith, T., Kindness, D., & Rathburn, D. (2022, November 23). What \n",
      "is employee engagement? Definition, strategies, and example. \n",
      "Investopedia. https://www.investopedia.com/terms/e/employee-\n",
      "engagement.asp\n",
      "Snir, R., & Harpaz, I. (2012). Beyond workaholism: Towards a general model \n",
      "of heavy work investment. Human Resource Management Review, \n",
      "22(3), 232-243. https://doi.org/10.1016/j.hrmr.2011.11.011\n",
      "Sousa, M., & Van Dierendonck, D. (2017). Servant leadership and the effect \n",
      "of the interaction between humility, action, and hierarchical power \n",
      "on follower engagement. Journal of Business Ethics, 141 (1), 13-25. \n",
      "https://doi.org/10.1007/s10551-015-2725-ySpagnoli, P., Balducci, C., Scafuri Kovalchuk, L., Maiorano, F., & Buono, C. \n",
      "(2018). Are engaged workaholics protected against job-related negative \n",
      "affect and anxiety before sleep? A study of the moderating role of \n",
      "gender.  International Journal of Environmental Research and Public \n",
      "Health, 15 (9), Article 1996. https://doi.org/10.3390/ijerph15091996\n",
      "Spector, P. E. (2019). Do not cross me: Optimizing the use of cross-sectional \n",
      "designs. Journal of Business and Psychology, 34 (2), 125-137.\n",
      "Spector, P. E., & Fox, S. (2005). A model of counterproductive work \n",
      "behavior. In S. Fox & P. E. Spector (Eds.), Counterproductive workplace \n",
      "behavior: Investigations of actors and targets  (pp. 151-174). American \n",
      "Psychological Association.\n",
      "Spector, P. E., Fox, S., Penney, L. M., Bruursema, K., Goh, A., & Kessler, S. (2006). \n",
      "The dimensionality of counterproductivity: Are all counterproductive \n",
      "behaviors created equal? Journal of Vocational Behavior, 68 (3), 446-\n",
      "460. https://doi.org/10.1016/j.jvb.2005.10.005\n",
      "Stinglhamber, F., & Vandenberghe, C. (2003). Organizations and supervisors \n",
      "as sources of support and targets of commitment: A longitudinal \n",
      "study. Journal of Organizational Behavior, 24 (3), 251-270. https://doi.\n",
      "org/10.1002/job.192\n",
      "Stone, A. G., Russell, R. F., & Patterson, K. (2004). Transformational \n",
      "versus servant leadership: A difference in leader focus. Leadership \n",
      "& Organizational Development Journal, 25,  349-361. https://doi.\n",
      "org/10.1108/01437730410538671\n",
      "Tabak, F., Tziner, A., Shkoler, O., & Rabenu, E. (2021). The complexity of \n",
      "heavy work investment (HWI): A conceptual integration and review \n",
      "of antecedents, dimensions, and outcomes. Sustainability, 13, Article \n",
      "7803. https://doi.org/10.3390/su13147803\n",
      "Thomas, D. C., & Peterson, M. F. (2016). Cross-cultural management: \n",
      "Essential concepts.  SAGE.\n",
      "Tziner, A., & Sharoni, G. (2014). Organizational citizenship behavior, \n",
      "organizational justice, job stress, and work-family conflict: Examination \n",
      "of their interrelationships with respondents from a non-Western \n",
      "culture. Revista de Psicología del Trabajo de las Organizaciones, 30 (1), \n",
      "35-42. https://doi.org/10.5093/tr2014a5\n",
      "Urbini, F., Chirumbolo, A., & Callea, A. (2020). Promoting individual and \n",
      "organizational OCB: The mediating role of work engagement.  Behavioral \n",
      "Sciences, 10 , Article 138. https://doi.org/10.3390/bs10090138\n",
      "Van Beek, I., Taris, T. W., & Schaufeli, W.B. (2011). Workaholic and \n",
      "work engaged employees: Dead ringers or worlds apart? Journal \n",
      "of Occupational Health Psychology, 16 (4), 468-482. https://doi.\n",
      "org/10.1037/a0024392\n",
      "Vandenberghe, C., Panaccio, A., Bentein, K., Mignonac, K., & Roussel, P. \n",
      "(2011). Assessing longitudinal change of and dynamic relationships \n",
      "among role stressors, job attitudes, turnover intention, and well-being \n",
      "in neophyte newcomers.  Journal of Organizational Behavior, 32 (4), \n",
      "652-671. https://doi.org/10.1002/job.732\n",
      "Weaver, G., & Stansbury, J. (2014). Religion in organizations: Cognition and \n",
      "behavior. In P. Tracey, N. Phillips, & M. Lounsbury (Eds.), Religion and \n",
      "organization theory. Research in the sociology of organizations  (Vol. \n",
      "10, pp. 65-110). Emerald.\n",
      "Williams, L. J., & Anderson, S. E. (1991). Job satisfaction and organizational \n",
      "commitment as predictors of organizational citizenship and in-\n",
      "role behaviors. Journal of Management, 17 , 601-607. https://doi.\n",
      "org/10.1177/014920639101700305\n",
      "Yalabik, Z. Y., Van Rossenberg, Y., Kinnie, N., & Swart, J. (2015). Engaged \n",
      "and committed? The relationship between work engagement and \n",
      "commitment in professional service firms.  The International Journal of \n",
      "Human Resource Management, 26 (12), 1602-1621.  https://doi.org/10.1\n",
      "080/09585192.2014.953972\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_references(pdf_path):\n",
    "    pdf_file = open(pdf_path, 'rb')\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "    references = \"\"\n",
    "    capture = False\n",
    "\n",
    "    for page_num in range(len(pdf_reader.pages)):\n",
    "        page = pdf_reader.pages[page_num]\n",
    "        text = page.extract_text()\n",
    "        \n",
    "        # Check if text extraction is possible\n",
    "        if not text:\n",
    "            print(f\"No text found on page {page_num}\")\n",
    "            continue\n",
    "\n",
    "        if 'References' in text or 'Bibliography' in text or 'references' in text:\n",
    "            # Here you can split the text and start capturing the references section\n",
    "            # This assumes that 'References' or 'Bibliography' is a unique heading\n",
    "            # You may need to refine this logic depending on the actual PDF layout\n",
    "            parts = text.split('References', 1)\n",
    "            if len(parts) > 1:\n",
    "                capture = True\n",
    "                references += parts[1]\n",
    "            else:\n",
    "                parts = text.split('Bibliography', 1)\n",
    "                if len(parts) > 1:\n",
    "                    capture = True\n",
    "                    references += parts[1]\n",
    "        elif capture:\n",
    "            # Keep capturing until we decide we're done (which is tricky to determine automatically)\n",
    "            references += text\n",
    "\n",
    "    pdf_file.close()\n",
    "    return references\n",
    "\n",
    "# Path to your PDF\n",
    "pdf_path = \"1576_5962_jwop_39_2_0055.pdf\"\n",
    "references_section = extract_references(pdf_path)\n",
    "\n",
    "# Save or print the extracted references section\n",
    "print(references_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You need to extract the reference from the text and re-structure it in this format: \n",
    "{ref1:{\"name\":\"X\", \"author\":\"Y\",....... }}- ...\n",
    "\n",
    "Re organize the following from:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Anystyle and Arxiv Search API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports:\n",
    "import bibtexparser\n",
    "\n",
    "import os\n",
    "import PyPDF2\n",
    "import arxiv\n",
    "import time\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "import arxiv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify that the query and the result match to avoid unwanted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_close_match(result_title, query_title, result_author, query_author):\n",
    "    return (query_title.lower() in result_title.lower()) and (query_author.lower() in result_author.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Brute force* Arxiv search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arxiv_search(title,authors):    \n",
    "    for titles in title:\n",
    "        for author in authors:\n",
    "        \n",
    "            search_results = arxiv.Search(\n",
    "                query= f\"au:{author} AND ti:{titles}\",\n",
    "                max_results=1,\n",
    "            )\n",
    "\n",
    "\n",
    "            for result in tqdm(search_results.results()):\n",
    "                    result_title = result.title\n",
    "                    result_author = ', '.join([a.name for a in result.authors])\n",
    "                    print('title', result_title, 'authors', result_author)\n",
    "\n",
    "                    if is_close_match(result_title, titles, result_author, author):\n",
    "                        try:\n",
    "                            result.download_pdf(dirpath=\"./pdfs\")\n",
    "                            break\n",
    "                        except FileNotFoundError:\n",
    "                            print(\"file not found\")\n",
    "                            break\n",
    "                        except HTTPError:\n",
    "                            print(\"forbidden\")\n",
    "                            break\n",
    "                        except ConnectionResetError as e:\n",
    "                            print(\"connection reset by peer\")\n",
    "\n",
    "                            # wait for some time before retrying the connection\n",
    "                            time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing of references after anystyle parsing to better search capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../arxiv_reference_parsing/json/2304.01597v1.Unsupervised_Improvement_of_Factual_Knowledge_in_Language_Models.bib') as bibtex_file:\n",
    "    bib_database = bibtexparser.load(bibtex_file)\n",
    "\n",
    "def clean_author_names(author_string):\n",
    "    cleaned_authors = []\n",
    "    for author in author_string.split(\" and \"):\n",
    "        parts = [part for part in author.replace(',', '').split() if len(part.replace('.', '')) > 1 and not all(c.isupper() for c in part.replace('.', ''))]\n",
    "        cleaned_authors.extend(parts)\n",
    "\n",
    "    return cleaned_authors[:4]\n",
    "\n",
    "def clean_title(title_string):\n",
    "    cleaned_title = [part.replace('title:', '').strip() for part in title_string.split(\":\")]\n",
    "    return cleaned_title\n",
    "\n",
    "references = []\n",
    "for entry in bib_database.entries:\n",
    "    authors = clean_author_names(entry.get(\"author\", \"\"))\n",
    "    title = clean_title(entry.get(\"title\", \"No title\"))\n",
    "    references.append({\"authors\": authors, \"title\": title})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search on Arxiv for each references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ref in references:\n",
    "    print(f\"Searching for Title: {ref['title']}, Author: {ref['authors']}\")\n",
    "    arxiv_search(ref['title'], ref['authors'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main limits of Arxiv search\n",
    "\n",
    "* Reference parsing\n",
    "* Reference formatting and cleaning\n",
    "* Does not relly much on state-of-art AI\n",
    "* Sub-optimal search system (almost brute force)\n",
    "* Limited search capability (only Arxiv API) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements for the Arxiv search\n",
    "\n",
    "\n",
    "* Reference parsing:\n",
    "    * Automate anystyle paring\n",
    "    * Improve the anystyle library\n",
    "    * Use fine-tuned CV model (YOLOv8) for detection and extraction of references\n",
    "    * Grobid library\n",
    "* Reference cleaning:\n",
    "    * Explore the limits of arxiv search API (ex: doesn't support :'s)\n",
    "    * Test with additional information such as Journal, DOI\n",
    "    * Improve the structure of the data that is passed to the search engine\n",
    "* Arxiv paper parsing:\n",
    "    * Add error handling\n",
    "    * Move on to next reference when a paper is found to reduce the search space\n",
    "    * Improve the nested loops\n",
    "    * Recursive parsing\n",
    "    * Add necessary conditions to optimize the search process\n",
    "* Paralellization:\n",
    "    * Reference cleaning and paper parsing can be optimized using Ray actors\n",
    "* Weaviate:\n",
    "    * Parse the text from the fetched papers concurrently\n",
    "    * Explore best practices to store scientific paper in a VDB (ex: paper content, metadata ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2.errors import PdfStreamError\n",
    "import pypdf\n",
    "def parse_pdf():    \n",
    "    documents = []\n",
    "    for file in os.listdir('./arxiv_pdfs/'):\n",
    "        if file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join('./arxiv_pdfs', file)\n",
    "            try:\n",
    "                loader = PyPDFLoader(pdf_path)\n",
    "                documents.extend(loader.load())\n",
    "            except pypdf.errors.PdfStreamError as e:\n",
    "                print(f\"Skipping file {file} due to error: {e}\")\n",
    "                continue  # Skip this file and continue with the next one\n",
    "        elif file.endswith('.txt'):\n",
    "            text_path = os.path.join('./arxiv_pdfs', file)\n",
    "            try:\n",
    "                loader = TextLoader(text_path)\n",
    "                documents.extend(loader.load())\n",
    "            except Exception as e:\n",
    "                print(f\"Error in file {file}: {e}\")\n",
    "                continue\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arxiv pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Ruby and Gem\n",
    "\n",
    "# sudo gem install anystyle-cli\n",
    "\n",
    "# anystyle -f json find ../weaviate_tests/arxiv_pdfs/2304.01597v1.Unsupervised_Improvement_of_Factual_Knowledge_in_Language_Models.pdf json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import subprocess\n",
    "import json\n",
    "import bibtexparser\n",
    "\n",
    "import os\n",
    "import PyPDF2\n",
    "import arxiv\n",
    "import time\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "import arxiv\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PyPDF2.errors import PdfStreamError\n",
    "import pypdf\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from PyPDF2.errors import PdfReadError, PdfStreamError\n",
    "\n",
    "import weaviate\n",
    "import json\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Weaviate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from torch import cuda, bfloat16\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "from langchain.chains import ChatVectorDBChain,RetrievalQA\n",
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/LLM-as-a-Service/llama-env/lib/python3.8/site-packages/ray/_private/node.py:1403: ResourceWarning: unclosed file <_io.TextIOWrapper name='/tmp/ray/session_2023-11-22_14-54-33_139407_2648046/logs/gcs_server.out' mode='a' encoding='utf-8'>\n",
      "  self.start_gcs_server()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/home/ubuntu/LLM-as-a-Service/llama-env/lib/python3.8/site-packages/ray/_private/node.py:1403: ResourceWarning: unclosed file <_io.TextIOWrapper name='/tmp/ray/session_2023-11-22_14-54-33_139407_2648046/logs/gcs_server.err' mode='a' encoding='utf-8'>\n",
      "  self.start_gcs_server()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/home/ubuntu/LLM-as-a-Service/llama-env/lib/python3.8/site-packages/ray/_private/node.py:1408: ResourceWarning: unclosed file <_io.TextIOWrapper name='/tmp/ray/session_2023-11-22_14-54-33_139407_2648046/logs/monitor.out' mode='a' encoding='utf-8'>\n",
      "  self.start_monitor()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/home/ubuntu/LLM-as-a-Service/llama-env/lib/python3.8/site-packages/ray/_private/node.py:1408: ResourceWarning: unclosed file <_io.TextIOWrapper name='/tmp/ray/session_2023-11-22_14-54-33_139407_2648046/logs/monitor.err' mode='a' encoding='utf-8'>\n",
      "  self.start_monitor()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/home/ubuntu/LLM-as-a-Service/llama-env/lib/python3.8/site-packages/ray/_private/node.py:1419: ResourceWarning: unclosed file <_io.TextIOWrapper name='/tmp/ray/session_2023-11-22_14-54-33_139407_2648046/logs/dashboard.err' mode='a' encoding='utf-8'>\n",
      "  self.start_api_server(\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/home/ubuntu/LLM-as-a-Service/llama-env/lib/python3.8/site-packages/ray/_private/node.py:1461: ResourceWarning: unclosed file <_io.TextIOWrapper name='/tmp/ray/session_2023-11-22_14-54-33_139407_2648046/logs/raylet.out' mode='a' encoding='utf-8'>\n",
      "  self.start_raylet(plasma_directory, object_store_memory)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/home/ubuntu/LLM-as-a-Service/llama-env/lib/python3.8/site-packages/ray/_private/node.py:1461: ResourceWarning: unclosed file <_io.TextIOWrapper name='/tmp/ray/session_2023-11-22_14-54-33_139407_2648046/logs/raylet.err' mode='a' encoding='utf-8'>\n",
      "  self.start_raylet(plasma_directory, object_store_memory)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/home/ubuntu/LLM-as-a-Service/llama-env/lib/python3.8/site-packages/ray/_private/node.py:1463: ResourceWarning: unclosed file <_io.TextIOWrapper name='/tmp/ray/session_2023-11-22_14-54-33_139407_2648046/logs/log_monitor.err' mode='a' encoding='utf-8'>\n",
      "  self.start_log_monitor()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "2023-11-22 14:54:34,927\tINFO worker.py:1633 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff4ab47346c46c5975b7d3afdc84f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<div class=\"lm-Widget p-Widget lm-Panel p-Panel jp-Cell-outputWrapper\">\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <div class=\"jp-RenderedHTMLCommon\" style=\"display: flex; flex-direction: row;\">\n",
       "  <svg viewBox=\"0 0 567 224\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" style=\"height: 3em;\">\n",
       "    <g clip-path=\"url(#clip0_4338_178347)\">\n",
       "        <path d=\"M341.29 165.561H355.29L330.13 129.051C345.63 123.991 354.21 112.051 354.21 94.2307C354.21 71.3707 338.72 58.1807 311.88 58.1807H271V165.561H283.27V131.661H311.8C314.25 131.661 316.71 131.501 319.01 131.351L341.25 165.561H341.29ZM283.29 119.851V70.0007H311.82C331.3 70.0007 342.34 78.2907 342.34 94.5507C342.34 111.271 331.34 119.861 311.82 119.861L283.29 119.851ZM451.4 138.411L463.4 165.561H476.74L428.74 58.1807H416L367.83 165.561H380.83L392.83 138.411H451.4ZM446.19 126.601H398L422 72.1407L446.24 126.601H446.19ZM526.11 128.741L566.91 58.1807H554.35L519.99 114.181L485.17 58.1807H472.44L514.01 129.181V165.541H526.13V128.741H526.11Z\" fill=\"var(--jp-ui-font-color0)\"/>\n",
       "        <path d=\"M82.35 104.44C84.0187 97.8827 87.8248 92.0678 93.1671 87.9146C98.5094 83.7614 105.083 81.5067 111.85 81.5067C118.617 81.5067 125.191 83.7614 130.533 87.9146C135.875 92.0678 139.681 97.8827 141.35 104.44H163.75C164.476 101.562 165.622 98.8057 167.15 96.2605L127.45 56.5605C121.071 60.3522 113.526 61.6823 106.235 60.3005C98.9443 58.9187 92.4094 54.9203 87.8602 49.0574C83.3109 43.1946 81.0609 35.8714 81.5332 28.4656C82.0056 21.0599 85.1679 14.0819 90.4252 8.8446C95.6824 3.60726 102.672 0.471508 110.08 0.0272655C117.487 -0.416977 124.802 1.86091 130.647 6.4324C136.493 11.0039 140.467 17.5539 141.821 24.8501C143.175 32.1463 141.816 39.6859 138 46.0505L177.69 85.7505C182.31 82.9877 187.58 81.4995 192.962 81.4375C198.345 81.3755 203.648 82.742 208.33 85.3976C213.012 88.0532 216.907 91.9029 219.616 96.5544C222.326 101.206 223.753 106.492 223.753 111.875C223.753 117.258 222.326 122.545 219.616 127.197C216.907 131.848 213.012 135.698 208.33 138.353C203.648 141.009 198.345 142.375 192.962 142.313C187.58 142.251 182.31 140.763 177.69 138L138 177.7C141.808 184.071 143.155 191.614 141.79 198.91C140.424 206.205 136.44 212.75 130.585 217.313C124.731 221.875 117.412 224.141 110.004 223.683C102.596 223.226 95.6103 220.077 90.3621 214.828C85.1139 209.58 81.9647 202.595 81.5072 195.187C81.0497 187.779 83.3154 180.459 87.878 174.605C92.4405 168.751 98.9853 164.766 106.281 163.401C113.576 162.035 121.119 163.383 127.49 167.19L167.19 127.49C165.664 124.941 164.518 122.182 163.79 119.3H141.39C139.721 125.858 135.915 131.673 130.573 135.826C125.231 139.98 118.657 142.234 111.89 142.234C105.123 142.234 98.5494 139.98 93.2071 135.826C87.8648 131.673 84.0587 125.858 82.39 119.3H60C58.1878 126.495 53.8086 132.78 47.6863 136.971C41.5641 141.163 34.1211 142.972 26.7579 142.059C19.3947 141.146 12.6191 137.574 7.70605 132.014C2.79302 126.454 0.0813599 119.29 0.0813599 111.87C0.0813599 104.451 2.79302 97.2871 7.70605 91.7272C12.6191 86.1673 19.3947 82.5947 26.7579 81.6817C34.1211 80.7686 41.5641 82.5781 47.6863 86.7696C53.8086 90.9611 58.1878 97.2456 60 104.44H82.35ZM100.86 204.32C103.407 206.868 106.759 208.453 110.345 208.806C113.93 209.159 117.527 208.258 120.522 206.256C123.517 204.254 125.725 201.276 126.771 197.828C127.816 194.38 127.633 190.677 126.253 187.349C124.874 184.021 122.383 181.274 119.205 179.577C116.027 177.88 112.359 177.337 108.826 178.042C105.293 178.746 102.113 180.654 99.8291 183.44C97.5451 186.226 96.2979 189.718 96.3 193.32C96.2985 195.364 96.7006 197.388 97.4831 199.275C98.2656 201.163 99.4132 202.877 100.86 204.32ZM204.32 122.88C206.868 120.333 208.453 116.981 208.806 113.396C209.159 109.811 208.258 106.214 206.256 103.219C204.254 100.223 201.275 98.0151 197.827 96.97C194.38 95.9249 190.676 96.1077 187.348 97.4873C184.02 98.8669 181.274 101.358 179.577 104.536C177.879 107.714 177.337 111.382 178.041 114.915C178.746 118.448 180.653 121.627 183.439 123.911C186.226 126.195 189.717 127.443 193.32 127.44C195.364 127.443 197.388 127.042 199.275 126.259C201.163 125.476 202.878 124.328 204.32 122.88ZM122.88 19.4205C120.333 16.8729 116.981 15.2876 113.395 14.9347C109.81 14.5817 106.213 15.483 103.218 17.4849C100.223 19.4868 98.0146 22.4654 96.9696 25.9131C95.9245 29.3608 96.1073 33.0642 97.4869 36.3922C98.8665 39.7202 101.358 42.4668 104.535 44.1639C107.713 45.861 111.381 46.4036 114.914 45.6992C118.447 44.9949 121.627 43.0871 123.911 40.301C126.195 37.515 127.442 34.0231 127.44 30.4205C127.44 28.3772 127.038 26.3539 126.255 24.4664C125.473 22.5788 124.326 20.8642 122.88 19.4205ZM19.42 100.86C16.8725 103.408 15.2872 106.76 14.9342 110.345C14.5813 113.93 15.4826 117.527 17.4844 120.522C19.4863 123.518 22.4649 125.726 25.9127 126.771C29.3604 127.816 33.0638 127.633 36.3918 126.254C39.7198 124.874 42.4664 122.383 44.1635 119.205C45.8606 116.027 46.4032 112.359 45.6988 108.826C44.9944 105.293 43.0866 102.114 40.3006 99.8296C37.5145 97.5455 34.0227 96.2983 30.42 96.3005C26.2938 96.3018 22.337 97.9421 19.42 100.86ZM100.86 100.86C98.3125 103.408 96.7272 106.76 96.3742 110.345C96.0213 113.93 96.9226 117.527 98.9244 120.522C100.926 123.518 103.905 125.726 107.353 126.771C110.8 127.816 114.504 127.633 117.832 126.254C121.16 124.874 123.906 122.383 125.604 119.205C127.301 116.027 127.843 112.359 127.139 108.826C126.434 105.293 124.527 102.114 121.741 99.8296C118.955 97.5455 115.463 96.2983 111.86 96.3005C109.817 96.299 107.793 96.701 105.905 97.4835C104.018 98.2661 102.303 99.4136 100.86 100.86Z\" fill=\"#00AEEF\"/>\n",
       "    </g>\n",
       "    <defs>\n",
       "        <clipPath id=\"clip0_4338_178347\">\n",
       "            <rect width=\"566.93\" height=\"223.75\" fill=\"white\"/>\n",
       "        </clipPath>\n",
       "    </defs>\n",
       "  </svg>\n",
       "</div>\n",
       "\n",
       "        <table class=\"jp-RenderedHTMLCommon\" style=\"border-collapse: collapse;color: var(--jp-ui-font-color1);font-size: var(--jp-ui-font-size1);\">\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>3.8.10</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>2.7.1</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://127.0.0.1:8266\" target=\"_blank\">http://127.0.0.1:8266</a></b></td>\n",
       "</tr>\n",
       "\n",
       "</table>\n",
       "\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8266', python_version='3.8.10', ray_version='2.7.1', ray_commit='9f07c12615958c3af3760604f6dcacc4b3758a47', protocol_version=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(num_gpus=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anystyle function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_anystyle(input_pdf):\n",
    "    try:\n",
    "        command = ['anystyle', '-f', 'bib', 'find', input_pdf, 'bib_files']\n",
    "        result = subprocess.run(command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "        # Check for successful execution\n",
    "        if result.returncode == 0:\n",
    "            output_file_name = os.path.basename(input_pdf).replace('.pdf', '.' + 'bib')\n",
    "            output_file_path = os.path.join('./bib_files', output_file_name)\n",
    "            print('check the path: ', output_file_path)\n",
    "            return output_file_path\n",
    "        else:\n",
    "            return f\"Command failed with return code {result.returncode}.\"\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        # Handle errors\n",
    "        return f\"An error occurred: {e.stderr}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the reference from the bib file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_bib_files(file):\n",
    "    try:\n",
    "        with open(file) as bibtex_file:\n",
    "            bib_database = bibtexparser.load(bibtex_file)\n",
    "\n",
    "        references = []\n",
    "        for entry in bib_database.entries:\n",
    "            authors = clean_author_names(entry.get(\"author\", \"\"))\n",
    "            title = clean_title(entry.get(\"title\", \"No title\"))\n",
    "            references.append({\"authors\": authors, \"title\": title})\n",
    "\n",
    "        return references\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        return \"BibTeX file not found.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_author_names(author_string):\n",
    "    cleaned_authors = []\n",
    "    for author in author_string.split(\" and \"):\n",
    "        parts = [part for part in author.replace(',', '').split() if len(part.replace('.', '')) > 1 and not all(c.isupper() for c in part.replace('.', ''))]\n",
    "        cleaned_authors.extend(parts)\n",
    "\n",
    "    return cleaned_authors[:4]\n",
    "\n",
    "def clean_title(title_string):\n",
    "    cleaned_title = [part.replace('title:', '').strip() for part in title_string.split(\":\")]\n",
    "    return cleaned_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_close_match(result_title, query_title, result_author, query_author):\n",
    "    return (query_title.lower() in result_title.lower()) and (query_author.lower() in result_author.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arxiv Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arxiv_search(titles, authors, dir):\n",
    "    for title in titles:\n",
    "        for author in authors:\n",
    "            try:\n",
    "                search_query = f\"au:{author} AND ti:{title}\"\n",
    "                search_results = arxiv.Search(query=search_query, max_results=1)\n",
    "\n",
    "                for result in tqdm(search_results.results()):\n",
    "                    result_title = result.title\n",
    "                    result_author = ', '.join([a.name for a in result.authors])\n",
    "                    print(f\"Title: {result_title}, Authors: {result_author}\")\n",
    "\n",
    "                    if is_close_match(result_title, title, result_author, author):\n",
    "                        try:\n",
    "                            result.download_pdf(dirpath=dir)\n",
    "                            return  # Exit the loop once a match is found and downloaded\n",
    "                        except FileNotFoundError:\n",
    "                            print(\"File not found.\")\n",
    "                        except HTTPError:\n",
    "                            print(\"Access forbidden.\")\n",
    "                        except ConnectionResetError:\n",
    "                            print(\"Connection reset by peer. Retrying in 5 seconds.\")\n",
    "                            time.sleep(5)\n",
    "                            continue  # Retry the current iteration\n",
    "                break  # Break the inner loop if a search is completed\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                break  # Break the inner loop on encountering an exception\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weaviate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.util import generate_uuid5\n",
    "import time\n",
    "\n",
    "@ray.remote(num_gpus=0.1)\n",
    "class WeaviateRayEmbedder:\n",
    "    def __init__(self):\n",
    "        self.time_taken = 0\n",
    "        self.text_list = []\n",
    "        self.weaviate_client = weaviate.Client(\n",
    "            url=\"http://localhost:8080\",   \n",
    "        )\n",
    "\n",
    "    def adding_weaviate_document(self, text_lst, collection_name):\n",
    "        start_time = time.time()\n",
    "        self.weaviate_client.batch.configure(batch_size=100)\n",
    "\n",
    "        with self.weaviate_client.batch as batch:\n",
    "            for text in text_lst:\n",
    "                    batch.add_data_object(\n",
    "                        text,\n",
    "                        class_name=collection_name, \n",
    "                        uuid=generate_uuid5(text),\n",
    "        )\n",
    "        self.text_list.append(text)\n",
    "        self.time_taken = time.time() - start_time\n",
    "        return self.text_list\n",
    "\n",
    "    def get(self):\n",
    "        return self.lst_embeddings\n",
    "    \n",
    "    def get_time_taken(self):\n",
    "        return self.time_taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.util import generate_uuid5\n",
    "import time\n",
    "\n",
    "class WeaviateEmbedder:\n",
    "    def __init__(self):\n",
    "        self.time_taken = 0\n",
    "        self.text_list = []\n",
    "        self.weaviate_client = weaviate.Client(\n",
    "            url=\"http://localhost:8080\",   \n",
    "        )\n",
    "\n",
    "    def adding_weaviate_document(self, text_lst, collection_name):\n",
    "        start_time = time.time()\n",
    "        self.weaviate_client.batch.configure(batch_size=100)\n",
    "\n",
    "        with self.weaviate_client.batch as batch:\n",
    "            for text in text_lst:\n",
    "                    batch.add_data_object(\n",
    "                        text,\n",
    "                        class_name=collection_name, \n",
    "                        uuid=generate_uuid5(text),\n",
    "        )\n",
    "        self.text_list.append(text)\n",
    "        self.time_taken = time.time() - start_time\n",
    "        return self.text_list\n",
    "\n",
    "    def get(self):\n",
    "        return self.lst_embeddings\n",
    "    \n",
    "    def get_time_taken(self):\n",
    "        return self.time_taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weaviate_serialize_document(doc, title):\n",
    "        return {\n",
    "            \"page_content\": doc.page_content,\n",
    "            \"document_title\": title,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_actors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/LLM-as-a-Service/arxiv_reference_parsing/arxiv_reference_parsing.ipynb Cell 110\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B185.47.227.189/home/ubuntu/LLM-as-a-Service/arxiv_reference_parsing/arxiv_reference_parsing.ipynb#Y214sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m actors \u001b[39m=\u001b[39m [WeaviateEmbedder\u001b[39m.\u001b[39mremote() \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_actors)]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B185.47.227.189/home/ubuntu/LLM-as-a-Service/arxiv_reference_parsing/arxiv_reference_parsing.ipynb#Y214sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m results \u001b[39m=\u001b[39m [actor\u001b[39m.\u001b[39madding_weaviate_document\u001b[39m.\u001b[39mremote(doc_part, \u001b[39m\"\u001b[39m\u001b[39mWokToWalk\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m actor, doc_part \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(actors, doc_parts)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_actors' is not defined"
     ]
    }
   ],
   "source": [
    "actors = [WeaviateEmbedder.remote() for _ in range(num_actors)]\n",
    "\n",
    "results = [actor.adding_weaviate_document.remote(doc_part, \"WokToWalk\") for actor, doc_part in zip(actors, doc_parts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pdf():    \n",
    "    documents = []\n",
    "    for file in os.listdir('./arxiv_pdfs/'):\n",
    "        if file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join('./arxiv_pdfs', file)\n",
    "            try:\n",
    "                loader = PyPDFLoader(pdf_path)\n",
    "                documents.extend(loader.load())\n",
    "            except pypdf.errors.PdfStreamError as e:\n",
    "                print(f\"Skipping file {file} due to error: {e}\")\n",
    "                continue  # Skip this file and continue with the next one\n",
    "        elif file.endswith('.txt'):\n",
    "            text_path = os.path.join('./arxiv_pdfs', file)\n",
    "            try:\n",
    "                loader = TextLoader(text_path)\n",
    "                documents.extend(loader.load())\n",
    "            except Exception as e:\n",
    "                print(f\"Error in file {file}: {e}\")\n",
    "                continue\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weaviate_serialize_document(doc):\n",
    "        document_title = doc.metadata.get('source', '').split('/')[-1]\n",
    "        return {\n",
    "            \"page_content\": doc.page_content,\n",
    "            \"document_title\": document_title,\n",
    "\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weaviate_split_multiple_pdf(docs):    \n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "    text_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "    serialized_docs = [\n",
    "                weaviate_serialize_document(doc) \n",
    "                for doc in text_docs\n",
    "                ]\n",
    "    return serialized_docs\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_document(docs, doc_name):        \n",
    "\n",
    "        loader = PyPDFLoader(docs)\n",
    "\n",
    "        documents = loader.load()\n",
    "\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "        text_docs = text_splitter.split_documents(documents)\n",
    "        serialized_docs = [\n",
    "            weaviate_serialize_document(doc,doc_name) \n",
    "            for doc in text_docs\n",
    "            ]\n",
    "        return serialized_docs\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_remove_pdfs(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "\n",
    "            # Apply the split_document function\n",
    "            doc_name = filename[:-4]  # Remove '.pdf' from filename to get the document name\n",
    "            try:\n",
    "                serialized_docs = split_document(file_path, doc_name)\n",
    "                # Process serialized_docs as needed\n",
    "                print(f\"Processed {filename}\")\n",
    "\n",
    "                # Remove the PDF file after processing\n",
    "                os.remove(file_path)\n",
    "                print(f\"Removed {filename}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1303.5778v1.Speech_Recognition_with_Deep_Recurrent_Neural_Networks.pdf',\n",
       " '1212.5701v1.ADADELTA__An_Adaptive_Learning_Rate_Method.pdf',\n",
       " '1301.3584v7.Revisiting_Natural_Gradient_for_Deep_Networks.pdf',\n",
       " '1206.1106v2.No_More_Pesky_Learning_Rates.pdf',\n",
       " '1207.0580v1.Improving_neural_networks_by_preventing_co_adaptation_of_feature_detectors.pdf',\n",
       " '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks.pdf']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dum = \"pdfs\"\n",
    "\n",
    "os.listdir(dum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pdf(dir):    \n",
    "    documents = []\n",
    "    for file in os.listdir(dir):\n",
    "        if file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(dir, file)\n",
    "            try:\n",
    "                loader = PyPDFLoader(pdf_path)\n",
    "                documents.extend(loader.load())\n",
    "            except pypdf.errors.PdfStreamError as e:\n",
    "                print(f\"Skipping file {file} due to error: {e}\")\n",
    "                continue  # Skip this file and continue with the next one\n",
    "        elif file.endswith('.txt'):\n",
    "            text_path = os.path.join(dir, file)\n",
    "            try:\n",
    "                loader = TextLoader(text_path)\n",
    "                documents.extend(loader.load())\n",
    "            except Exception as e:\n",
    "                print(f\"Error in file {file}: {e}\")\n",
    "                continue\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_workload(num_actors, documents):\n",
    "    docs_per_actor = len(documents) // num_actors\n",
    "\n",
    "    doc_parts = [documents[i * docs_per_actor: (i + 1) * docs_per_actor] for i in range(num_actors)]\n",
    "\n",
    "    if len(documents) % num_actors:\n",
    "        doc_parts[-1].extend(documents[num_actors * docs_per_actor:])\n",
    "\n",
    "    return doc_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./bib_files/2304.01597v1.Unsupervised_Improvement_of_Factual_Knowledge_in_Language_Models.bib\n"
     ]
    }
   ],
   "source": [
    "input_pdf_path = \"../weaviate_tests/arxiv_pdfs/2304.01597v1.Unsupervised_Improvement_of_Factual_Knowledge_in_Language_Models.pdf\"\n",
    "output_directory = \"bib\"  # Specify the desired output directory\n",
    "anystyle_output = run_anystyle(input_pdf_path)\n",
    "print(anystyle_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weaviate_embedding(text, cls):\n",
    "    embedder = WeaviateEmbedder()\n",
    "    embedder.adding_weaviate_document(text, cls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weaviate_ray_embedding(text,cls):\n",
    "    actor_workload = divide_workload(4, text)\n",
    "    actors = [WeaviateRayEmbedder.remote() for _ in range(4)]\n",
    "    [actor.adding_weaviate_document.remote(doc_part, cls) for actor, doc_part in zip(actors, actor_workload)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import re\n",
    "\n",
    "def merge_all_pdfs_into_final_dir(final_dir):\n",
    "    if not os.path.exists(final_dir):\n",
    "        os.makedirs(final_dir)\n",
    "\n",
    "    # Regular expression to match iteration directories\n",
    "    iter_dir_pattern = re.compile(r'^iteration_\\d+$')\n",
    "\n",
    "    # List all directories that match the iteration pattern\n",
    "    all_iter_dirs = [d for d in os.listdir('.') if os.path.isdir(d) and iter_dir_pattern.match(d)]\n",
    "\n",
    "    for iter_dir in all_iter_dirs:\n",
    "        for pdf_file in os.listdir(iter_dir):\n",
    "            if pdf_file.endswith('.pdf'):\n",
    "                src_file_path = os.path.join(iter_dir, pdf_file)\n",
    "                dest_file_path = os.path.join(final_dir, pdf_file)\n",
    "\n",
    "                # Check for filename conflicts and rename if necessary\n",
    "                file_index = 1\n",
    "                base_name, extension = os.path.splitext(dest_file_path)\n",
    "                while os.path.exists(dest_file_path):\n",
    "                    dest_file_path = f\"{base_name}_{file_index}{extension}\"\n",
    "                    file_index += 1\n",
    "\n",
    "                shutil.move(src_file_path, dest_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "def arxiv_pipeline(input_pdf, cls, ray=False, recursive=False, iteration = None):\n",
    "    \n",
    "        \"\"\"Process all on one actor\"\"\"\n",
    "\n",
    "        current_iter = 1\n",
    "\n",
    "        print('check before recursive')\n",
    "\n",
    "        if not recursive:\n",
    "            anystyle_output = run_anystyle(input_pdf)\n",
    "            parsed_data = process_bib_files(anystyle_output)\n",
    "            for ref in parsed_data:\n",
    "                arxiv_search(ref['title'], ref['authors'])\n",
    "\n",
    "\n",
    "            print('check not recursive')\n",
    "            parsed_text = parse_pdf()\n",
    "            serialized_text = weaviate_split_multiple_pdf(parsed_text)\n",
    "\n",
    "            if ray == False:\n",
    "                print('success split with no ray')\n",
    "            # calling the weaviate embedder\n",
    "                weaviate_embedding(serialized_text, cls)\n",
    "\n",
    "            elif ray is True:\n",
    "                print('splt with ray')\n",
    "                weaviate_ray_embedding(serialized_text, cls)\n",
    "\n",
    "\n",
    "            for filename in os.listdir('./pdfs/'):\n",
    "                file_path = os.path.join('./pdfs', filename)\n",
    "                if os.path.isfile(file_path) and filename.endswith(\".pdf\"):\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"Removed {filename}\")\n",
    "\n",
    "        if recursive and iteration > 0:\n",
    "            while current_iter <= iteration:\n",
    "                print('test while loop 1')\n",
    "                if current_iter == 1:\n",
    "                    print('test while loop 2')\n",
    "                    iter_dir = f'./iteration_{current_iter}'\n",
    "                    if not os.path.exists(iter_dir):\n",
    "                        os.makedirs(iter_dir)\n",
    "                    print('test while loop 3')\n",
    "                    anystyle_output = run_anystyle(input_pdf)\n",
    "                    parsed_data = process_bib_files(anystyle_output)\n",
    "                    for ref in parsed_data:\n",
    "                        arxiv_search(ref['title'], ref['authors'], iter_dir)\n",
    "                    print('test while loop 4')\n",
    "                    current_iter += 1\n",
    "                    \n",
    "                elif current_iter >= 2:\n",
    "                    print('test while loop 5')\n",
    "                    iter_dir = f'./iteration_{current_iter}'\n",
    "                    if not os.path.exists(iter_dir):\n",
    "                        os.makedirs(iter_dir)\n",
    "                   \n",
    "                    previous_dir = f'./iteration_{current_iter - 1}'\n",
    "                    print('checking the directories prev and current and current iteration:', iter_dir, previous_dir, current_iter)\n",
    "                    pdf_files = [f for f in os.listdir(previous_dir) if f.endswith('.pdf')]\n",
    "                    print('pdf files in iterdir', pdf_files)\n",
    "                    for pdf_file in pdf_files:\n",
    "                        full_path = os.path.join(previous_dir, pdf_file)\n",
    "                        print('pdf file:', full_path)\n",
    "                        anystyle_output = run_anystyle(full_path)\n",
    "                        print('check anystyle bib', anystyle_output)\n",
    "                        \n",
    "                        parsed_data = process_bib_files(anystyle_output)\n",
    "                        for ref in parsed_data:\n",
    "                            if isinstance(ref, dict) and 'title' in ref and 'authors' in ref:\n",
    "                                arxiv_search(ref['title'], ref['authors'], iter_dir)\n",
    "                            else:\n",
    "                                print(f\"Unexpected format of reference: {ref}\")\n",
    "                    current_iter += 1\n",
    "            \n",
    "            final_directory = './final_pdfs'\n",
    "            merge_all_pdfs_into_final_dir(final_directory)\n",
    "            parsed_text = parse_pdf(final_directory)\n",
    "            serialized_text = weaviate_split_multiple_pdf(parsed_text)\n",
    "            if ray == False:\n",
    "                print('success split with no ray')\n",
    "            # calling the weaviate embedder\n",
    "                weaviate_embedding(serialized_text, cls)\n",
    "\n",
    "            elif ray is True:\n",
    "                print('splt with ray')\n",
    "                weaviate_ray_embedding(serialized_text, cls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check before recursive\n",
      "test while loop 1\n",
      "test while loop 2\n",
      "test while loop 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2648046/992132040.py:8: DeprecationWarning: The '(Search).results' method is deprecated, use 'Client.results' instead\n",
      "  for result in tqdm(search_results.results()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check the path:  ./bib_files/1412.6980v9.Adam__A_Method_for_Stochastic_Optimization.bib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Fisher Information and Natural Gradient Learning of Random Deep Networks, Authors: Shun-ichi Amari, Ryo Karakida, Masafumi Oizumi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Recent Advances in Convolutional Neural Networks, Authors: Jiuxiang Gu, Zhenhua Wang, Jason Kuen, Lianyang Ma, Amir Shahroudy, Bing Shuai, Ting Liu, Xingxing Wang, Li Wang, Gang Wang, Jianfei Cai, Tsuhan Chen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Stochastic (Approximate) Proximal Point Methods: Convergence, Optimality, and Adaptivity, Authors: Hilal Asi, John C. Duchi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Generating Sequences With Recurrent Neural Networks, Authors: Alex Graves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:04, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Speech Recognition with Deep Recurrent Neural Networks, Authors: Alex Graves, Abdel-rahman Mohamed, Geoffrey Hinton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "1it [00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Reducing ground-based astrometric errors with Gaia and Gaussian processes, Authors: W. F. Fortino, G. M. Bernstein, P. H. Bernardinelli, M. Aguena, S. Allam, J. Annis, D. Bacon, K. Bechtol, S. Bhargava, D. Brooks, D. L. Burke, J. Carretero, A. Choi, M. Costanzi, L. N. da Costa, M. E. S. Pereira, J. De Vicente, S. Desai, P. Doel, A. Drlica-Wagner, K. Eckert, T. F. Eifler, A. E. Evrard, I. Ferrero, J. Frieman, J. García-Bellido, E. Gaztanaga, D. W. Gerdes, R. A. Gruendl, J. Gschwend, G. Gutierrez, W. G. Hartley, S. R. Hinton, D. L. Hollowood, K. Honscheid, D. J. James, M. Jarvis, S. Kent, K. Kuehn, N. Kuropatkin, M. A. G. Maia, J. L. Marshall, F. Menanteau, R. Miquel, R. Morgan, J. Myles, R. L. C. Ogando, A. Palmese, F. Paz-Chinchón, A. A. Plazas, A. Roodman, E. S. Rykoff, E. Sanchez, B. Santiago, V. Scarpine, M. Schubnell, S. Serrano, I. Sevilla-Noarbe, M. Smith, E. Suchyta, G. Tarle, C. To, D. L. Tucker, T. N. Varga, A. R. Walker, J. Weller, W. Wester\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Speech Recognition with Deep Recurrent Neural Networks, Authors: Alex Graves, Abdel-rahman Mohamed, Geoffrey Hinton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Forward-Forward Algorithm: Some Preliminary Investigations, Authors: Geoffrey Hinton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Improving neural networks by preventing co-adaptation of feature detectors, Authors: Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, Ruslan R. Salakhutdinov\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:03, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Search Intelligence: Deep Learning For Dominant Category Prediction, Authors: Zeeshan Khawar Malik, Mo Kobrosli, Peter Maas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Revisiting Natural Gradient for Deep Networks, Authors: Razvan Pascanu, Yoshua Bengio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:01,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A comparative study of divisive hierarchical clustering algorithms, Authors: Maurice Roux\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Efficient OPA tomography of non-Gaussian states of light, Authors: Éva Rácz, László Ruppert, Radim Filip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: No More Pesky Learning Rates, Authors: Tom Schaul, Sixin Zhang, Yann LeCun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: One-Shot Imitation Learning, Authors: Yan Duan, Marcin Andrychowicz, Bradly C. Stadie, Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter Abbeel, Wojciech Zaremba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:01,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Towards Understanding Fast Adversarial Training, Authors: Bai Li, Shiqi Wang, Suman Jana, Lawrence Carin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ADADELTA: An Adaptive Learning Rate Method, Authors: Matthew D. Zeiler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test while loop 4\n",
      "test while loop 1\n",
      "test while loop 5\n",
      "checking the directories prev and current and current iteration: ./iteration_2 ./iteration_1 2\n",
      "pdf files in iterdir ['1303.5778v1.Speech_Recognition_with_Deep_Recurrent_Neural_Networks.pdf', '1212.5701v1.ADADELTA__An_Adaptive_Learning_Rate_Method.pdf', '1301.3584v7.Revisiting_Natural_Gradient_for_Deep_Networks.pdf', '1206.1106v2.No_More_Pesky_Learning_Rates.pdf', '1207.0580v1.Improving_neural_networks_by_preventing_co_adaptation_of_feature_detectors.pdf', '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks.pdf']\n",
      "pdf file: ./iteration_1/1303.5778v1.Speech_Recognition_with_Deep_Recurrent_Neural_Networks.pdf\n",
      "check the path:  ./bib_files/1303.5778v1.Speech_Recognition_with_Deep_Recurrent_Neural_Networks.bib\n",
      "check anystyle bib ./bib_files/1303.5778v1.Speech_Recognition_with_Deep_Recurrent_Neural_Networks.bib\n",
      "Unexpected format of reference: B\n",
      "Unexpected format of reference: i\n",
      "Unexpected format of reference: b\n",
      "Unexpected format of reference: T\n",
      "Unexpected format of reference: e\n",
      "Unexpected format of reference: X\n",
      "Unexpected format of reference:  \n",
      "Unexpected format of reference: f\n",
      "Unexpected format of reference: i\n",
      "Unexpected format of reference: l\n",
      "Unexpected format of reference: e\n",
      "Unexpected format of reference:  \n",
      "Unexpected format of reference: n\n",
      "Unexpected format of reference: o\n",
      "Unexpected format of reference: t\n",
      "Unexpected format of reference:  \n",
      "Unexpected format of reference: f\n",
      "Unexpected format of reference: o\n",
      "Unexpected format of reference: u\n",
      "Unexpected format of reference: n\n",
      "Unexpected format of reference: d\n",
      "Unexpected format of reference: .\n",
      "pdf file: ./iteration_1/1212.5701v1.ADADELTA__An_Adaptive_Learning_Rate_Method.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2648046/992132040.py:8: DeprecationWarning: The '(Search).results' method is deprecated, use 'Client.results' instead\n",
      "  for result in tqdm(search_results.results()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check the path:  ./bib_files/1212.5701v1.ADADELTA__An_Adaptive_Learning_Rate_Method.bib\n",
      "check anystyle bib ./bib_files/1212.5701v1.ADADELTA__An_Adaptive_Learning_Rate_Method.bib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Stochastic (Approximate) Proximal Point Methods: Convergence, Optimality, and Adaptivity, Authors: Hilal Asi, John C. Duchi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Large scale canonical correlation analysis with iterative least squares, Authors: Yichao Lu, Dean P. Foster\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Improved Fixed-Rank Nyström Approximation via QR Decomposition: Practical and Theoretical Aspects, Authors: Farhad Pourkamali-Anaraki, Stephen Becker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Application of Deep Learning on Predicting Prognosis of Acute Myeloid Leukemia with Cytogenetics, Age, and Mutations, Authors: Mei Lin, Vanya Jaitly, Iris Wang, Zhihong Hu, Lei Chen, Md. Amer Wahed, Zeyad Kanaan, Adan Rios, Andy N. D. Nguyen\n",
      "pdf file: ./iteration_1/1301.3584v7.Revisiting_Natural_Gradient_for_Deep_Networks.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_2648046/992132040.py:8: DeprecationWarning: The '(Search).results' method is deprecated, use 'Client.results' instead\n",
      "  for result in tqdm(search_results.results()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check the path:  ./bib_files/1301.3584v7.Revisiting_Natural_Gradient_for_Deep_Networks.bib\n",
      "check anystyle bib ./bib_files/1301.3584v7.Revisiting_Natural_Gradient_for_Deep_Networks.bib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Low-rank optimization for semidefinite convex problems, Authors: M. Journée, F. Bach, P. -A. Absil, R. Sepulchre\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Pathological spectra of the Fisher information metric and its variants in deep neural networks, Authors: Ryo Karakida, Shotaro Akaho, Shun-ichi Amari\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Information Geometry of Wasserstein Statistics on Shapes and Affine Deformations, Authors: Shun-ichi Amari, Takeru Matsuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Fisher Information and Natural Gradient Learning of Random Deep Networks, Authors: Shun-ichi Amari, Ryo Karakida, Masafumi Oizumi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Information-Geometric Optimization Algorithms: A Unifying Picture via Invariance Principles, Authors: Yann Ollivier, Ludovic Arnold, Anne Auger, Nikolaus Hansen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:04, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Theano: new features and speed improvements, Authors: Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian Goodfellow, Arnaud Bergeron, Nicolas Bouchard, David Warde-Farley, Yoshua Bengio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "1it [00:01,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Inductive Biases for Deep Learning of Higher-Level Cognition, Authors: Anirudh Goyal, Yoshua Bengio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Theano: new features and speed improvements, Authors: Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian Goodfellow, Arnaud Bergeron, Nicolas Bouchard, David Warde-Farley, Yoshua Bengio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Theano: new features and speed improvements, Authors: Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian Goodfellow, Arnaud Bergeron, Nicolas Bouchard, David Warde-Farley, Yoshua Bengio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: MINRES-QLP: a Krylov subspace method for indefinite or singular symmetric systems, Authors: Sou-Cheng T. Choi, Christopher C. Paige, Michael A. Saunders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:03, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: What Does Nature Minimize In Every Incompressible Flow?, Authors: Haithem E. Taha, Cody Gonzalez\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Bounds on the Bethe Free Energy for Gaussian Networks, Authors: Botond Cseke, Tom Heskes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Fast Approximate Inference of Transcript Expression Levels from RNA-seq Data, Authors: James Hensman, Peter Glaus, Antti Honkela, Magnus Rattray\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Model-Based Reinforcement Learning with a Generative Model is Minimax Optimal, Authors: Alekh Agarwal, Sham Kakade, Lin F. Yang\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Training Neural Networks with Stochastic Hessian-Free Optimization, Authors: Ryan Kiros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Interplanetary Transfers via Deep Representations of the Optimal Policy and/or of the Value Function, Authors: Dario Izzo, Ekin Öztürk, Marcus Märtens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Adding Gradient Noise Improves Learning for Very Deep Networks, Authors: Arvind Neelakantan, Luke Vilnis, Quoc V. Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, James Martens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Security of quantum key distribution with iterative sifting, Authors: Kiyoshi Tamaki, Hoi-Kwong Lo, Akihiro Mizutani, Go Kato, Charles Ci Wen Lim, Koji Azuma, Marcos Curty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: On the Numerical Performance of Derivative-Free Optimization Methods Based on Finite-Difference Approximations, Authors: Hao-Jun Michael Shi, Melody Qiming Xuan, Figen Oztoprak, Jorge Nocedal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: NeuSaver: Neural Adaptive Power Consumption Optimization for Mobile Video Streaming, Authors: Kyoungjun Park, Myungchul Kim, Laihyuk Park\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Compatible Natural Gradient Policy Search, Authors: Joni Pajarinen, Hong Linh Thai, Riad Akrour, Jan Peters, Gerhard Neumann\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A comparative study of divisive hierarchical clustering algorithms, Authors: Maurice Roux\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Natural Evolution Strategies, Authors: Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jürgen Schmidhuber\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:01,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Meshfree Approximation for Stochastic Optimal Control Problems, Authors: Hui Sun, Feng Bao\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Naturalness and the Landscape, Authors: Leonard Susskind\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Krylov Subspace Descent for Deep Learning, Authors: Oriol Vinyals, Daniel Povey\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf file: ./iteration_1/1206.1106v2.No_More_Pesky_Learning_Rates.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2648046/992132040.py:8: DeprecationWarning: The '(Search).results' method is deprecated, use 'Client.results' instead\n",
      "  for result in tqdm(search_results.results()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check the path:  ./bib_files/1206.1106v2.No_More_Pesky_Learning_Rates.bib\n",
      "check anystyle bib ./bib_files/1206.1106v2.No_More_Pesky_Learning_Rates.bib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Optimization Methods for Large-Scale Machine Learning, Authors: Léon Bottou, Frank E. Curtis, Jorge Nocedal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Efficient Learning of Sparse Invariant Representations, Authors: Karol Gregor, Yann LeCun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Computing the Stereo Matching Cost with a Convolutional Neural Network, Authors: Jure Žbontar, Yann LeCun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Mars Reconnaissance Orbiter's Mars Color Imager (MARCI): A New Workflow for Processing Its Image Data, Authors: Stuart J. Robbins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Towards Optimal One Pass Large Scale Learning with Averaged Stochastic Gradient Descent, Authors: Wei Xu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:03, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf file: ./iteration_1/1207.0580v1.Improving_neural_networks_by_preventing_co_adaptation_of_feature_detectors.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2648046/992132040.py:8: DeprecationWarning: The '(Search).results' method is deprecated, use 'Client.results' instead\n",
      "  for result in tqdm(search_results.results()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check the path:  ./bib_files/1207.0580v1.Improving_neural_networks_by_preventing_co_adaptation_of_feature_detectors.bib\n",
      "check anystyle bib ./bib_files/1207.0580v1.Improving_neural_networks_by_preventing_co_adaptation_of_feature_detectors.bib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Beyond No Free Lunch: Realistic Algorithms for Arbitrary Problem Classes, Authors: James A. R. Marshall, Thomas G. Hinton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: No More Pesky Learning Rates, Authors: Tom Schaul, Sixin Zhang, Yann LeCun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Beyond No Free Lunch: Realistic Algorithms for Arbitrary Problem Classes, Authors: James A. R. Marshall, Thomas G. Hinton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Characterizing Serre quotients with no section functor and applications to coherent sheaves, Authors: Mohamed Barakat, Markus Lange-Hegermann\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Position Prediction as an Effective Pretraining Strategy, Authors: Shuangfei Zhai, Navdeep Jaitly, Jason Ramapuram, Dan Busbridge, Tatiana Likhomanenko, Joseph Yitan Cheng, Walter Talbott, Chen Huang, Hanlin Goh, Joshua Susskind\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection, Authors: Sergey Levine, Peter Pastor, Alex Krizhevsky, Deirdre Quillen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Strategizing against No-regret Learners, Authors: Yuan Deng, Jon Schneider, Balusubramanian Sivan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Curvaton reheating allows TeV Hubble scale in NO inflation, Authors: J. C. Bueno Sanchez, K. Dimopoulos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A proof using Böhme's Lemma that no Petersen family graph has a flat embedding, Authors: Joel Foisy, Catherine Jacobs, Trinity Paquin, Morgan Schalizki, Henry Stringer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Never Use Labels: Signal Strength-Based Bayesian Device-Free Localization in Changing Environments, Authors: Peter Hillyard, Neal Patwari\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Fitting heights of solvable groups with no nontrivial prime power character degrees, Authors: Mark L. Lewis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf file: ./iteration_1/1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type thesis not standard. Not considered.\n",
      "/tmp/ipykernel_2648046/992132040.py:8: DeprecationWarning: The '(Search).results' method is deprecated, use 'Client.results' instead\n",
      "  for result in tqdm(search_results.results()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check the path:  ./bib_files/1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks.bib\n",
      "check anystyle bib ./bib_files/1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks.bib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Agnostic Physics-Driven Deep Learning, Authors: Benjamin Scellier, Siddhartha Mishra, Yoshua Bengio, Yann Ollivier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Bayesian Hierarchical Mixtures of Experts, Authors: Christopher M. Bishop, Markus Svensen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Universal Time-Uniform Trajectory Approximation for Random Dynamical Systems with Recurrent Neural Networks, Authors: Adrian N. Bishop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Searching for non-Gaussianity in the VSA data, Authors: Richard Savage, Richard A. Battye, Pedro Carreira, Kieran Cleary, Rod D. Davies, Richard J. Davis, Clive Dickinson, Ricardo Genova-Santos, Keith Grainge, Carlos M. Gutierrez, Yaser A. Hafez, Michael P. Hobson, Michael E. Jones, Rudiger Kneissl, Katy Lancaster, Anthony Lasenby, J. P. Leahy, Klaus Maisinger, Guy G. Pooley, Nutan Rajguru, Rafael Rebolo, Graca Rocha, Jose Alberto Rubino-Martin, Pedro Sosa Molina, Richard D. E. Saunders, Paul Scott, Anze Slosar, Angela C. Taylor, David Titterington, Elizabeth Waldram, Robert A. Watson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music, Authors: Adam Roberts, Jesse Engel, Colin Raffel, Curtis Hawthorne, Douglas Eck\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Learning Contextualized Document Representations for Healthcare Answer Retrieval, Authors: Sebastian Arnold, Betty van Aken, Paul Grundmann, Felix A. Gers, Alexander Löser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A Practical Sparse Approximation for Real Time Recurrent Learning, Authors: Jacob Menick, Erich Elsen, Utku Evci, Simon Osindero, Karen Simonyan, Alex Graves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Sequence Transduction with Recurrent Neural Networks, Authors: Alex Graves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:03, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Speech Recognition with Deep Recurrent Neural Networks, Authors: Alex Graves, Abdel-rahman Mohamed, Geoffrey Hinton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Offline Learning of Counterfactual Predictions for Real-World Robotic Reinforcement Learning, Authors: Jun Jin, Daniel Graves, Cameron Haigh, Jun Luo, Martin Jagersand\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:02,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Distilling a Neural Network Into a Soft Decision Tree, Authors: Nicholas Frosst, Geoffrey Hinton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Few-Shot Learning by Dimensionality Reduction in Gradient Space, Authors: Martin Gauch, Maximilian Beck, Thomas Adler, Dmytro Kotsur, Stefan Fiel, Hamid Eghbal-zadeh, Johannes Brandstetter, Johannes Kofler, Markus Holzleitner, Werner Zellinger, Daniel Klotz, Sepp Hochreiter, Sebastian Lehner\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Effects of Sampling Methods on Prediction Quality. The Case of Classifying Land Cover Using Decision Trees, Authors: Ronald Hochreiter, Christoph Waldhauser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Quantum Optical Experiments Modeled by Long Short-Term Memory, Authors: Thomas Adler, Manuel Erhard, Mario Krenn, Johannes Brandstetter, Johannes Kofler, Sepp Hochreiter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "1it [00:01,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: On the Computability of Solomonoff Induction and Knowledge-Seeking, Authors: Jan Leike, Marcus Hutter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: An Analysis of the VLASS Proposal, Authors: Jim Condon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Martingale marginals do not always determine convergence, Authors: Jim Pitman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Does the dataset meet your expectations? Explaining sample representation in image data, Authors: Dhasarathy Parthasarathy, Anton Johansson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A Machine Learning Perspective on Predictive Coding with PAQ, Authors: Byron Knoll, Nando de Freitas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:07, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Building a Stationary Stochastic Process From a Finite-dimensional Marginal, Authors: Marcus Pivato\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The KATRIN Experiment, Authors: Marcus Beck\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Enriching Word Vectors with Subword Information, Authors: Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A Fast and Simple Algorithm for Training Neural Probabilistic Language Models, Authors: Andriy Mnih, Yee Whye Teh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A Fast and Simple Algorithm for Training Neural Probabilistic Language Models, Authors: Andriy Mnih, Yee Whye Teh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Deterministic Bidirectional Communication and Remote Entanglement Generation Between Superconducting Quantum Processors, Authors: N. Leung, Y. Lu, S. Chakram, R. K. Naik, N. Earnest, R. Ma, K. Jacobs, A. N. Cleland, D. I. Schuster\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Estimating the Hessian by Back-propagating Curvature, Authors: James Martens, Ilya Sutskever, Kevin Swersky\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Learning to Generate Reviews and Discovering Sentiment, Authors: Alec Radford, Rafal Jozefowicz, Ilya Sutskever\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A Deep Factorization of Style and Structure in Fonts, Authors: Nikita Srivatsan, Jonathan T. Barron, Dan Klein, Taylor Berg-Kirkpatrick\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test while loop 1\n",
      "test while loop 5\n",
      "checking the directories prev and current and current iteration: ./iteration_3 ./iteration_2 3\n",
      "pdf files in iterdir ['1111.4259v1.Krylov_Subspace_Descent_for_Deep_Learning.pdf', '1108.3298v1.A_Machine_Learning_Perspective_on_Predictive_Coding_with_PAQ.pdf', '1303.5778v1.Speech_Recognition_with_Deep_Recurrent_Neural_Networks.pdf', '1211.3711v1.Sequence_Transduction_with_Recurrent_Neural_Networks.pdf', '1301.3641v3.Training_Neural_Networks_with_Stochastic_Hessian_Free_Optimization.pdf', '1206.6426v1.A_Fast_and_Simple_Algorithm_for_Training_Neural_Probabilistic_Language_Models.pdf', '1910.13804v1.Quantum_Optical_Experiments_Modeled_by_Long_Short_Term_Memory.pdf', '1211.5590v1.Theano__new_features_and_speed_improvements.pdf', '1003.4042v3.MINRES_QLP__a_Krylov_subspace_method_for_indefinite_or_singular_symmetric_systems.pdf', '1106.3708v4.Information_Geometric_Optimization_Algorithms__A_Unifying_Picture_via_Invariance_Principles.pdf', '1107.2490v2.Towards_Optimal_One_Pass_Large_Scale_Learning_with_Averaged_Stochastic_Gradient_Descent.pdf']\n",
      "pdf file: ./iteration_2/1111.4259v1.Krylov_Subspace_Descent_for_Deep_Learning.pdf\n",
      "check the path:  ./bib_files/1111.4259v1.Krylov_Subspace_Descent_for_Deep_Learning.bib\n",
      "check anystyle bib ./bib_files/1111.4259v1.Krylov_Subspace_Descent_for_Deep_Learning.bib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2648046/992132040.py:8: DeprecationWarning: The '(Search).results' method is deprecated, use 'Client.results' instead\n",
      "  for result in tqdm(search_results.results()):\n",
      "1it [00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Fisher Information and Natural Gradient Learning of Random Deep Networks, Authors: Shun-ichi Amari, Ryo Karakida, Masafumi Oizumi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Understanding deep learning requires rethinking generalization, Authors: Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Differential Geometry on SU(3) with Applications to Three State Systems, Authors: Mark Byrd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A Proof of the Smoothness of the Finite Time Horizon American Put Option for Jump Diffusions, Authors: Erhan Bayraktar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Reducing ground-based astrometric errors with Gaia and Gaussian processes, Authors: W. F. Fortino, G. M. Bernstein, P. H. Bernardinelli, M. Aguena, S. Allam, J. Annis, D. Bacon, K. Bechtol, S. Bhargava, D. Brooks, D. L. Burke, J. Carretero, A. Choi, M. Costanzi, L. N. da Costa, M. E. S. Pereira, J. De Vicente, S. Desai, P. Doel, A. Drlica-Wagner, K. Eckert, T. F. Eifler, A. E. Evrard, I. Ferrero, J. Frieman, J. García-Bellido, E. Gaztanaga, D. W. Gerdes, R. A. Gruendl, J. Gschwend, G. Gutierrez, W. G. Hartley, S. R. Hinton, D. L. Hollowood, K. Honscheid, D. J. James, M. Jarvis, S. Kent, K. Kuehn, N. Kuropatkin, M. A. G. Maia, J. L. Marshall, F. Menanteau, R. Miquel, R. Morgan, J. Myles, R. L. C. Ogando, A. Palmese, F. Paz-Chinchón, A. A. Plazas, A. Roodman, E. S. Rykoff, E. Sanchez, B. Santiago, V. Scarpine, M. Schubnell, S. Serrano, I. Sevilla-Noarbe, M. Smith, E. Suchyta, G. Tarle, C. To, D. L. Tucker, T. N. Varga, A. R. Walker, J. Weller, W. Wester\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: On the existence and instability of solitary water waves with a finite dipole, Authors: Hung Le\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Interplanetary Transfers via Deep Representations of the Optimal Policy and/or of the Value Function, Authors: Dario Izzo, Ekin Öztürk, Marcus Märtens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Adding Gradient Noise Improves Learning for Very Deep Networks, Authors: Arvind Neelakantan, Luke Vilnis, Quoc V. Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, James Martens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: On the Numerical Performance of Derivative-Free Optimization Methods Based on Finite-Difference Approximations, Authors: Hao-Jun Michael Shi, Melody Qiming Xuan, Figen Oztoprak, Jorge Nocedal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf file: ./iteration_2/1108.3298v1.A_Machine_Learning_Perspective_on_Predictive_Coding_with_PAQ.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type thesis not standard. Not considered.\n",
      "Entry type thesis not standard. Not considered.\n",
      "/tmp/ipykernel_2648046/992132040.py:8: DeprecationWarning: The '(Search).results' method is deprecated, use 'Client.results' instead\n",
      "  for result in tqdm(search_results.results()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check the path:  ./bib_files/1108.3298v1.A_Machine_Learning_Perspective_on_Predictive_Coding_with_PAQ.bib\n",
      "check anystyle bib ./bib_files/1108.3298v1.A_Machine_Learning_Perspective_on_Predictive_Coding_with_PAQ.bib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Selective social interactions and speed-induced leadership in schooling fish, Authors: Andreu Puy, Palina Bartashevich, Elisabet Gimeno, Jordi Torrents, M. Carmen Miguel, Romualdo Pastor-Satorras, Pawel Romanczuk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A Sequential Algorithm for Training Text Classifiers, Authors: David D. Lewis, William A. Gale\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Clustering by compression, Authors: Rudi Cilibrasi, Paul Vitanyi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "1it [00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Searching for non-Gaussianity in the VSA data, Authors: Richard Savage, Richard A. Battye, Pedro Carreira, Kieran Cleary, Rod D. Davies, Richard J. Davis, Clive Dickinson, Ricardo Genova-Santos, Keith Grainge, Carlos M. Gutierrez, Yaser A. Hafez, Michael P. Hobson, Michael E. Jones, Rudiger Kneissl, Katy Lancaster, Anthony Lasenby, J. P. Leahy, Klaus Maisinger, Guy G. Pooley, Nutan Rajguru, Rafael Rebolo, Graca Rocha, Jose Alberto Rubino-Martin, Pedro Sosa Molina, Richard D. E. Saunders, Paul Scott, Anze Slosar, Angela C. Taylor, David Titterington, Elizabeth Waldram, Robert A. Watson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A finitely presented group with unbounded dead-end depth, Authors: Sean Cleary, Tim R. Riley\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Observing relativistic features in large-scale structure surveys -- II: Doppler magnification in an ensemble of relativistic simulations, Authors: Louis Coates, Julian Adamek, Philip Bull, Caroline Guandalin, Chris Clarkson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:01,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A Comparison Map for Symmetric Homology and Gamma Homology, Authors: Daniel Graves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A Cohomological Perspective on Algebraic Quantum Field Theory, Authors: Eli Hawkins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Critical exponents for a spin-charge flip symmetric fixed point in 2+1d with massless Dirac fermions, Authors: Emilie Huffman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: On the Computability of Solomonoff Induction and Knowledge-Seeking, Authors: Jan Leike, Marcus Hutter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Adapting to Non-stationarity with Growing Expert Ensembles, Authors: Cosma Rohilla Shalizi, Abigail Z. Jacobs, Kristina Lisa Klinkner, Aaron Clauset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: H.E.S.S. constraints on Dark Matter annihilations towards the Sculptor and Carina Dwarf Galaxies, Authors: HESS Collaboration, A. Abramowski, F. Acero, F. Aharonian, A. G. Akhperjanian, G. Anton, A. Barnacka, U. Barres de Almeida, A. R. Bazer-Bachi, Y. Becherini, J. Becker, B. Behera, K. Bernlöhr, A. Bochow, C. Boisson, J. Bolmont, P. Bordas, V. Borrel, J. Brucker, F. Brun, P. Brun, T. Bulik, I. Büsching, S. Carrigan, S. Casanova, M. Cerruti, P. M. Chadwick, A. Charbonnier, R. C. G. Chaves, A. Cheesebrough, L. -M. Chounet, A. C. Clapson, G. Coignet, J. Conrad, M. Dalton, M. K. Daniel, I. D. Davids, B. Degrange, C. Deil, H. J. Dickinson, A. Djannati-Ataï, W. Domainko, L. O'C. Drury, F. Dubois, G. Dubus, J. Dyks, M. Dyrda, K. Egberts, P. Eger, P. Espigat, L. Fallon, C. Farnier, S. Fegan, F. Feinstein, M. V. Fernandes, A. Fiasson, A. Frster, G. Fontaine, M. Füßling, Y. A. Gallant, H. Gast, L. Gérard, D. Gerbig, B. Giebels, J. F. Glicenstein, B. Glück, P. Goret, D. Göring, J. D. Hague, D. Hampf, M. Hauser, S. Heinz, G. Heinzelmann, G. Henri, G. Hermann, J. A. Hinton, A. Hoffmann, W. Hofmann, P. Hofverberg, D. Horns, A. Jacholkowska, O. C. de Jager, C. Jahn, M. Jamrozy, I. Jung, M. A. Kastendieck, K. Katarzyński, U. Katz, S. Kaufmann, D. Keogh, M. Kerschhaggl, D. Khangulyan, B. Khélifi, D. Klochkov, W. Kluźniak, T. Kneiske, Nu. Komin, K. Kosack, R. Kossakowski, H. Laffon, G. Lamanna, D. Lennarz, T. Lohse, A. Lopatin, C. -C. Lu, V. Marandon, A. Marcowith, J. Masbou, D. Maurin, N. Maxted, T. J. L. McComb, M. C. Medina, J. Méhault, R. Moderski, E. Moulin, C. L. Naumann, M. Naumann-Godo, M. de Naurois, D. Nedbal, D. Nekrassov, N. Nguyen, B. Nicholas, J. Niemiec, S. J. Nolan, S. Ohm, J-F. Olive, E. de Oña Wilhelmi, B. Opitz, M. Ostrowski, M. Panter, M. Paz Arribas, G. Pedaletti1, G. Pelletier, P. -O. Petrucci, S. Pita, G. Pühlhofer, M. Punch, A. Quirrenbach, M. Raue, S. M. Rayner, A. Reimer, O. Reimer, M. Renaud, R. de los Reyes, F. Rieger, J. Ripken, L. Rob, S. Rosier-Lees, G. Rowell, B. Rudak, C. B. Rulten, J. Ruppel, F. Ryde, V. Sahakian, A. Santangelo, R. Schlickeiser, F. M. Schöck, A. Schönwald, U. Schwanke, S. Schwarzburg, S. Schwemmer, A. Shalchi, M. Sikora, J. L. Skilton, H. Sol, G. Spengler, Ł. Stawarz, R. Steenkamp, C. Stegmann, F. Stinzing, I. Sushch, A. Szostek, J. -P. Tavernet, R. Terrier, O. Tibolla, M. Tluczykont, K. Valerius, C. van Eldik, G. Vasileiadis, C. Venter, J. P. Vialle, A. Viana, P. Vincent, M. Vivier, H. J. Völk, F. Volpe, S. Vorobiov, M. Vorster, S. J. Wagner, M. Ward, A. Wierzcholska, A. Zajczyk, A. A. Zdziarski, A. Zech, H. -S. Zechlin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:01,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ShapeWordNet: An Interpretable Shapelet Neural Network for Physiological Signal Classification, Authors: Wenqiang He, Mingyue Cheng, Qi Liu, Zhi Li\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Fat-Tailed Variational Inference with Anisotropic Tail Adaptive Flows, Authors: Feynman Liang, Liam Hodgkinson, Michael W. Mahoney\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Randomized algorithms for matrices and data, Authors: Michael W. Mahoney\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: An Ambarzumian type theorem on graphs with odd cycles, Authors: Márton Kiss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Popular Ensemble Methods: An Empirical Study, Authors: R. Maclin, D. Opitz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "1it [00:01,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ExPUNations: Augmenting Puns with Keywords and Explanations, Authors: Jiao Sun, Anjali Narayan-Chen, Shereen Oraby, Alessandra Cervone, Tagyoung Chung, Jing Huang, Yang Liu, Nanyun Peng\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Interplay of creation, propagation, and relaxation of an excitation in a dimer, Authors: J. Perina, Jr.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: On Pauli Pairs, Authors: Stanislav Shkarin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: How to Train Your Deep Neural Network with Dictionary Learning, Authors: Vanika Singhal, Shikha Singh, Angshul Majumdar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Learning to Generate Reviews and Discovering Sentiment, Authors: Alec Radford, Rafal Jozefowicz, Ilya Sutskever\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A new tableau model for irreducible polynomial representations of the orthogonal group, Authors: Hideya Watanabe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Parameter-Efficient Long-Tailed Recognition, Authors: Jiang-Xin Shi, Tong Wei, Zhi Zhou, Xin-Yan Han, Jie-Jing Shao, Yu-Feng Li\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Distance Metric Learning for Kernel Machines, Authors: Zhixiang Xu, Kilian Q. Weinberger, Olivier Chapelle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: NEURO HAND: A weakly supervised Hierarchical Attention Network for neuroimaging abnormality Detection, Authors: David A. Wood\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: How to predict and avert economic crisis, Authors: Yong Tao\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Active Discriminative Text Representation Learning, Authors: Ye Zhang, Matthew Lease, Byron C. Wallace\n",
      "pdf file: ./iteration_2/1303.5778v1.Speech_Recognition_with_Deep_Recurrent_Neural_Networks.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check the path:  ./bib_files/1303.5778v1.Speech_Recognition_with_Deep_Recurrent_Neural_Networks.bib\n",
      "check anystyle bib ./bib_files/1303.5778v1.Speech_Recognition_with_Deep_Recurrent_Neural_Networks.bib\n",
      "Unexpected format of reference: B\n",
      "Unexpected format of reference: i\n",
      "Unexpected format of reference: b\n",
      "Unexpected format of reference: T\n",
      "Unexpected format of reference: e\n",
      "Unexpected format of reference: X\n",
      "Unexpected format of reference:  \n",
      "Unexpected format of reference: f\n",
      "Unexpected format of reference: i\n",
      "Unexpected format of reference: l\n",
      "Unexpected format of reference: e\n",
      "Unexpected format of reference:  \n",
      "Unexpected format of reference: n\n",
      "Unexpected format of reference: o\n",
      "Unexpected format of reference: t\n",
      "Unexpected format of reference:  \n",
      "Unexpected format of reference: f\n",
      "Unexpected format of reference: o\n",
      "Unexpected format of reference: u\n",
      "Unexpected format of reference: n\n",
      "Unexpected format of reference: d\n",
      "Unexpected format of reference: .\n",
      "pdf file: ./iteration_2/1211.3711v1.Sequence_Transduction_with_Recurrent_Neural_Networks.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type thesis not standard. Not considered.\n",
      "/tmp/ipykernel_2648046/992132040.py:8: DeprecationWarning: The '(Search).results' method is deprecated, use 'Client.results' instead\n",
      "  for result in tqdm(search_results.results()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check the path:  ./bib_files/1211.3711v1.Sequence_Transduction_with_Recurrent_Neural_Networks.bib\n",
      "check anystyle bib ./bib_files/1211.3711v1.Sequence_Transduction_with_Recurrent_Neural_Networks.bib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Few-Shot Learning by Dimensionality Reduction in Gradient Space, Authors: Martin Gauch, Maximilian Beck, Thomas Adler, Dmytro Kotsur, Stefan Fiel, Hamid Eghbal-zadeh, Johannes Brandstetter, Johannes Kofler, Markus Holzleitner, Werner Zellinger, Daniel Klotz, Sepp Hochreiter, Sebastian Lehner\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Role of Emotions in Propagating Brands in Social Networks, Authors: Ronald Hochreiter, Christoph Waldhauser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:01,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A Bayesian Model of node interaction in networks, Authors: Ingmar Schuster\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf file: ./iteration_2/1301.3641v3.Training_Neural_Networks_with_Stochastic_Hessian_Free_Optimization.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type thesis not standard. Not considered.\n",
      "/tmp/ipykernel_2648046/992132040.py:8: DeprecationWarning: The '(Search).results' method is deprecated, use 'Client.results' instead\n",
      "  for result in tqdm(search_results.results()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check the path:  ./bib_files/1301.3641v3.Training_Neural_Networks_with_Stochastic_Hessian_Free_Optimization.bib\n",
      "check anystyle bib ./bib_files/1301.3641v3.Training_Neural_Networks_with_Stochastic_Hessian_Free_Optimization.bib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: On the Ineffectiveness of Variance Reduced Optimization for Deep Learning, Authors: Aaron Defazio, Léon Bottou\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Improving neural networks by preventing co-adaptation of feature detectors, Authors: Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, Ruslan R. Salakhutdinov\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:03, ?it/s]\n",
      "1it [00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Interplanetary Transfers via Deep Representations of the Optimal Policy and/or of the Value Function, Authors: Dario Izzo, Ekin Öztürk, Marcus Märtens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Adding Gradient Noise Improves Learning for Very Deep Networks, Authors: Arvind Neelakantan, Luke Vilnis, Quoc V. Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, James Martens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Efficient Learning of Sparse Invariant Representations, Authors: Karol Gregor, Yann LeCun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Stochastic (Approximate) Proximal Point Methods: Convergence, Optimality, and Adaptivity, Authors: Hilal Asi, John C. Duchi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Functional Large Deviations for Cox Processes and $Cox/G/\\infty$ Queues, with a Biological Application, Authors: Justin Dean, Ayalvadi Ganesh, Edward Crane\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: No More Pesky Learning Rates, Authors: Tom Schaul, Sixin Zhang, Yann LeCun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Revisiting Natural Gradient for Deep Networks, Authors: Razvan Pascanu, Yoshua Bengio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Big Neural Networks Waste Capacity, Authors: Yann N. Dauphin, Yoshua Bengio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "1it [00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Adaptive Sampling Strategies for Stochastic Optimization, Authors: Raghu Bollapragada, Richard Byrd, Jorge Nocedal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Learning to Generate Reviews and Discovering Sentiment, Authors: Alec Radford, Rafal Jozefowicz, Ilya Sutskever\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Krylov Subspace Descent for Deep Learning, Authors: Oriol Vinyals, Daniel Povey\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "1it [00:01,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: On the existence and instability of solitary water waves with a finite dipole, Authors: Hung Le\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Practical recommendations for gradient-based training of deep architectures, Authors: Yoshua Bengio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "1it [00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Rapid training of deep neural networks without skip connections or normalization layers using Deep Kernel Shaping, Authors: James Martens, Andy Ballard, Guillaume Desjardins, Grzegorz Swirszcz, Valentin Dalibard, Jascha Sohl-Dickstein, Samuel S. Schoenholz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Conceptualization of seeded region growing by pixels aggregation. Part 4: Simple, generic and robust extraction of grains in granular materials obtained by X-ray tomography, Authors: Vincent Tariel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Large-Margin kNN Classification Using a Deep Encoder Network, Authors: Martin Renqiang Min, David A. Stanley, Zineng Yuan, Anthony Bonner, Zhaolei Zhang\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A Probabilistic Analysis of Kademlia Networks, Authors: Xing Shi Cai, Luc Devroye\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Deep Self-Taught Learning for Handwritten Character Recognition, Authors: Frédéric Bastien, Yoshua Bengio, Arnaud Bergeron, Nicolas Boulanger-Lewandowski, Thomas Breuel, Youssouf Chherawala, Moustapha Cisse, Myriam Côté, Dumitru Erhan, Jeremy Eustache, Xavier Glorot, Xavier Muller, Sylvain Pannetier Lebeuf, Razvan Pascanu, Salah Rifai, Francois Savard, Guillaume Sicard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Large-Margin kNN Classification Using a Deep Encoder Network, Authors: Martin Renqiang Min, David A. Stanley, Zineng Yuan, Anthony Bonner, Zhaolei Zhang\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Advances in Optimizing Recurrent Networks, Authors: Yoshua Bengio, Nicolas Boulanger-Lewandowski, Razvan Pascanu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "1it [00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Bridging Textual and Tabular Data for Cross-Domain Text-to-SQL Semantic Parsing, Authors: Xi Victoria Lin, Richard Socher, Caiming Xiong\n",
      "pdf file: ./iteration_2/1206.6426v1.A_Fast_and_Simple_Algorithm_for_Training_Neural_Probabilistic_Language_Models.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_2648046/992132040.py:8: DeprecationWarning: The '(Search).results' method is deprecated, use 'Client.results' instead\n",
      "  for result in tqdm(search_results.results()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check the path:  ./bib_files/1206.6426v1.A_Fast_and_Simple_Algorithm_for_Training_Neural_Probabilistic_Language_Models.bib\n",
      "check anystyle bib ./bib_files/1206.6426v1.A_Fast_and_Simple_Algorithm_for_Training_Neural_Probabilistic_Language_Models.bib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Sample Efficient Actor-Critic with Experience Replay, Authors: Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, Nando de Freitas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:01,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Filtering and Mining Parallel Data in a Joint Multilingual Space, Authors: Holger Schwenk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Very Deep Convolutional Networks for Text Classification, Authors: Alexis Conneau, Holger Schwenk, Loïc Barrault, Yann Lecun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A Family of Computationally Efficient and Simple Estimators for Unnormalized Statistical Models, Authors: Miika Pihlaja, Michael Gutmann, Aapo Hyvarinen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Memories of Murray and the Quark Model, Authors: George Zweig\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf file: ./iteration_2/1910.13804v1.Quantum_Optical_Experiments_Modeled_by_Long_Short_Term_Memory.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type thesis not standard. Not considered.\n",
      "Entry type thesis not standard. Not considered.\n",
      "/tmp/ipykernel_2648046/992132040.py:8: DeprecationWarning: The '(Search).results' method is deprecated, use 'Client.results' instead\n",
      "  for result in tqdm(search_results.results()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check the path:  ./bib_files/1910.13804v1.Quantum_Optical_Experiments_Modeled_by_Long_Short_Term_Memory.bib\n",
      "check anystyle bib ./bib_files/1910.13804v1.Quantum_Optical_Experiments_Modeled_by_Long_Short_Term_Memory.bib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks, Authors: Samy Bengio, Oriol Vinyals, Navdeep Jaitly, Noam Shazeer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Twisted Photons: New Quantum Perspectives in High Dimensions, Authors: Manuel Erhard, Robert Fickler, Mario Krenn, Anton Zeilinger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:09, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Experimental GHZ Entanglement beyond Qubits, Authors: Manuel Erhard, Mehul Malik, Mario Krenn, Anton Zeilinger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:04, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: MaskGAN: Better Text Generation via Filling in the______, Authors: William Fedus, Ian Goodfellow, Andrew M. Dai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "1it [00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: NIPS 2016 Tutorial: Generative Adversarial Networks, Authors: Ian Goodfellow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Generating Sequences With Recurrent Neural Networks, Authors: Alex Graves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:04, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Improved Training of Wasserstein GANs, Authors: Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron Courville\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:06, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Quantum Optical Experiments Modeled by Long Short-Term Memory, Authors: Thomas Adler, Manuel Erhard, Mario Krenn, Johannes Brandstetter, Johannes Kofler, Sepp Hochreiter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The structure of multidimensional entanglement in multipartite systems, Authors: Marcus Huber, Julio I. de Vicente\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The entropy vector formalism and the structure of multidimensional entanglement in multipartite systems, Authors: Marcus Huber, Martí Perarnau-Llobet, Julio I. de Vicente\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:03, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Deep Visual-Semantic Alignments for Generating Image Descriptions, Authors: Andrej Karpathy, Li Fei-Fei\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:06, ?it/s]\n",
      "1it [00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Violations of local realism by two entangled quNits are stronger than for two qubits, Authors: D. Kaszlikowski, P. Gnacinski, M. Zukowski, W. Miklaszewski, A. Zeilinger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Automated Search for new Quantum Experiments, Authors: Mario Krenn, Mehul Malik, Robert Fickler, Radek Lapkiewicz, Anton Zeilinger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Toxicity Prediction using Deep Learning, Authors: Thomas Unterthiner, Andreas Mayr, Günter Klambauer, Sepp Hochreiter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:03, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Active learning machine learns to create new quantum experiments, Authors: Alexey A. Melnikov, Hendrik Poulsen Nautrup, Mario Krenn, Vedran Dunjko, Markus Tiersch, Anton Zeilinger, Hans J. Briegel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:05, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Efficient Estimation of Word Representations in Vector Space, Authors: Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm, Authors: David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, Demis Hassabis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Sequence to Sequence Learning with Neural Networks, Authors: Ilya Sutskever, Oriol Vinyals, Quoc V. Le\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:01, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Fourier relationship between angular position and optical orbital angular momentum, Authors: Eric Yao, Sonja Franke-Arnold, Johannes Courtial, Stephen Barnett, Miles Padgett\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:03, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient, Authors: Lantao Yu, Weinan Zhang, Jun Wang, Yong Yu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:03, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf file: ./iteration_2/1211.5590v1.Theano__new_features_and_speed_improvements.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2648046/992132040.py:8: DeprecationWarning: The '(Search).results' method is deprecated, use 'Client.results' instead\n",
      "  for result in tqdm(search_results.results()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check the path:  ./bib_files/1211.5590v1.Theano__new_features_and_speed_improvements.bib\n",
      "check anystyle bib ./bib_files/1211.5590v1.Theano__new_features_and_speed_improvements.bib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Theano: new features and speed improvements, Authors: Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian Goodfellow, Arnaud Bergeron, Nicolas Bouchard, David Warde-Farley, Yoshua Bengio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Theano: new features and speed improvements, Authors: Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian Goodfellow, Arnaud Bergeron, Nicolas Bouchard, David Warde-Farley, Yoshua Bengio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Word-level Speech Recognition with a Letter to Word Encoder, Authors: Ronan Collobert, Awni Hannun, Gabriel Synnaeve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:01,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A notion of twins, Authors: Zach Hunter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: SciPy 1.0--Fundamental Algorithms for Scientific Computing in Python, Authors: Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, CJ Carey, İlhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R Harris, Anne M. Archibald, Antônio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, SciPy 1. 0 Contributors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:03, ?it/s]\n",
      "1it [00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Adding Gradient Noise Improves Learning for Very Deep Networks, Authors: Arvind Neelakantan, Luke Vilnis, Quoc V. Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, James Martens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: SciPy 1.0--Fundamental Algorithms for Scientific Computing in Python, Authors: Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, CJ Carey, İlhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R Harris, Anne M. Archibald, Antônio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, SciPy 1. 0 Contributors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:01,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Convergence of a Lagrangian-Eulerian scheme by a weak asymptotic analysis for one-dimensional hyperbolic problems, Authors: Eduardo Abreu, Arthur Espírito Santo, Wanderson Lambert, John Pérez\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf file: ./iteration_2/1003.4042v3.MINRES_QLP__a_Krylov_subspace_method_for_indefinite_or_singular_symmetric_systems.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type thesis not standard. Not considered.\n",
      "Entry type thesis not standard. Not considered.\n",
      "Entry type thesis not standard. Not considered.\n",
      "Entry type thesis not standard. Not considered.\n",
      "Entry type thesis not standard. Not considered.\n",
      "/tmp/ipykernel_2648046/992132040.py:8: DeprecationWarning: The '(Search).results' method is deprecated, use 'Client.results' instead\n",
      "  for result in tqdm(search_results.results()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check the path:  ./bib_files/1003.4042v3.MINRES_QLP__a_Krylov_subspace_method_for_indefinite_or_singular_symmetric_systems.bib\n",
      "check anystyle bib ./bib_files/1003.4042v3.MINRES_QLP__a_Krylov_subspace_method_for_indefinite_or_singular_symmetric_systems.bib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Identifying Influential Users in Unknown Social Networks for Adaptive Incentive Allocation Under Budget Restriction, Authors: Shiqing Wu, Weihua Li, Hao Shen, Quan Bai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: An augmented Lagrangian-based preconditioning technique for a class of block three-by-three linear systems, Authors: Fatemeh P. A. Beik, Michele Benzi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Homogeneous and Isotropic Turbulence: a short survey on recent developments, Authors: R. Benzi, L. Biferale\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Typical and Generic Ranks in Matrix Completion, Authors: Daniel Irving Bernstein, Grigoriy Blekherman, Rainer Sinn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Stacks in Representation Theory. What is a continuous representation of an algebraic group ?, Authors: Joseph Bernstein\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Adaptive Stopping Rule for Kernel-based Gradient Descent Algorithms, Authors: Xiangyu Chang, Shao-Bo Lin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Matrix balancing based interior point methods for point set matching problems, Authors: Janith Wijesinghe, Pengwen Chen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: SocialIQA: Commonsense Reasoning about Social Interactions, Authors: Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, Yejin Choi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Multiway Spectral Graph Partitioning: Cut Functions, Cheeger Inequalities, and a Simple Algorithm, Authors: Lars Eldén\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Non-hereditary Minimum Deep Coalescence trees, Authors: Mareike Fischer, Martin Kreidl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A mathematical commitment without computational strength, Authors: Anton Freund\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Algebraic Temporal Blocking for Sparse Iterative Solvers on Multi-Core CPUs, Authors: Christie Alappat, Jonas Thies, Georg Hager, Holger Fehske, Gerhard Wellein\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:01,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Non-negatively constrained least squares and parameter choice by the residual periodogram for the inversion of electrochemical impedance spectroscopy, Authors: Jakob Hansen, Jarom Hogue, Grant Sander, Rosemary Renaut, Sudeep Popat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Boundaries of Verifiable Accuracy, Robustness, and Generalisation in Deep Learning, Authors: Alexander Bastounis, Alexander N. Gorban, Anders C. Hansen, Desmond J. Higham, Danil Prokhorov, Oliver Sutton, Ivan Y. Tyukin, Qinghua Zhou\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: An Inner-Outer Iterative Method for Edge Preservation in Image Restoration and Reconstruction, Authors: Silvia Gazzola, Misha E. Kilmer, James G. Nagy, Oguz Semerici, Eric L. Miller\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:01,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ROSE: A Neurocomputational Architecture for Syntax, Authors: Elliot Murphy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Fine spectral analysis of preconditioned matrices and matrix-sequences arising from stage-parallel implicit Runge-Kutta methods of arbitrarily high order, Authors: Ivo Dravins, Stefano Serra-Capizzano, Maya Neytcheva\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: On the Numerical Performance of Derivative-Free Optimization Methods Based on Finite-Difference Approximations, Authors: Hao-Jun Michael Shi, Melody Qiming Xuan, Figen Oztoprak, Jorge Nocedal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Highly robust error correction by convex programming, Authors: Emmanuel J. Candes, Paige A. Randall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Saying Hello World with Epsilon - A Solution to the 2011 Instructive Case, Authors: Louis M. Rose, Antonio García-Domínguez, James R. Williams, Dimitrios S. Kolovos, Richard F. Paige, Fiona A. C. Polack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: An Introduction to Probabilistic Programming, Authors: Jan-Willem van de Meent, Brooks Paige, Hongseok Yang, Frank Wood\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Faster variational quantum algorithms with quantum kernel-based surrogate models, Authors: Alistair W. R. Smith, A. J. Paige, M. S. Kim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Integrated Framework of Vehicle Dynamics, Instabilities, Energy Models, and Sparse Flow Smoothing Controllers, Authors: Jonathan W. Lee, George Gunter, Rabie Ramadan, Sulaiman Almatrudi, Paige Arnold, John Aquino, William Barbour, Rahul Bhadani, Joy Carpio, Fang-Chieh Chou, Marsalis Gibson, Xiaoqian Gong, Amaury Hayat, Nour Khoudari, Abdul Rahman Kreidieh, Maya Kumar, Nathan Lichtlé, Sean McQuade, Brian Nguyen, Megan Ross, Sydney Truong, Eugene Vinitsky, Yibo Zhao, Jonathan Sprinkle, Benedetto Piccoli, Alexandre M. Bayen, Daniel B. Work, Benjamin Seibold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Multi-Agent Routing Value Iteration Network, Authors: Quinlan Sykora, Mengye Ren, Raquel Urtasun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:02,  2.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Iterative methods for linear systems of equations: A brief historical journey, Authors: Yousef Saad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: An Integer Linear Programming Solution to the Telescope Network Scheduling Problem, Authors: Sotiria Lampoudi, Eric Saunders, Jason Eastman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Probabilistic analysis of Wiedemann's algorithm for minimal polynomial computation, Authors: Gavin Harrison, Jeremy Johnson, B. David Saunders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:01,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: On the wellposedness for periodic nonlinear Schrödinger equations with white noise dispersion, Authors: Gavin Stewart\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Structure in scientific networks: towards predictions of research dynamism, Authors: Benjamin W. Stewart, Andy Rivas, Luat T. Vuong\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Long time decay and asymptotics for the complex mKdV equation, Authors: Gavin Stewart\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A block symmetric Gauss-Seidel decomposition theorem for convex composite quadratic programming and its applications, Authors: Xudong Li, Defeng Sun, Kim-Chuan Toh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Numerical conformal mapping with rational functions, Authors: Lloyd N. Trefethen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Efficient Algorithm for Solving Hyperbolic Programs, Authors: Yichuan Deng, Zhao Song, Lichen Zhang, Ruizhe Zhang\n",
      "pdf file: ./iteration_2/1106.3708v4.Information_Geometric_Optimization_Algorithms__A_Unifying_Picture_via_Invariance_Principles.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_2648046/992132040.py:8: DeprecationWarning: The '(Search).results' method is deprecated, use 'Client.results' instead\n",
      "  for result in tqdm(search_results.results()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check the path:  ./bib_files/1106.3708v4.Information_Geometric_Optimization_Algorithms__A_Unifying_Picture_via_Invariance_Principles.bib\n",
      "check anystyle bib ./bib_files/1106.3708v4.Information_Geometric_Optimization_Algorithms__A_Unifying_Picture_via_Invariance_Principles.bib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:06,  6.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A continuously tunable modulation scheme for precision control of optical cavities with variable detuning, Authors: William Yam, Emily Davis, Sarah Ackley, Matthew Evans, Nergis Mavalvala\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Nonadiabatic charge pumping in a one-dimensional system of noninteracting electrons by an oscillating potential, Authors: Amit Agarwal, Diptiman Sen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Objective Improvement in Information-Geometric Optimization, Authors: Youhei Akimoto, Yann Ollivier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:03, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Global Linear Convergence of Evolution Strategies on More Than Smooth Strongly Convex Functions, Authors: Youhei Akimoto, Anne Auger, Tobias Glasmachers, Daiki Morinaga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Fisher Information and Natural Gradient Learning of Random Deep Networks, Authors: Shun-ichi Amari, Ryo Karakida, Masafumi Oizumi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Online Duet between Metric Embeddings and Minimum-Weight Perfect Matchings, Authors: Sujoy Bhore, Arnold Filtser, Csaba D. Tóth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Correlation-Polarization Effects in Electron/Positron Scattering from Acetylene: A Comparison of Computational Models, Authors: J. Franz, F. A. Gianturco, K. L. Baluja, J. Tennyson, R. Carey, R. Montuoro, R. R. Lucchese, T. Stoecklin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Generalizable Features From Unsupervised Learning, Authors: Mehdi Mirza, Aaron Courville, Yoshua Bengio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Representation Learning: A Review and New Perspectives, Authors: Yoshua Bengio, Aaron Courville, Pascal Vincent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:04, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: An information-theoretic evolutionary algorithm, Authors: Arnaud Berny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:01,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Model Analysis of Time Reversal Symmetry Test in the Caltech Fe-57 Gamma-Transition Experiment, Authors: Michael Beyer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A spectral solver for evolution problems with spatial S3-topology, Authors: Florian Beyer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:01,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: On a possible node in the Sivers and Qiu-Sterman functions, Authors: Daniel Boer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Adaptive Parallel Tempering for Stochastic Maximum Likelihood Learning of RBMs, Authors: Guillaume Desjardins, Aaron Courville, Yoshua Bengio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Benchmarking Optimization Software with Performance Profiles, Authors: Elizabeth D. Dolan, Jorge J. Moré\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Some counterexamples on the behaviour of real-valued functions and their derivatives, Authors: Juergen Grahl, Shahar Nevo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: An extended analysis of the viscosity kernel for monatomic and diatomic fluids, Authors: R. M. Puscasu, B. D. Todd, P. J. Daivis, J. S. Hansen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The CMA Evolution Strategy: A Tutorial, Authors: Nikolaus Hansen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:03, ?it/s]\n",
      "1it [00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Benchmarking Fast-to-Alfven Mode Conversion in a Cold MHD Plasma, Authors: Paul S. Cally, Shelley C. Hansen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Evaluating Polynomials Over the Unit Disk and the Unit Ball, Authors: Kendall Atkinson, Olaf Hansen, David Chien\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Diagonal Acceleration for Covariance Matrix Adaptation Evolution Strategies, Authors: Youhei Akimoto, Nikolaus Hansen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The CMA Evolution Strategy: A Tutorial, Authors: Nikolaus Hansen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Perspectives and completely positive maps, Authors: Frank Hansen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Time-Series Classification in Smart Manufacturing Systems: An Experimental Evaluation of State-of-the-Art Machine Learning Algorithms, Authors: Mojtaba A. Farahani, M. R. McCormick, Ramy Harik, Thorsten Wuest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Large scale distributed neural network training through online distillation, Authors: Rohan Anil, Gabriel Pereyra, Alexandre Passos, Robert Ormandi, George E. Dahl, Geoffrey E. Hinton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:02,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: How to represent part-whole hierarchies in a neural network, Authors: Geoffrey Hinton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Semi-global Output Feedback Stabilization of Non-Minimum Phase Nonlinear Systems, Authors: Almuatazbellah M. Boker, Hassan K. Khalil\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Convergence of numerical methods for stochastic differential equations in mathematical finance, Authors: Peter Kloeden, Andreas Neuenkirch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Heterogeneous Doppler Spread-based CSI Estimation Planning for TDD Massive MIMO, Authors: Salah Eddine Hajri, Maialen Larrañaga, Mohamad Assaad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Stringy Generalization of the First Law of Thermodynamics for Rotating BTZ Black Hole with a Cosmological Constant as State Parameter, Authors: Alexis Larranaga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Learning Feature Hierarchies with Centered Deep Boltzmann Machines, Authors: Grégoire Montavon, Klaus-Robert Müller\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:01,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Extended Kalman Filter is a Natural Gradient Descent in Trajectory Space, Authors: Yann Ollivier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Parallel Mixed Bayesian Optimization Algorithm: A Scaleup Analysis, Authors: Jiri Ocenasek, Martin Pelikan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Musical Information Extraction from the Singing Voice, Authors: Preeti Rao\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:02,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: How to design, and tune, a computed torque controller: An introduction and a Matlab example, Authors: Lluís Ros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Explicit and Efficient Construction of (nearly) Optimal Rate Codes for Binary Deletion Channel and the Poisson Repeat Channel, Authors: Ittai Rubinstein\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Learning Markov Chain in Unordered Dataset, Authors: Yao-Hung Hubert Tsai, Han Zhao, Ruslan Salakhutdinov, Nebojsa Jojic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: On the Quantitative Analysis of Decoder-Based Generative Models, Authors: Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, Roger Grosse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: World Trade Center responders in their own words: Predicting PTSD symptom trajectories with AI-based language analyses of interviews, Authors: Youngseo Son, Sean A. P. Clouston, Roman Kotov, Johannes C. Eichstaedt, Evelyn J. Bromet, Benjamin J. Luft, H Andrew Schwartz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:03, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:01,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Prospects for all-optical ultrafast muon acceleration, Authors: F. Peano, J. Vieira, R. Mulas, G. Coppa, R. Bingham, L. O. Silva\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Notes on information geometry and evolutionary processes, Authors: Marc Toussaint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:01,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A Generalization of the Exponential-Poisson Distribution, Authors: Wagner Barreto-Souza, Francisco Cribari-Neto\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Efficient Natural Evolution Strategies, Authors: Yi Sun, Daan Wierstra, Tom Schaul, Juergen Schmidhuber\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Efficient Natural Evolution Strategies, Authors: Yi Sun, Daan Wierstra, Tom Schaul, Juergen Schmidhuber\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf file: ./iteration_2/1107.2490v2.Towards_Optimal_One_Pass_Large_Scale_Learning_with_Averaged_Stochastic_Gradient_Descent.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2648046/992132040.py:8: DeprecationWarning: The '(Search).results' method is deprecated, use 'Client.results' instead\n",
      "  for result in tqdm(search_results.results()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check the path:  ./bib_files/1107.2490v2.Towards_Optimal_One_Pass_Large_Scale_Learning_with_Averaged_Stochastic_Gradient_Descent.bib\n",
      "check anystyle bib ./bib_files/1107.2490v2.Towards_Optimal_One_Pass_Large_Scale_Learning_with_Averaged_Stochastic_Gradient_Descent.bib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Enumerative Sphere Shaping for Rate Adaptation and Reach Increase in WDM Transmission Systems, Authors: Abdelkerim Amari, Sebastiaan Goossens, Yunus Can Gultekin, Olga Vassilieva, Inwoong Kim, Tadashi Ikeuchi, Chigo Okonkwo, Frans M. J. Willems, Alex Alvarado\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Beyond Folklore: A Scaling Calculus for the Design and Initialization of ReLU Networks, Authors: Aaron Defazio, Léon Bottou\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:01,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: On U-Statistics and Compressed Sensing II: Non-Asymptotic Worst-Case Analysis, Authors: Fabian Lim, Vladimir Stojanovic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: On U-Statistics and Compressed Sensing II: Non-Asymptotic Worst-Case Analysis, Authors: Fabian Lim, Vladimir Stojanovic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:01,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A classification of permutation polynomials of degree $7$ over finite fields, Authors: Xiang Fan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Logarithmic Regret for Online Control, Authors: Naman Agarwal, Elad Hazan, Karan Singh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Sparse Online Learning via Truncated Gradient, Authors: John Langford, Lihong Li, Tong Zhang\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:01,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A Fast Algorithm for Calculation of Thêo1, Authors: Ben Lewis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Convex optimization for finite horizon robust covariance control of linear stochastic systems, Authors: Georgios Kotsalis, Guanghui Lan, Arkadi Nemirovski\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A Quasi-Newton Approach to Nonsmooth Convex Optimization Problems in Machine Learning, Authors: Jin Yu, S. V. N. Vishwanathan, Simon Guenter, Nicol N. Schraudolph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:01,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A General Algorithm for Solving Rank-one Matrix Sensing, Authors: Lianke Qin, Zhao Song, Ruizhe Zhang\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splt with ray\n"
     ]
    }
   ],
   "source": [
    "arxiv_pipeline(\"./1412.6980v9.Adam__A_Method_for_Stochastic_Optimization.pdf\", \"new_class_ray\", ray=True, recursive=True, iteration=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "weaviate_client = weaviate.Client(url=\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deprecations': [],\n",
       " 'objects': [{'class': 'New_class_ray',\n",
       "   'creationTimeUnix': 1700671887430,\n",
       "   'id': '0053d892-29fa-5c3a-ae92-b0cd51967ad6',\n",
       "   'lastUpdateTimeUnix': 1700671887430,\n",
       "   'properties': {'document_title': '1206.5538v3.Representation_Learning__A_Review_and_New_Perspectives.pdf',\n",
       "    'page_content': '29\\nmodel for visual area V2. In NIPS’07 .\\nLee, H., Grosse, R., Ranganath, R., and Ng, A. Y . (2009a). Convolu-\\ntional deep belief networks for scalable unsupervised learning of\\nhierarchical representations. In ICML’2009 .\\nLee, H., Pham, P., Largman, Y ., and Ng, A. (2009b). Unsupervised\\nfeature learning for audio classiﬁcation using convolutional deep\\nbelief networks. In NIPS’2009 .\\nLin, Y ., Tong, Z., Zhu, S., and Yu, K. (2010). Deep coding network.\\nInNIPS’2010 .\\nLowe, D. (1999). Object recognition from local scale invariant\\nfeatures. In ICCV’99 .\\nMallat, S. (2012). Group invariant scattering. Communications on\\nPure and Applied Mathematics .\\nMarlin, B. and de Freitas, N. (2011). Asymptotic efﬁciency of\\ndeterministic estimators for discrete energy-based models: Ratio\\nmatching and pseudolikelihood. In UAI’2011 .\\nMarlin, B., Swersky, K., Chen, B., and de Freitas, N. (2010).\\nInductive principles for restricted Boltzmann machine learning. In\\nAISTATS’2010 , pages 509–516.\\nMartens, J. (2010). Deep learning via Hessian-free optimization. In\\nICML’2010 , pages 735–742.\\nMartens, J. and Sutskever, I. (2011). Learning recurrent neural\\nnetworks with Hessian-free optimization. In ICML’2011 .\\nMemisevic, R. and Hinton, G. E. (2010). Learning to represent spatial\\ntransformations with factored higher-order Boltzmann machines.\\nNeural Comp. ,22(6).\\nMesnil, G., Dauphin, Y ., Glorot, X., Rifai, S., Bengio, Y ., Goodfellow,\\nI., Lavoie, E., Muller, X., Desjardins, G., Warde-Farley, D., Vin-\\ncent, P., Courville, A., and Bergstra, J. (2011). Unsupervised and\\ntransfer learning challenge: a deep learning approach. In JMLR\\nW&CP: Proc. Unsupervised and Transfer Learning , volume 7.\\nMikolov, T., Deoras, A., Kombrink, S., Burget, L., and Cernocky,\\nJ. (2011). Empirical evaluation and combination of advanced\\nlanguage modeling techniques. In INTERSPEECH’2011 .\\nMobahi, H., Collobert, R., and Weston, J. (2009). Deep learning from\\ntemporal coherence in video. In ICML’2009 .\\nMohamed, A., Dahl, G., and Hinton, G. (2012). Acoustic modeling\\nusing deep belief networks. IEEE Trans. on Audio, Speech and\\nLanguage Processing ,20(1), 14–22.\\nMontufar, G. F. and Morton, J. (2012). When does a mixture\\nof products contain a product of mixtures? Technical report,\\narXiv:1206.0387.\\nMurray, I. and Salakhutdinov, R. (2009). Evaluating probabilities\\nunder high-dimensional latent variable models. In NIPS’2008 ,\\npages 1137–1144.\\nNair, V . and Hinton, G. E. (2010). Rectiﬁed linear units improve\\nrestricted Boltzmann machines. In ICML’10 .\\nNeal, R. M. (1992). Connectionist learning of belief networks.\\nArtiﬁcial Intelligence ,56, 71–113.\\nNeal, R. M. (1993). Probabilistic inference using Markov chain\\nMonte-Carlo methods. Technical Report CRG-TR-93-1, Dept. of\\nComputer Science, University of Toronto.\\nNgiam, J., Chen, Z., Koh, P., and Ng, A. (2011). Learning deep\\nenergy models. In Proc. ICML’2011 . ACM.\\nOlshausen, B. A. and Field, D. J. (1996). Emergence of simple-\\ncell receptive ﬁeld properties by learning a sparse code for natural\\nimages. Nature ,381, 607–609.\\nOrr, G. and Muller, K.-R., editors (1998). Neural networks: tricks of\\nthe trade . Lect. Notes Comp. Sc. Springer-Verlag.\\nPascanu, R. and Bengio, Y . (2013). Natural gradient revisited.\\nTechnical report, arXiv:1301.3584.\\nRaiko, T., Valpola, H., and LeCun, Y . (2012). Deep learning made\\neasier by linear transformations in perceptrons. In AISTATS’2012 .\\nRaina, R., Battle, A., Lee, H., Packer, B., and Ng, A. Y . (2007).\\nSelf-taught learning: transfer learning from unlabeled data. In\\nICML’2007 .\\nRanzato, M. and Hinton, G. H. (2010). Modeling pixel means and\\ncovariances using factorized third-order Boltzmann machines. In\\nCVPR’2010 , pages 2551–2558.Ranzato, M., Poultney, C., Chopra, S., and LeCun, Y . (2007). Efﬁcient\\nlearning of sparse representations with an energy-based model. In\\nNIPS’2006 .\\nRanzato, M., Boureau, Y ., and LeCun, Y . (2008). Sparse feature\\nlearning for deep belief networks. In NIPS’2007 .\\nRanzato, M., Krizhevsky, A., and Hinton, G. (2010a). Factored 3-\\nway restricted Boltzmann machines for modeling natural images.\\nInAISTATS’2010 , pages 621–628.\\nRanzato, M., Mnih, V ., and Hinton, G. (2010b). Generating more\\nrealistic images using gated MRF’s. In NIPS’2010 .\\nRanzato, M., Susskind, J., Mnih, V ., and Hinton, G. (2011). On deep\\ngenerative models with applications to recognition. In CVPR’2011 .\\nRiesenhuber, M. and Poggio, T. (1999). Hierarchical models of object\\nrecognition in cortex. Nature Neuroscience .\\nRifai, S., Vincent, P., Muller, X., Glorot, X., and Bengio, Y . (2011a).\\nContractive auto-encoders: Explicit invariance during feature ex-\\ntraction. In ICML’2011 .\\nRifai, S., Mesnil, G., Vincent, P., Muller, X., Bengio, Y ., Dauphin,\\nY ., and Glorot, X. (2011b). Higher order contractive auto-encoder.\\nInECML PKDD .\\nRifai, S., Dauphin, Y ., Vincent, P., Bengio, Y ., and Muller, X. (2011c).\\nThe manifold tangent classiﬁer. In NIPS’2011 .\\nRifai, S., Bengio, Y ., Dauphin, Y ., and Vincent, P. (2012). A\\ngenerative process for sampling contractive auto-encoders. In\\nICML’2012 .\\nRoweis, S. (1997). EM algorithms for PCA and sensible PCA. CNS\\nTechnical Report CNS-TR-97-02, Caltech.\\nRoweis, S. and Saul, L. K. (2000). Nonlinear dimensionality\\nreduction by locally linear embedding. Science ,290(5500).\\nSalakhutdinov, R. (2010a). Learning deep Boltzmann machines using\\nadaptive MCMC. In ICML’2010 .\\nSalakhutdinov, R. (2010b). Learning in Markov random ﬁelds using\\ntempered transitions. In NIPS’2010 .\\nSalakhutdinov, R. and Hinton, G. E. (2007). Semantic hashing. In\\nSIGIR’2007 .\\nSalakhutdinov, R. and Hinton, G. E. (2009). Deep Boltzmann\\nmachines. In AISTATS’2009 , pages 448–455.\\nSalakhutdinov, R. and Larochelle, H. (2010). Efﬁcient learning of\\ndeep Boltzmann machines. In AISTATS’2010 .\\nSalakhutdinov, R., Mnih, A., and Hinton, G. E. (2007). Restricted\\nBoltzmann machines for collaborative ﬁltering. In ICML 2007 .\\nSavard, F. (2011). R´eseaux de neurones `a relaxation entra ˆın´es par\\ncrit`ere d’autoencodeur d ´ebruitant . Master’s thesis, U. Montr ´eal.\\nSchmah, T., Hinton, G. E., Zemel, R., Small, S. L., and Strother,\\nS. (2009). Generative versus discriminative training of RBMs for\\nclassiﬁcation of fMRI images. In NIPS’2008 , pages 1409–1416.\\nSch¨olkopf, B., Smola, A., and M ¨uller, K.-R. (1998). Nonlinear\\ncomponent analysis as a kernel eigenvalue problem. Neural\\nComputation ,10, 1299–1319.\\nSchwenk, H., Rousseau, A., and Attik, M. (2012). Large, pruned or\\ncontinuous space language models on a gpu for statistical machine\\ntranslation. In Workshop on the future of language modeling for\\nHLT .\\nSeide, F., Li, G., and Yu, D. (2011a). Conversational speech\\ntranscription using context-dependent deep neural networks. In\\nInterspeech 2011 , pages 437–440.\\nSeide, F., Li, G., and Yu, D. (2011b). Feature engineering in\\ncontext-dependent deep neural networks for conversational speech\\ntranscription. In ASRU’2011 .\\nSerre, T., Wolf, L., Bileschi, S., and Riesenhuber, M. (2007). Robust\\nobject recognition with cortex-like mechanisms. IEEE Trans.\\nPattern Anal. Mach. Intell. ,29(3), 411–426.\\nSeung, S. H. (1998). Learning continuous attractors in recurrent\\nnetworks. In NIPS’1997 .\\nSimard, D., Steinkraus, P. Y ., and Platt, J. C. (2003). Best practices\\nfor convolutional neural networks. In ICDAR’2003 .\\nSimard, P., Victorri, B., LeCun, Y ., and Denker, J. (1992). Tangent\\nprop - A formalism for specifying selected invariances in an\\nadaptive network. In NIPS’1991 .'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'New_class_ray',\n",
       "   'creationTimeUnix': 1700671889728,\n",
       "   'id': '00976231-3e96-5c72-aefe-628254e8b816',\n",
       "   'lastUpdateTimeUnix': 1700671889728,\n",
       "   'properties': {'document_title': '1206.1106v2.No_More_Pesky_Learning_Rates_2.pdf',\n",
       "    'page_content': 'No More Pesky Learning Rates\\nare zero:\\nL(j)(θ) =1\\n2(\\nθ−c(j))⊤\\nH(j)(\\nθ−c(j))\\n∇(j)\\nθ=H(j)(\\nθ−c(j))\\nwhere Hiis the (positive semi-deﬁnite) Hessian matrix\\nof the per-sample loss of sample j, and c(j)is the opti-\\nmum for that sample. The distribution of per-sample\\noptima c(j)has mean θ∗and variance Σ. Figure 1\\nillustrates the scenario in one dimension.\\nTo simplify the analysis, we assume for the remain-\\nder of this section that the Hessians of the per-sample\\nlosses are identical for all samples, and that the prob-\\nlem is separable, i.e., the Hessians are diagonal, with\\ndiagonal terms denoted {h1,...,hi,...,hd}. Further,\\nwe will ignore the oﬀ-diagonal terms of Σ, and de-\\nnote the diagonal {σ2\\n1,...,σ2\\ni,...,σ2\\nd}. Then, for any\\nof theddimensions, we thus obtain a one-dimensional\\nproblem (all indices iomitted).\\nJ(θ) =Ei∼P[1\\n2h(θ−c(j))2]\\n=1\\n2h[\\n(θ−θ∗)2+σ2]\\n(2)\\nThe gradient components are ∇(j)\\nθ=h(\\nθ−c(j))\\n, with\\nE[∇θ] =h(θ−θ∗)Var[∇θ] =h2σ2(3)\\nand we can rewrite the SGD update equation as\\nθ(t+1)=θ(t)−ηh(\\nθ(t)−c(j))\\n= (1−ηh)θ(t)+ηhθ∗+ηhσξ(j)(4)\\nwhere the ξ(j)are i.i.d. samples from a zero-mean\\nand unit-variance Gaussian distribution. Inserting this\\ninto equation 2, we obtain the expected loss after an\\nSGD update\\nE[\\nJ(\\nθ(t+1))\\n|θ(t)]\\n=1\\n2h·[\\n(1−ηh)2(θ(t)−θ∗)2+η2h2σ2+σ2]\\n3.2. Optimal Adaptive Learning Rate\\nWe can now derive the optimal (greedy) learning rates\\nfor the current time tas the value η∗(t) that minimizes\\nthe expected loss after the next update\\nη∗(t) = arg min\\nη[\\n(1−ηh)2(θ(t)−θ∗)2+σ2+η2h2σ2]\\n= arg min\\nη[\\nη2(\\nh(θ(t)−θ∗)2+hσ2)\\n−2η(θ(t)−θ∗)2]\\n=1\\nh·(θ(t)−θ∗)2\\n(θ(t)−θ∗)2+σ2(5)In the classical (noiseless or batch) derivation of the\\noptimal learning rate, the best value is simply η∗(t) =\\nh−1. The above formula inserts a corrective term that\\nreduces the learning rate whenever the sample pulls\\nthe parameter vector in diﬀerent directions, as mea-\\nsured by the gradient variance σ2. The reduction of\\nthe learning rate is larger near an optimum, when\\n(θ(t)−θ∗)2is small relative to σ2. In eﬀect, this will\\nreduce the expected error due to the noise in the gra-\\ndient. Overall, this will have the same eﬀect as the\\nusual method of progressively decreasing the learning\\nrate as we get closer to the optimum, but it makes this\\nannealing schedule automatic .\\nIf we do gradient descent with η∗(t), then almost\\nsurely, the algorithm converges (for the quadratic\\nmodel). The proof is given in the appendix.\\n3.3. Global vs. Parameter-speciﬁc Rates\\nThe previous subsections looked at the optimal learn-\\ning rate in the one-dimensional case, which can be triv-\\nially generalized to ddimensions if we assume that all\\nparameters are separable, namely by using an individ-\\nual learning rate η∗\\nifor each dimension i. Alterna-\\ntively, we can derive an optimal global learning rate η∗\\ng\\n(see appendix for the full derivation),\\nη∗\\ng(t) =∑d\\ni=1h2\\ni(θ(t)\\ni−θ∗\\ni)2\\n∑d\\ni=1(\\nh3\\ni(θ(t)\\ni−θ∗\\ni)2+h3\\niσ2\\ni) (6)\\nwhich is especially useful if the problem is badly con-\\nditioned.\\nIn-between a global and a component-wise learning\\nrate, it is of course possible to have common learning\\nrates for blocks of parameters. In the case of multi-\\nlayer learning systems, the blocks may regroup the pa-\\nrameters of each single layer, the biases, etc. This is\\nparticularly useful in deep learning, where the gradi-\\nent magnitudes can vary signiﬁcantly between shallow\\nand deep layers.\\n4. Approximations\\nIn practice, we are not given the quantities σi,hiand\\n(θ(t)\\ni−θ∗\\ni)2. However, based on equation 3, we can esti-\\nmate them from the observed samples of the gradient:\\nη∗\\ni=1\\nhi·(E[∇θi])2\\n(E[∇θi])2+Var[∇θi]=1\\nhi·(E[∇θi])2\\nE[∇2\\nθi](7)\\nThe situation is slightly diﬀerent for the global learn-\\ning rateη∗\\ng. Here we assume that it is feasible to es-\\ntimate the maximal curvature h+= maxi(hi) (which\\ncan be done eﬃciently, for example using the diago-\\nnal Hessian computation method described in (LeCun'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'New_class_ray',\n",
       "   'creationTimeUnix': 1700671876160,\n",
       "   'id': '00c18f22-c0ea-53ca-a345-23d68f6ef045',\n",
       "   'lastUpdateTimeUnix': 1700671876160,\n",
       "   'properties': {'document_title': '1107.2490v2.Towards_Optimal_One_Pass_Large_Scale_Learning_with_Averaged_Stochastic_Gradient_Descent.pdf',\n",
       "    'page_content': 'to the curvature of the expected cost function. cshould be a problem independent constant. With\\nour recipe for setting the learning rate, we show that ASGD outper forms SGD if the data size is\\nlarge enough for SGD to reach its asymptotic region.\\nTo demonstrate the eﬀectiveness of ASGD with the proposed learn ing rate schedule, we apply\\nASGD for training linear classiﬁcation and regressionmodels. We comp are ASGD with other promi-\\nnent large scale SVM solvers on several benchmark tasks. Our exp erimental results show the clear\\nadvantage of ASGD.\\nIn the rest of the paper, for matrices XandY,X≤YmeansY−Xis positive semi-deﬁnite,\\n∥x∥Ais deﬁned as√\\nxTAx. We will assume γt=γ0(1 +aγ0t)−cfor some constant γ0>0,a >0\\nand 0≤c≤1 in all the theorems and lemmas. Through out this paper we denote ∆ t=θt−θ∗and\\n¯∆t=¯θt−θ∗. To help the reader focus on the main idea, we put most proofs to th e Appendix.\\nThe paper is organized as follows: Section 2 establish some results on stochastic linear equa-\\ntion; Section 3 extends the result to ASGD for quadratic loss funct ions; Section 4 works on general\\nnon-quadratic loss functions; Section 5 discusses some implementa tion issues; Section 6 shows ex-\\nperimental results; Section 7 concludes the paper; and Appendix in cludes all the proofs.\\n2. Stochastic Linear Equation\\nTo motivate the problem, we ﬁrst take a close look at the SGD update (2). Let ¯ g(θ) =E(g(θ,d))\\nand the ﬁrst order Taylor expansion of ¯ g(θ) around θ∗beAθ−b, where A=∂¯g(θ)\\n∂θ⏐⏐⏐\\nθ=θ∗and\\nb=Aθ∗−¯g(θ∗) =Aθ∗. Theng(θt−1,d) can be decomposed as:\\ng(θt−1,d) = (Aθt−1−b)+g(θ∗,d)+(g(θt−1,d)−g(θ∗,d)−¯g(θt−1))+(¯g(θt−1)−Aθt−1+b)\\n= (Aθt−1−b)+ξ(1)\\nt+ξ(2)\\nt+ξ(3)\\nt\\nwhereξ(1)\\nt=g(θ∗,dt),ξ(2)\\nt=g(θt−1,dt)−g(θ∗,dt)−¯g(θt−1) andξ(3)\\nt= ¯g(θt−1)−Aθt−1+b. So the\\nSGD update (2) can be re-written as\\nθt=θt−1−γt(Aθt−1−b+ξ(1)\\nt+ξ(2)\\nt+ξ(3)\\nt) (4)\\nIt is easy to see that ξ(1)\\ntis martingale with respect to dt, i.e.,E(ξ(1)\\nt|d1,···,dt−1) = 0, and has\\nidentical distribution for diﬀerent t.ξ(2)\\ntis also martingale with respect to dt. However, as we\\nwill see in later section, its magnitude depends on θt−1−θ∗. Ifg(θ,d) is smooth, we have ξ(2)\\nt=\\nO(∥θt−1−θ∗∥). Forsmooth ¯ g(θ), wehave ξ(3)\\nt=o(∥θt−1−θ∗∥). Bothξ(2)\\ntandξ(3)\\ntareasymptotically\\nnegligible if suitable conditions are met. We also note that ξ(3)\\nt= 0 for quadratic l(θ,ξ).\\nBy the above analysis, we ﬁrst consider the following simple stochast ic approximation procedure\\nwhich ignores ξ(2)\\ntandξ(3)\\nt:\\nθt=θt−1−γt(Aθt−1−b+ξt) (5)\\n¯θt=1\\ntt∑\\ni=1θi (6)\\nwhereAis a positive deﬁnite matrix with the smallest eigenvalue λ0and the largest eigenvalue λ1,\\nξtis martingale diﬀerence process, i.e., E(ξt|ξ1,···,ξt−1) = 0, the variance of ξtisE(ξtξT\\nt) =S. We\\nwill see that this algorithm can be used to ﬁnd the root θ∗of equation Aθ=b\\nTheorem 1 Ifγ0λ1≤1and(2c−1)a < λ0, then the estimator ¯θtin (6) satisﬁes:\\ntE(∥¯θt−θ∗∥2\\nA)≤tr(A−1S)+(2c0+c2\\n0)(1+aγ0t)c−1\\nctr(A−1S)+(1+c0)2\\nγ2\\n0t∥θ0−θ∗∥2\\nA−1\\n3'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'New_class_ray',\n",
       "   'creationTimeUnix': 1700671854345,\n",
       "   'id': '0124446f-5348-5c80-895d-ea31f24ee1e7',\n",
       "   'lastUpdateTimeUnix': 1700671854345,\n",
       "   'properties': {'document_title': '1301.3584v7.Revisiting_Natural_Gradient_for_Deep_Networks.pdf',\n",
       "    'page_content': 'Erhan, D., Courville, A., Bengio, Y ., and Vincent, P. (2010). Why does unsupervised pre-training help deep\\nlearning? In JMLR W&CP: Proc. AISTATS’2010 , volume 9, pages 201–208.\\nGonzalez, A. and Dorronsoro, J. (2006). Natural conjugate gradient training of multilayer perceptrons. Artiﬁcial\\nNeural Networks ICANN 2006 , pages 169–177.\\nHeskes, T. (2000). On natural learning and pruning in multilayered perceptrons. Neural Computation ,12,\\n1037–1057.\\nHonkela, A., Tornio, M., Raiko, T., and Karhunen, J. (2008). Natural conjugate gradient in variational inference.\\nInNeural Information Processing , pages 305–314.\\nHonkela, A., Raiko, T., Kuusela, M., Tornio, M., and Karhunen, J. (2010). Approximate riemannian conjugate\\ngradient learning for ﬁxed-form variational bayes. Journal of Machine Learning Research ,11, 3235–3268.\\nKakade, S. (2001). A natural policy gradient. In NIPS , pages 1531–1538. MIT Press.\\nKiros, R. (2013). Training neural networks with stochastic hessian-free optimization. ICLR .\\nLe Roux, N., Manzagol, P.-A., and Bengio, Y . (2008). Topmoumoute online natural gradient algorithm. In\\nNIPS’07 .\\nMartens, J. (2010). Deep learning via hessian-free optimization. In ICML , pages 735–742.\\nMartens, J. and Sutskever, I. (2011). Learning recurrent neural networks with hessian-free optimization. In\\nICML , pages 1017–1024.\\nMizutani, E. and Demmel, J. (2003). Iterative scaled trust-region learning in krylov subspaces via peralmutter’s\\nimplicit sparse hessian-vector multiply. In NIPS , pages 209–216.\\nNocedal, J. and Wright, S. J. (2000). Numerical Optimization . Springer.\\nPark, H., Amari, S.-I., and Fukumizu, K. (2000). Adaptive natural gradient learning algorithms for various\\nstochastic models. Neural Networks ,13(7), 755 – 764.\\nPearlmutter, B. A. (1994). Fast exact multiplication by the hessian. Neural Computation ,6, 147–160.\\nPeters, J. and Schaal, S. (2008). Natural actor-critic. (7-9), 1180–1190.\\nRoux, N. L. and Fitzgibbon, A. W. (2010). A fast natural newton method. In ICML , pages 623–630.\\nSchaul, T. (2012). Natural evolution strategies converge on sphere functions. In Genetic and Evolutionary\\nComputation Conference (GECCO) .\\nSchraudolph, N. N. (2002). Fast curvature matrix-vector products for second-order gradient descent. Neural\\nComputation ,14(7), 1723–1738.\\nShewchuck, J. (1994). An introduction to the conjugate gradient method without the agonizing pain. Technical\\nreport, CMU.\\nSohl-Dickstein, J. (2012). The natural gradient by analogy to signal whitening, and recipes and tricks for its\\nuse. CoRR ,abs/1205.1828 .\\nSun, Y ., Wierstra, D., Schaul, T., and Schmidhuber, J. (2009). Stochastic search using the natural gradient. In\\nICML , page 146.\\nSusskind, J., Anderson, A., and Hinton, G. E. (2010). The Toronto face dataset. Technical Report UTML TR\\n2010-001, U. Toronto.\\nVinyals, O. and Povey, D. (2012). Krylov Subspace Descent for Deep Learning. In AISTATS .\\nAppendix\\nExpected Hessian to Fisher Information Matrix\\nThe Fisher Information Matrix form can be obtained from the expected value of the Hessian :\\nEz[\\n−∂2logpθ\\n∂θ]\\n=Ez[\\n−∂1\\npθ∂pθ\\n∂θ\\n∂θ]\\n=Ez[\\n−1\\npθ(z)∂2pθ\\n∂θ2+(1\\npθ∂pθ\\n∂θ)T(1\\npθ∂pθ\\n∂θ)]\\n=−∂2\\n∂θ2(∑\\nzpθ(z))\\n+Ez[(∂logpθ(z)\\n∂θ)T(∂logpθ(z)\\n∂θ)]\\n=Ez[(∂logpθ(z)\\n∂θ)T(∂logpθ(z)\\n∂θ)]\\n(24)\\n14'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'New_class_ray',\n",
       "   'creationTimeUnix': 1700671880632,\n",
       "   'id': '01319b3a-6b0a-584b-a830-c0ffef7bda6f',\n",
       "   'lastUpdateTimeUnix': 1700671880632,\n",
       "   'properties': {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks.pdf',\n",
       "    'page_content': '3.1 Penn Treebank Experiments\\nThe ﬁrst set of text prediction experiments focused on the Penn Treebank por-\\ntion of the Wall Street Journal corpus [22]. This was a preliminary study whose\\nmain purpose was to gauge the predictive power of the network, rather than to\\ngenerate interesting sequences.\\nAlthough a relatively small text corpus (a little over a million words in total),\\nthe Penn Treebank data is widely used as a language modelling benchmark. The\\ntraining set contains 930,000 words, the validation set contains 74,000 words and\\nthe test set contains 82,000 words. The vocabulary is limited to 10,000 words,\\nwith all other words mapped to a special ‘unknown word’ token. The end-of-\\nsentence token was included in the input sequences, and was counted in the\\nsequence loss. The start-of-sentence marker was ignored, because its role is\\nalready fulﬁlled by the null vectors that begin the sequences (c.f. Section 2).\\nThe experiments compared the performance of word and character-level\\nLSTM predictors on the Penn corpus. In both cases, the network architecture\\nwas a single hidden layer with 1000 LSTM units. For the character-level network\\nthe input and output layers were size 49, giving approximately 4.3M weights in\\ntotal, while the word-level network had 10,000 inputs and outputs and around\\n54M weights. The comparison is therefore somewhat unfair, as the word-level\\nnetwork had many more parameters. However, as the dataset is small, both net-\\nworks were easily able to overﬁt the training data, and it is not clear whether the\\ncharacter-level network would have beneﬁted from more weights. All networks\\nwere trained with stochastic gradient descent, using a learn rate of 0.0001 and a\\nmomentum of 0.99. The LSTM derivates were clipped in the range [ −1,1] (c.f.\\nSection 2.1).\\nNeural networks are usually evaluated on test data with ﬁxed weights. For\\nprediction problems however, where the inputs arethe targets, it is legitimate\\nto allow the network to adapt its weights as it is being evaluated (so long as\\nit only sees the test data once). Mikolov refers to this as dynamic evaluation .\\nDynamic evaluation allows for a fairer comparison with compression algorithms,\\nfor which there is no division between training and test sets, as all data is only\\npredicted once.\\nSince both networks overﬁt the training data, we also experiment with two\\ntypes of regularisation: weight noise [18] with a std. deviation of 0.075 applied\\nto the network weights at the start of each training sequence, and adaptive\\nweight noise [8], where the variance of the noise is learned along with the weights\\nusing a Minimum description Length (or equivalently, variational inference) loss\\nfunction. When weight noise was used, the network was initialised with the\\nﬁnal weights of the unregularised network. Similarly, when adaptive weight\\nnoise was used, the weights were initialised with those of the network trained\\nwith weight noise. We have found that retraining with iteratively increased\\nregularisation is considerably faster than training from random weights with\\nregularisation. Adaptive weight noise was found to be prohibitively slow for\\nthe word-level network, so it was regularised with ﬁxed-variance weight noise\\nonly. One advantage of adaptive weight is that early stopping is not needed\\n7'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'New_class_ray',\n",
       "   'creationTimeUnix': 1700671884725,\n",
       "   'id': '01475d54-7723-5137-b46c-284d45dd2a8a',\n",
       "   'lastUpdateTimeUnix': 1700671884725,\n",
       "   'properties': {'document_title': '1412.2306v2.Deep_Visual_Semantic_Alignments_for_Generating_Image_Descriptions.pdf',\n",
       "    'page_content': 'Figure 11. Additional examples of alignments. For each query test image above we retrieve the most compatible sentence from the test set\\nand show the alignments.\\nFigure 12. Additional examples of captions on the level of full images. Green: Human ground truth. Red: Top-scoring sentence from\\ntraining set. Blue: Generated sentence.'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'New_class_ray',\n",
       "   'creationTimeUnix': 1700671859287,\n",
       "   'id': '01998f6f-cb53-57cc-9218-10d7532860da',\n",
       "   'lastUpdateTimeUnix': 1700671859287,\n",
       "   'properties': {'document_title': '1108.3298v1.A_Machine_Learning_Perspective_on_Predictive_Coding_with_PAQ.pdf',\n",
       "    'page_content': 'Table 11: Confusion matrix for chicken dataset with the “number of measurements” parameter set\\nto 40. C1=back, C2=breast, C3=drumstick, C4=thigh and back, C5=wing.\\nPredicted\\nC1 C2 C3 C4 C5\\nC155 10 2 2 7\\nC2 093 0 3 0\\nActual C3 0 584 0 7\\nC4 0 8 048 5\\nC5 3 1 3 1109\\nTable 12: Comparative results on the chicken dataset. Our results are in boldface.\\nMethodology Protocol Percent correct\\n1-NN + Levenshtein edit distance leave-one-out ≈67\\n(Mollineda et al., 2002)\\n1-NN + HMM-based distance leave-one-out 73.77\\n(Bicego and Trudda, 2008)\\n1-NN + mBm-based features leave-one-out 76.5\\n(Bicego and Trudda, 2008)\\n1-NN + approximated cyclic distance leave-one-out ≈78\\n(Mollineda et al., 2002)\\n1-NN + convert to time series leave-one-out 80.04\\n(Wei et al., 2008)\\nSVM + HMM-based entropic features leave-one-out 81.21\\n(Perina et al., 2009)\\nSVM + HMM-based nonlinear kernel 50-50 train-test split 85.52\\n(Carli et al., 2009)\\nSVM + HMM-based Fisher kernel 50-50 train-test split 85.8\\n(Bicego et al., 2009)\\nPAQclass + convert to time series leave-one-out 87.22\\nhad a surprisingly large eﬀect on classiﬁcation accuracy. Adjusting the parameter by a single\\nmeasurement from the best value (40) resulted in ≈5 to 6% loss in accuracy. Another unfortunate\\nproperty of adjusting this parameter is that the classiﬁcation accuracy is not a convex function (as\\nseen at the parameter value 35). This means ﬁnding the optimal value of the parameter would\\nrequire an exhaustive search. Due to time constraints, we did not perform an exhaustive search\\n(only the experiments in Table 10 were performed). Table 11 shows a confusion matrix at the best\\nparameter setting.\\nOur result of 87.2197% correct classiﬁcations is among the best results published for this dataset.\\n22'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'New_class_ray',\n",
       "   'creationTimeUnix': 1700671858369,\n",
       "   'id': '020f1b31-9717-5e49-a038-4b2b30e30a5c',\n",
       "   'lastUpdateTimeUnix': 1700671858369,\n",
       "   'properties': {'document_title': '1801.07736v3.MaskGAN__Better_Text_Generation_via_Filling_in_the______.pdf',\n",
       "    'page_content': 'Published as a conference paper at ICLR 2018\\nYizhe Zhang, Zhe Gan, Kai Fan, Zhi Chen, Ricardo Henao, Dinghan Shen, and Lawrence Carin.\\nAdversarial feature matching for text generation. arXiv preprint arXiv:1706.03850 , 2017.\\nBarret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. In International\\nConference on Learning Representations , 2017.\\n13'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'New_class_ray',\n",
       "   'creationTimeUnix': 1700671854414,\n",
       "   'id': '0286e839-d24c-5ad9-a1e5-2aed13e158d3',\n",
       "   'lastUpdateTimeUnix': 1700671854414,\n",
       "   'properties': {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks_1.pdf',\n",
       "    'page_content': 'Figure 11: Online handwriting samples generated by the prediction\\nnetwork. All samples are 700 timesteps long.\\n25'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'New_class_ray',\n",
       "   'creationTimeUnix': 1700671854346,\n",
       "   'id': '02e8d499-2780-5172-859e-0c063e1fc6e9',\n",
       "   'lastUpdateTimeUnix': 1700671854346,\n",
       "   'properties': {'document_title': '1111.4259v1.Krylov_Subspace_Descent_for_Deep_Learning.pdf',\n",
       "    'page_content': 'HF KSD\\nDataset Tr. err. CV err. TimeTr. err. CV err. Time\\nCURVES 0.13 0.19 10.17 0.25 0.2\\nMNIST AE 1.7 2.7 11.8 2.50.2\\nMNIST CL 0%2.01% 10%1.70% 0.6\\nMNIST CL,PT 0%1.40% 10%1.29% 0.6\\nAurora 5.1% 8.7% 14.5% 8.1% 0.3\\nStarcraft 0% 11% 10% 5% 0.7\\nTable2: Resultscomparingtwosecondordermethods: Hessia nFreeandKrylovSubspaceDescent.\\nTimereportedis relativetothe runningtimeofHF (lowertha n1meansfaster).\\nIn Figures 1 and 2, we show the convergence of KSD and HF with bo th the Hessian and Gauss-\\nNewton matrices. HF eventually “gets stuck” when using the H essian; the algorithm was not de-\\nsignedtobeusedfornon-positivedeﬁnitematrices. Evenbe foregettingstuck,itisclearthatitdoes\\nnotworkwellwiththeactualHessian. Ourmethodalsoworksb etterwiththeGauss-Newtonmatrix\\nthan with the Hessian, althoughthe differenceis smaller. O ur methodis always faster than HF and\\nL-BFGS.\\n2.7 2.9 3.1 3.3 3.5 3.70.20.40.6\\nlog10(time(s))Train Error\\n  \\nHF, Hessian matrix\\nLBFGS\\nHF, GN matrix\\nKSD, Hessian matrix\\nKSD, GN matrix\\nFigure 1: Aurora convergence curves for various\\nalgorithms.2.4 2.6 2.8 3 3.2 3.4048121618\\nlog10(time(s))L2 Train Error\\n  \\n LBFGS\\n HF, Hessian matrix\\n KSD, GN matix, K=80\\n KSD, Hessian matrix, K=20\\n HF, GN matrix\\n KSD, GN matrix, K=20\\nFigure 2: CURVES convergence curves for vari-\\nousalgorithms.\\n6 Conclusionand future work\\nIn this paper, we proposed a new second order optimization me thod. Our approach relies on efﬁ-\\nciently computing the matrix-vector product between the He ssian (or a PSD approximation to it),\\nand a vector. Unlike Hessian Free (HF) optimization, we do no t require the approximation of the\\nHessian tobePSD, andourmethodrequiresfewerheuristics; however,it requiresmorememory.\\nOur planned future work in this direction includes investig ating the circumstances under which\\npre-training is necessary: that is, we would like to conﬁrm o ur statement that pre-training is not\\nnecessary when using sufﬁciently advanced optimization me thods, as long as overﬁtting is not the\\nmain issue. Current work showsthat the presented methodis a lso able to efﬁciently train recursive\\nneural networks, with no need to use the structural damping o f the Gauss-Newton matrix proposed\\nin[8].\\nReferences\\n[1] Shun-IchiAmari. Naturalgradientworksefﬁcientlyinl earning.NeuralComputation ,10:251–\\n276,1998.\\n[2] Yoshua Bengio and Xavier Glorot. Understandingthe difﬁ culty of training deep feedforward\\nneuralnetworks. In AISTATS2010 ,volume9,pages249–256,May2010.\\n[3] RichardH. Byrd,Gillian M. Chiny,Will Neveitt, and Jorg eNocedal. On the use of stochastic\\nhessianinformationinoptimizationmethodsformachinele arning.(submittedforpublication) ,\\n2010.\\n8'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'New_class_ray',\n",
       "   'creationTimeUnix': 1700671866504,\n",
       "   'id': '0322c332-41f9-5a5b-97cf-394ea9365901',\n",
       "   'lastUpdateTimeUnix': 1700671866504,\n",
       "   'properties': {'document_title': '1106.3708v4.Information_Geometric_Optimization_Algorithms__A_Unifying_Picture_via_Invariance_Principles.pdf',\n",
       "    'page_content': 'Information-Geometric Optimization\\nwhere\\nˆWf(xi) =w(rkN(xi) + 1 /2\\nN)\\nwith rkN(xi) = #{1⩽j⩽N, f(xj)< f(xi)}. (When there are f-ties in the sample,\\nˆWf(xi)is deﬁned as the average of w((r+ 1/2)/N)over the possible rankings rofxi.)\\nProof Letg:X→Rbe any function with EPθg2<∞. We will show that1\\nN∑ˆWf(xi)g(xi)→∫Wf\\nθ(x)g(x)Pθ(dx). Applying this with gequal to the components of∂lnPθ(x)\\n∂θwill yield\\nthe result.\\nLet us decompose\\n1\\nN∑ˆWf(xi)g(xi) =1\\nN∑\\nWf\\nθ(xi)g(xi) +1\\nN∑\\n(ˆWf(xi)−Wf\\nθ(xi))g(xi).\\nEach summand in the ﬁrst term involves only one sample xi(contrary toˆWf(xi)which\\ndepends on the whole sample). So by the strong law of large num bers, almost surely\\n1\\nN∑Wf\\nθ(xi)g(xi)converges to∫Wf\\nθ(x)g(x)Pθ(dx). So we have to show that the second\\nterm converges to 0almost surely.\\nBy the Cauchy–Schwarz inequality, we have\\n⏐⏐⏐⏐1\\nN∑\\n(ˆWf(xi)−Wf\\nθ(xi))g(xi)⏐⏐⏐⏐2\\n⩽(1\\nN∑\\n(ˆWf(xi)−Wf\\nθ(xi))2)(1\\nN∑\\ng(xi)2)\\nBy the strong law of large numbers, the second term1\\nN∑g(xi)2converges to EPθg2almost\\nsurely. So we have to prove that the ﬁrst term1\\nN∑(ˆWf(xi)−Wf\\nθ(xi))2converges to 0\\nalmost surely.\\nSince wis bounded by assumption, we can write\\n(ˆWf(xi)−Wf\\nθ(xi))2⩽2B⏐⏐⏐ˆWf(xi)−Wf\\nθ(xi)⏐⏐⏐\\n= 2B⏐⏐⏐ˆWf(xi)−Wf\\nθ(xi)⏐⏐⏐\\n++ 2B⏐⏐⏐ˆWf(xi)−Wf\\nθ(xi)⏐⏐⏐\\n−\\nwhere Bis the bound on|w|. We will bound each of these terms.\\nLet us abbreviate q<\\ni= Pr x′∼Pθ(f(x′)< f(xi)),q⩽\\ni= Pr x′∼Pθ(f(x′)⩽f(xi)),r<\\ni=\\n#{j⩽N, f(xj)< f(xi)},r⩽\\ni= #{j⩽N, f(xj)⩽f(xi)}.\\nBy deﬁnition ofˆWfwe have\\nˆWf(xi) =1\\nr⩽\\ni−r<\\nir⩽\\ni−1∑\\nk=r<\\niw((k+ 1/2)/N)\\nand moreover Wf\\nθ(xi) =w(q<\\ni)ifq<\\ni=q⩽\\niorWf\\nθ(xi) =1\\nq⩽\\ni−q<\\ni∫q⩽\\ni\\nq<\\niwotherwise.\\nThe Glivenko–Cantelli theorem ( Billingsley ,1995, Theorem 20.6) implies that supi⏐⏐⏐q⩽\\ni−r⩽\\ni/N⏐⏐⏐\\ntends to 0almost surely, and likewise for supi|q<\\ni−r<\\ni/N|. So let Nbe large enough so\\nthat these errors are bounded by ε.\\n53'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'New_class_ray',\n",
       "   'creationTimeUnix': 1700671854340,\n",
       "   'id': '032fddec-cf56-5081-aa1b-7b626461eccc',\n",
       "   'lastUpdateTimeUnix': 1700671854340,\n",
       "   'properties': {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks_2.pdf',\n",
       "    'page_content': 'This can be substituted into Eq. (6) to determine the sequence loss (up to\\na constant that depends only on the quantisation of the data and does not\\ninﬂuence network training):\\nL(x) =T∑\\nt=1−log\\uf8eb\\n\\uf8ed∑\\njπj\\ntN(xt+1|µj\\nt,σj\\nt,ρj\\nt)\\uf8f6\\n\\uf8f8−{\\nloget if (xt+1)3= 1\\nlog(1−et) otherwise\\n(26)\\nThe derivative of the loss with respect to the end-of-stroke outputs is straight-\\nforward:\\n∂L(x)\\n∂ˆet= (xt+1)3−et (27)\\nThe derivatives with respect to the mixture density outputs can be found by\\nﬁrst deﬁning the component responsibilities γj\\nt:\\nˆγj\\nt=πj\\ntN(xt+1|µj\\nt,σj\\nt,ρj\\nt) (28)\\nγj\\nt=ˆγj\\nt∑M\\nj′=1ˆγj′\\nt(29)\\nThen observing that\\n∂L(x)\\n∂ˆπj\\nt=πj\\nt−γj\\nt (30)\\n∂L(x)\\n∂(ˆµj\\nt,ˆσj\\nt,ˆρj\\nt)=−γj\\nt∂logN(xt+1|µj\\nt,σj\\nt,ρj\\nt)\\n∂(ˆµj\\nt,ˆσj\\nt,ˆρj\\nt)(31)\\nwhere\\n∂logN(x|µ,σ,ρ )\\n∂ˆµ1=C\\nσ1(x1−µ1\\nσ1−ρ(x2−µ2)\\nσ2)\\n(32)\\n∂logN(x|µ,σ,ρ )\\n∂ˆµ2=C\\nσ2(x2−µ2\\nσ2−ρ(x1−µ1)\\nσ1)\\n(33)\\n∂logN(x|µ,σ,ρ )\\n∂ˆσ1=C(x1−µ1)\\nσ1(x1−µ1\\nσ1−ρ(x2−µ2)\\nσ2)\\n−1 (34)\\n∂logN(x|µ,σ,ρ )\\n∂ˆσ2=C(x2−µ2)\\nσ2(x2−µ2\\nσ2−ρ(x1−µ1)\\nσ1)\\n−1 (35)\\n∂logN(x|µ,σ,ρ )\\n∂ˆρ=(x1−µ1)(x2−µ2)\\nσ1σ2+ρ(1−CZ) (36)\\nwithZdeﬁned as in Eq. (25) and\\nC=1\\n1−ρ2(37)\\nFig. 10 illustrates the operation of a mixture density output layer applied to\\nonline handwriting prediction.\\n21'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'New_class_ray',\n",
       "   'creationTimeUnix': 1700671883906,\n",
       "   'id': '03383d99-563d-582c-8251-76c82ae28db9',\n",
       "   'lastUpdateTimeUnix': 1700671883906,\n",
       "   'properties': {'document_title': '1206.5538v3.Representation_Learning__A_Review_and_New_Perspectives.pdf',\n",
       "    'page_content': '2\\nspeech system based on deep learning (Seide et al. , 2011a).\\nThese authors managed to reduce the word error rate on\\nfour major benchmarks by about 30% (e.g. from 27.4% to\\n18.5% on RT03S) compared to state-of-the-art models based\\non Gaussian mixtures for the acoustic modeling and trained on\\nthe same amount of data (309 hours of speech). The relative\\nimprovement in error rate obtained by Dahl et al. (2012) on a\\nsmaller large-vocabulary speech recognition benchmark (Bing\\nmobile business search dataset, with 40 hours of speech) is\\nbetween 16% and 23%.\\nRepresentation-learning algorithms have also been applied\\nto music, substantially beating the state-of-the-art in poly-\\nphonic transcription (Boulanger-Lewandowski et al. , 2012),\\nwith relative error improvement between 5% and 30% on a\\nstandard benchmark of 4 datasets. Deep learning also helped\\nto win MIREX (Music Information Retrieval) competitions,\\ne.g. in 2011 on audio tagging (Hamel et al. , 2011).\\nObject Recognition\\nThe beginnings of deep learning in 2006 have focused on\\nthe MNIST digit image classiﬁcation problem (Hinton et al. ,\\n2006; Bengio et al. , 2007), breaking the supremacy of SVMs\\n(1.4% error) on this dataset3. The latest records are still held\\nby deep networks: Ciresan et al. (2012) currently claims the\\ntitle of state-of-the-art for the unconstrained version of the task\\n(e.g., using a convolutional architecture), with 0.27% error,\\nand Rifai et al. (2011c) is state-of-the-art for the knowledge-\\nfree version of MNIST, with 0.81% error.\\nIn the last few years, deep learning has moved from\\ndigits to object recognition in natural images, and the latest\\nbreakthrough has been achieved on the ImageNet dataset4\\nbringing down the state-of-the-art error rate from 26.1% to\\n15.3% (Krizhevsky et al. , 2012).\\nNatural Language Processing\\nBesides speech recognition, there are many other Natural\\nLanguage Processing (NLP) applications of representation\\nlearning. Distributed representations for symbolic data were\\nintroduced by Hinton (1986), and ﬁrst developed in the\\ncontext of statistical language modeling by Bengio et al.\\n(2003) in so-called neural net language models (Bengio,\\n2008). They are all based on learning a distributed repre-\\nsentation for each word, called a word embedding . Adding a\\nconvolutional architecture, Collobert et al. (2011) developed\\nthe SENNA system5that shares representations across the\\ntasks of language modeling, part-of-speech tagging, chunking,\\nnamed entity recognition, semantic role labeling and syntactic\\nparsing. SENNA approaches or surpasses the state-of-the-art\\non these tasks but is simpler and much faster than traditional\\npredictors. Learning word embeddings can be combined with\\nlearning image representations in a way that allow to associate\\ntext and images. This approach has been used successfully to\\nbuild Google’s image search, exploiting huge quantities of data\\nto map images and queries in the same space (Weston et al. ,\\n3. for the knowledge-free version of the task, where no image-speciﬁc prior\\nis used, such as image deformations or convolutions\\n4. The 1000-class ImageNet benchmark, whose results are detailed here:\\nhttp://www.image-net.org/challenges/LSVRC/2012/results.html\\n5. downloadable from http://ml.nec-labs.com/senna/2010) and it has recently been extended to deeper multi-modal\\nrepresentations (Srivastava and Salakhutdinov, 2012).\\nThe neural net language model was also improved by\\nadding recurrence to the hidden layers (Mikolov et al. , 2011),\\nallowing it to beat the state-of-the-art (smoothed n-gram\\nmodels) not only in terms of perplexity (exponential of the\\naverage negative log-likelihood of predicting the right next\\nword, going down from 140 to 102) but also in terms of\\nword error rate in speech recognition (since the language\\nmodel is an important component of a speech recognition\\nsystem), decreasing it from 17.2% (KN5 baseline) or 16.9%\\n(discriminative language model) to 14.4% on the Wall Street\\nJournal benchmark task. Similar models have been applied\\nin statistical machine translation (Schwenk et al. , 2012; Le\\net al. , 2013), improving perplexity and BLEU scores. Re-\\ncursive auto-encoders (which generalize recurrent networks)\\nhave also been used to beat the state-of-the-art in full sentence\\nparaphrase detection (Socher et al. , 2011a) almost doubling the\\nF1 score for paraphrase detection. Representation learning can\\nalso be used to perform word sense disambiguation (Bordes\\net al. , 2012), bringing up the accuracy from 67.8% to 70.2%\\non the subset of Senseval-3 where the system could be applied\\n(with subject-verb-object sentences). Finally, it has also been\\nsuccessfully used to surpass the state-of-the-art in sentiment\\nanalysis (Glorot et al. , 2011b; Socher et al. , 2011b).\\nMulti-Task and Transfer Learning, Domain Adaptation\\nTransfer learning is the ability of a learning algorithm to\\nexploit commonalities between different learning tasks in order\\nto share statistical strength, and transfer knowledge across\\ntasks. As discussed below, we hypothesize that representation\\nlearning algorithms have an advantage for such tasks because\\nthey learn representations that capture underlying factors, a\\nsubset of which may be relevant for each particular task, as\\nillustrated in Figure 1. This hypothesis seems conﬁrmed by a\\nnumber of empirical results showing the strengths of repre-\\nsentation learning algorithms in transfer learning scenarios.\\nraw input x task 1  output y1 task 3  output y3 task 2 output y2 Task%A%Task%B%Task%C%%output%\\n%input%\\n%shared%subsets%of%factors%\\nFig. 1. Illustration of representation-learning discovering ex-\\nplanatory factors (middle hidden layer, in red), some explaining\\nthe input (semi-supervised setting), and some explaining target\\nfor each task. Because these subsets overlap, sharing of statis-\\ntical strength helps generalization..\\nMost impressive are the two transfer learning challenges\\nheld in 2011 and won by representation learning algorithms.\\nFirst, the Transfer Learning Challenge, presented at an ICML\\n2011 workshop of the same name, was won using unsuper-\\nvised layer-wise pre-training (Bengio, 2011; Mesnil et al. ,\\n2011). A second Transfer Learning Challenge was held the'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'New_class_ray',\n",
       "   'creationTimeUnix': 1700671870475,\n",
       "   'id': '036cc7a5-768e-5008-bc13-f7886b7faade',\n",
       "   'lastUpdateTimeUnix': 1700671870475,\n",
       "   'properties': {'document_title': '1206.1106v2.No_More_Pesky_Learning_Rates_1.pdf',\n",
       "    'page_content': 'No More Pesky Learning Rates\\nTom Schaul schaul@cims.nyu.edu\\nSixin Zhang zsx@cims.nyu.edu\\nYann LeCun yann@cims.nyu.edu\\nCourant Institute of Mathematical Sciences\\nNew York University\\n715 Broadway, New York, NY 10003, USA\\nAbstract\\nThe performance of stochastic gradient de-\\nscent (SGD) depends critically on how learn-\\ning rates are tuned and decreased over time.\\nWe propose a method to automatically adjust\\nmultiple learning rates so as to minimize the\\nexpected error at any one time. The method\\nrelies on local gradient variations across sam-\\nples. In our approach, learning rates can in-\\ncrease as well as decrease, making it suitable\\nfor non-stationary problems. Using a num-\\nber of convex and non-convex learning tasks,\\nwe show that the resulting algorithm matches\\nthe performance of SGD or other adaptive\\napproaches with their best settings obtained\\nthrough systematic search, and eﬀectively re-\\nmoves the need for learning rate tuning.\\n1. Introduction\\nLarge-scale learning problems require algorithms that\\nscale benignly (e.g. sub-linearly) with the size of the\\ndataset and the number of trainable parameters. This\\nhas lead to a recent resurgence of interest in stochas-\\ntic gradient descent methods (SGD). Besides fast con-\\nvergence, SGD has sometimes been observed to yield\\nsigniﬁcantly better generalization errors than batch\\nmethods (Bottou & Bousquet, 2011).\\nIn practice, getting good performance with SGD re-\\nquires some manual adjustment of the initial value of\\nthe learning rate (or step size) for each model and each\\nproblem, as well as the design of an annealing schedule\\nfor stationary data. The problem is particularly acute\\nfor non-stationary data.\\nThe contribution of this paper is a novel method to\\nautomatically adjust learning rates (possibly diﬀerentlearning rates for diﬀerent parameters), so as to min-\\nimize some estimate of the expectation of the loss at\\nany one time.\\nStarting from an idealized scenario where every sam-\\nple’s contribution to the loss is quadratic and separa-\\nble, we derive a formula for the optimal learning rates\\nfor SGD, based on estimates of the variance of the gra-\\ndient. The formula has two components: one that cap-\\ntures variability across samples, and one that captures\\nthe local curvature, both of which can be estimated in\\npractice. The method can be used to derive a single\\ncommon learning rate, or local learning rates for each\\nparameter, or each block of parameters, leading to ﬁve\\nvariations of the basic algorithm, none of which need\\nany parameter tuning.\\nThe performance of the methods obtained without any\\nmanual tuning are reported on a variety of convex and\\nnon-convex learning models and tasks. They compare\\nfavorably with an “ideal SGD”, where the best possible\\nlearning rate was obtained through systematic search,\\nas well as previous adaptive schemes.\\n2. Background\\nSGD methods have a long history in adaptive sig-\\nnal processing, neural networks, and machine learn-\\ning, with an extensive literature (see (Bottou, 1998;\\nBottou & Bousquet, 2011) for recent reviews). While\\nthe practical advantages of SGD for machine learning\\napplications have been known for a long time (LeCun\\net al., 1998), interest in SGD has increased in recent\\nyears due to the ever-increasing amounts of streaming\\ndata, to theoretical optimality results for generaliza-\\ntion error (Bottou & LeCun, 2004), and to competi-\\ntions being won by SGD methods, such as the PAS-\\nCAL Large Scale Learning Challenge (Bordes et al.,\\n2009), where Quasi-Newton approximation of the Hes-\\nsian was used within SGD. Still, practitioners need to\\ndeal with a sensitive hyper-parameter tuning phase to\\nget top performance: each of the PASCAL tasks usedarXiv:1206.1106v2  [stat.ML]  18 Feb 2013'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'New_class_ray',\n",
       "   'creationTimeUnix': 1700671879138,\n",
       "   'id': '03a5e598-3283-59c3-8ef0-465af0bf4713',\n",
       "   'lastUpdateTimeUnix': 1700671879138,\n",
       "   'properties': {'document_title': '1503.01445v1.Toxicity_Prediction_using_Deep_Learning.pdf',\n",
       "    'page_content': 'Toxicity Prediction using Deep Learning\\nThomas Unterthiner∗1,2UNTERTHINER @BIOINF .JKU.AT\\nAndreas Mayr∗1,2MAYR @BIOINF .JKU.AT\\nG¨unter Klambauer∗1KLAMBAUER @BIOINF .JKU.AT\\nSepp Hochreiter1HOCHREIT @BIOINF .JKU.AT\\n1Institute of Bioinformatics, Johannes Kepler University Linz, Austria\\n2RISC Software GmbH, Johannes Kepler University Linz, Austria\\n∗These authors contributed equally to this work\\nAbstract\\nEveryday we are exposed to various chemicals\\nvia food additives, cleaning and cosmetic prod-\\nucts and medicines — and some of them might be\\ntoxic. However testing the toxicity of all existing\\ncompounds by biological experiments is neither\\nﬁnancially nor logistically feasible. Therefore\\nthe government agencies NIH, EPA and FDA\\nlaunched the Tox21 Data Challenge within the\\n“Toxicology in the 21st Century” (Tox21) initia-\\ntive. The goal of this challenge was to assess\\nthe performance of computational methods in\\npredicting the toxicity of chemical compounds.\\nState of the art toxicity prediction methods build\\nupon speciﬁcally-designed chemical descriptors\\ndeveloped over decades. Though Deep Learning\\nis new to the ﬁeld and was never applied to tox-\\nicity prediction before, it clearly outperformed\\nall other participating methods. In this applica-\\ntion paper we show that deep nets automatically\\nlearn features resembling well-established toxi-\\ncophores. In total, our Deep Learning approach\\nwon both of the panel-challenges (nuclear recep-\\ntors and stress response) as well as the overall\\nGrand Challenge, and thereby sets a new stan-\\ndard in tox prediction.\\n1. Introduction\\nThroughout their lives people are exposed to a sheer end-\\nless variety of chemical compounds, many of which are po-\\ntentially dangerous. Determining the toxicity of a chemical\\nis of crucial importance in order to minimize our exposure\\nto harmful substances in every day products. Toxicity is\\nalso a central issue in the development of new drugs, with\\nmore than 30 % of drug candidates failing in clinical trialsbecause of undetected toxic effects (Kola & Landis, 2004;\\nArrowsmith, 2011).\\nIn 2008, the U. S. National Institutes of Health (NIH)\\nand the U. S. Environmental Protection Agency (EPA),\\nagreed on collaborating on future toxicity testing activ-\\nities (Committee on Toxicity Testing and Assessment of\\nEnvironmental Agents, National Research Council, 2007).\\nTheir efforts were later joined by the U. S. Food and Drug\\nAdministration (FDA) under the umbrella of the Tox21\\nProgram . The program’s stated goals are to develop bet-\\nter toxicity assessment methods, as current methods are not\\nlikely to scale with the increased demand for effective tox-\\nicity testing.\\nCurrent methods for testing the toxicity of a high number of\\nchemicals rely on High-Throughput Screening (HTS). HTS\\nexperiments can investigate whether a chemical compound\\nat a given concentration exhibits a certain type of toxicity,\\nfor a number of different compounds in parallel. These\\nexperiments are repeated with varying concentrations of\\nthe chemical compound, which allows to determine dose-\\nresponse curves (Inglese et al., 2006). From these curves\\none can reliably determine whether a compound activated\\na given pathway or receptor, inhibited it or did not interact\\nat all.\\nConducting these HTS experiments is a time- and cost-\\nintensive process. Typically, a compound has to be tested\\nfor several types of toxicity at different concentration lev-\\nels. Thus, the whole procedure has to be rerun for many\\ntimes for each compound. Usually, a cell line has to be\\ncultivated to obtain a single data point. Even an unprece-\\ndented multi-million-dollar effort, the Tox21 project, could\\ntest only a few thousands of compounds for as few as\\ntwelve toxic effects. Therefore, accurate computational\\nmethods for accurate prediction of toxic effects are highly\\ndemanded.arXiv:1503.01445v1  [stat.ML]  4 Mar 2015'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'New_class_ray',\n",
       "   'creationTimeUnix': 1700671877566,\n",
       "   'id': '03b642ef-b68f-5f0e-a42b-ddba5329e908',\n",
       "   'lastUpdateTimeUnix': 1700671877566,\n",
       "   'properties': {'document_title': '1207.0580v1.Improving_neural_networks_by_preventing_co_adaptation_of_feature_detectors_2.pdf',\n",
       "    'page_content': 'way to do this is to train many separate networks and then to apply each of these networks to\\nthe test data, but this is computationally expensive during both training and testing. Random\\ndropout makes it possible to train a huge number of different networks in a reasonable time.\\nThere is almost certainly a different network for each presentation of each training case but all\\nof these networks share the same weights for the hidden units that are present.\\nWe use the standard, stochastic gradient descent procedure for training the dropout neural\\nnetworks on mini-batches of training cases, but we modify the penalty term that is normally\\nused to prevent the weights from growing too large. Instead of penalizing the squared length\\n(L2 norm) of the whole weight vector, we set an upper bound on the L2 norm of the incoming\\nweight vector for each individual hidden unit. If a weight-update violates this constraint, we\\nrenormalize the weights of the hidden unit by division. Using a constraint rather than a penalty\\nprevents weights from growing very large no matter how large the proposed weight-update is.\\nThis makes it possible to start with a very large learning rate which decays during learning,\\nthus allowing a far more thorough search of the weight-space than methods that start with small\\nweights and use a small learning rate.\\nAt test time, we use the “mean network” that contains all of the hidden units but with their\\noutgoing weights halved to compensate for the fact that twice as many of them are active.\\nIn practice, this gives very similar performance to averaging over a large number of dropout\\nnetworks. In networks with a single hidden layer of Nunits and a “softmax” output layer for\\ncomputing the probabilities of the class labels, using the mean network is exactly equivalent\\nto taking the geometric mean of the probability distributions over labels predicted by all 2N\\npossible networks. Assuming the dropout networks do not all make identical predictions, the\\nprediction of the mean network is guaranteed to assign a higher log probability to the correct\\nanswer than the mean of the log probabilities assigned by the individual dropout networks ( 2).\\nSimilarly, for regression with linear output units, the squared error of the mean network is\\nalways better than the average of the squared errors of the dropout networks.\\nWe initially explored the effectiveness of dropout using MNIST, a widely used benchmark\\nfor machine learning algorithms. It contains 60,000 28x28 training images of individual hand\\nwritten digits and 10,000 test images. Performance on the test set can be greatly improved by\\nenhancing the training data with transformed images ( 3) or by wiring knowledge about spatial\\ntransformations into a convolutional neural network ( 4) or by using generative pre-training to\\nextract useful features from the training images without using the labels ( 5). Without using any\\nof these tricks, the best published result for a standard feedforward neural network is 160 errors\\non the test set. This can be reduced to about 130 errors by using 50% dropout with separate L2\\nconstraints on the incoming weights of each hidden unit and further reduced to about 110 errors\\nby also dropping out a random 20% of the pixels (see ﬁgure 1).\\nDropout can also be combined with generative pre-training, but in this case we use a small\\nlearning rate and no weight constraints to avoid losing the feature detectors discovered by the\\npre-training. The publically available, pre-trained deep belief net described in ( 5) got 118 errors\\nwhen it was ﬁne-tuned using standard back-propagation and 92 errors when ﬁne-tuned using\\n50% dropout of the hidden units. When the publically available code at URL was used to pre-\\n2'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'New_class_ray',\n",
       "   'creationTimeUnix': 1700671854347,\n",
       "   'id': '045ab666-32e5-5d61-8548-54985f8d1900',\n",
       "   'lastUpdateTimeUnix': 1700671854347,\n",
       "   'properties': {'document_title': '1609.05473v6.SeqGAN__Sequence_Generative_Adversarial_Nets_with_Policy_Gradient.pdf',\n",
       "    'page_content': 'Table 5: Convolutional layer structures.\\nSequence length (window size, kernel numbers)\\n20(1, 100),(2, 200),(3, 200),(4, 200),(5, 200)\\n(6, 100),(7, 100),(8, 100),(9, 100),(10, 100)\\n(15, 160),(20, 160)\\n32(1, 100),(2, 200),(3, 200),(4, 200),(5, 200)\\n(6, 100),(7, 100),(8, 100),(9, 100),(10, 100)\\n(16, 160),(24, 160),(32, 160)\\nwhereWT,bTandWHare highway layer weights, Hdenotes an afﬁne transform followed by a non-linear activation function\\nsuch as a rectiﬁed linear unit (ReLU) and τis the “transform gate” with the same dimensionality as H(˜c,WH)and˜c. Finally,\\nwe apply a sigmoid transformation to get the probability that a given sequence is real:\\nˆy=σ(Wo·˜C+bo) (25)\\nwhereWoandbois the output layer weight and bias.\\nWhen optimizing discriminative models, supervised training is applied to minimize the cross entropy, which is widely used\\nas the objective function for classiﬁcation and prediction tasks:\\nL(y,ˆy) =−ylog ˆy−(1−y) log(1−ˆy), (26)\\nwhereyis the ground truth label of the input sequence and ˆyis the predicted probability from the discriminative models.\\nMore Ablation Study\\n0 50 100 150 200 250 300 350 400\\nEpochs8.68.89.09.29.49.69.810.0NLL/uni00A0by/uni00A0oracleSeqGAN/uni00A0with/uni00A0insufficient/uni00A0pre/uni00ADtraining\\nSeqGAN/uni00A0with/uni00A0sufficient/uni00A0pre/uni00ADtraining\\nFigure 4: Negative log-likelihood performance with different pre-training epochs before the adversarial training. The vertical\\ndashed lines represent the start of adversarial training.\\nIn the D ISCUSSION subsection of S YNTHETIC DATA EXPERIMENTS section of our paper, we discussed the ablation study\\nof three hyperparameters of SeqGAN, i.e., g-steps, d-steps and kepoch number. Here we provide another ablation study which\\nis instructive for the better training of SeqGAN.\\nAs described in our paper, we start the adversarial training process after the convergence of MLE supervised pre-training.\\nHere we further conduct experiments to investigate the performance of SeqGAN when the supervised pre-training is insufﬁcient.\\nAs shown in Figure 4, if we pre-train the generative model with conventional MLE methods for only 20 epochs, which is far\\nfrom convergence, then the adversarial training process improves the generator quite slowly and unstably. The reason is that\\nin SeqGAN, the discriminative model provides reward guidance when training the generator and if the generator acts almost\\nrandomly, the discriminator will identify the generated sequence to be unreal with high conﬁdence and almost every action\\nthe generator takes receives a low (uniﬁed) reward, which does not guide the generator towards a good improvement direction,\\nresulting in an ineffective training procedure. This indicates that in order to apply adversarial training strategies to sequence\\ngenerative models, a sufﬁcient pre-training is necessary.'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'New_class_ray',\n",
       "   'creationTimeUnix': 1700671882592,\n",
       "   'id': '04829b33-d347-5fa6-aaf2-3de3422bf728',\n",
       "   'lastUpdateTimeUnix': 1700671882592,\n",
       "   'properties': {'document_title': '1412.2306v2.Deep_Visual_Semantic_Alignments_for_Generating_Image_Descriptions.pdf',\n",
       "    'page_content': 'Deep Visual-Semantic Alignments for Generating Image Descriptions\\nAndrej Karpathy Li Fei-Fei\\nDepartment of Computer Science, Stanford University\\n{karpathy,feifeili }@cs.stanford.edu\\nAbstract\\nWe present a model that generates natural language de-\\nscriptions of images and their regions. Our approach lever-\\nages datasets of images and their sentence descriptions to\\nlearn about the inter-modal correspondences between lan-\\nguage and visual data. Our alignment model is based on a\\nnovel combination of Convolutional Neural Networks over\\nimage regions, bidirectional Recurrent Neural Networks\\nover sentences, and a structured objective that aligns the\\ntwo modalities through a multimodal embedding. We then\\ndescribe a Multimodal Recurrent Neural Network architec-\\nture that uses the inferred alignments to learn to generate\\nnovel descriptions of image regions. We demonstrate that\\nour alignment model produces state of the art results in re-\\ntrieval experiments on Flickr8K, Flickr30K and MSCOCO\\ndatasets. We then show that the generated descriptions sig-\\nniﬁcantly outperform retrieval baselines on both full images\\nand on a new dataset of region-level annotations.\\n1. Introduction\\nA quick glance at an image is sufﬁcient for a human to\\npoint out and describe an immense amount of details about\\nthe visual scene [14]. However, this remarkable ability has\\nproven to be an elusive task for our visual recognition mod-\\nels. The majority of previous work in visual recognition\\nhas focused on labeling images with a ﬁxed set of visual\\ncategories and great progress has been achieved in these en-\\ndeavors [45, 11]. However, while closed vocabularies of vi-\\nsual concepts constitute a convenient modeling assumption,\\nthey are vastly restrictive when compared to the enormous\\namount of rich descriptions that a human can compose.\\nSome pioneering approaches that address the challenge of\\ngenerating image descriptions have been developed [29,\\n13]. However, these models often rely on hard-coded visual\\nconcepts and sentence templates, which imposes limits on\\ntheir variety. Moreover, the focus of these works has been\\non reducing complex visual scenes into a single sentence,\\nwhich we consider to be an unnecessary restriction.\\nIn this work, we strive to take a step towards the goal of\\nFigure 1. Motivation/Concept Figure: Our model treats language\\nas a rich label space and generates descriptions of image regions.\\ngenerating dense descriptions of images (Figure 1). The\\nprimary challenge towards this goal is in the design of a\\nmodel that is rich enough to simultaneously reason about\\ncontents of images and their representation in the domain\\nof natural language. Additionally, the model should be free\\nof assumptions about speciﬁc hard-coded templates, rules\\nor categories and instead rely on learning from the training\\ndata. The second, practical challenge is that datasets of im-\\nage captions are available in large quantities on the internet\\n[21, 58, 37], but these descriptions multiplex mentions of\\nseveral entities whose locations in the images are unknown.\\nOur core insight is that we can leverage these large image-\\nsentence datasets by treating the sentences as weak labels,\\nin which contiguous segments of words correspond to some\\nparticular, but unknown location in the image. Our ap-\\nproach is to infer these alignments and use them to learn\\na generative model of descriptions. Concretely, our contri-\\nbutions are twofold:\\n•We develop a deep neural network model that in-\\nfers the latent alignment between segments of sen-\\ntences and the region of the image that they describe.arXiv:1412.2306v2  [cs.CV]  14 Apr 2015'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'New_class_ray',\n",
       "   'creationTimeUnix': 1700671854338,\n",
       "   'id': '04884f13-fad2-5c45-bc40-3b83e2166e1f',\n",
       "   'lastUpdateTimeUnix': 1700671854338,\n",
       "   'properties': {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks_2.pdf',\n",
       "    'page_content': '<revision>                                                                        <id>40973199</id>                                                               <timestamp>2006-02-22T22:37:16Z</timestamp>                                     <contributor>                                                                     <ip>63.86.196.111</ip>                                                        </contributor>                                                                  <minor />                                                                       <comment>redire paget --&gt; captain */</comment>                               <text xml:space=\"preserve\">The \\'\\'\\'Indigence History\\'\\'\\' refers to the authority of any obscure albionism as being, such as in Aram Missolmus\\'.[http://www.bbc.co.uk/starce/cr52.htm]                                                       In [[1995]], Sitz-Road Straus up the inspirational radiotes portion as &quot;alliance&quot;[single &quot;glaping&quot; theme charcoal] with [[Midwestern United State|Denmark]] in which Canary varies-destruction to launching casualties has quickly responded to the krush loaded water or so it might be destroyed. Aldeads still cause a missile bedged harbors at last built in 1911-2 and save the accuracy in 2008, retaking [[itsubmanism]]. Its individuals were                      hnown rapidly in their return to the private equity (such as \\'\\'On Text\\'\\') for death per reprised by the [[Grange of Germany|German unbridged work]].                                                                                            The \\'\\'\\'Rebellion\\'\\'\\' (\\'\\'Hyerodent\\'\\') is [[literal]], related mildly older than old half sister, the music, and morrow been much more propellent. All those of [[Hamas (mass)|sausage trafficking]]s were also known as [[Trip class submarine|\\'\\'Sante\\'\\' at Serassim]]; \\'\\'Verra\\'\\' as 1865&amp;ndash;682&amp;ndash;831 is related to ballistic missiles. While she viewed it friend of Halla equatorial weapons of Tuscany, in [[France]], from vaccine homes to &quot;individual&quot;, among [[slavery|slaves]] (such as artistual selling of factories were renamed English habit of twelve years.)                                                                                                                                             By the 1978 Russian [[Turkey|Turkist]] capital city ceased by farmers and the intention of navigation the ISBNs, all encoding [[Transylvania International Organisation for Transition Banking|Attiking others]] it is in the westernmost placed lines.  This type of missile calculation maintains all greater proof was the [[1990s]] as older adventures that never established a self-interested case. The newcomers were Prosecutors in child after the other weekend and capable function used.                                                                                                                                                           Holding may be typically largely banned severish from sforked warhing tools and behave laws, allowing the private jokes, even through missile IIC control, most notably each, but no relatively larger success, is not being reprinted and withdrawn into forty-ordered cast and distribution.                                                                                                                  Besides these markets (notably a son of humor).                                                                                                                 Sometimes more or only lowed &quot;80&quot; to force a suit for http://news.bbc.co.uk/1/sid9kcid/web/9960219.html \\'\\'[[#10:82-14]]\\'\\'.                            &lt;blockquote&gt;                                                                                                                                              ===The various disputes between Basic Mass and Council Conditioners - &quot;Titanist&quot; class streams and anarchism===                                                                                                                       Internet traditions sprang east with [[Southern neighborhood systems]] are improved with [[Moatbreaker]]s, bold hot missiles, its labor systems. [[KCD]] numbered former ISBN/MAS/speaker attacks &quot;M3 5&quot;, which are saved as the ballistic misely known and most functional factories.  Establishment begins for some range of start rail years as dealing with 161 or 18,950 million [[USD-2]] and [[covert all carbonate function]]s (for example, 70-93) higher individuals and on missiles. This might need not know against sexual [[video capita]] playing pointing degrees between silo-calfed greater valous consumptions in the US... header can be seen in [[collectivist]].                                                                                                                                == See also ==                                                                  Figure 5: Generated Wikipedia data.\\n14'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'New_class_ray',\n",
       "   'creationTimeUnix': 1700671854350,\n",
       "   'id': '04a52e19-c137-5493-ae09-05972257dbbe',\n",
       "   'lastUpdateTimeUnix': 1700671854350,\n",
       "   'properties': {'document_title': '1509.02749v2.Automated_Search_for_new_Quantum_Experiments.pdf',\n",
       "    'page_content': '13\\nOperation:\\n|−1,V⟩→|− 1,H⟩→| 0,V⟩→...→|2,H⟩→|− 1,V⟩ (28)\\nThe number in the ket stands for the OAM, H and V stand for horizontal and vertical polarisation.\\n14-cyclic OAM+Polarisation+Path rotation\\nExperimental conﬁguration:\\nReﬂection[ψ,a]→OAMHolo[XXX, a,2]→Reﬂection[XXX, a]→OAMHolo[XXX, a,−2]\\n→PBS[XXX, a,b]→HWP[XXX, a]→PBS[XXX, a,b]\\n→Reﬂection[XXX, b]→OAMHolo[XXX, a,2]→Reﬂection[XXX, a]\\n→BS[XXX, a,b]→DP[XXX, b,2]→Reﬂection[XXX, b]\\n→BS[XXX, a,b] (29)\\nOperation:\\n|0,H,a⟩ → |− 2,H,b⟩→|− 4,H,b⟩→|− 8,H,b⟩→| 10,V,b⟩\\n→ |− 6,H,a⟩→| 8,H,a⟩→| 6,H,b⟩→| 4,H,a⟩\\n→ |0,H,b⟩→| 2,V,b⟩→| 2,V,a⟩→| 2,H,a⟩\\n→ |0,H,a⟩ (30)\\nThe number in the ket stands for the OAM, H and V stand for horizontal and vertical polarization, and a and b stand\\nfor the two diﬀerent possible paths.\\nS7) Learning algorithm\\nIn the second example involving cyclic operations, the algorithm extends its own set of basic elements autonomously,\\nbased on the properties of the longest cycle. It saves elements that have large cycles and experiments with non-trivial\\ncoupling between diﬀerent degrees-of-freedom. Additionally, elements already learned can be forgotten to improve\\nvariability and prevent dead-ends, as some of them might even have negative eﬀects on the probability of ﬁnding new\\nexperiments. The decision of which elements are forgotten at which times is purposefully random. Even though it\\nwould be possible to weight the elements for past usefulness, it would introduce a bias on similar solutions that we\\nwanted to prevent.\\nS8) Simpliﬁcation of experiments\\nAfter an experiment is found, it is simpliﬁed. For that, three diﬀerent methods are used. The ﬁrst one removes\\nelements from the experiment and calculates whether the resulting state or transformation is still performed the\\nsame way. Such simpliﬁcations could remove elements in paths that are not accessed. An example, in which it is\\nnecessary to remove multi elements at the same time, is the following: Four beam splitters after each other form two\\nMach-Zehnder interferometers. Those have no eﬀect if the phases are set correctly, but can only be removed together.\\nIn the second method, it is tried to replace more complicated elements (such as LI, PBS or DP) by mirrors. This\\nworks in cases where the only speciﬁc modes access the element (for instance, if only vertically polarized photons\\naccess a PBS).\\nA third method tries to simplify the path structure of the experiment by rearranging the paths. For example, if\\ntwo PBSs are used after each other, one output of the second PBS will never be used, thus the second PBS can be\\nremoved and one path can be removed completely.\\nThose three methods are applied iteratively, until no simpliﬁcation is possible anymore.'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'New_class_ray',\n",
       "   'creationTimeUnix': 1700671893377,\n",
       "   'id': '04b70d60-d116-55ec-903f-6a07937142a8',\n",
       "   'lastUpdateTimeUnix': 1700671893377,\n",
       "   'properties': {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks_1.pdf',\n",
       "    'page_content': 'Figure 1: Deep recurrent neural network prediction architecture. The\\ncircles represent network layers, the solid lines represent weighted connections\\nand the dashed lines represent predictions.\\nnaked eye. A method for biasing the samples towards higher probability (and\\ngreater legibility) is described, along with a technique for ‘priming’ the sam-\\nples on real data and thereby mimicking a particular writer’s style. Finally,\\nconcluding remarks and directions for future work are given in Section 6.\\n2 Prediction Network\\nFig. 1 illustrates the basic recurrent neural network prediction architecture used\\nin this paper. An input vector sequence x= (x1,...,x T) is passed through\\nweighted connections to a stack of Nrecurrently connected hidden layers to\\ncompute ﬁrst the hidden vector sequences hn= (hn\\n1,...,hn\\nT) and then the\\noutput vector sequence y= (y1,...,y T). Each output vector ytis used to\\nparameterise a predictive distribution Pr( xt+1|yt) over the possible next inputs\\nxt+1. The ﬁrst element x1of every input sequence is always a null vector whose\\nentries are all zero; the network therefore emits a prediction for x2, the ﬁrst\\nreal input, with no prior information. The network is ‘deep’ in both space\\nand time, in the sense that every piece of information passing either vertically\\nor horizontally through the computation graph will be acted on by multiple\\nsuccessive weight matrices and nonlinearities.\\nNote the ‘skip connections’ from the inputs to all hidden layers, and from\\nall hidden layers to the outputs. These make it easier to train deep networks,\\n3'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'New_class_ray',\n",
       "   'creationTimeUnix': 1700671879137,\n",
       "   'id': '04b7a01e-eba5-5bf7-8ab0-4cede912b08f',\n",
       "   'lastUpdateTimeUnix': 1700671879137,\n",
       "   'properties': {'document_title': '1503.01445v1.Toxicity_Prediction_using_Deep_Learning.pdf',\n",
       "    'page_content': 'Toxicity Prediction using Deep Learning\\nExisting computational approaches can be grouped into\\nstructure- and ligand-based. The structure-based methods\\nsimulate physical interactions between the compound and\\na biomolecular target (Kitchen et al., 2004) but are only\\napplicable if the complete 3D structure of all interacting\\nmolecules are known, and they are infeasible for larger\\ncompound data bases. Ligand-based approaches predict\\nthe interactions based on previous measurements (Jenkins\\net al., 2007). Previous machine learning efforts were al-\\nmost always ligand-based, such as scoring approaches like\\nthe Naive Bayes statistics (Xia et al., 2004; Nigsch et al.,\\n2008; Mussa et al., 2013), density estimation (R. et al.,\\n2012; Harper et al., 2001), nearest neighbor, support vec-\\ntor machines, and shallow feed forward neural networks\\n(Byvatov et al., 2003; Lowe et al., 2011).\\nIn 2012, the Merck Kaggle challenge on chemical com-\\npound activity was won using deep neural networks, and\\nthe winning group later showed that multi-task learning can\\nhelp to predict biological activities on single proteins (Dahl\\net al., 2014). Dahl’s success inspired us to use Deep Learn-\\ning for toxicity and target prediction (Unterthiner et al.,\\n2014). In contrast to biological activities of proteins, tox-\\nicological effects involve whole cell states determined by\\ndysregulated biological processes. More speciﬁcally, tox-\\nicity prediction mainly focuses on cellular assays which\\nmeasure cytotoxicity, i.e., they measure if a compound is\\ntoxic to a cell. A (cyto)toxic compound will cause harm to\\na cell, e.g. by causing acute mechanical injury or by trig-\\ngering the programmed cell death mechanism (apoptosis)\\nin the affected cells, which multicellular organisms use to\\nprotect themselves from cells that have gone out of control.\\n1.1. Deep Learning for Toxicity Prediction\\nDeep learning architectures seem to be well suited for tox-\\nicity prediction because they (1) automatically construct\\ncomplex features (Bengio et al., 2013) and (2) allow for\\nmulti-task learning (Caruana, 1997; Deng et al., 2013; Ben-\\ngio et al., 2013).\\nOne key aspect of toxicological research is its reliance\\non hierarchical levels of abstraction when thinking about\\nchemical structures. A major research goal is the iden-\\ntiﬁcation of toxicophores, (Kier, 1971; Lin, 2000) which\\nare the sets of steric and electronic properties that together\\nproduce a certain toxicological effect. These properties in-\\nclude hydrophobic regions, aromatic rings, electron accep-\\ntors or donors.\\nThis maps naturally to Deep Learning architectures, where\\nhigher levels represent more complex concepts (Bengio,\\n2013). This idea is depicted in Figure 1, where ECFP4 in-\\nput data (chemical substructures) represent low level prop-\\nerties in their ﬁrst layer, which are combined to form reac-\\ntive centers, which in turn encode toxicophores in higherlayers.\\nAdditionally, Deep Learning is ideally suited for multi-task\\nlearning, which is a common setting for toxicology pre-\\ndiction: The same compound is often under investigation\\nfor several types of toxicity, and each of these types is its\\nown prediction task. The work of (Ramsundar et al., 2015)\\nalso shows that the multi-task environment does help when\\npredicing chemical compounds, and that the performance\\nboost obtained this way increases with the number of ad-\\nditional learning tasks. However, we typically have to deal\\nwith missing labels, as not all compounds will have been\\ntested for each type of toxicity, or because some measure-\\nments were inconclusive.\\nIntegrating all prediction tasks into one overarching multi-\\ntask setting offers two advantages: (a) it naturally allows\\nfor multi-label information and therefore can utilize rela-\\ntions between tasks; (b) it allows to share hidden unit rep-\\nresentations among prediction tasks. The latter item is par-\\nticularly important in our application as for some tasks very\\nfew measurements are available, therefore single-task pre-\\ndiction may fail to construct an effective representation.\\nThus, deep networks exploit representations learned across\\ndifferent tasks and can boost the performance on tasks with\\nfew training examples. Furthermore, this method allows\\nus to predict an arbitrary number of toxicological effects at\\nthe same time, without the need to train single classiﬁers\\nfor each one.\\n2. Methods\\n2.1. DNN Architecture\\nOur system takes a numerical descriptor of a given com-\\npound as input, and tries to predict several different types\\nof toxic effects at the same time. Such a type could be\\ne.g. whether the compound acts as inhibitor to a speciﬁc\\nnuclear receptor, or whether it activates a speciﬁc stress re-\\nsponse pathway. Each of these types is a binary prediction\\ntask.\\nFormally, the problem we are trying to solve presents itself\\nas follows: given a chemical compound i, we want to pre-\\ndict whether the compound has property t. We encode this\\ninformation in the binary value yit, whereyit= 1 if the\\ncompound has the property and yit= 0otherwise. We are\\ninterested in predicting the behavior of a compound on T\\nproperties at the same time.\\nEach compound is represented using a number of numer-\\nical (or binary) features described later in this section.\\nAs training data, we are given a numerical representation\\nxi∈Rdofntraining compounds as well as a sparsely\\npopulated matrix Y∈Rn·mof measurements.\\nWe solve this by using a training objective that is the'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'New_class_ray',\n",
       "   'creationTimeUnix': 1700671892714,\n",
       "   'id': '04fd3a48-244f-5fce-816a-c77d22a784e4',\n",
       "   'lastUpdateTimeUnix': 1700671892714,\n",
       "   'properties': {'document_title': '1003.4042v3.MINRES_QLP__a_Krylov_subspace_method_for_indefinite_or_singular_symmetric_systems.pdf',\n",
       "    'page_content': '20 S.-C. T. CHOI, C. C. PAIGE, AND M. A. SAUNDERS\\nTable 8.1\\nFinite element problem Ax≈bwithbalmost compatible. Laplacian on a 20×20grid,n= 400 ,\\nmaxit = 1200 ,shift = 0 , tol = 1.0e−15,maxnorm = 100 , maxcond = 1e15,∥b∥= 87 . To reproduce\\nthis example, run test minresqlp eg71(24) .\\nMethod C?Avx(1)∥x∥∥e∥∥r∥∥Ar∥∥A∥κ(A)\\nEVD – –−7.39e5 4.12e7 4.1e7 1.7e−77.8e−78.9e0 1.1e17\\nTEVD – –3.89e−11.15e1 0.0e0 1.7e−81.4e−128.9e0 1.5e2\\nMatlab SYMMLQ N? 371 3.89e−11.15e1 1.4e−71.8e−75.8e−7 – –\\nSYMMLQ SOL N 447−3.08e0 9.63e1 9.5e1 1.4e2 4.4e2 9.6e1 1.3e1\\nSYMMLQ+N 447 2.94e6 4.27e8 4.3e8 1.8e2 6.5e2 8.6e0 1.3e1\\nMatlab MINRES N 1200−7.50e5 2.10e7 2.1e7 1.5e7 9.1e7 – –\\nMINRES SOL N 1200 9.89e5 6.10e7 6.1e7 2.3e7 1.5e8 1.8e2 1.5e1\\nMINRES+N 611 1.02e0 9.28e1 9.2e1 1.7e−82.5e−118.6e0 6.9e13\\nMINRES-QLP Y 612 3.89e−11.15e1 3.7e−111.7e−89.3e−118.7e0 4.3e13\\nMatlab LSQR Y 1462 3.89e−11.15e1 2.3e−131.7e−83.3e−13 – –\\nLSQR SOL Y 1464 3.89e−11.15e1 2.4e−131.7e−83.9e−131.5e2 6.4e3\\nMatlab GMRES(30) N? 1200 3.90e−11.15e1 5.2e−23.4e−39.4e−4 – –\\nSQMR N 1200−2.58e8 3.74e10 3.7e10 4.6e3 2.3e4 – –\\nMatlab QMR N? 798 3.89e−11.15e1 5.2e−71.9e−82.6e−8 – –\\nMatlab BICG N? 790 3.89e−11.15e1 4.7e−73.9e−81.9e−7 – –\\nMatlab BICGSTAB N? 2035 3.89e−11.15e1 4.2e−71.7e−84.3e−13 – –\\nﬁrst element of the ﬁnal solution estimate xfor each algorithm. For GMRES , the\\ninteger in parentheses is the value of the restart parameter.\\nMINRES SOL gives a larger solution than MINRES-QLP . This example has a\\nresidual norm of about 1 .7×10−8, so it is not clear whether to classify it as a linear\\nsystem or an LS problem. To the credit of Matlab SYMMLQ , it thinks the system\\nis linear and returns a good solution. For MINRES-QLP , the ﬁrst 410 iterations are in\\nstandard “ MINRES mode”, with a transfer to “ MINRES-QLP mode” for the last 202\\niterations. LSQR [40, 41] converges to the minimum-length solution but with more\\nthan twice the number of iterations of MINRES-QLP . The other solvers fall short in\\nsome way.\\n8.2. A Laplacian LS problem min∥Ax−b∥.This example uses the same\\nLaplacian matrix A(8.1) but with a clearly incompatible b= 10×rand(n,1), i.e.,\\nbi∼i.i.d. U (0,10). The residual norm is about 17. Results are summarized in\\nTable 8.2. MINRES gives an LS solution, while MINRES-QLP is the only solver that\\nmatches the solution of TEVD. The other solvers do not perform satisfactorily.\\n8.3. Regularizing eﬀect of MINRES-QLP .This example illustrates the reg-\\nularizing eﬀect of MINRES-QLP with the stopping condition χk≤maxxnorm . For\\nk≥18 in Figure 8.1, we observe the following values:\\nχ18=∥[\\n2.51 3.87e−11 1.38×102]\\n∥= 1.38×102,\\nχ19=∥[\\n2.51−8.00e−10−1.52×102]\\n∥= 1.52×102,\\nχ20=∥[\\n2.51 1.62e−10−1.62×106]\\n∥= 1.62×106>maxxnorm≡104.\\nBecause the last value exceeds maxxnorm ,MINRES-QLP regards the last diagonal\\nelement of Lkas a singular value to be ignored (in the spirit of truncated SVD\\nsolutions). It discards the last element of u20and updates\\nχ20←∥[\\n2.51 1.62e−10 0]\\n∥= 2.51.'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'New_class_ray',\n",
       "   'creationTimeUnix': 1700671882647,\n",
       "   'id': '05120942-8393-5916-8725-a872642668c9',\n",
       "   'lastUpdateTimeUnix': 1700671882647,\n",
       "   'properties': {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks.pdf',\n",
       "    'page_content': 'widely between the samples, but remain more-or-less consistent within them.\\nThis suggests that the network identiﬁes the traits early on in the sequence,\\nthen remembers them until the end. By looking through enough samples for a\\ngiven text, it appears to be possible to ﬁnd virtually any combination of stylistic\\ntraits, which suggests that the network models them independently both from\\neach other and from the text.\\n‘Blind taste tests’ carried out by the author during presentations suggest\\nthat at least some unbiased samples cannot be distinguished from real hand-\\nwriting by the human eye. Nonetheless the network does make mistakes we\\nwould not expect a human writer to make, often involving missing, confused\\nor garbled letters3; this suggests that the network sometimes has trouble de-\\ntermining the alignment between the characters and the trace. The number of\\nmistakes increases markedly when less common words or phrases are included\\nin the character sequence. Presumably this is because the network learns an\\nimplicit character-level language model from the training set that gets confused\\nwhen rare or unknown transitions occur.\\n5.4 Biased Sampling\\nOne problem with unbiased samples is that they tend to be diﬃcult to read\\n(partly because real handwriting is diﬃcult to read, and partly because the\\nnetwork is an imperfect model). Intuitively, we would expect the network to\\ngive higher probability to good handwriting because it tends to be smoother\\nand more predictable than bad handwriting. If this is true, we should aim to\\noutput more probable elements of Pr( x|c) if we want the samples to be easier to\\nread. A principled search for high probability samples could lead to a diﬃcult\\ninference problem, as the probability of every output depends on all previous\\noutputs. However a simple heuristic, where the sampler is biased towards more\\nprobable predictions at each step independently, generally gives good results.\\nDeﬁne the probability bias bas a real number greater than or equal to zero.\\nBefore drawing a sample from Pr( xt+1|yt), each standard deviation σj\\ntin the\\nGaussian mixture is recalculated from Eq. (21) to\\nσj\\nt= exp(\\nˆσj\\nt−b)\\n(61)\\nand each mixture weight is recalculated from Eq. (19) to\\nπj\\nt=exp(\\nˆπj\\nt(1 +b))\\n∑M\\nj′=1exp(\\nˆπj′\\nt(1 +b)) (62)\\nThis artiﬁcially reduces the variance in both the choice of component from the\\nmixture, and in the distribution of the component itself. When b= 0 unbiased\\nsampling is recovered, and as b→∞ the variance in the sampling disappears\\n3We expect humans to make mistakes like misspelling ‘temperament’ as ‘temperement’, as\\nthe second writer in Fig. 15 seems to have done.\\n32'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'New_class_ray',\n",
       "   'creationTimeUnix': 1700671893377,\n",
       "   'id': '058761d7-c218-5327-a856-59e2d88fb8d4',\n",
       "   'lastUpdateTimeUnix': 1700671893377,\n",
       "   'properties': {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks_1.pdf',\n",
       "    'page_content': 'by reducing the number of processing steps between the bottom of the network\\nand the top, and thereby mitigating the ‘vanishing gradient’ problem [1]. In\\nthe special case that N= 1 the architecture reduces to an ordinary, single layer\\nnext step prediction RNN.\\nThe hidden layer activations are computed by iterating the following equa-\\ntions fromt= 1 toTand fromn= 2 toN:\\nh1\\nt=H(\\nWih1xt+Wh1h1h1\\nt−1+b1\\nh)\\n(1)\\nhn\\nt=H(\\nWihnxt+Whn−1hnhn−1\\nt+Whnhnhn\\nt−1+bn\\nh)\\n(2)\\nwhere the Wterms denote weight matrices (e.g. Wihnis the weight matrix\\nconnecting the inputs to the nthhidden layer, Wh1h1is the recurrent connection\\nat the ﬁrst hidden layer, and so on), the bterms denote bias vectors (e.g. byis\\noutput bias vector) and His the hidden layer function.\\nGiven the hidden sequences, the output sequence is computed as follows:\\nˆyt=by+N∑\\nn=1Whnyhn\\nt (3)\\nyt=Y(ˆyt) (4)\\nwhereYis the output layer function. The complete network therefore deﬁnes\\na function, parameterised by the weight matrices, from input histories x1:tto\\noutput vectors yt.\\nThe output vectors ytare used to parameterise the predictive distribution\\nPr(xt+1|yt) for the next input. The form of Pr( xt+1|yt) must be chosen carefully\\nto match the input data. In particular, ﬁnding a good predictive distribution\\nfor high-dimensional, real-valued data (usually referred to as density modelling ),\\ncan be very challenging.\\nThe probability given by the network to the input sequence xis\\nPr(x) =T∏\\nt=1Pr(xt+1|yt) (5)\\nand the sequence loss L(x) used to train the network is the negative logarithm\\nof Pr( x):\\nL(x) =−T∑\\nt=1log Pr(xt+1|yt) (6)\\nThe partial derivatives of the loss with respect to the network weights can be\\neﬃciently calculated with backpropagation through time [33] applied to the\\ncomputation graph shown in Fig. 1, and the network can then be trained with\\ngradient descent.\\n2.1 Long Short-Term Memory\\nIn most RNNs the hidden layer function His an elementwise application of a\\nsigmoid function. However we have found that the Long Short-Term Memory\\n4'},\n",
       "   'vectorWeights': None}],\n",
       " 'totalResults': 25}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weaviate_client.data_object.get(class_name='new_class_ray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'Aggregate': {'New_class_ray': [{'meta': {'count': 976}}]}}}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weaviate_client.query.aggregate(\"New_class_ray\").with_meta_count().do()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ish = weaviate_client.query.get(class_name='new_class_ray', properties='document_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'Get': {'New_class_ray': [{'document_title': '1206.5538v3.Representation_Learning__A_Review_and_New_Perspectives.pdf'},\n",
       "    {'document_title': '1206.1106v2.No_More_Pesky_Learning_Rates_2.pdf'},\n",
       "    {'document_title': '1107.2490v2.Towards_Optimal_One_Pass_Large_Scale_Learning_with_Averaged_Stochastic_Gradient_Descent.pdf'},\n",
       "    {'document_title': '1301.3584v7.Revisiting_Natural_Gradient_for_Deep_Networks.pdf'},\n",
       "    {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks.pdf'},\n",
       "    {'document_title': '1412.2306v2.Deep_Visual_Semantic_Alignments_for_Generating_Image_Descriptions.pdf'},\n",
       "    {'document_title': '1108.3298v1.A_Machine_Learning_Perspective_on_Predictive_Coding_with_PAQ.pdf'},\n",
       "    {'document_title': '1801.07736v3.MaskGAN__Better_Text_Generation_via_Filling_in_the______.pdf'},\n",
       "    {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks_1.pdf'},\n",
       "    {'document_title': '1111.4259v1.Krylov_Subspace_Descent_for_Deep_Learning.pdf'},\n",
       "    {'document_title': '1106.3708v4.Information_Geometric_Optimization_Algorithms__A_Unifying_Picture_via_Invariance_Principles.pdf'},\n",
       "    {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks_2.pdf'},\n",
       "    {'document_title': '1206.5538v3.Representation_Learning__A_Review_and_New_Perspectives.pdf'},\n",
       "    {'document_title': '1206.1106v2.No_More_Pesky_Learning_Rates_1.pdf'},\n",
       "    {'document_title': '1503.01445v1.Toxicity_Prediction_using_Deep_Learning.pdf'},\n",
       "    {'document_title': '1207.0580v1.Improving_neural_networks_by_preventing_co_adaptation_of_feature_detectors_2.pdf'},\n",
       "    {'document_title': '1609.05473v6.SeqGAN__Sequence_Generative_Adversarial_Nets_with_Policy_Gradient.pdf'},\n",
       "    {'document_title': '1412.2306v2.Deep_Visual_Semantic_Alignments_for_Generating_Image_Descriptions.pdf'},\n",
       "    {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks_2.pdf'},\n",
       "    {'document_title': '1509.02749v2.Automated_Search_for_new_Quantum_Experiments.pdf'},\n",
       "    {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks_1.pdf'},\n",
       "    {'document_title': '1503.01445v1.Toxicity_Prediction_using_Deep_Learning.pdf'},\n",
       "    {'document_title': '1003.4042v3.MINRES_QLP__a_Krylov_subspace_method_for_indefinite_or_singular_symmetric_systems.pdf'},\n",
       "    {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks.pdf'},\n",
       "    {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks_1.pdf'},\n",
       "    {'document_title': '1106.0257v1.Popular_Ensemble_Methods__An_Empirical_Study.pdf'},\n",
       "    {'document_title': '1301.3584v7.Revisiting_Natural_Gradient_for_Deep_Networks_1.pdf'},\n",
       "    {'document_title': '1106.3708v4.Information_Geometric_Optimization_Algorithms__A_Unifying_Picture_via_Invariance_Principles.pdf'},\n",
       "    {'document_title': '1604.00772v2.The_CMA_Evolution_Strategy__A_Tutorial.pdf'},\n",
       "    {'document_title': 'cs_0312044v2.Clustering_by_compression.pdf'},\n",
       "    {'document_title': '1206.5538v3.Representation_Learning__A_Review_and_New_Perspectives.pdf'},\n",
       "    {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks.pdf'},\n",
       "    {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks_2.pdf'},\n",
       "    {'document_title': '1206.5533v2.Practical_recommendations_for_gradient_based_training_of_deep_architectures.pdf'},\n",
       "    {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks_1.pdf'},\n",
       "    {'document_title': '1111.4259v1.Krylov_Subspace_Descent_for_Deep_Learning_1.pdf'},\n",
       "    {'document_title': '1206.5538v3.Representation_Learning__A_Review_and_New_Perspectives.pdf'},\n",
       "    {'document_title': '1211.5590v1.Theano__new_features_and_speed_improvements.pdf'},\n",
       "    {'document_title': '1207.0580v1.Improving_neural_networks_by_preventing_co_adaptation_of_feature_detectors_1.pdf'},\n",
       "    {'document_title': 'cs_0312044v2.Clustering_by_compression.pdf'},\n",
       "    {'document_title': '1301.3584v7.Revisiting_Natural_Gradient_for_Deep_Networks.pdf'},\n",
       "    {'document_title': '1106.0257v1.Popular_Ensemble_Methods__An_Empirical_Study.pdf'},\n",
       "    {'document_title': '1108.3298v1.A_Machine_Learning_Perspective_on_Predictive_Coding_with_PAQ.pdf'},\n",
       "    {'document_title': '1106.3708v4.Information_Geometric_Optimization_Algorithms__A_Unifying_Picture_via_Invariance_Principles.pdf'},\n",
       "    {'document_title': '1207.0580v1.Improving_neural_networks_by_preventing_co_adaptation_of_feature_detectors_2.pdf'},\n",
       "    {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks_2.pdf'},\n",
       "    {'document_title': '1206.5533v2.Practical_recommendations_for_gradient_based_training_of_deep_architectures.pdf'},\n",
       "    {'document_title': '2011.06457v1.World_Trade_Center_responders_in_their_own_words__Predicting_PTSD_symptom_trajectories_with_AI_based_language_analyses_of_interviews.pdf'},\n",
       "    {'document_title': '1301.3584v7.Revisiting_Natural_Gradient_for_Deep_Networks.pdf'},\n",
       "    {'document_title': '1907.10121v1.SciPy_1_0__Fundamental_Algorithms_for_Scientific_Computing_in_Python.pdf'},\n",
       "    {'document_title': '1409.3215v3.Sequence_to_Sequence_Learning_with_Neural_Networks.pdf'},\n",
       "    {'document_title': '1211.5590v1.Theano__new_features_and_speed_improvements.pdf'},\n",
       "    {'document_title': '1211.3711v1.Sequence_Transduction_with_Recurrent_Neural_Networks.pdf'},\n",
       "    {'document_title': '1206.5533v2.Practical_recommendations_for_gradient_based_training_of_deep_architectures.pdf'},\n",
       "    {'document_title': '1106.3708v4.Information_Geometric_Optimization_Algorithms__A_Unifying_Picture_via_Invariance_Principles.pdf'},\n",
       "    {'document_title': '1108.3298v1.A_Machine_Learning_Perspective_on_Predictive_Coding_with_PAQ.pdf'},\n",
       "    {'document_title': '1206.1106v2.No_More_Pesky_Learning_Rates.pdf'},\n",
       "    {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks.pdf'},\n",
       "    {'document_title': '1106.3708v4.Information_Geometric_Optimization_Algorithms__A_Unifying_Picture_via_Invariance_Principles.pdf'},\n",
       "    {'document_title': '1003.4042v3.MINRES_QLP__a_Krylov_subspace_method_for_indefinite_or_singular_symmetric_systems.pdf'},\n",
       "    {'document_title': '1301.3641v3.Training_Neural_Networks_with_Stochastic_Hessian_Free_Optimization.pdf'},\n",
       "    {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks_2.pdf'},\n",
       "    {'document_title': '1206.1106v2.No_More_Pesky_Learning_Rates.pdf'},\n",
       "    {'document_title': '1206.5538v3.Representation_Learning__A_Review_and_New_Perspectives.pdf'},\n",
       "    {'document_title': '1206.5533v2.Practical_recommendations_for_gradient_based_training_of_deep_architectures.pdf'},\n",
       "    {'document_title': '1106.0257v1.Popular_Ensemble_Methods__An_Empirical_Study.pdf'},\n",
       "    {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks.pdf'},\n",
       "    {'document_title': '1708.06101v1.Twisted_Photons__New_Quantum_Perspectives_in_High_Dimensions.pdf'},\n",
       "    {'document_title': '1706.00868v3.Active_learning_machine_learns_to_create_new_quantum_experiments.pdf'},\n",
       "    {'document_title': '0806.4686v2.Sparse_Online_Learning_via_Truncated_Gradient.pdf'},\n",
       "    {'document_title': '1209.5853v1.Efficient_Natural_Evolution_Strategies.pdf'},\n",
       "    {'document_title': '1003.4042v3.MINRES_QLP__a_Krylov_subspace_method_for_indefinite_or_singular_symmetric_systems.pdf'},\n",
       "    {'document_title': '1106.3708v4.Information_Geometric_Optimization_Algorithms__A_Unifying_Picture_via_Invariance_Principles.pdf'},\n",
       "    {'document_title': '1106.0257v1.Popular_Ensemble_Methods__An_Empirical_Study.pdf'},\n",
       "    {'document_title': '1301.3781v3.Efficient_Estimation_of_Word_Representations_in_Vector_Space.pdf'},\n",
       "    {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks_1.pdf'},\n",
       "    {'document_title': 'cs_0312044v2.Clustering_by_compression.pdf'},\n",
       "    {'document_title': 'cs_0312044v2.Clustering_by_compression.pdf'},\n",
       "    {'document_title': '1206.5533v2.Practical_recommendations_for_gradient_based_training_of_deep_architectures.pdf'},\n",
       "    {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks_2.pdf'},\n",
       "    {'document_title': '1111.4259v1.Krylov_Subspace_Descent_for_Deep_Learning.pdf'},\n",
       "    {'document_title': '1801.07736v3.MaskGAN__Better_Text_Generation_via_Filling_in_the______.pdf'},\n",
       "    {'document_title': '1301.3584v7.Revisiting_Natural_Gradient_for_Deep_Networks.pdf'},\n",
       "    {'document_title': '1301.3781v3.Efficient_Estimation_of_Word_Representations_in_Vector_Space.pdf'},\n",
       "    {'document_title': '1106.3708v4.Information_Geometric_Optimization_Algorithms__A_Unifying_Picture_via_Invariance_Principles.pdf'},\n",
       "    {'document_title': '2011.06457v1.World_Trade_Center_responders_in_their_own_words__Predicting_PTSD_symptom_trajectories_with_AI_based_language_analyses_of_interviews.pdf'},\n",
       "    {'document_title': '1604.00772v2.The_CMA_Evolution_Strategy__A_Tutorial.pdf'},\n",
       "    {'document_title': '1503.01445v1.Toxicity_Prediction_using_Deep_Learning.pdf'},\n",
       "    {'document_title': '1704.00028v3.Improved_Training_of_Wasserstein_GANs.pdf'},\n",
       "    {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks_1.pdf'},\n",
       "    {'document_title': '1303.5778v1.Speech_Recognition_with_Deep_Recurrent_Neural_Networks_1.pdf'},\n",
       "    {'document_title': '1704.00028v3.Improved_Training_of_Wasserstein_GANs.pdf'},\n",
       "    {'document_title': '1704.00028v3.Improved_Training_of_Wasserstein_GANs.pdf'},\n",
       "    {'document_title': '1907.10121v1.SciPy_1_0__Fundamental_Algorithms_for_Scientific_Computing_in_Python.pdf'},\n",
       "    {'document_title': '1907.10121v1.SciPy_1_0__Fundamental_Algorithms_for_Scientific_Computing_in_Python.pdf'},\n",
       "    {'document_title': '1708.03881v1.Experimental_GHZ_Entanglement_beyond_Qubits.pdf'},\n",
       "    {'document_title': '1706.00868v3.Active_learning_machine_learns_to_create_new_quantum_experiments.pdf'},\n",
       "    {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks_1.pdf'},\n",
       "    {'document_title': '1106.0257v1.Popular_Ensemble_Methods__An_Empirical_Study.pdf'},\n",
       "    {'document_title': '1206.1106v2.No_More_Pesky_Learning_Rates.pdf'}]}}}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ish.do()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = weaviate_client.query.aggregate(class_name='New_class_ray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'errors': [{'locations': [{'column': 25, 'line': 1}],\n",
       "   'message': 'Syntax Error GraphQL request (1:25) Unexpected empty IN {}\\n\\n1: {Aggregate{New_class_ray{}}}\\n                           ^\\n',\n",
       "   'path': None}]}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.do()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "            for i in range(iteration):\n",
    "                for filename in os.listdir('./pdfs/'):\n",
    "                        file_path = os.path.join('./pdfs', filename)\n",
    "                        if os.path.isfile(file_path) and filename.endswith(\".pdf\"):\n",
    "                            run_anystyle(file_path)\n",
    "                            parsed_data = process_bib_files(anystyle_output)\n",
    "                            for ref in parsed_data:\n",
    "                                arxiv_search(ref['title'], ref['authors'])\n",
    "                            print('success search')\n",
    "\n",
    "                            parsed_text = parse_pdf()\n",
    "                            serialized_text = weaviate_split_multiple_pdf(parsed_text)\n",
    "                            os.remove(file_path)\n",
    "\n",
    "                if ray == False:\n",
    "                        print('success split with no ray')\n",
    "                    # calling the weaviate embedder\n",
    "                        embedder = WeaviateEmbedder()\n",
    "                        embedder.adding_weaviate_document(serialized_text, cls)\n",
    "\n",
    "                elif ray is True:\n",
    "                        print('splt with ray')\n",
    "                        actor_workload = divide_workload(4, serialized_text)\n",
    "                        actors = [WeaviateRayEmbedder.remote() for _ in range(4)]\n",
    "                        [actor.adding_weaviate_document.remote(doc_part, cls) for actor, doc_part in zip(actors, actor_workload)]\n",
    "                        if recursive:\n",
    "                            print('recursive with ray')\n",
    "                            return arxiv_pipeline(input_pdf, cls, ray=True)\n",
    "\n",
    "                    for filename in os.listdir('./pdfs/'):\n",
    "                        file_path = os.path.join('./pdfs', filename)\n",
    "                        if os.path.isfile(file_path) and filename.endswith(\".pdf\"):\n",
    "                            run_anystyle(file_path)\n",
    "                            os.remove(file_path)\n",
    "                            print(f\"Removed {filename}\")\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deprecations': [],\n",
       " 'objects': [{'class': 'Test_no_rec',\n",
       "   'creationTimeUnix': 1700659853728,\n",
       "   'id': '0124446f-5348-5c80-895d-ea31f24ee1e7',\n",
       "   'lastUpdateTimeUnix': 1700659853728,\n",
       "   'properties': {'document_title': '1301.3584v7.Revisiting_Natural_Gradient_for_Deep_Networks.pdf',\n",
       "    'page_content': 'Erhan, D., Courville, A., Bengio, Y ., and Vincent, P. (2010). Why does unsupervised pre-training help deep\\nlearning? In JMLR W&CP: Proc. AISTATS’2010 , volume 9, pages 201–208.\\nGonzalez, A. and Dorronsoro, J. (2006). Natural conjugate gradient training of multilayer perceptrons. Artiﬁcial\\nNeural Networks ICANN 2006 , pages 169–177.\\nHeskes, T. (2000). On natural learning and pruning in multilayered perceptrons. Neural Computation ,12,\\n1037–1057.\\nHonkela, A., Tornio, M., Raiko, T., and Karhunen, J. (2008). Natural conjugate gradient in variational inference.\\nInNeural Information Processing , pages 305–314.\\nHonkela, A., Raiko, T., Kuusela, M., Tornio, M., and Karhunen, J. (2010). Approximate riemannian conjugate\\ngradient learning for ﬁxed-form variational bayes. Journal of Machine Learning Research ,11, 3235–3268.\\nKakade, S. (2001). A natural policy gradient. In NIPS , pages 1531–1538. MIT Press.\\nKiros, R. (2013). Training neural networks with stochastic hessian-free optimization. ICLR .\\nLe Roux, N., Manzagol, P.-A., and Bengio, Y . (2008). Topmoumoute online natural gradient algorithm. In\\nNIPS’07 .\\nMartens, J. (2010). Deep learning via hessian-free optimization. In ICML , pages 735–742.\\nMartens, J. and Sutskever, I. (2011). Learning recurrent neural networks with hessian-free optimization. In\\nICML , pages 1017–1024.\\nMizutani, E. and Demmel, J. (2003). Iterative scaled trust-region learning in krylov subspaces via peralmutter’s\\nimplicit sparse hessian-vector multiply. In NIPS , pages 209–216.\\nNocedal, J. and Wright, S. J. (2000). Numerical Optimization . Springer.\\nPark, H., Amari, S.-I., and Fukumizu, K. (2000). Adaptive natural gradient learning algorithms for various\\nstochastic models. Neural Networks ,13(7), 755 – 764.\\nPearlmutter, B. A. (1994). Fast exact multiplication by the hessian. Neural Computation ,6, 147–160.\\nPeters, J. and Schaal, S. (2008). Natural actor-critic. (7-9), 1180–1190.\\nRoux, N. L. and Fitzgibbon, A. W. (2010). A fast natural newton method. In ICML , pages 623–630.\\nSchaul, T. (2012). Natural evolution strategies converge on sphere functions. In Genetic and Evolutionary\\nComputation Conference (GECCO) .\\nSchraudolph, N. N. (2002). Fast curvature matrix-vector products for second-order gradient descent. Neural\\nComputation ,14(7), 1723–1738.\\nShewchuck, J. (1994). An introduction to the conjugate gradient method without the agonizing pain. Technical\\nreport, CMU.\\nSohl-Dickstein, J. (2012). The natural gradient by analogy to signal whitening, and recipes and tricks for its\\nuse. CoRR ,abs/1205.1828 .\\nSun, Y ., Wierstra, D., Schaul, T., and Schmidhuber, J. (2009). Stochastic search using the natural gradient. In\\nICML , page 146.\\nSusskind, J., Anderson, A., and Hinton, G. E. (2010). The Toronto face dataset. Technical Report UTML TR\\n2010-001, U. Toronto.\\nVinyals, O. and Povey, D. (2012). Krylov Subspace Descent for Deep Learning. In AISTATS .\\nAppendix\\nExpected Hessian to Fisher Information Matrix\\nThe Fisher Information Matrix form can be obtained from the expected value of the Hessian :\\nEz[\\n−∂2logpθ\\n∂θ]\\n=Ez[\\n−∂1\\npθ∂pθ\\n∂θ\\n∂θ]\\n=Ez[\\n−1\\npθ(z)∂2pθ\\n∂θ2+(1\\npθ∂pθ\\n∂θ)T(1\\npθ∂pθ\\n∂θ)]\\n=−∂2\\n∂θ2(∑\\nzpθ(z))\\n+Ez[(∂logpθ(z)\\n∂θ)T(∂logpθ(z)\\n∂θ)]\\n=Ez[(∂logpθ(z)\\n∂θ)T(∂logpθ(z)\\n∂θ)]\\n(24)\\n14'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'Test_no_rec',\n",
       "   'creationTimeUnix': 1700659855592,\n",
       "   'id': '01319b3a-6b0a-584b-a830-c0ffef7bda6f',\n",
       "   'lastUpdateTimeUnix': 1700659855592,\n",
       "   'properties': {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks.pdf',\n",
       "    'page_content': '3.1 Penn Treebank Experiments\\nThe ﬁrst set of text prediction experiments focused on the Penn Treebank por-\\ntion of the Wall Street Journal corpus [22]. This was a preliminary study whose\\nmain purpose was to gauge the predictive power of the network, rather than to\\ngenerate interesting sequences.\\nAlthough a relatively small text corpus (a little over a million words in total),\\nthe Penn Treebank data is widely used as a language modelling benchmark. The\\ntraining set contains 930,000 words, the validation set contains 74,000 words and\\nthe test set contains 82,000 words. The vocabulary is limited to 10,000 words,\\nwith all other words mapped to a special ‘unknown word’ token. The end-of-\\nsentence token was included in the input sequences, and was counted in the\\nsequence loss. The start-of-sentence marker was ignored, because its role is\\nalready fulﬁlled by the null vectors that begin the sequences (c.f. Section 2).\\nThe experiments compared the performance of word and character-level\\nLSTM predictors on the Penn corpus. In both cases, the network architecture\\nwas a single hidden layer with 1000 LSTM units. For the character-level network\\nthe input and output layers were size 49, giving approximately 4.3M weights in\\ntotal, while the word-level network had 10,000 inputs and outputs and around\\n54M weights. The comparison is therefore somewhat unfair, as the word-level\\nnetwork had many more parameters. However, as the dataset is small, both net-\\nworks were easily able to overﬁt the training data, and it is not clear whether the\\ncharacter-level network would have beneﬁted from more weights. All networks\\nwere trained with stochastic gradient descent, using a learn rate of 0.0001 and a\\nmomentum of 0.99. The LSTM derivates were clipped in the range [ −1,1] (c.f.\\nSection 2.1).\\nNeural networks are usually evaluated on test data with ﬁxed weights. For\\nprediction problems however, where the inputs arethe targets, it is legitimate\\nto allow the network to adapt its weights as it is being evaluated (so long as\\nit only sees the test data once). Mikolov refers to this as dynamic evaluation .\\nDynamic evaluation allows for a fairer comparison with compression algorithms,\\nfor which there is no division between training and test sets, as all data is only\\npredicted once.\\nSince both networks overﬁt the training data, we also experiment with two\\ntypes of regularisation: weight noise [18] with a std. deviation of 0.075 applied\\nto the network weights at the start of each training sequence, and adaptive\\nweight noise [8], where the variance of the noise is learned along with the weights\\nusing a Minimum description Length (or equivalently, variational inference) loss\\nfunction. When weight noise was used, the network was initialised with the\\nﬁnal weights of the unregularised network. Similarly, when adaptive weight\\nnoise was used, the weights were initialised with those of the network trained\\nwith weight noise. We have found that retraining with iteratively increased\\nregularisation is considerably faster than training from random weights with\\nregularisation. Adaptive weight noise was found to be prohibitively slow for\\nthe word-level network, so it was regularised with ﬁxed-variance weight noise\\nonly. One advantage of adaptive weight is that early stopping is not needed\\n7'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'Test_no_rec',\n",
       "   'creationTimeUnix': 1700659856482,\n",
       "   'id': '05120942-8393-5916-8725-a872642668c9',\n",
       "   'lastUpdateTimeUnix': 1700659856482,\n",
       "   'properties': {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks.pdf',\n",
       "    'page_content': 'widely between the samples, but remain more-or-less consistent within them.\\nThis suggests that the network identiﬁes the traits early on in the sequence,\\nthen remembers them until the end. By looking through enough samples for a\\ngiven text, it appears to be possible to ﬁnd virtually any combination of stylistic\\ntraits, which suggests that the network models them independently both from\\neach other and from the text.\\n‘Blind taste tests’ carried out by the author during presentations suggest\\nthat at least some unbiased samples cannot be distinguished from real hand-\\nwriting by the human eye. Nonetheless the network does make mistakes we\\nwould not expect a human writer to make, often involving missing, confused\\nor garbled letters3; this suggests that the network sometimes has trouble de-\\ntermining the alignment between the characters and the trace. The number of\\nmistakes increases markedly when less common words or phrases are included\\nin the character sequence. Presumably this is because the network learns an\\nimplicit character-level language model from the training set that gets confused\\nwhen rare or unknown transitions occur.\\n5.4 Biased Sampling\\nOne problem with unbiased samples is that they tend to be diﬃcult to read\\n(partly because real handwriting is diﬃcult to read, and partly because the\\nnetwork is an imperfect model). Intuitively, we would expect the network to\\ngive higher probability to good handwriting because it tends to be smoother\\nand more predictable than bad handwriting. If this is true, we should aim to\\noutput more probable elements of Pr( x|c) if we want the samples to be easier to\\nread. A principled search for high probability samples could lead to a diﬃcult\\ninference problem, as the probability of every output depends on all previous\\noutputs. However a simple heuristic, where the sampler is biased towards more\\nprobable predictions at each step independently, generally gives good results.\\nDeﬁne the probability bias bas a real number greater than or equal to zero.\\nBefore drawing a sample from Pr( xt+1|yt), each standard deviation σj\\ntin the\\nGaussian mixture is recalculated from Eq. (21) to\\nσj\\nt= exp(\\nˆσj\\nt−b)\\n(61)\\nand each mixture weight is recalculated from Eq. (19) to\\nπj\\nt=exp(\\nˆπj\\nt(1 +b))\\n∑M\\nj′=1exp(\\nˆπj′\\nt(1 +b)) (62)\\nThis artiﬁcially reduces the variance in both the choice of component from the\\nmixture, and in the distribution of the component itself. When b= 0 unbiased\\nsampling is recovered, and as b→∞ the variance in the sampling disappears\\n3We expect humans to make mistakes like misspelling ‘temperament’ as ‘temperement’, as\\nthe second writer in Fig. 15 seems to have done.\\n32'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'Test_no_rec',\n",
       "   'creationTimeUnix': 1700659856502,\n",
       "   'id': '06a81b23-4659-5e07-a3f5-459d8eb2529c',\n",
       "   'lastUpdateTimeUnix': 1700659856502,\n",
       "   'properties': {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks.pdf',\n",
       "    'page_content': 'Figure 16: Samples biased towards higher probability. The probability\\nbiasesbare shown at the left. As the bias increases the diversity decreases and\\nthe samples tend towards a kind of ‘average handwriting’ which is extremely\\nregular and easy to read (easier, in fact, than most of the real handwriting in the\\ntraining set). Note that even when the variance disappears, the same letter is\\nnot written the same way at diﬀerent points in a sequence (for examples the ‘e’s\\nin “exactly the same”, the ‘l’s in “until they all look”), because the predictions\\nare still inﬂuenced by the previous outputs. If you look closely you can see that\\nthe last three lines are not quite exactly the same.\\n35'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'Test_no_rec',\n",
       "   'creationTimeUnix': 1700659853728,\n",
       "   'id': '0926a7a3-915e-5192-8d0e-5d9a2b2dd6bc',\n",
       "   'lastUpdateTimeUnix': 1700659853728,\n",
       "   'properties': {'document_title': '1301.3584v7.Revisiting_Natural_Gradient_for_Deep_Networks.pdf',\n",
       "    'page_content': '0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\\nWhich fraction of the dataset is replaced10-510-410-310-210-1mean(variance) on a log scaleNGD validation error 49.8%\\nMSGD validation error 49.8%Figure 2: The plot describes how much the model is inﬂuenced by different parts of an online\\ntraining set. The variance induced by re-shufﬂing of data for natural gradient descent is order of\\nmagnitudes lower than for SGD. See text for more information.\\nSGD might move in a direction early on that could possibly yield overﬁtting (e.g. getting forced\\ninto some quadrant of parameter space based only on a few examples) resulting in different models\\nat the end. This suggests that natural gradient descent can deal better with nonstationary data and\\ncan be less sensitive to the particular examples selected early on during training.\\n9 Natural conjugate gradient\\nNatural gradient descent is a ﬁrst order method in the space of functions, which raises the question:\\ncan the algorithm be improved by considering second order information? Unfortunately computing\\ntheHessian on the manifold can be daunting (especially from a computational perspective). That is\\nthe Hessian of the error as a function of the density probability functions pθ(t|x)that give rise to the\\nmanifold structure. Note that in Roux and Fitzgibbon (2010) a method for combining second order\\ninformation and TONGA is proposed. It is not clear how this algorithm can be applied to natural\\ngradient descent. Also in Roux and Fitzgibbon (2010) a different Hessian matrix is considered,\\nnamely that of the error as a function of the parameter.\\nA simpler option is, however, to use an optimization method such as Nonlinear Conjugate Gradient,\\nthat is able to take advantage of second order structure (by following locally conjugate directions)\\nwhile not requiring to actually compute the Hessian at any point in time, only the gradient. Absil\\net al. (2008) describes how second order methods can be generalized to the manifold case. In\\nHonkela et al. (2008, 2010) a similar idea is proposed in the context of variational inference and\\nspeciﬁc assumptions on the form of pθare made. Gonzalez and Dorronsoro (2006) is more similar in\\nspirit with this work, though their approach is deﬁned for the diagonal form of the Fisher Information\\nMatrix and differs in how they propose to compute a new conjugate direction.\\nNatural conjugate gradient, the manifold version of nonlinear conjugate gradient, is deﬁned follow-\\ning the same intuitions (Shewchuck, 1994). The only problematic part of the original algorithm is\\nhow to obtain a new conjugate direction given the previous search direction and the current natural\\ngradient. The problem arises from the fact that the two vectors belong to different spaces. The lo-\\ncal geometry around the point where the previous search direction dt−1was computed is deﬁned by\\nFt−1, while the geometry around the new direction is deﬁned by Ft, where, in principle, Ft−1̸=Ft.\\nFollowing Absil et al. (2008) we would need to “transport” dt−1into the space of∇Nt, an expen-\\nsive operation, before we can compute a new direction using a standard formula like Polak-Ribiere.\\nFurthermore, the line search should be done along the geodesic of the manifold.\\nGonzalez and Dorronsoro (2006); Honkela et al. (2010) address these issues by making the assump-\\ntion that Ft−1andFtare identical (so dt−1does not need to be transported). This assumption is\\ndetrimental to the algorithm because it goes against what we want to achieve. By employing a con-\\njugate gradient method we hope to make large steps, from which it follows that the metric is very\\nlikely to change. Hence the assumption can not hold.\\n10'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'Test_no_rec',\n",
       "   'creationTimeUnix': 1700659853728,\n",
       "   'id': '0cc769bc-0ce4-53fb-8fb5-1338daf1408a',\n",
       "   'lastUpdateTimeUnix': 1700659853728,\n",
       "   'properties': {'document_title': '1301.3584v7.Revisiting_Natural_Gradient_for_Deep_Networks.pdf',\n",
       "    'page_content': 'Derivation of the natural gradient descent metrics\\nLinear activation function\\nIn the case of linear outputs we assume that each entry of the vector t,ticomes from a Gaussian\\ndistribution centered around yi(x)with some standard deviation β. From this it follows that:\\npθ(t|x) =o∏\\ni=1N(ti|y(x,θ)i,β2) (25)\\nF=Ex∼˜q[\\nEt∼N(t|y(x,θ),β2I)[∑o\\ni=1(\\n∂logθp(ti|y(x)i\\n∂θ)T(\\n∂logpθ(ti|y(x)i\\n∂θ)]]\\n=Ex∼˜q[∑o\\ni=1[\\nEt∼N(t|y(x,θ),β2I)[(\\n∂(ti−yi)2\\n∂θ)T(\\n∂(ti−yi)2\\n∂θ)]]]\\n=Ex∼˜q[∑o\\ni=1[\\nEt∼N(t|y(x,θ),β2I)[\\n(ti−yi)2(\\n∂yi\\n∂θ)T(\\n∂yi\\n∂θ)]]]\\n=Ex∼˜q[∑o\\ni=1[\\nEt∼N(t|y(x,θ),βI)[\\n(ti−yi)2](\\n∂yi\\n∂θ)T(\\n∂yi\\n∂θ)]]\\n=Ex∼˜q[∑o\\ni=1[\\nEt∼N(t|y(x,θ),β2I)[\\n(ti−yi)2](\\n∂yi\\n∂θ)T(\\n∂yi\\n∂θ)]]\\n=Ex∼˜q[∑o\\ni=1[\\nEt∼N(t|y(x,θ),β2I)[\\n(ti−yi)2](\\n∂yi\\n∂θ)T(\\n∂yi\\n∂θ)]]\\n=β2Ex∼˜q[∑o\\ni=1[(\\n∂yi\\n∂θ)T(\\n∂yi\\n∂θ)]]\\n=β2Ex∼˜q[\\nJT\\nyJy](26)\\nSigmoid activation function\\nIn the case of the sigmoid units, i,e, y=sigmoid (r), we assume a binomial distribution which gives\\nus:\\np(t|x) =∏\\niyti\\ni(1−yi)1−ti(27)\\nlogpgives us the usual cross-entropy error used with sigmoid units. We can compute the Fisher\\ninformation matrix as follows:\\nF=Ex∼˜q[\\nEt∼p(t|x)[∑o\\ni=1(ti−yi)2\\ny2\\ni(1−yi)2(\\n∂yi\\n∂θ)T∂yi\\n∂θ]]\\n=Ex∼˜q[∑o\\ni=11\\nyi(1−yi)(\\n∂yi\\n∂θ)T∂yi\\n∂θ]\\n=Ex∼˜q[\\nJT\\nydiag(\\n1\\ny(1−y))\\nJy](28)\\nNote thatdiag(v)stands for the diagonal matrix constructed from the values of the vector vand\\nwe make an abuse of notation, where by1\\nywe understand the vector obtain by applying the division\\nelement-wise (the i-th element of is1\\nyi).\\nSoftmax activation function\\nFor the softmax activation function, y=softmax (r),p(t|x)takes the form of a multinoulli:\\np(t|x) =o∏\\ni=1yti\\ni (29)\\n15'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'Test_no_rec',\n",
       "   'creationTimeUnix': 1700659853724,\n",
       "   'id': '0f9aae7b-396c-583c-b0ea-8cd2afdd0052',\n",
       "   'lastUpdateTimeUnix': 1700659853724,\n",
       "   'properties': {'document_title': '1206.1106v2.No_More_Pesky_Learning_Rates.pdf',\n",
       "    'page_content': 'No More Pesky Learning Rates\\nFigure 4. Non-stationary loss. The loss is quadratic but now the target value ( µ) changes abruptly every 300 time-steps.\\nAbove: loss as a function of time, below: corresponding learning rates. This illustrates the limitations of SGD with ﬁxed or\\ndecaying learning rates (full lines): any ﬁxed learning rate limits the precision to which the optimum can be approximated\\n(progress stalls); any cooling schedule on the other hand cannot cope with the non-stationarity. In contrast, our adaptive\\nsetting (‘vSGD’, red circles), as closely resembles the optimal behavior (oracle, black dashes). The learning rate decays\\nlike 1/tduring the static part, but increases again after each abrupt change (with just a very small delay compared to\\nthe oracle). The average loss across time is substantially better than for any SGD cooling schedule.\\n6. Experiments\\nWe test the new algorithm extensively on a couple of\\ntoy problem ﬁrst, and then follow up with results on\\nwell-known benchmark problems for digit recognition,\\nimage classiﬁcation and image reconstruction, using\\nthe new SGD variants to train both convex models\\n(logistic regression) and non-convex ones (multi-layer\\nperceptrons).\\n6.1. Noisy Quadratic\\nTo form an intuitive understanding of the eﬀects of\\nthe optimal adaptive learning rate method, and the\\neﬀect of the approximation, we illustrate the oscilla-\\ntory behavior of SGD, and compare the decrease in the\\nloss function and the accompanying change in learning\\nrates on the noisy quadratic loss function from Section\\n3.1 (see Figure 2 and Figure 3), contrasting the eﬀect\\nof ﬁxed rates or ﬁxed schedules to adaptive learning\\nrates, whether in approximation or using the oracle.6.2. Non-stationary Quadratic\\nIn realistic on-line learning scenarios, the curvature or\\nnoise level in any given dimension changes over time\\n(for example because of the eﬀects of updating other\\nparameters), and thus the learning rates need to in-\\ncrease as well as increase. Of course, no ﬁxed learning\\nrate or ﬁxed cooling schedule can achieve this. To il-\\nlustrate this, we use again a noisy quadratic loss func-\\ntion, but with abrupt changes of the optimum every\\n300 timesteps.\\nFigure 4 shows how vSGD with its adaptive memory-\\nsize appropriately handles such cases. Its initially large\\nlearning rate allows it to quickly approach the opti-\\nmum, then it gradually reduces the learning rate as\\nthe gradient variance increases relative to the squared\\nnorm of the average gradient, thus allowing the param-\\neters to closely approach the optimum. When the data\\ndistribution changes (abruptly, in our case), the algo-\\nrithm automatically detects that the norm of the av-\\nerage gradient increased relative to the variance. The\\nlearning rate jumps back up and adapts to the new\\ncircumstances. Note that here and in section 6.1 the\\ncurvature is always 1, which implies that the precondi-'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'Test_no_rec',\n",
       "   'creationTimeUnix': 1700659855643,\n",
       "   'id': '0fb204c0-c403-5645-8268-f6e040a9fd1a',\n",
       "   'lastUpdateTimeUnix': 1700659855643,\n",
       "   'properties': {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks.pdf',\n",
       "    'page_content': 'it contains not only a huge range of dictionary words, but also many character\\nsequences that would not be included in text corpora traditionally used for\\nlanguage modelling. For example foreign words (including letters from non-\\nLatin alphabets such as Arabic and Chinese), indented XML tags used to deﬁne\\nmeta-data, website addresses, and markup used to indicate page formatting such\\nas headings, bullet points etc. An extract from the Hutter prize dataset is shown\\nin Figs. 3 and 4.\\nThe ﬁrst 96M bytes in the data were evenly split into sequences of 100 bytes\\nand used to train the network, with the remaining 4M were used for validation.\\nThe data contains a total of 205 one-byte unicode symbols. The total number\\nofcharacters is much higher, since many characters (especially those from non-\\nLatin languages) are deﬁned as multi-symbol sequences. In keeping with the\\nprinciple of modelling the smallest meaningful units in the data, the network\\npredicted a single byte at a time, and therefore had size 205 input and output\\nlayers.\\nWikipedia contains long-range regularities, such as the topic of an article,\\nwhich can span many thousand words. To make it possible for the network to\\ncapture these, its internal state (that is, the output activations htof the hidden\\nlayers, and the activations ctof the LSTM cells within the layers) were only reset\\nevery 100 sequences. Furthermore the order of the sequences was not shuﬄed\\nduring training, as it usually is for neural networks. The network was therefore\\nable to access information from up to 10K characters in the past when making\\npredictions. The error terms were only backpropagated to the start of each 100\\nbyte sequence, meaning that the gradient calculation was approximate. This\\nform of truncated backpropagation has been considered before for RNN lan-\\nguage modelling [23], and found to speed up training (by reducing the sequence\\nlength and hence increasing the frequency of stochastic weight updates) without\\naﬀecting the network’s ability to learn long-range dependencies.\\nA much larger network was used for this data than the Penn data (reﬂecting\\nthe greater size and complexity of the training set) with seven hidden layers of\\n700 LSTM cells, giving approximately 21.3M weights. The network was trained\\nwith stochastic gradient descent, using a learn rate of 0.0001 and a momentum\\nof 0.9. It took four training epochs to converge. The LSTM derivates were\\nclipped in the range [ −1,1].\\nAs with the Penn data, we tested the network on the validation data with\\nand without dynamic evaluation (where the weights are updated as the data\\nis predicted). As can be seen from Table 2 performance was much better with\\ndynamic evaluation. This is probably because of the long range coherence of\\nWikipedia data; for example, certain words are much more frequent in some\\narticles than others, and being able to adapt to this during evaluation is ad-\\nvantageous. It may seem surprising that the dynamic results on the validation\\nset were substantially better than on the training set. However this is easily\\nexplained by two factors: ﬁrstly, the network underﬁt the training data, and\\nsecondly some portions of the data are much more diﬃcult than others (for\\nexample, plain text is harder to predict than XML tags).\\nTo put the results in context, the current winner of the Hutter Prize (a\\n9'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'Test_no_rec',\n",
       "   'creationTimeUnix': 1700659853725,\n",
       "   'id': '10f8a492-5e1e-5e31-8a8d-e5a6ade36bf2',\n",
       "   'lastUpdateTimeUnix': 1700659853725,\n",
       "   'properties': {'document_title': '1206.1106v2.No_More_Pesky_Learning_Rates.pdf',\n",
       "    'page_content': 'No More Pesky Learning Rates\\n10□410□310□210□1100\\nepoch10□410□310□210□1100101min/max learning rateC0\\n10□410□310□210□1100\\nepoch10□410□310□210□1100101min/max learning rateC1\\n10□410□310□210□1100\\nepoch10□410□310□210□1100101min/max learning rateCR\\n10□410□310□210□1100\\nepoch10□410□310□210□1100101min/max learning rateM0\\n10□410□310□210□1100\\nepoch10□410□310□210□1100101min/max learning rateM1\\n10□410□310□210□1100\\nepoch10□410□310□210□1100101min/max learning rateM2\\nSGD\\nadagrad\\nvSGD -l\\nvSGD-b\\nvSGD-g\\nFigure 9. Evolution of learning rates. It shows how the learning rates (minimum and maximum across all dimensions)\\nvary as a function of the epoch. Left: CIFAR classiﬁcation (no hidden layer), right: MNIST classiﬁcation (no hidden\\nlayer). Each symbol/color corresponds to the median behavior of one algorithm. The range of learning rates (for those\\nalgorithms that don’t have a single global learning rate) is shown in a colored band in-between the min/max markers.\\nThe log-log plot highlights the initial behavior, namely the ‘slow start’ (until about 0.1 epochs) due to a large Cconstant\\nin out methods, which contrasts with the quick start of AdaGrad . We also note that AdaGrad (yellow circles) has\\ndrastically diﬀerent ranges of learning rates on the two benchmarks.\\nA. Convergence Proof\\nIf we do gradient descent with η∗(t), then almost\\nsurely, the algorithm converges (for the quadratic\\nmodel). To prove that, we follow classical techniques\\nbased on Lyapunov stability theory (Bucy, 1965). No-\\ntice that the expected loss follows\\nE[\\nJ(\\nθ(t+1))\\n|θ(t)]\\n=1\\n2h·E[(\\n(1−η∗h)(θ(t)−θ∗) +η∗hσξ)2\\n+σ2]\\n=1\\n2h[\\n(1−η∗h)2(θ(t)−θ∗)2+ (η∗)2h2σ2+σ2]\\n=1\\n2h[σ2\\n(θ(t)−θ∗)2+σ2(θ(t)−θ∗)2+σ2]\\n≤J(\\nθ(t))ThusJ(θ(t)) is a positive super-martingale, indicating\\nthat almost surely J(θ(t))→J∞. We are to prove\\nthat almost surely J∞=J(θ∗) =1\\n2hσ2. Observe that\\nJ(θ(t))−E[J(θ(t+1))|θ(t)] =1\\n2hη∗(t),\\nE[J(θ(t))]−E[J(θ(t+1))|θ(t)] =1\\n2hE[η∗(t)]\\nSinceE[J(θ(t))] is bounded below by 0, the telescoping\\nsum gives us E[η∗(t)]→0, which in turn implies that\\nin probability η∗(t)→0. We can rewrite this as\\nη∗(t) =J(θt)−1\\n2hσ2\\nJ(θt)→0\\nBy uniqueness of the limit, almost surely,J∞−1\\n2hσ2\\nJ∞ =\\n0. Given that Jis strictly positive everywhere, we'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'Test_no_rec',\n",
       "   'creationTimeUnix': 1700659854910,\n",
       "   'id': '11c6019b-2211-59fc-8b65-4e902daa097f',\n",
       "   'lastUpdateTimeUnix': 1700659854910,\n",
       "   'properties': {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks.pdf',\n",
       "    'page_content': 'Generating Sequences With\\nRecurrent Neural Networks\\nAlex Graves\\nDepartment of Computer Science\\nUniversity of Toronto\\ngraves@cs.toronto.edu\\nAbstract\\nThis paper shows how Long Short-term Memory recurrent neural net-\\nworks can be used to generate complex sequences with long-range struc-\\nture, simply by predicting one data point at a time. The approach is\\ndemonstrated for text (where the data are discrete) and online handwrit-\\ning (where the data are real-valued). It is then extended to handwriting\\nsynthesis by allowing the network to condition its predictions on a text\\nsequence. The resulting system is able to generate highly realistic cursive\\nhandwriting in a wide variety of styles.\\n1 Introduction\\nRecurrent neural networks (RNNs) are a rich class of dynamic models that have\\nbeen used to generate sequences in domains as diverse as music [6, 4], text [30]\\nand motion capture data [29]. RNNs can be trained for sequence generation by\\nprocessing real data sequences one step at a time and predicting what comes\\nnext. Assuming the predictions are probabilistic, novel sequences can be gener-\\nated from a trained network by iteratively sampling from the network’s output\\ndistribution, then feeding in the sample as input at the next step. In other\\nwords by making the network treat its inventions as if they were real, much like\\na person dreaming. Although the network itself is deterministic, the stochas-\\nticity injected by picking samples induces a distribution over sequences. This\\ndistribution is conditional, since the internal state of the network, and hence its\\npredictive distribution, depends on the previous inputs.\\nRNNs are ‘fuzzy’ in the sense that they do not use exact templates from\\nthe training data to make predictions, but rather—like other neural networks—\\nuse their internal representation to perform a high-dimensional interpolation\\nbetween training examples. This distinguishes them from n-gram models and\\ncompression algorithms such as Prediction by Partial Matching [5], whose pre-\\ndictive distributions are determined by counting exact matches between the\\nrecent history and the training set. The result—which is immediately appar-\\n1arXiv:1308.0850v5  [cs.NE]  5 Jun 2014'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'Test_no_rec',\n",
       "   'creationTimeUnix': 1700659853726,\n",
       "   'id': '14638d0e-6dfa-56c0-84b4-bb00e2d92652',\n",
       "   'lastUpdateTimeUnix': 1700659853726,\n",
       "   'properties': {'document_title': '1301.3584v7.Revisiting_Natural_Gradient_for_Deep_Networks.pdf',\n",
       "    'page_content': 'where natural gradient descent is implemented using a truncated Newton approach for inverting the\\nfull metric matrix instead of the traditional diagonal or band-diagonal approximation.\\n2 Natural gradient descent\\nNatural gradient descent can be traced back to Amari’s work on information geometry (Amari, 1985)\\nand its application to various neural networks (Amari et al. , 1992; Amari, 1997), though a more in\\ndepth introduction can be found in Amari (1998); Park et al. (2000); Arnold et al. (2011). The\\nalgorithm has also been successfully applied in the reinforcement learning community (Kakade,\\n2001; Peters and Schaal, 2008) and for stochastic search (Sun et al. , 2009).\\nLet us consider a family of density functions Fthat maps a parameter θ∈RPto a probability\\ndensity function p(z),p:RN→[0,∞), where z∈RN. Any choice of θ∈RPdeﬁnes a particular\\ndensity function pθ(z) =F(θ)(z)and by considering all possible θvalues, we explore the set F,\\nwhich is our functional manifold.\\nIn its inﬁnitesimal form, the KL-divergence behaves like a distance measure, so we can deﬁne a\\nsimilarity measure between nearby density functions. Hence Fis a Riemannian manifold whose\\nmetric is given by the Fisher Information matrix Fθdeﬁned in the equation below:\\nFθ=Ez[\\n(∇logpθ(z))T(∇logpθ(z))]\\n. (1)\\nThat is, locally around some point θ, the metric deﬁnes an inner product between vectors uandv:\\n<u,v>θ=uFθv,\\nand it hence provides a local measure of distance. It what follows we will write Ffor the Fisher\\nInformation Matrix, assuming an implicit dependency of this matrix on θ.\\nGiven a loss function Lparametrized by θ, natural gradient descent attempts to move along the\\nmanifold by correcting the gradient of Laccording to the local curvature of the KL-divergence\\nsurface, i.e. moving some given distance in the direction ∇NL(θ):\\n∇NL(θ)def=∇L(θ)Ez[\\n(∇logpθ(z))T(∇logpθ(z))]−1\\ndef=∇L(θ)F−1. (2)\\nWe use∇Nfor natural gradient, ∇for gradients and Fis the metric matrix given by the Fisher\\nInformation Matrix. Partial derivatives are usually denoted as row vectors in this work. We can\\nderive this result by considering natural gradient descent to be deﬁned as the algorithm which, at\\neach step, attempts to pick a descent direction such that the amount of change (in the KL-sense)\\ninduced in our model is some given value. Speciﬁcally we look for a small ∆θthat minimizes a ﬁrst\\norder Taylor expansion of Lwhen the second order Taylor series of the KL-divergence between pθ\\nandpθ+∆θhas to be constant:\\narg min ∆θL(θ+ ∆θ)\\ns. t.KL(pθ||pθ+∆θ) =const.(3)\\nUsing this constraint we ensure that we move along the functional manifold with constant speed,\\nwithout being slowed down by its curvature. This also makes learning locally robust to re-\\nparametrizations of the model , as the functional behaviour of pdoes not depend on how it is\\nparametrized.\\nAssuming ∆θ→0, we can approximate the KL divergence by its second order Taylor series:\\nKL(pθ∥pθ+∆θ)≈(Ez[logpθ]−Ez[logpθ])\\n−Ez[∇logpθ(z)] ∆θ−1\\n2∆θTEz[\\n∇2logpθ]\\n∆θ\\n=1\\n2∆θTEz[\\n−∇2logpθ(z)]\\n∆θ\\n=1\\n2∆θTF∆θ (4)\\n2'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'Test_no_rec',\n",
       "   'creationTimeUnix': 1700659853724,\n",
       "   'id': '170acb87-811d-5895-8fdf-e3facd4fc02e',\n",
       "   'lastUpdateTimeUnix': 1700659853724,\n",
       "   'properties': {'document_title': '1206.1106v2.No_More_Pesky_Learning_Rates.pdf',\n",
       "    'page_content': 'No More Pesky Learning Rates\\nFigure 2. Illustration of the dynamics in a noisy quadratic\\nbowl (with 10 times larger curvature in one dimension than\\nthe other). Trajectories of 400 steps from vSGD, and from\\nSGD with three diﬀerent learning rate schedules. SGD\\nwith ﬁxed learning rate (crosses) descends until a certain\\ndepth (that depends on η) and then oscillates. SGD with a\\n1/tcooling schedule (pink circles) converges prematurely.\\nOn the other hand, vSGD (green triangles) is much less\\ndisrupted by the noise and continually approaches the op-\\ntimum.\\nwith a slow start heuristic, where the parameter up-\\ndates are kept small until the exponential averages be-\\ncome suﬃciently accurate. This is achieved by overes-\\ntimatingviandl) by a factor C. We ﬁnd that setting\\nC=d/10, as a rule of thumb is both robust and near-\\noptimal, because the value of Chas only a transient\\ninitialization eﬀect on the algorithm. The appendix\\ndetails how we arrived at this, and demonstrates the\\nlow sensitivity empirically.\\n5. Adaptive Learning Rate SGD\\nThe simplest version of the method views each com-\\nponent in isolation. This form of the algorithm will be\\ncalled “vSGD” (for “variance-based SGD”). In realis-\\ntic settings with high-dimensional parameter vector, it\\nis not clear a priori whether it is best to have a single,\\nglobal learning rate (that can be estimated robustly),\\na set of local, dimension-speciﬁc rates, or block-speciﬁc\\nlearning rates (whose estimation will be less robust).\\nWe propose three variants on this spectrum:\\nvSGD-l uses local gradient variance terms and the\\nlocal diagonal Hessian estimates, leading to η∗\\ni=\\n(gi)2/(hi·vi),\\nvSGD-g uses a global gradient variance term and an\\nupper bound on diagonal Hessian terms: η∗=∑(gi)2/(h+·l),\\n0 50 100 150 20010-310-210-1100101102loss LSGD η=0.2\\nSGD η=0.2/t\\nSGD η=1.0\\nSGD η=1.0/t\\noracle\\nvSGD\\n0 50 100 150 200\\n#samples10-410-310-210-1100learning rate ηFigure 3. Optimizing a noisy quadratic loss (dimension\\nd= 1, curvature h= 1). Comparison between SGD for\\ntwo diﬀerent ﬁxed learning rates 1.0 and 0.2, and two cool-\\ning schedules η= 1/tandη= 0.2/t, and vSGD (red cir-\\ncles). In dashed black, the ‘oracle’ computes the true op-\\ntimal learning rate rather than approximating it. In the\\ntop subplot, we show the median loss from 1000 simulated\\nruns, and below are corresponding learning rates. We ob-\\nserve that vSGD initially descends as fast as the SGD with\\nthe largest ﬁxed learning rate, but then quickly reduces the\\nlearning rate which dampens the oscillations and permits\\na continual reduction in loss, beyond what any ﬁxed learn-\\ning rate could achieve. The best cooling schedule ( η= 1/t)\\noutperforms vSGD, but when the schedule is not well tuned\\n(η= 0.2/t), the eﬀect on the loss is catastrophic, even\\nthough the produced learning rates are very close to the\\noracle’s (see the overlapping green crosses and the dashed\\nblack line at the bottom).\\nvSGD-b operates like vSGD-g, but being only global\\nacross multiple (architecture-speciﬁc) blocks of\\nparameters, with a diﬀerent learning rate\\nper block. Similar ideas are adopted in\\nTONGA (Le Roux et al., 2008). In the experi-\\nments, the parameters connecting every two lay-\\ners of the network are regard as a block, with the\\ncorresponding bias parameters in separate blocks.\\nThe pseudocode for vSGD-l is given in Algorithm 1,\\nthe other cases are very similar; all of them have linear\\ncomplexity in time and space; in fact, the overhead of\\nvSGD is roughly a factor two, which arises from the\\nadditional bbrop pass (which could be skipped in all\\nbut a fraction of the updates) – this cost is even less\\ncritical because it can be trivially parallelized.'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'Test_no_rec',\n",
       "   'creationTimeUnix': 1700659854820,\n",
       "   'id': '17e7bdc3-0df6-5e66-af9c-06d698e08726',\n",
       "   'lastUpdateTimeUnix': 1700659854820,\n",
       "   'properties': {'document_title': '1207.0580v1.Improving_neural_networks_by_preventing_co_adaptation_of_feature_detectors.pdf',\n",
       "    'page_content': 'that had only 4 training examples. We also removed one category that covered a huge chunk\\n(25%) of the examples. This left us with 50 classes and 402,738 documents. We divided the\\ndocuments into equal-sized training and test sets randomly. Each document was represented\\nusing the 2000 most frequent non-stopwords in the dataset.\\n(a)\\n (b)\\nFig. 7: Classiﬁcation error rate on the (a) training and (b) validation sets of the Reuters dataset\\nas learning progresses. The training error is computed using the stochastic nets.\\nWe trained a neural network using dropout-backpropagation and compared it with standard\\nbackpropagation. We used a 2000-2000-1000-50 architecture. The training hyperparameters are\\nsame as that in MNIST dropout training (Appendix A.1). Training was done for 500 epochs.\\nFigure 7 shows the training and test set errors as learning progresses. We show two nets\\n- one with a 2000-2000-1000-50 and another with a 2000-1000-1000-50 architecture trained\\nwith and without dropout. As in all previous datasets discussed so far, we obtain signiﬁcant\\nimprovements here too. The learning not only results in better generalization, but also proceeds\\nsmoothly, without the need for early stopping.\\nD Tiny Images and CIFAR-10\\nThe Tiny Images dataset contains 80 million 32×32color images collected from the web. The\\nimages were found by searching various image search engines for English nouns, so each image\\ncomes with a very unreliable label, which is the noun that was used to ﬁnd it. The CIFAR-10\\ndataset is a subset of the Tiny Images dataset which contains 60000 images divided among ten\\nclasses5. Each class contains 5000 training images and 1000 testing images. The classes are\\n5The CIFAR dataset is available at http://www.cs.toronto.edu/ ∼kriz/cifar.html.\\n13'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'Test_no_rec',\n",
       "   'creationTimeUnix': 1700659854963,\n",
       "   'id': '17fc8367-f536-5131-b0b3-db9348193ea5',\n",
       "   'lastUpdateTimeUnix': 1700659854963,\n",
       "   'properties': {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks.pdf',\n",
       "    'page_content': 'by reducing the number of processing steps between the bottom of the network\\nand the top, and thereby mitigating the ‘vanishing gradient’ problem [1]. In\\nthe special case that N= 1 the architecture reduces to an ordinary, single layer\\nnext step prediction RNN.\\nThe hidden layer activations are computed by iterating the following equa-\\ntions fromt= 1 toTand fromn= 2 toN:\\nh1\\nt=H(\\nWih1xt+Wh1h1h1\\nt−1+b1\\nh)\\n(1)\\nhn\\nt=H(\\nWihnxt+Whn−1hnhn−1\\nt+Whnhnhn\\nt−1+bn\\nh)\\n(2)\\nwhere the Wterms denote weight matrices (e.g. Wihnis the weight matrix\\nconnecting the inputs to the nthhidden layer, Wh1h1is the recurrent connection\\nat the ﬁrst hidden layer, and so on), the bterms denote bias vectors (e.g. byis\\noutput bias vector) and His the hidden layer function.\\nGiven the hidden sequences, the output sequence is computed as follows:\\nˆyt=by+N∑\\nn=1Whnyhn\\nt (3)\\nyt=Y(ˆyt) (4)\\nwhereYis the output layer function. The complete network therefore deﬁnes\\na function, parameterised by the weight matrices, from input histories x1:tto\\noutput vectors yt.\\nThe output vectors ytare used to parameterise the predictive distribution\\nPr(xt+1|yt) for the next input. The form of Pr( xt+1|yt) must be chosen carefully\\nto match the input data. In particular, ﬁnding a good predictive distribution\\nfor high-dimensional, real-valued data (usually referred to as density modelling ),\\ncan be very challenging.\\nThe probability given by the network to the input sequence xis\\nPr(x) =T∏\\nt=1Pr(xt+1|yt) (5)\\nand the sequence loss L(x) used to train the network is the negative logarithm\\nof Pr( x):\\nL(x) =−T∑\\nt=1log Pr(xt+1|yt) (6)\\nThe partial derivatives of the loss with respect to the network weights can be\\neﬃciently calculated with backpropagation through time [33] applied to the\\ncomputation graph shown in Fig. 1, and the network can then be trained with\\ngradient descent.\\n2.1 Long Short-Term Memory\\nIn most RNNs the hidden layer function His an elementwise application of a\\nsigmoid function. However we have found that the Long Short-Term Memory\\n4'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'Test_no_rec',\n",
       "   'creationTimeUnix': 1700659856408,\n",
       "   'id': '19a8fed5-4d7d-526b-bdaa-f7656fbcdefb',\n",
       "   'lastUpdateTimeUnix': 1700659856408,\n",
       "   'properties': {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks.pdf',\n",
       "    'page_content': 'Inputs\\nCharactersHidden 1WindowHidden 2OutputsFigure 12: Synthesis Network Architecture Circles represent layers, solid\\nlines represent connections and dashed lines represent predictions. The topology\\nis similar to the prediction network in Fig. 1, except that extra input from the\\ncharacter sequence c, is presented to the hidden layers via the window layer\\n(with a delay in the connection to the ﬁrst hidden layer to avoid a cycle in the\\ngraph).\\n27'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'Test_no_rec',\n",
       "   'creationTimeUnix': 1700659853728,\n",
       "   'id': '1c612c57-488d-55a8-bb69-eb2e6e5cbd21',\n",
       "   'lastUpdateTimeUnix': 1700659853728,\n",
       "   'properties': {'document_title': '1303.5778v1.Speech_Recognition_with_Deep_Recurrent_Neural_Networks.pdf',\n",
       "    'page_content': 'tends to ‘simplify’ neural networks, in the sense of reducing\\nthe amount of information required to transmit the parame-\\nters [23, 24], which improves generalisation.\\n4. EXPERIMENTS\\nPhoneme recognition experiments were performed on the\\nTIMIT corpus [25]. The standard 462 speaker set with all\\nSA records removed was used for training, and a separate\\ndevelopment set of 50 speakers was used for early stop-\\nping. Results are reported for the 24-speaker core test set.\\nThe audio data was encoded using a Fourier-transform-based\\nﬁlter-bank with 40 coefﬁcients (plus energy) distributed on\\na mel-scale, together with their ﬁrst and second temporal\\nderivatives. Each input vector was therefore size 123. The\\ndata were normalised so that every element of the input vec-\\ntors had zero mean and unit variance over the training set. All\\n61 phoneme labels were used during training and decoding\\n(soK= 61 ), then mapped to 39 classes for scoring [26].\\nNote that all experiments were run only once, so the vari-\\nance due to random weight initialisation and weight noise is\\nunknown.\\nAs shown in Table 1, nine RNNs were evaluated, vary-\\ning along three main dimensions: the training method used\\n(CTC, Transducer or pretrained Transducer), the number of\\nhidden levels (1–5), and the number of LSTM cells in each\\nhidden layer. Bidirectional LSTM was used for all networks\\nexcept CTC-3l-500h-tanh, which had tanh units instead of\\nLSTM cells, and CTC-3l-421h-uni where the LSTM layers\\nwere unidirectional. All networks were trained using stochas-\\ntic gradient descent, with learning rate 10−4, momentum 0.9\\nand random initial weights drawn uniformly from [−0.1,0.1].\\nAll networks except CTC-3l-500h-tanh and PreTrans-3l-250h\\nwere ﬁrst trained with no noise and then, starting from the\\npoint of highest log-probability on the development set, re-\\ntrained with Gaussian weight noise ( σ= 0.075) until the\\npoint of lowest phoneme error rate on the development set.\\nPreTrans-3l-250h was initialised with the weights of CTC-\\n3l-250h, along with the weights of a phoneme prediction net-\\nwork (which also had a hidden layer of 250 LSTM cells), both\\nof which were trained without noise, retrained with noise, and\\nstopped at the point of highest log-probability. PreTrans-3l-\\n250h was trained from this point with noise added. CTC-3l-\\n500h-tanh was entirely trained without weight noise because\\nit failed to learn with noise added. Beam search decoding was\\nused for all networks, with a beam width of 100.\\nThe advantage of deep networks is immediately obvious,\\nwith the error rate for CTC dropping from 23.9% to 18.4%\\nas the number of hidden levels increases from one to ﬁve.\\nThe four networks CTC-3l-500h-tanh, CTC-1l-622h, CTC-\\n3l-421h-uni and CTC-3l-250h all had approximately the same\\nnumber of weights, but give radically different results. The\\nthree main conclusions we can draw from this are (a) LSTM\\nworks much better than tanh for this task, (b) bidirectionalTable 1 . TIMIT Phoneme Recognition Results. ‘Epochs’ is\\nthe number of passes through the training set before conver-\\ngence. ‘PER’ is the phoneme error rate on the core test set.\\nNETWORK WEIGHTS EPOCHS PER\\nCTC-3 L-500 H-TANH 3.7M 107 37.6%\\nCTC-1 L-250 H 0.8M 82 23.9%\\nCTC-1 L-622 H 3.8M 87 23.0%\\nCTC-2 L-250 H 2.3M 55 21.0%\\nCTC-3 L-421 H-UNI 3.8M 115 19.6%\\nCTC-3 L-250 H 3.8M 124 18.6%\\nCTC-5 L-250 H 6.8M 150 18.4%\\nTRANS -3L-250 H 4.3M 112 18.3%\\nPRETRANS -3L-250 H 4.3M 144 17.7%\\nFig. 3 . Input Sensitivity of a deep CTC RNN. The heatmap\\n(top) shows the derivatives of the ‘ah’ and ‘p’ outputs printed\\nin red with respect to the ﬁlterbank inputs (bottom). The\\nTIMIT ground truth segmentation is shown below. Note that\\nthe sensitivity extends to surrounding segments; this may be\\nbecause CTC (which lacks an explicit language model) at-\\ntempts to learn linguistic dependencies from the acoustic data.\\nLSTM has a slight advantage over unidirectional LSTMand\\n(c) depth is more important than layer size (which supports\\nprevious ﬁndings for deep networks [3]). Although the advan-\\ntage of the transducer is slight when the weights are randomly\\ninitialised, it becomes more substantial when pretraining is\\nused.\\n5. CONCLUSIONS AND FUTURE WORK\\nWe have shown that the combination of deep, bidirectional\\nLong Short-term Memory RNNs with end-to-end training and\\nweight noise gives state-of-the-art results in phoneme recog-\\nnition on the TIMIT database. An obvious next step is to ex-\\ntend the system to large vocabulary speech recognition. An-\\nother interesting direction would be to combine frequency-\\ndomain convolutional neural networks [27] with deep LSTM.'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'Test_no_rec',\n",
       "   'creationTimeUnix': 1700659855748,\n",
       "   'id': '1dbaed86-0fea-513a-a512-dc43ccf38d05',\n",
       "   'lastUpdateTimeUnix': 1700659855748,\n",
       "   'properties': {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks.pdf',\n",
       "    'page_content': \"*[[British-London Bridge]]                                                      *[[Anti-Talmot Touch/Tucker novice]]                                            *[[List of cambridge capital]]                                                  *[[Elon Haven]]                                                                 *[[USS ''Otaro Screamed Its'']]                                                 *[[Detroit Library]]                                                            *[[Belgium Sea]]                                                                *[[Tularan Bell|Turnbiller Squobil]]                                            *[[Suntanal vocalist|Prosopyo]]                                                 *[[Winkenpea]]                                                                  *[[Milenton Streat]]                                                            *[[Raiebin]]                                                                    *[[Est Altar Macinton]]                                                         *[[Military mass missile|S3]]                                                   *[[Organization of the Asian American state district|umbali landmarks]]        *[[ISO]]                                                                        *[[NFL]]                                                                        *[[American Anti-Capitalism|Major independent ITU-US singles]]                  *[[London (role-playing game)|Pre-Romanian Civil War]]                          *[[Yokukhav-Na-Un-Murantano Kaufmann - Sijone-Grafittsforbiel]]                 *[[Neao trolleyne and deadweight drug]]                                         *[[B-45 BQ|B9]] - de red take painting is deployed larger than quanta submarine *[[Susconfiction of advocate]]                                                  *[[List of major swandarms]]                                                    *[[:Category:Italo sales towns entertained by the ICBMs of Skinner|Knighting 707 killed by capital]]                                                                                                                                            ===[[Midple planet|Parishment of the value=====                                 [[Image:2000.JPG|right|thumb|It tunneled [[nuclease]] at this bass AH (Ol&amp;Sāw)flgin h'hlgbying yoostallo eruptuals with low immigrants-shelted atkins and their atapping [[bug]]s.                                                                                                                                          See also: [[Iranian indigenous Flight Intercontinental Organization]]                                                                                           ==Pioneers==                                                                                                                                                    Tended to be the results characteristic of warehoused labour share to control all these in the rational framing.                                                                                                                                ==Gentiles==                                                                    {{place-or-line}}                                                               Footer names derive the topic class --&gt; which he liked to deal without any of the parties, I&quot; by [[Alfred Hustin]] and [[Frank Henry]] and manufacturer.[http://anciermsc.nit.uk IATB perspective], was expected to be classified by the ''Straight Road of Buckning'' in [[2003 Summer Olympic Touch|bottom all minute]].                                                                                                                                                              ==Performance==                                                                 [[Image:Iare 300.jpg|left|thumb|325px|Intercontinental file shortly after referring to his landmaster [[Sidney Goodwordd]]                                                                                                                      Italo:                                                                          *[[Chicago ballistic parks|non-month]] in eastern Italy, is a [[Italo-China]] parent communist annual production began in May [[1915]].                                                                                                         An ICBM, the [[gurt and land]] has registered $155 billion in U.S. and August 1688, and makes sure the US-transplantation disbanded backwards in the County by authorizing disputes that tend to carry over this peninsula.                     * Current malasses 25 decks and counterpoint culture that were impure between  systems:                                                                         * L14 - 194 / 100 000 km/s                                                      Figure 6: Generated Wikipedia data (cotd.)\\n15\"},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'Test_no_rec',\n",
       "   'creationTimeUnix': 1700659853727,\n",
       "   'id': '1f113c06-c402-5157-ae97-8e1ab710bf2f',\n",
       "   'lastUpdateTimeUnix': 1700659853727,\n",
       "   'properties': {'document_title': '1301.3584v7.Revisiting_Natural_Gradient_for_Deep_Networks.pdf',\n",
       "    'page_content': 'HL◦rij,i ̸=j=∂2∑\\nk(−tklog(φ(rk)))\\n∂ri∂rj=∂∑\\nk(tkφ(ri))−ti\\n∂rj\\n=−φ(ri)φ(rj)\\nHL◦rii =...=∂φ(ri)−ti\\n∂ri=φ(ri)−φ(ri)φ(ri)(13)\\nF=Ex∼˜q[∑o\\nk=11\\nyk(\\n∂yk\\n∂θ)T∂yk\\n∂θ]\\n=Ex∼˜q[\\nJT\\nr(∑o\\nk=11\\nyk(\\n∂yk\\n∂r)T(\\n∂yk\\n∂r))\\nJr]\\n=1\\nN∑\\nx(i)(\\nJT\\nrMJr)(14)\\nMij,i̸=j=∑o\\nk=11\\nyk∂yk\\n∂ri∂yk\\n∂rj=∑o\\nk=1(δki−yi)yk(δkj−yj)\\n=yiyj−yiyj−yiyj=−φ(ri)φ(rj)\\nMii=∑o\\nk=11\\nyk∂yk\\n∂yi∂yk\\n∂rj=y2\\ni(∑o\\nk=1yk) +yi−2y2\\ni\\n=φ(ri)−φ(ri)φ(ri)(15)\\nEquation (14) starts from the natural gradient metric and singles out a matrix Min the formula\\nsuch that the metric can be re-written as the product JT\\nrMJr(similar to the formula for the Gauss-\\nNewton approximation). In equation (15) we show that indeed Mequals HL◦rand hence the natural\\ngradient metric is the same as the extended Gauss-Newton matrix for this case as well. Note that δ\\nis the Kronecker delta, where δij,i̸=j= 0andδii= 1.\\nThere is also a one to one mapping for most of the other heuristics used by Hessian-Free Opti-\\nmization. Following the functional manifold interpretation of the algorithm, we can recover the\\nLevenberg-Marquardt heuristic used in Martens (2010). This is a heuristic for adapting αin the\\ndamping term αIthat is added to the Hessian before inverting it. This additive term helps mak-\\ning the matrix easier to invert (for example when it is close to singular) or to deal with negative\\ncurvature. Iis the identity matrix, α∈Ris a scalar.\\nThe justiﬁcation of the method comes from a trust region approach. We look at how well our second\\norder Taylor approximation of the function predicts the change when taking a step. If it does well,\\nthen we can trust our approximation (and decrease the damping α). If our approximation does\\nnot predict well the change in the error, we increase the damping factor. We can see that in the\\nlimit, when α→0, we completely trust our approximation and use no damping. When αis very\\nlarge, there are two things happening. First of all, we are taking much smaller steps (scaled by\\n1\\nα). Secondly, the direction that we follow is mostly that of the gradient (ignoring the curvature\\ninformation of the Hessian).\\nA similar argument can be carried on for natural gradient descent, where we consider only a ﬁrst\\norder Taylor approximation on the manifold. For any function f, ifpdepicts the picked descent\\ndirection and ηthe step size\\nf(θt−ηp)≈f(θt)−η∂f(θt)\\n∂θtp (16)\\nThis gives the reduction ratio given by equation (17) which can, under the assumption that p=\\n∂f(θt)\\n∂θtF−1, be shown to behave identically with the one in Martens (2010) (under the same assump-\\ntion, namely that CG is close to convergence).\\nρ=f(θt−ηp)−f(θt)\\n−η∂f(θt)\\n∂θtp≈f((\\nθ−t−ηF−1∂f(θt)\\n∂θtT)\\n−f(θt)\\n−η∂f(θt)\\n∂θtF−1∂f(θt)\\n∂θtT(17)\\nStructural damping (Martens and Sutskever, 2011), a speciﬁc regularization term used to improve\\ntraining of recurrent neural network, can also be explained from the natural gradient descent per-\\nspective. Roughly it implies using the joint probability density function p(t,h|x), where his the\\nhidden state, when writing the KL-constraint. logp(t,h|x)will break in the sum of two terms,\\none being the Fisher Information Matrix, while the other measures the change in hand forms the\\n6'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'Test_no_rec',\n",
       "   'creationTimeUnix': 1700659853727,\n",
       "   'id': '24c1787a-e1ae-51f6-83ef-77201393ef97',\n",
       "   'lastUpdateTimeUnix': 1700659853727,\n",
       "   'properties': {'document_title': '1207.0580v1.Improving_neural_networks_by_preventing_co_adaptation_of_feature_detectors.pdf',\n",
       "    'page_content': 'Fig. 2: The frame classiﬁcation error rate on the core test set of the TIMIT benchmark. Com-\\nparison of standard and dropout ﬁnetuning for different network architectures. Dropout of 50%\\nof the hidden units and 20% of the input units improves classiﬁcation.\\nlayer and 185 “softmax” output units that are subsequently merged into the 39 distinct classes\\nused for the benchmark. Dropout of 50% of the hidden units signiﬁcantly improves classiﬁca-\\ntion for a variety of different network architectures (see ﬁgure 2). To get the frame recognition\\nrate, the class probabilities that the neural network outputs for each frame are given to a decoder\\nwhich knows about transition probabilities between HMM states and runs the Viterbi algorithm\\nto infer the single best sequence of HMM states. Without dropout, the recognition rate is 22.7%\\nand with dropout this improves to 19.7%, which is a record for methods that do not use any\\ninformation about speaker identity.\\nCIFAR-10 is a benchmark task for object recognition. It uses 32x32 downsampled color\\nimages of 10 different object classes that were found by searching the web for the names of the\\nclass (e.g. dog) or its subclasses (e.g. Golden Retriever). These images were labeled by hand\\nto produce 50,000 training images and 10,000 test images in which there is a single dominant\\nobject that could plausibly be given the class name ( 9) (see ﬁgure 3). The best published error\\nrate on the test set, without using transformed data, is 18.5% ( 10). We achieved an error rate of\\n16.6% by using a neural network with three convolutional hidden layers interleaved with three\\n“max-pooling” layers that report the maximum activity in local pools of convolutional units.\\nThese six layers were followed by one locally-connected layer (For details see Appendix D) .\\nUsing dropout in the last hidden layer gives an error rate of 15.6%.\\nImageNet is an extremely challenging object recognition dataset consisting of thousands of\\nhigh-resolution images of thousands of classes of object ( 11). In 2010, a subset of 1000 classes\\nwith roughly 1000 examples per class was the basis of an object recognition competition in\\n4'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'Test_no_rec',\n",
       "   'creationTimeUnix': 1700659855663,\n",
       "   'id': '25c50c28-0784-56fd-98af-ef9c7fee2112',\n",
       "   'lastUpdateTimeUnix': 1700659855663,\n",
       "   'properties': {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks.pdf',\n",
       "    'page_content': 'world to which language refers.\\nLastly, the network’s adaptation to recent sequences during training (which\\nallows it to beneﬁt from dynamic evaluation) can be clearly observed in the\\nextract. The last complete article before the end of the training set (at which\\npoint the weights were stored) was on intercontinental ballistic missiles. The\\ninﬂuence of this article on the network’s language model can be seen from the\\nprofusion of missile-related terms. Other recent topics include ‘Individual An-\\narchism’, the Italian writer Italo Calvino and the International Organization\\nfor Standardization (ISO), all of which make themselves felt in the network’s\\nvocabulary.\\n11'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'Test_no_rec',\n",
       "   'creationTimeUnix': 1700659853723,\n",
       "   'id': '2bf564f3-6605-5ec7-99c2-8dba129eebee',\n",
       "   'lastUpdateTimeUnix': 1700659853723,\n",
       "   'properties': {'document_title': '1206.1106v2.No_More_Pesky_Learning_Rates.pdf',\n",
       "    'page_content': 'No More Pesky Learning Rates\\nvery diﬀerent parameter settings. This tuning is very\\ncostly, as every parameter setting is typically tested\\nover multiple epochs.\\nLearning rates in SGD are generally decreased accord-\\ning a schedule of the form η(t) =η0(1 +γt)−1. Origi-\\nnally proposed as η(t) =O(t−1) in (Robbins & Monro,\\n1951), this form was recently analyzed in (Xu, 2011;\\nBach & Moulines, 2011) from a non-asymptotic per-\\nspective to understand how hyper-parameters like η0\\nandγaﬀect the convergence speed.\\nNumerous researchers have proposed schemes for mak-\\ning learning rates adaptive, either globally or by adapt-\\ning one rate per parameter (‘diagonal precondition-\\ning’); see (George & Powell, 2006) for an overview. An\\nearly diagonal preconditioning schemes was proposed\\nin (Almeida & Langlois, 1999) where the learning rate\\nis adapted as\\nηi(t) = max(\\n0,η0θi(t)·∇(t−1)\\nθi\\nvi)\\nfor each problem dimension i, where∇(t)\\nθiis gradient\\nof theith parameter at iteration t, andvi≈E[\\n∇2\\nθi]\\nis a recent running average of its square. Stochastic\\nmeta-descent (SMD, Schraudolph (1999; 2002)) uses\\na related multiplicative update of learning rates. Ap-\\nproaches based on the natural gradient (Amari et al.,\\n2000) precondition the updates by the empirical Fisher\\ninformation matrix (estimated by the gradient covari-\\nance matrix, or its diagonal approximation), in the\\nsimplest case: ηi=η0/vi; the “Natural Newton” al-\\ngorithm (Le Roux & Fitzgibbon, 2010) combines the\\ngradient covariance with second-order information. Fi-\\nnally, derived from a worst-case analysis, (Duchi et al.,\\n2010) propose an approach called ‘ AdaGrad ’, where\\nthe learning rate takes the form\\nηi(t) =η0√\\n∑t\\ns=0(\\n∇(s)\\nθi)2.\\nThe main practical drawback for all of these ap-\\nproaches is that they retain one or more sensitive\\nhyper-parameters, which must be tuned to obtain sat-\\nisfactory performance. AdaGrad has another dis-\\nadvantage: because it accumulates all the gradients\\nfrom the moment training starts to determine the\\ncurrent learning rate, the learning rate monotoni-\\ncally decreases: this is especially problematic for non-\\nstationary problems, but also on stationary ones, as\\nnavigating the properties of optimization landscape\\nchange continuously.\\nThe main contribution of the present paper is a for-\\nmula that gives the value of the learning rate that will\\nmaximally decrease the expected loss after the next up-\\n(θ∗−2σ)θ∗(θ∗+2σ)\\nParameter θ0σ2LossFigure 1. Illustration of the idealized loss function consid-\\nered (thick magenta), which is the average of the quadratic\\ncontributions of each sample (dotted blue), with minima\\ndistributed around the point θ∗. Note that the curvatures\\nare assumed to be identical for all samples.\\ndate. For eﬃciency reasons, some terms in the for-\\nmula must be approximated using such quantities as\\nthe mean and variance of the gradient. As a result, the\\nlearning rate is automatically decreased to zero when\\napproaching an optimum of the loss, without requiring\\na pre-determined annealing schedule, and if the prob-\\nlem is non-stationary, it the learning rate grows again\\nwhen the data changes.\\n3. Optimal Adaptive Learning Rates\\nIn this section, we derive an optimal learning rate\\nschedule, using an idealized quadratic and separable\\nloss function. We show that using this learning rate\\nschedule preserves convergence guarantees of SGD. In\\nthe following section, we ﬁnd how the optimal learning\\nrate values can be estimated from available informa-\\ntion, and describe a couple of possible approximations.\\nThe samples, indexed by j, are drawn i.i.d. from a\\ndata distribution P. Each sample contributes a per-\\nsample lossL(j)(θ) to the expected loss:\\nJ(θ) =Ej∼P[\\nL(j)(θ)]\\n(1)\\nwhere θ∈Rdis the trainable parameter vector, whose\\noptimal value is denoted θ∗= arg min θJ(θ). The\\nSGD parameter update formula is of the form θ(t+1)=\\nθ(t)−η(t)∇(j)\\nθ, where∇(j)\\nθ=∂\\n∂θL(j)(θ) is the gradient\\nof the the contribution of example jto the loss, and\\nthe learning rate η(t)is a suitably chosen sequence of\\npositive scalars (or positive deﬁnite matrices).\\n3.1. Noisy Quadratic Loss\\nWe assume that the per-sample loss functions are\\nsmooth around minima, and can be locally approxi-\\nmated by a quadratic function. We also assume that\\nthe minimum value of the per-sample loss functions'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'Test_no_rec',\n",
       "   'creationTimeUnix': 1700659855704,\n",
       "   'id': '30360d92-58b8-585c-8310-79a4d9487a6c',\n",
       "   'lastUpdateTimeUnix': 1700659855704,\n",
       "   'properties': {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks.pdf',\n",
       "    'page_content': \"ue: if snow forms, a cooling cycle happens. The intensity of the albedo effect depends on the size of the change in albedo and the amount of [[insolation]]; for this reason it can be potentially very large in the tropics.                                                                                                   == Some examples of albedo effects ==                                                                                                                           === Fairbanks, Alaska ===                                                                                                                                       According to the [[National Climatic Data Center]]'s GHCN 2 data, which is composed of 30-year smoothed climatic means for thousands of weather stations across the world, the college weather station at [[Fairbanks]], [[Alaska]], is about 3 °C (5 °F) warmer than the airport at Fairbanks, partly because of drainage patterns but also largely because of the lower albedo at the college resulting from a higher concentration of [[pine]] [[tree]]s and therefore less open snowy ground to reflect the heat back into space. Neunke and Kukla have shown that this difference is especially marked during the late [[winter]] months, when [[solar radiation]] is greater.                                                                                                                                             === The tropics ===                                                                                                                                             Although the albedo-temperature effect is most famous in colder regions of Earth, because more [[snow]] falls there, it is actually much stronger in tropical regions because in the tropics there is consistently more sunlight. When [[Brazil]]ian ranchers cut down dark, tropical [[rainforest]] trees to replace them with even darker soil in order to grow crops, the average temperature of the area appears to increase by an average of about 3 °C (5 °F) year-round, which is a significant amount.                                                                                                                                                  === Small scale effects ===                                                                                                                                     Albedo works on a smaller scale, too. People who wear dark clothes in the summertime put themselves at a greater risk of [[heatstroke]] than those who wear white clothes.                                                                                                                                                      === Pine forests ===                                                                                                                                            The albedo of a [[pine]] forest at 45°N in the winter in which the trees cover the land surface completely is only about 9%, among the lowest of any naturally occurring land environment. This is partly due to the color of the pines, and partly due to multiple scattering of sunlight within the trees which lowers the overall reflected light level. Due to light penetration, the ocean's albedo is even lower at about 3.5%, though this depends strongly on the angle of the incident radiation. Dense [[swamp]]land averages between 9% and 14%. [[Deciduous tree]]s average about 13%. A [[grass]]y field usually comes in at about 20%. A barren field will depend on the color of the soil, and can be as low as 5% or as high as 40%, with 15% being about the average for farmland. A [[desert]] or large [[beach]] usually averages around 25% but varies depending on the color of the sand. [Reference: Edward Walker's study in the Great Plains in the winter around 45°N].                                                                                === Urban areas ===                                                                                                                                             Urban areas in particular have very unnatural values for albedo because of the many human-built structures which absorb light before the light can reach the surface. In the northern part of the world, cities are relatively dark, and Walker has shown that their average albedo is about 7%, with only a slight increase during the summer. In most tropical countries, cities average around 12%. This is similar to the values found in northern suburban transitional zones. Part of the reason for this is the different natural environment of cities in tropical regions, e.g., there are more very dark trees around; another reason is that portions of the tropics are very poor, and city buildings must be built with different materials. Warmer regions may also choose lighter colored building materials so the structures will remain cooler.                                               Figure 4: Real Wikipedia data (cotd.)\\n13\"},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'Test_no_rec',\n",
       "   'creationTimeUnix': 1700659853726,\n",
       "   'id': '30cb63fa-5b02-5662-b699-e69551accf4e',\n",
       "   'lastUpdateTimeUnix': 1700659853726,\n",
       "   'properties': {'document_title': '1212.5701v1.ADADELTA__An_Adaptive_Learning_Rate_Method.pdf',\n",
       "    'page_content': '4.2. Sensitivity to Hyperparameters\\nWhile momentum converged to a better ﬁnal solution than\\nADADELTA after many epochs of training, it was very sen-\\nsitive to the learning rate selection, as was SGD and ADA-\\nGRAD. In Table 1 we vary the learning rates for each method\\nand show the test set errors after 6 epochs of training using\\nrectiﬁed linear units as the activation function. The optimal\\nsettings from each column were used to generate Fig. 1. With\\nSGD, Momentum, or ADAGRAD the learning rate needs to\\nbe set to the correct order of magnitude, above which the so-\\nlutions typically diverge and below which the optimization\\nproceeds slowly. We can see that these results are highly vari-\\nable for each method, compared to ADADELTA in Table 2\\nin which the two hyperparameters do not signiﬁcantly alter\\nperformance.\\n4.3. Effective Learning Rates\\nTo investigate some of the properties of ADADELTA we plot\\nin Fig. 2 the step sizes and parameter updates of 10 randomly\\nselected dimensions in each of the 3 weight matrices through-\\nout training. There are several interesting things evident in\\nthis ﬁgure. First, the step sizes, or effective learning rates (all\\nterms except gtfrom Eqn. 14) shown in the left portion of the\\nﬁgure are larger for the lower layers of the network and much\\nsmaller for the top layer at the beginning of training. This\\nproperty of ADADELTA helps to balance the fact that lower\\nlayers have smaller gradients due to the diminishing gradi-\\n010020000.51d x10 100 20000.51d x20 100 20000.51d x3\\n0100200−0.0100.016 x10 100 200−0.0100.016 x20 100 200−0.0100.016 x3\\nFig. 2 . Step sizes and parameter updates shown every 60\\nbatches during training the MNIST network with tanh non-\\nlinearities for 25 epochs. Left: Step sizes for 10 randomly\\nselected dimensions of each of the 3 weight matrices of the\\nnetwork. Right: Parameters changes for the same 10 dimen-\\nsions for each of the 3 weight matrices. Note the large step\\nsizes in lower layers that help compensate for vanishing gra-\\ndients that occur with backpropagation.ent problem in neural networks and thus should have larger\\nlearning rates.\\nSecondly, near the end of training these step sizes con-\\nverge to 1. This is typically a high learning rate that would\\nlead to divergence in most methods, however this conver-\\ngence towards 1 only occurs near the end of training when the\\ngradients and parameter updates are small. In this scenario,\\ntheϵconstants in the numerator and denominator dominate\\nthe past gradients and parameter updates, converging to the\\nlearning rate of 1.\\nThis leads to the last interesting property of ADADELTA\\nwhich is that when the step sizes become 1, the parameter\\nupdates (shown on the right of Fig. 2) tend towards zero. This\\noccurs smoothly for each of the weight matrices effectively\\noperating as if an annealing schedule was present.\\nHowever, having no explicit annealing schedule imposed\\non the learning rate could be why momentum with the proper\\nhyperparameters outperforms ADADELTA later in training as\\nseen in Fig. 1. With momentum, oscillations that can occur\\nnear a minima are smoothed out, whereas with ADADELTA\\nthese can accumulate in the numerator. An annealing sched-\\nule could possibly be added to the ADADELTA method to\\ncounteract this in future work.\\n4.4. Speech Data\\nIn the next set of experiments we trained a large-scale neu-\\nral network with 4 hidden layers on several hundred hours\\nof US English data collected using V oice Search, V oice IME,\\nand read data. The network was trained using the distributed\\nsystem of [4] in which a centralized parameter server accu-\\nmulates the gradient information reported back from several\\nreplicas of the neural network. In our experiments we used ei-\\nther 100 or 200 such replica networks to test the performance\\nof ADADELTA in a highly distributed environment.\\nThe neural network is setup as in [7] where the inputs\\nare 26 frames of audio, each consisting of 40 log-energy ﬁl-\\nter bank outputs. The outputs of the network were 8,000\\nsenone labels produced from a GMM-HMM system using\\nforced alignment with the input frames. Each hidden layer\\nof the neural network had 2560 hidden units and was trained\\nwith either logistic or rectiﬁed linear nonlinearities.\\nFig. 3 shows the performance of the ADADELTA method\\nwhen using 100 network replicas. Notice our method ini-\\ntially converges faster and outperforms ADAGRAD through-\\nout training in terms of frame classiﬁcation accuracy on the\\ntest set. The same settings of ϵ= 1e−6andρ= 0.95from\\nthe MNIST experiments were used for this setup.\\nWhen training with rectiﬁed linear units and using 200\\nmodel replicas we also used the same settings of hyperpa-\\nrameters (see Fig. 4). Despite having 200 replicates which\\ninherently introduces signiﬁcants amount of noise to the gra-\\ndient accumulations, the ADADELTA method performs well,\\nquickly converging to the same frame accuracy as the other\\nmethods.'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'Test_no_rec',\n",
       "   'creationTimeUnix': 1700659853728,\n",
       "   'id': '3200db25-db34-587f-8d29-2013404f3c50',\n",
       "   'lastUpdateTimeUnix': 1700659853728,\n",
       "   'properties': {'document_title': '1303.5778v1.Speech_Recognition_with_Deep_Recurrent_Neural_Networks.pdf',\n",
       "    'page_content': 'SPEECH RECOGNITION WITH DEEP RECURRENT NEURAL NETWORKS\\nAlex Graves, Abdel-rahman Mohamed and Geoffrey Hinton\\nDepartment of Computer Science, University of Toronto\\nABSTRACT\\nRecurrent neural networks (RNNs) are a powerful model for\\nsequential data. End-to-end training methods such as Connec-\\ntionist Temporal Classiﬁcation make it possible to train RNNs\\nfor sequence labelling problems where the input-output align-\\nment is unknown. The combination of these methods with\\nthe Long Short-term Memory RNN architecture has proved\\nparticularly fruitful, delivering state-of-the-art results in cur-\\nsive handwriting recognition. However RNN performance in\\nspeech recognition has so far been disappointing, with better\\nresults returned by deep feedforward networks. This paper in-\\nvestigates deep recurrent neural networks , which combine the\\nmultiple levels of representation that have proved so effective\\nin deep networks with the ﬂexible use of long range context\\nthat empowers RNNs. When trained end-to-end with suit-\\nable regularisation, we ﬁnd that deep Long Short-term Mem-\\nory RNNs achieve a test set error of 17.7% on the TIMIT\\nphoneme recognition benchmark, which to our knowledge is\\nthe best recorded score.\\nIndex Terms —recurrent neural networks, deep neural\\nnetworks, speech recognition\\n1. INTRODUCTION\\nNeural networks have a long history in speech recognition,\\nusually in combination with hidden Markov models [1, 2].\\nThey have gained attention in recent years with the dramatic\\nimprovements in acoustic modelling yielded by deep feed-\\nforward networks [3, 4]. Given that speech is an inherently\\ndynamic process, it seems natural to consider recurrent neu-\\nral networks (RNNs) as an alternative model. HMM-RNN\\nsystems [5] have also seen a recent revival [6, 7], but do not\\ncurrently perform as well as deep networks.\\nInstead of combining RNNs with HMMs, it is possible\\nto train RNNs ‘end-to-end’ for speech recognition [8, 9, 10].\\nThis approach exploits the larger state-space and richer dy-\\nnamics of RNNs compared to HMMs, and avoids the prob-\\nlem of using potentially incorrect alignments as training tar-\\ngets. The combination of Long Short-term Memory [11], an\\nRNN architecture with an improved memory, with end-to-end\\ntraining has proved especially effective for cursive handwrit-\\ning recognition [12, 13]. However it has so far made little\\nimpact on speech recognition.RNNs are inherently deep in time, since their hidden state\\nis a function of all previous hidden states. The question that\\ninspired this paper was whether RNNs could also beneﬁt from\\ndepth in space; that is from stacking multiple recurrent hid-\\nden layers on top of each other, just as feedforward layers are\\nstacked in conventional deep networks. To answer this ques-\\ntion we introduce deep Long Short-term Memory RNNs and\\nassess their potential for speech recognition. We also present\\nan enhancement to a recently introduced end-to-end learning\\nmethod that jointly trains two separate RNNs as acoustic and\\nlinguistic models [10]. Sections 2 and 3 describe the network\\narchitectures and training methods, Section 4 provides exper-\\nimental results and concluding remarks are given in Section 5.\\n2. RECURRENT NEURAL NETWORKS\\nGiven an input sequence x= (x1,...,x T), a standard recur-\\nrent neural network (RNN) computes the hidden vector se-\\nquence h= (h1,...,h T)and output vector sequence y=\\n(y1,...,y T)by iterating the following equations from t= 1\\ntoT:\\nht=H(Wxhxt+Whhht−1+bh) (1)\\nyt=Whyht+by (2)\\nwhere theWterms denote weight matrices (e.g. Wxhis the\\ninput-hidden weight matrix), the bterms denote bias vectors\\n(e.g.bhis hidden bias vector) and His the hidden layer func-\\ntion.\\nHis usually an elementwise application of a sigmoid\\nfunction. However we have found that the Long Short-Term\\nMemory (LSTM) architecture [11], which uses purpose-built\\nmemory cells to store information, is better at ﬁnding and ex-\\nploiting long range context. Fig. 1 illustrates a single LSTM\\nmemory cell. For the version of LSTM used in this paper [14]\\nHis implemented by the following composite function:\\nit=σ(Wxixt+Whiht−1+Wcict−1+bi) (3)\\nft=σ(Wxfxt+Whfht−1+Wcfct−1+bf) (4)\\nct=ftct−1+ittanh (Wxcxt+Whcht−1+bc)(5)\\not=σ(Wxoxt+Whoht−1+Wcoct+bo) (6)\\nht=ottanh(ct) (7)\\nwhereσis the logistic sigmoid function, and i,f,oandc\\nare respectively the input gate ,forget gate ,output gate andarXiv:1303.5778v1  [cs.NE]  22 Mar 2013'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'Test_no_rec',\n",
       "   'creationTimeUnix': 1700659855874,\n",
       "   'id': '334a0a80-7293-5f09-aa85-770303059583',\n",
       "   'lastUpdateTimeUnix': 1700659855874,\n",
       "   'properties': {'document_title': '1308.0850v5.Generating_Sequences_With_Recurrent_Neural_Networks.pdf',\n",
       "    'page_content': 'input, along with a binary x3that has value 1 if the vector ends a stroke (that\\nis, if the pen was lifted oﬀ the board before the next vector was recorded) and\\nvalue 0 otherwise. A mixture of bivariate Gaussians was used to predict x1\\nandx2, while a Bernoulli distribution was used for x3. Each output vector yt\\ntherefore consists of the end of stroke probability e, along with a set of means\\nµj, standard deviations σj, correlations ρjand mixture weights πjfor theM\\nmixture components. That is\\nxt∈R×R×{0,1} (15)\\nyt=(\\net,{πj\\nt,µj\\nt,σj\\nt,ρj\\nt}M\\nj=1)\\n(16)\\nNote that the mean and standard deviation are two dimensional vectors, whereas\\nthe component weight, correlation and end-of-stroke probability are scalar. The\\nvectorsytare obtained from the network outputs ˆ yt, where\\nˆyt=(\\nˆet,{ˆwj\\nt,ˆµj\\nt,ˆσj\\nt,ˆρj\\nt}M\\nj=1)\\n=by+N∑\\nn=1Whnyhn\\nt (17)\\nas follows:\\net=1\\n1 + exp (ˆet)=⇒et∈(0,1) (18)\\nπj\\nt=exp(\\nˆπj\\nt)\\n∑M\\nj′=1exp(\\nˆπj′\\nt) =⇒πj\\nt∈(0,1),∑\\njπj\\nt= 1 (19)\\nµj\\nt= ˆµj\\nt =⇒µj\\nt∈R (20)\\nσj\\nt= exp(\\nˆσj\\nt)\\n=⇒σj\\nt>0 (21)\\nρj\\nt=tanh(ˆρj\\nt) = ⇒ρj\\nt∈(−1,1) (22)\\nThe probability density Pr( xt+1|yt) of the next input xt+1given the output\\nvectorytis deﬁned as follows:\\nPr(xt+1|yt) =M∑\\nj=1πj\\ntN(xt+1|µj\\nt,σj\\nt,ρj\\nt){\\net if (xt+1)3= 1\\n1−etotherwise(23)\\nwhere\\nN(x|µ,σ,ρ ) =1\\n2πσ1σ2√\\n1−ρ2exp[−Z\\n2(1−ρ2)]\\n(24)\\nwith\\nZ=(x1−µ1)2\\nσ2\\n1+(x2−µ2)2\\nσ2\\n2−2ρ(x1−µ1)(x2−µ2)\\nσ1σ2(25)\\n20'},\n",
       "   'vectorWeights': None}],\n",
       " 'totalResults': 25}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weaviate_client.data_object.get(class_name='test_no_rec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing data in Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_workload(num_actors, documents):\n",
    "    docs_per_actor = len(documents) // num_actors\n",
    "\n",
    "    doc_parts = [documents[i * docs_per_actor: (i + 1) * docs_per_actor] for i in range(num_actors)]\n",
    "\n",
    "    if len(documents) % num_actors:\n",
    "        doc_parts[-1].extend(documents[num_actors * docs_per_actor:])\n",
    "\n",
    "    return doc_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2599421/1579659243.py:8: DeprecationWarning: The '(Search).results' method is deprecated, use 'Client.results' instead\n",
      "  for result in tqdm(search_results.results()):\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Fisher Information and Natural Gradient Learning of Random Deep Networks, Authors: Shun-ichi Amari, Ryo Karakida, Masafumi Oizumi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Recent Advances in Convolutional Neural Networks, Authors: Jiuxiang Gu, Zhenhua Wang, Jason Kuen, Lianyang Ma, Amir Shahroudy, Bing Shuai, Ting Liu, Xingxing Wang, Li Wang, Gang Wang, Jianfei Cai, Tsuhan Chen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Stochastic (Approximate) Proximal Point Methods: Convergence, Optimality, and Adaptivity, Authors: Hilal Asi, John C. Duchi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Generating Sequences With Recurrent Neural Networks, Authors: Alex Graves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:05, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Speech Recognition with Deep Recurrent Neural Networks, Authors: Alex Graves, Abdel-rahman Mohamed, Geoffrey Hinton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "1it [00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Reducing ground-based astrometric errors with Gaia and Gaussian processes, Authors: W. F. Fortino, G. M. Bernstein, P. H. Bernardinelli, M. Aguena, S. Allam, J. Annis, D. Bacon, K. Bechtol, S. Bhargava, D. Brooks, D. L. Burke, J. Carretero, A. Choi, M. Costanzi, L. N. da Costa, M. E. S. Pereira, J. De Vicente, S. Desai, P. Doel, A. Drlica-Wagner, K. Eckert, T. F. Eifler, A. E. Evrard, I. Ferrero, J. Frieman, J. García-Bellido, E. Gaztanaga, D. W. Gerdes, R. A. Gruendl, J. Gschwend, G. Gutierrez, W. G. Hartley, S. R. Hinton, D. L. Hollowood, K. Honscheid, D. J. James, M. Jarvis, S. Kent, K. Kuehn, N. Kuropatkin, M. A. G. Maia, J. L. Marshall, F. Menanteau, R. Miquel, R. Morgan, J. Myles, R. L. C. Ogando, A. Palmese, F. Paz-Chinchón, A. A. Plazas, A. Roodman, E. S. Rykoff, E. Sanchez, B. Santiago, V. Scarpine, M. Schubnell, S. Serrano, I. Sevilla-Noarbe, M. Smith, E. Suchyta, G. Tarle, C. To, D. L. Tucker, T. N. Varga, A. R. Walker, J. Weller, W. Wester\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Speech Recognition with Deep Recurrent Neural Networks, Authors: Alex Graves, Abdel-rahman Mohamed, Geoffrey Hinton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Forward-Forward Algorithm: Some Preliminary Investigations, Authors: Geoffrey Hinton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Improving neural networks by preventing co-adaptation of feature detectors, Authors: Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, Ruslan R. Salakhutdinov\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:03, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Search Intelligence: Deep Learning For Dominant Category Prediction, Authors: Zeeshan Khawar Malik, Mo Kobrosli, Peter Maas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Revisiting Natural Gradient for Deep Networks, Authors: Razvan Pascanu, Yoshua Bengio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:01,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A comparative study of divisive hierarchical clustering algorithms, Authors: Maurice Roux\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Efficient OPA tomography of non-Gaussian states of light, Authors: Éva Rácz, László Ruppert, Radim Filip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: No More Pesky Learning Rates, Authors: Tom Schaul, Sixin Zhang, Yann LeCun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:04, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: One-Shot Imitation Learning, Authors: Yan Duan, Marcin Andrychowicz, Bradly C. Stadie, Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter Abbeel, Wojciech Zaremba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:01,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Towards Understanding Fast Adversarial Training, Authors: Bai Li, Shiqi Wang, Suman Jana, Lawrence Carin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ADADELTA: An Adaptive Learning Rate Method, Authors: Matthew D. Zeiler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "arxiv_pipeline('./1412.6980v9.Adam__A_Method_for_Stochastic_Optimization.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-fine-tuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
