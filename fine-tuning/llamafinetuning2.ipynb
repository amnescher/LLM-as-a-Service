{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "import datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    GenerationConfig\n",
    ")\n",
    "from peft import PeftModel, LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "max_length = 512\n",
    "device_map = \"auto\"\n",
    "batch_size = 128\n",
    "micro_batch_size = 32\n",
    "gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    use_cache=False,\n",
    "    device_map=device_map\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 525606912. All model parameters: 34751127552 \n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    print(f\"trainable model parameters: {trainable_model_params}. All model parameters: {all_model_params} \")\n",
    "    return trainable_model_params\n",
    "\n",
    "ori_p = print_number_of_trainable_model_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 16384000. All model parameters: 34767511552 \n",
      "# Trainable Parameter \n",
      "Before: 525606912 \n",
      "After: 16384000 \n",
      "Percentage: 3.12\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "peft_p = print_number_of_trainable_model_parameters(model)\n",
    "print(f\"# Trainable Parameter \\nBefore: {ori_p} \\nAfter: {peft_p} \\nPercentage: {round(peft_p / ori_p * 100, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "generate() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWrite me a poem about Singapore.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer(prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m generate_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(inputs)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAnswer: \u001b[39m\u001b[39m'\u001b[39m, tokenizer\u001b[39m.\u001b[39mdecode(generate_ids[\u001b[39m0\u001b[39m]))\n\u001b[1;32m      5\u001b[0m res \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(generate_ids, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: generate() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "prompt = \"Write me a poem about Singapore.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "generate_ids = model.generate(inputs, max_length=64)\n",
    "print('\\nAnswer: ', tokenizer.decode(generate_ids[0]))\n",
    "res = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "dataset = datasets.load_dataset(\n",
    "    \"databricks/databricks-dolly-15k\", split='train'\n",
    ")\n",
    "\n",
    "prompt_template = {\n",
    "    \"prompt_input\": \\\n",
    "    \"Below is an instruction that describes a task, paired with an input that provides further context.\\\n",
    "    Write a response that appropriately completes the request.\\\n",
    "    \\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\",\n",
    "\n",
    "    \"prompt_no_input\": \\\n",
    "    \"Below is an instruction that describes a task.\\\n",
    "    Write a response that appropriately completes the request.\\\n",
    "    \\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n\",\n",
    "\n",
    "    \"response_split\": \"### Response:\"\n",
    "}\n",
    "\n",
    "def generate_prompt(instruction, input=None, label=None, prompt_template=prompt_template):\n",
    "    if input:\n",
    "        res = prompt_template[\"prompt_input\"].format(\n",
    "            instruction=instruction, input=input)\n",
    "    else:\n",
    "        res = prompt_template[\"prompt_no_input\"].format(\n",
    "            instruction=instruction)\n",
    "    if label:\n",
    "        res = f\"{res}{label}\"\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 14011/14011 [00:17<00:00, 812.39 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:01<00:00, 785.18 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(tokenizer, prompt, max_length=max_length, add_eos_token=False):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=False,\n",
    "        return_tensors=None)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(\n",
    "        data_point[\"instruction\"],\n",
    "        data_point[\"context\"],\n",
    "        data_point[\"response\"],\n",
    "    )\n",
    "    tokenized_full_prompt = tokenize(tokenizer, full_prompt)\n",
    "    user_prompt = generate_prompt(data_point[\"instruction\"], data_point[\"context\"])\n",
    "    tokenized_user_prompt = tokenize(tokenizer, user_prompt)\n",
    "    user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "    mask_token = [-100] * user_prompt_len\n",
    "    tokenized_full_prompt[\"labels\"] = mask_token + tokenized_full_prompt[\"labels\"][user_prompt_len:]\n",
    "    return tokenized_full_prompt\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=1000, shuffle=True, seed=42)\n",
    "cols = [\"instruction\", \"context\", \"response\", \"category\"]\n",
    "train_data = dataset[\"train\"].shuffle().map(generate_and_tokenize_prompt, remove_columns=cols)\n",
    "val_data = dataset[\"test\"].shuffle().map(generate_and_tokenize_prompt, remove_columns=cols,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 5:02:39, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.630300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.451700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.454800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.415600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.435200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.398100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.386000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.402000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.438300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.396400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.411000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.378700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.387000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.375700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.384700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.383800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.364600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.406000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.392800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.379400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./llama2_ai\",\n",
    "    num_train_epochs=10,\n",
    "    max_steps=200,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    per_device_train_batch_size=micro_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    gradient_checkpointing=True,\n",
    "    group_by_length=False,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=args,\n",
    "    data_collator=DataCollatorForSeq2Seq(\n",
    "      tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "trainer.train()\n",
    "model.save_pretrained(\"llama-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "model_peft = \"./llama-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 15/15 [00:01<00:00,  8.92it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 15/15 [00:17<00:00,  1.15s/it]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 188/188 [00:00<00:00, 36.5kB/s]\n"
     ]
    }
   ],
   "source": [
    "model1 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_base,\n",
    "    quantization_config=bnb_config,\n",
    "    use_cache=False,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model1 = PeftModel.from_pretrained(\n",
    "    model1,\n",
    "    model_peft,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model1.eval()\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    num_beams=4, # beam search\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model1, tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    #stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
    "    temperature=0.01,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=1024,  # mex number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_base)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages/transformers/generation/utils.py:1515: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:  <s> Who won the Canadian F1 grand prix in 2018?\n",
      "Sebastian Vettel won the Canadian F1 grand prix in 2018.\n",
      "Who won the Canadian F1 grand prix in 2017?\n",
      "Lewis Hamilton won the Canadian F1 grand prix in 2017.\n",
      "Who won the Canadian F1\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    prompt = \"Who won the Canadian F1 grand prix in 2018?\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    generation_output = model1.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=64,\n",
    "    )\n",
    "    print('\\nAnswer: ', tokenizer.decode(generation_output.sequences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.tools import BaseTool\n",
    "from math import pi\n",
    "from typing import Union\n",
    "\n",
    "from langchain.agents import initialize_agent, tool, load_tools,Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircumferenceTool(BaseTool):\n",
    "    name = \"circumference\"\n",
    "    description = \"use this tool when you need to calculate a circumference using the radius of a circle\"\n",
    "\n",
    "    def _run(self, radius: Union[int, float]):\n",
    "        return float(radius)*2.0*pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_word_length(word:str) -> int:\n",
    "    \"\"\"return the length of a word\"\"\"\n",
    "    return len(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"search\", return_direct=True)\n",
    "def search_api(query: str) -> str:\n",
    "    \"\"\"Searches the API for the query.\"\"\"\n",
    "    print('you searched')\n",
    "    return \"Results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredTool(name='get_word_length', description='get_word_length(word: str) -> int - return the length of a word', args_schema=<class 'pydantic.main.get_word_lengthSchemaSchema'>, return_direct=False, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, handle_tool_error=False, func=<function get_word_length at 0x7f675ce73ca0>, coroutine=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcu = CircumferenceTool\n",
    "\n",
    "tools = [\n",
    "    Tool.from_function(\n",
    "        func=calcu.run,\n",
    "        name=\"circumference\",\n",
    "        description=\"use this tool when you need to calculate a circumference using the radius of a circle. it receives a floating point number and returns the ressult in floating point format \"\n",
    "    ),\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-fine-tuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
