{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "from transformers import AutoTokenizer, set_seed, pipeline,BitsAndBytesConfig, LlamaForCausalLM, LlamaTokenizer, GenerationConfig,LlamaConfig,LlamaModel,AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, BitsAndBytesConfig \n",
    "import torch\n",
    "\n",
    "from functools import partial\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM,PeftModel\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15011\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Which is a species of fish? Tope or Rope',\n",
       " 'context': '',\n",
       " 'response': 'Tope',\n",
       " 'category': 'classification'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Lora, BnB and Peft config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BnB config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bnb_config():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    return bnb_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peft and Lora config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_peft_config(modules):\n",
    "    \"\"\"\n",
    "    Create Parameter-Efficient Fine-Tuning config for your model\n",
    "    :param modules: Names of the modules to apply Lora to\n",
    "    \"\"\"\n",
    "    config = LoraConfig(\n",
    "        r=16,  # dimension of the updated matrices\n",
    "        lora_alpha=64,  # parameter for scaling\n",
    "        target_modules=modules,\n",
    "        lora_dropout=0.1,  # dropout probability for layers\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all linear names (target modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling the model and creating tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, bnb_config):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = f'{40960}MB' #TODO Change if necessary\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\", \n",
    "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'meta-llama/Llama-2-70b-chat-hf'\n",
    "\n",
    "bnb_config = create_bnb_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 15/15 [02:05<00:00,  8.38s/it]\n",
      "/home/ubuntu/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:628: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(model_id, bnb_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample):\n",
    "    \"\"\"\n",
    "    Format various fields of the sample ('instruction', 'context', 'response')\n",
    "    Then concatenate them using two newline characters \n",
    "    :param sample: Sample dictionnary\n",
    "    \"\"\"\n",
    "\n",
    "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"### Instruction:\"\n",
    "    INPUT_KEY = \"Input:\"\n",
    "    RESPONSE_KEY = \"### Response:\"\n",
    "    END_KEY = \"### End\"\n",
    "    \n",
    "    blurb = f\"{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\\n{sample['instruction']}\"\n",
    "    input_context = f\"{INPUT_KEY}\\n{sample['context']}\" if sample[\"context\"] else None\n",
    "    response = f\"{RESPONSE_KEY}\\n{sample['response']}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "    \n",
    "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
    "\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    \n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the max length of sequence (tokens) for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    print('configuration: ', conf)\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configuration:  LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-2-70b-chat-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 8192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 28672,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 64,\n",
      "  \"num_hidden_layers\": 80,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.32.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "Found max lenth: 4096\n"
     ]
    }
   ],
   "source": [
    "#test max length\n",
    "max_length = get_max_length(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizing batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: str):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)#, batched=True)\n",
    "    print('dataset check:', dataset)\n",
    "    \n",
    "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    print('preprocess func', _preprocessing_function)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"instruction\", \"context\", \"response\", \"category\",\"text\"] ,\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "    \n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Why can camels survive for long without water?',\n",
       " 'context': '',\n",
       " 'response': 'Camels use the fat in their humps to keep them filled with energy and hydration for long periods of time.',\n",
       " 'category': 'open_qa'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset...\n",
      "dataset check: Dataset({\n",
      "    features: ['instruction', 'context', 'response', 'category', 'text'],\n",
      "    num_rows: 15011\n",
      "})\n",
      "preprocess func functools.partial(<function preprocess_batch at 0x7fa174d8d8b0>, max_length=4096, tokenizer=LlamaTokenizerFast(name_or_path='meta-llama/Llama-2-70b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False))\n"
     ]
    }
   ],
   "source": [
    "dataset = preprocess_dataset(tokenizer, max_length, 9016, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1,\n",
       "  13866,\n",
       "  338,\n",
       "  385,\n",
       "  15278,\n",
       "  393,\n",
       "  16612,\n",
       "  263,\n",
       "  3414,\n",
       "  29889,\n",
       "  14350,\n",
       "  263,\n",
       "  2933,\n",
       "  393,\n",
       "  7128,\n",
       "  2486,\n",
       "  1614,\n",
       "  2167,\n",
       "  278,\n",
       "  2009,\n",
       "  29889,\n",
       "  13,\n",
       "  13,\n",
       "  2277,\n",
       "  29937,\n",
       "  2799,\n",
       "  4080,\n",
       "  29901,\n",
       "  13,\n",
       "  5328,\n",
       "  1784,\n",
       "  5633,\n",
       "  892,\n",
       "  14982,\n",
       "  716,\n",
       "  297,\n",
       "  278,\n",
       "  7102,\n",
       "  5955,\n",
       "  29871,\n",
       "  29929,\n",
       "  29929,\n",
       "  29941,\n",
       "  29973,\n",
       "  13,\n",
       "  13,\n",
       "  4290,\n",
       "  29901,\n",
       "  13,\n",
       "  1576,\n",
       "  29871,\n",
       "  29929,\n",
       "  29929,\n",
       "  29941,\n",
       "  471,\n",
       "  1568,\n",
       "  16710,\n",
       "  975,\n",
       "  322,\n",
       "  3755,\n",
       "  1422,\n",
       "  515,\n",
       "  967,\n",
       "  27978,\n",
       "  985,\n",
       "  272,\n",
       "  29889,\n",
       "  7579,\n",
       "  304,\n",
       "  7102,\n",
       "  5955,\n",
       "  29892,\n",
       "  1432,\n",
       "  760,\n",
       "  310,\n",
       "  278,\n",
       "  1559,\n",
       "  471,\n",
       "  8688,\n",
       "  515,\n",
       "  278,\n",
       "  5962,\n",
       "  701,\n",
       "  29892,\n",
       "  3704,\n",
       "  278,\n",
       "  6012,\n",
       "  322,\n",
       "  871,\n",
       "  29871,\n",
       "  29906,\n",
       "  29900,\n",
       "  29995,\n",
       "  310,\n",
       "  967,\n",
       "  5633,\n",
       "  892,\n",
       "  8988,\n",
       "  975,\n",
       "  515,\n",
       "  278,\n",
       "  3517,\n",
       "  12623,\n",
       "  29889,\n",
       "  7102,\n",
       "  5955,\n",
       "  14637,\n",
       "  304,\n",
       "  278,\n",
       "  29871,\n",
       "  29929,\n",
       "  29929,\n",
       "  29941,\n",
       "  408,\n",
       "  376,\n",
       "  29874,\n",
       "  7282,\n",
       "  6564,\n",
       "  29892,\n",
       "  451,\n",
       "  925,\n",
       "  515,\n",
       "  263,\n",
       "  16905,\n",
       "  29892,\n",
       "  541,\n",
       "  884,\n",
       "  263,\n",
       "  7604,\n",
       "  18520,\n",
       "  1213,\n",
       "  7102,\n",
       "  5955,\n",
       "  29915,\n",
       "  29879,\n",
       "  6012,\n",
       "  414,\n",
       "  316,\n",
       "  11292,\n",
       "  263,\n",
       "  716,\n",
       "  3578,\n",
       "  29899,\n",
       "  284,\n",
       "  2376,\n",
       "  1014,\n",
       "  2557,\n",
       "  411,\n",
       "  1302,\n",
       "  309,\n",
       "  322,\n",
       "  6398,\n",
       "  15933,\n",
       "  8872,\n",
       "  2673,\n",
       "  313,\n",
       "  273,\n",
       "  599,\n",
       "  716,\n",
       "  2473,\n",
       "  29899,\n",
       "  2324,\n",
       "  1788,\n",
       "  29892,\n",
       "  1334,\n",
       "  790,\n",
       "  496,\n",
       "  4853,\n",
       "  280,\n",
       "  511,\n",
       "  10594,\n",
       "  5742,\n",
       "  278,\n",
       "  3517,\n",
       "  13777,\n",
       "  29899,\n",
       "  2696,\n",
       "  975,\n",
       "  1655,\n",
       "  261,\n",
       "  322,\n",
       "  3907,\n",
       "  7282,\n",
       "  6728,\n",
       "  411,\n",
       "  278,\n",
       "  6012,\n",
       "  322,\n",
       "  11415,\n",
       "  29892,\n",
       "  4969,\n",
       "  263,\n",
       "  901,\n",
       "  7631,\n",
       "  1891,\n",
       "  1559,\n",
       "  12463,\n",
       "  322,\n",
       "  13138,\n",
       "  385,\n",
       "  16710,\n",
       "  19500,\n",
       "  7271,\n",
       "  29889,\n",
       "  450,\n",
       "  29871,\n",
       "  29929,\n",
       "  29929,\n",
       "  29941,\n",
       "  471,\n",
       "  884,\n",
       "  278,\n",
       "  937,\n",
       "  29871,\n",
       "  29929,\n",
       "  29896,\n",
       "  29896,\n",
       "  304,\n",
       "  7150,\n",
       "  263,\n",
       "  4832,\n",
       "  6210,\n",
       "  22713,\n",
       "  29889,\n",
       "  13,\n",
       "  13,\n",
       "  2277,\n",
       "  29937,\n",
       "  13291,\n",
       "  29901,\n",
       "  13,\n",
       "  1576,\n",
       "  29871,\n",
       "  29929,\n",
       "  29929,\n",
       "  29941,\n",
       "  1904,\n",
       "  471,\n",
       "  1754,\n",
       "  515,\n",
       "  29871,\n",
       "  29896,\n",
       "  29929,\n",
       "  29929,\n",
       "  29946,\n",
       "  29899,\n",
       "  29896,\n",
       "  29929,\n",
       "  29929,\n",
       "  29947,\n",
       "  29889,\n",
       "  739,\n",
       "  338,\n",
       "  2998,\n",
       "  408,\n",
       "  278,\n",
       "  1833,\n",
       "  310,\n",
       "  278,\n",
       "  4799,\n",
       "  1111,\n",
       "  29877,\n",
       "  839,\n",
       "  7102,\n",
       "  816,\n",
       "  267,\n",
       "  29889,\n",
       "  9333,\n",
       "  29871,\n",
       "  29906,\n",
       "  29900,\n",
       "  29995,\n",
       "  310,\n",
       "  967,\n",
       "  5633,\n",
       "  892,\n",
       "  8988,\n",
       "  975,\n",
       "  515,\n",
       "  278,\n",
       "  3517,\n",
       "  12623,\n",
       "  313,\n",
       "  1552,\n",
       "  29871,\n",
       "  29929,\n",
       "  29953,\n",
       "  29946,\n",
       "  467,\n",
       "  450,\n",
       "  29871,\n",
       "  29929,\n",
       "  29929,\n",
       "  29941,\n",
       "  471,\n",
       "  263,\n",
       "  7282,\n",
       "  6564,\n",
       "  297,\n",
       "  4958,\n",
       "  310,\n",
       "  2874,\n",
       "  322,\n",
       "  16905,\n",
       "  7117,\n",
       "  29889,\n",
       "  739,\n",
       "  338,\n",
       "  697,\n",
       "  310,\n",
       "  278,\n",
       "  1556,\n",
       "  10712,\n",
       "  17878,\n",
       "  29871,\n",
       "  29929,\n",
       "  29896,\n",
       "  29896,\n",
       "  29879,\n",
       "  304,\n",
       "  445,\n",
       "  2462,\n",
       "  29889,\n",
       "  13,\n",
       "  13,\n",
       "  2277,\n",
       "  29937,\n",
       "  2796],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the trainable parameters within the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model, use_4bit=False):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        # if using DS Zero 3 and the weights are initialized empty\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "    if use_4bit:\n",
    "        trainable_params /= 2\n",
    "    print(\n",
    "        f\"all params: {all_param:,d} || trainable params: {trainable_params:,d} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, tokenizer, dataset, output_dir):\n",
    "    # Apply preprocessing to the model to prepare it by\n",
    "    # 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "    model.gradient_checkpointing_enable()\n",
    "    print('gradient checkpointing check: ', model.gradient_checkpointing_enable())\n",
    "\n",
    "    # 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
    "    print('check model before kbit: ', model)\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    print('check model after kbit: ', model)\n",
    "\n",
    "    # Get lora module names\n",
    "    modules = find_all_linear_names(model)\n",
    "    print('check target modules: ', modules)\n",
    "\n",
    "    # Create PEFT config for these modules and wrap the model to PEFT\n",
    "    peft_config = create_peft_config(modules)\n",
    "    print('check peft config: ', peft_config)\n",
    "\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    print('check model after peft', model)\n",
    "    \n",
    "    # Print information about the percentage of trainable parameters\n",
    "    print_trainable_parameters(model)\n",
    "    \n",
    "    # Training parameters\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        args=TrainingArguments(\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=2,\n",
    "            max_steps=50,\n",
    "            learning_rate=2e-4,\n",
    "            fp16=True,\n",
    "            logging_steps=1,\n",
    "            output_dir=\"outputs\",\n",
    "            optim=\"paged_adamw_8bit\",\n",
    "        ),\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    )\n",
    "    \n",
    "    model.config.use_cache = False \n",
    "    \n",
    "    dtypes = {}\n",
    "    for _, p in model.named_parameters():\n",
    "        dtype = p.dtype\n",
    "        if dtype not in dtypes: dtypes[dtype] = 0\n",
    "        dtypes[dtype] += p.numel()\n",
    "    total = 0\n",
    "    for k, v in dtypes.items(): total+= v\n",
    "    for k, v in dtypes.items():\n",
    "        print(k, v, v/total)\n",
    "    print('dtypes:', dtypes)\n",
    "\n",
    "    do_train = True\n",
    "    \n",
    "    # Launch training\n",
    "    print(\"Training...\")\n",
    "    \n",
    "    if do_train:\n",
    "        train_result = trainer.train()\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        print(metrics)    \n",
    "    \n",
    "    ###\n",
    "    \n",
    "    # Saving model\n",
    "    print(\"Saving last checkpoint of the model...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "    \n",
    "    # Free memory for merging weights\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"results/llama2/final_checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient checkpointing check:  None\n",
      "check model before kbit:  LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 8192)\n",
      "    (layers): ModuleList(\n",
      "      (0-79): 80 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=8192, out_features=28672, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=8192, out_features=28672, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=28672, out_features=8192, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=8192, out_features=32000, bias=False)\n",
      ")\n",
      "check model after kbit:  LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 8192)\n",
      "    (layers): ModuleList(\n",
      "      (0-79): 80 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=8192, out_features=28672, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=8192, out_features=28672, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=28672, out_features=8192, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=8192, out_features=32000, bias=False)\n",
      ")\n",
      "check target modules:  ['q_proj', 'k_proj', 'v_proj', 'down_proj', 'gate_proj', 'o_proj', 'up_proj']\n",
      "check peft config:  LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=16, target_modules=['q_proj', 'k_proj', 'v_proj', 'down_proj', 'gate_proj', 'o_proj', 'up_proj'], lora_alpha=64, lora_dropout=0.1, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)\n",
      "check model after peft PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 8192)\n",
      "        (layers): ModuleList(\n",
      "          (0-79): 80 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=8192, out_features=8192, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=8192, out_features=1024, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=8192, out_features=1024, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=8192, out_features=8192, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=8192, out_features=28672, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=28672, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=8192, out_features=28672, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=28672, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=28672, out_features=8192, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=28672, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=8192, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "all params: 34,958,221,312 || trainable params: 207,093,760 || trainable%: 0.592403595571127\n",
      "torch.float32 732700672 0.020959323572577994\n",
      "torch.uint8 34225520640 0.979040676427422\n",
      "dtypes: {torch.float32: 732700672, torch.uint8: 34225520640}\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 11:01, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.227100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.539100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.407900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.252000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.104500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.108200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.417900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.242100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.875300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.941300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.210300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.433100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.856200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.252400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.853200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.828200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.963800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.837400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.037100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.942800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.959600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.904100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.181900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.752900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.809000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.155700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.142400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.181300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.126200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.349100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.044900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.859800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.926300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.918500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.101400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.833500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.196600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.034900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.274000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.054500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.909100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.149200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.001300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       0.01\n",
      "  total_flos               =  9766870GF\n",
      "  train_loss               =     1.1332\n",
      "  train_runtime            = 0:11:15.70\n",
      "  train_samples_per_second =      0.296\n",
      "  train_steps_per_second   =      0.074\n",
      "{'train_runtime': 675.7061, 'train_samples_per_second': 0.296, 'train_steps_per_second': 0.074, 'total_flos': 1.0487097544015872e+16, 'train_loss': 1.1331523597240447, 'epoch': 0.01}\n",
      "Saving last checkpoint of the model...\n"
     ]
    }
   ],
   "source": [
    "train(model, tokenizer, dataset, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_NAME = 'meta-llama/Llama-2-70b-chat-hf'\n",
    "new_model = \"results/llama2/final_checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 15/15 [00:10<00:00,  1.44it/s]\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_NAME,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map = \"auto\",    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"fine-tuned-llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68976648192"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params_b = sum(p.numel() for p in model.parameters())\n",
    "total_params_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68976648192"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params_n = sum(p.numel() for p in base_model.parameters())\n",
    "total_params_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 15/15 [00:06<00:00,  2.22it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 11\u001b[0m\n\u001b[1;32m      2\u001b[0m MODEL \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmeta-llama/Llama-2-70b-chat-hf\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      4\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      5\u001b[0m     MODEL,\n\u001b[1;32m      6\u001b[0m     load_in_8bit\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     offload_folder\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./offload/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m model \u001b[39m=\u001b[39m PeftModel\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m     12\u001b[0m         model, \n\u001b[1;32m     13\u001b[0m         WEIGHTS, \n\u001b[1;32m     14\u001b[0m         torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat16,\n\u001b[1;32m     15\u001b[0m         device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     16\u001b[0m         offload_folder\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39moffload\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m     17\u001b[0m         \n\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     19\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mmerge_and_unload()\n\u001b[1;32m     20\u001b[0m model\u001b[39m.\u001b[39msave_pretrained(\u001b[39m\"\u001b[39m\u001b[39mllama_evo\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages/peft/peft_model.py:270\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(model, config, adapter_name)\n\u001b[1;32m    269\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     model \u001b[39m=\u001b[39m MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config\u001b[39m.\u001b[39;49mtask_type](model, config, adapter_name)\n\u001b[1;32m    271\u001b[0m model\u001b[39m.\u001b[39mload_adapter(model_id, adapter_name, is_trainable\u001b[39m=\u001b[39mis_trainable, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    272\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages/peft/peft_model.py:893\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, model, peft_config: PeftConfig, adapter_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 893\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(model, peft_config, adapter_name)\n\u001b[1;32m    894\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model_prepare_inputs_for_generation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model\u001b[39m.\u001b[39mprepare_inputs_for_generation\n",
      "File \u001b[0;32m~/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages/peft/peft_model.py:112\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(peft_config, PromptLearningConfig):\n\u001b[1;32m    111\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpeft_config[adapter_name] \u001b[39m=\u001b[39m peft_config\n\u001b[0;32m--> 112\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model \u001b[39m=\u001b[39m PEFT_TYPE_TO_MODEL_MAPPING[peft_config\u001b[39m.\u001b[39;49mpeft_type](\n\u001b[1;32m    113\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpeft_config, adapter_name\n\u001b[1;32m    114\u001b[0m     )\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[1;32m    116\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages/peft/tuners/lora.py:180\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[0;34m(self, model, config, adapter_name)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mforward\n\u001b[1;32m    179\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpeft_config \u001b[39m=\u001b[39m config\n\u001b[0;32m--> 180\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_adapter(adapter_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpeft_config[adapter_name])\n\u001b[1;32m    182\u001b[0m \u001b[39m# transformers models have a .config attribute, whose presence is assumed later on\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages/peft/tuners/lora.py:194\u001b[0m, in \u001b[0;36mLoraModel.add_adapter\u001b[0;34m(self, adapter_name, config)\u001b[0m\n\u001b[1;32m    192\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_lora_config(config, model_config)\n\u001b[1;32m    193\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpeft_config[adapter_name] \u001b[39m=\u001b[39m config\n\u001b[0;32m--> 194\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_find_and_replace(adapter_name)\n\u001b[1;32m    195\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpeft_config) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpeft_config[adapter_name]\u001b[39m.\u001b[39mbias \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    196\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLoraModel supports only 1 adapter with bias. When using multiple adapters, set bias to \u001b[39m\u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m\u001b[39m for all adapters.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m     )\n",
      "File \u001b[0;32m~/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages/peft/tuners/lora.py:352\u001b[0m, in \u001b[0;36mLoraModel._find_and_replace\u001b[0;34m(self, adapter_name)\u001b[0m\n\u001b[1;32m    344\u001b[0m         target\u001b[39m.\u001b[39mupdate_layer(\n\u001b[1;32m    345\u001b[0m             adapter_name,\n\u001b[1;32m    346\u001b[0m             lora_config\u001b[39m.\u001b[39mr,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    349\u001b[0m             lora_config\u001b[39m.\u001b[39minit_lora_weights,\n\u001b[1;32m    350\u001b[0m         )\n\u001b[1;32m    351\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 352\u001b[0m         new_module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_new_module(lora_config, adapter_name, target)\n\u001b[1;32m    353\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_replace_module(parent, target_name, new_module, target)\n\u001b[1;32m    355\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_target_modules_in_base_model:\n",
      "File \u001b[0;32m~/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages/peft/tuners/lora.py:309\u001b[0m, in \u001b[0;36mLoraModel._create_new_module\u001b[0;34m(self, lora_config, adapter_name, target)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    306\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTarget module \u001b[39m\u001b[39m{\u001b[39;00mtarget\u001b[39m}\u001b[39;00m\u001b[39m is not supported. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCurrently, only `torch.nn.Linear` and `Conv1D` are supported.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m         )\n\u001b[0;32m--> 309\u001b[0m     new_module \u001b[39m=\u001b[39m Linear(adapter_name, in_features, out_features, bias\u001b[39m=\u001b[39;49mbias, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    311\u001b[0m \u001b[39mreturn\u001b[39;00m new_module\n",
      "File \u001b[0;32m~/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages/peft/tuners/lora.py:774\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, adapter_name, in_features, out_features, r, lora_alpha, lora_dropout, fan_in_fan_out, is_target_conv_1d_layer, **kwargs)\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[39mif\u001b[39;00m fan_in_fan_out:\n\u001b[1;32m    772\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mT\n\u001b[0;32m--> 774\u001b[0m nn\u001b[39m.\u001b[39;49mLinear\u001b[39m.\u001b[39;49mreset_parameters(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    775\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_layer(adapter_name, r, lora_alpha, lora_dropout, init_lora_weights)\n\u001b[1;32m    776\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactive_adapter \u001b[39m=\u001b[39m adapter_name\n",
      "File \u001b[0;32m~/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages/torch/nn/modules/linear.py:107\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset_parameters\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[39m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[39m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[39m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     init\u001b[39m.\u001b[39;49mkaiming_uniform_(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, a\u001b[39m=\u001b[39;49mmath\u001b[39m.\u001b[39;49msqrt(\u001b[39m5\u001b[39;49m))\n\u001b[1;32m    108\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         fan_in, _ \u001b[39m=\u001b[39m init\u001b[39m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages/torch/nn/init.py:412\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    410\u001b[0m bound \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39m3.0\u001b[39m) \u001b[39m*\u001b[39m std  \u001b[39m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 412\u001b[0m     \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49muniform_(\u001b[39m-\u001b[39;49mbound, bound)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "WEIGHTS = \"./results/llama2/final_checkpoint/\"\n",
    "MODEL = 'meta-llama/Llama-2-70b-chat-hf'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"./offload/\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "        model, \n",
    "        WEIGHTS, \n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        offload_folder=\"offload\", \n",
    "        \n",
    "    )\n",
    "model = model.merge_and_unload()\n",
    "model.save_pretrained(\"llama_evo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 8192)\n",
       "    (layers): ModuleList(\n",
       "      (0-79): 80 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(\n",
       "            in_features=8192, out_features=8192, bias=False\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (k_proj): Linear4bit(\n",
       "            in_features=8192, out_features=1024, bias=False\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (v_proj): Linear4bit(\n",
       "            in_features=8192, out_features=1024, bias=False\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (o_proj): Linear4bit(\n",
       "            in_features=8192, out_features=8192, bias=False\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(\n",
       "            in_features=8192, out_features=28672, bias=False\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=28672, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (up_proj): Linear4bit(\n",
       "            in_features=8192, out_features=28672, bias=False\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=28672, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (down_proj): Linear4bit(\n",
       "            in_features=28672, out_features=8192, bias=False\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=28672, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=8192, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from accelerate import disk_offload\n",
    "disk_offload(model, offload_dir=\"offload/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 15/15 [00:28<00:00,  1.92s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "We need an `offload_dir` to dispatch this model according to this `device_map`, the following submodules need to be offloaded: base_model.model.model.layers.77, base_model.model.model.layers.78, base_model.model.model.layers.79, base_model.model.model.norm, base_model.model.lm_head.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m AutoPeftModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(output_dir, device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m, torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mbfloat16)\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mmerge_and_unload()\n\u001b[1;32m      4\u001b[0m output_merged_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mresults/llama2/final_merged_checkpoint\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages/peft/auto.py:103\u001b[0m, in \u001b[0;36m_BaseAutoPeftModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, adapter_name, is_trainable, config, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     98\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot infer the auto class from the config, please make sure that you are loading the correct model for your task type.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m     )\n\u001b[1;32m    101\u001b[0m base_model \u001b[39m=\u001b[39m target_class\u001b[39m.\u001b[39mfrom_pretrained(base_model_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_target_peft_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    104\u001b[0m     base_model,\n\u001b[1;32m    105\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m    106\u001b[0m     adapter_name\u001b[39m=\u001b[39;49madapter_name,\n\u001b[1;32m    107\u001b[0m     is_trainable\u001b[39m=\u001b[39;49mis_trainable,\n\u001b[1;32m    108\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    109\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    110\u001b[0m )\n",
      "File \u001b[0;32m~/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages/peft/peft_model.py:271\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     model \u001b[39m=\u001b[39m MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config\u001b[39m.\u001b[39mtask_type](model, config, adapter_name)\n\u001b[0;32m--> 271\u001b[0m model\u001b[39m.\u001b[39;49mload_adapter(model_id, adapter_name, is_trainable\u001b[39m=\u001b[39;49mis_trainable, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    272\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages/peft/peft_model.py:591\u001b[0m, in \u001b[0;36mPeftModel.load_adapter\u001b[0;34m(self, model_id, adapter_name, is_trainable, **kwargs)\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(device_map, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    588\u001b[0m     device_map \u001b[39m=\u001b[39m infer_auto_device_map(\n\u001b[1;32m    589\u001b[0m         \u001b[39mself\u001b[39m, max_memory\u001b[39m=\u001b[39mmax_memory, no_split_module_classes\u001b[39m=\u001b[39mno_split_module_classes\n\u001b[1;32m    590\u001b[0m     )\n\u001b[0;32m--> 591\u001b[0m dispatch_model(\n\u001b[1;32m    592\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    593\u001b[0m     device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m    594\u001b[0m     offload_dir\u001b[39m=\u001b[39;49moffload_dir,\n\u001b[1;32m    595\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdispatch_model_kwargs,\n\u001b[1;32m    596\u001b[0m )\n\u001b[1;32m    597\u001b[0m hook \u001b[39m=\u001b[39m AlignDevicesHook(io_same_device\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    598\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpeft_config[adapter_name], PromptLearningConfig):\n",
      "File \u001b[0;32m~/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages/accelerate/big_modeling.py:343\u001b[0m, in \u001b[0;36mdispatch_model\u001b[0;34m(model, device_map, main_device, state_dict, offload_dir, offload_index, offload_buffers, skip_keys, preload_module_classes)\u001b[0m\n\u001b[1;32m    341\u001b[0m disk_modules \u001b[39m=\u001b[39m [name \u001b[39mfor\u001b[39;00m name, device \u001b[39min\u001b[39;00m device_map\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m device \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdisk\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    342\u001b[0m \u001b[39mif\u001b[39;00m offload_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m offload_index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(disk_modules) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 343\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    344\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWe need an `offload_dir` to dispatch this model according to this `device_map`, the following submodules \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    345\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mneed to be offloaded: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(disk_modules)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m     )\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    348\u001b[0m     \u001b[39mlen\u001b[39m(disk_modules) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    349\u001b[0m     \u001b[39mand\u001b[39;00m offload_index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    350\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(offload_dir) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(offload_dir, \u001b[39m\"\u001b[39m\u001b[39mindex.json\u001b[39m\u001b[39m\"\u001b[39m)))\n\u001b[1;32m    351\u001b[0m ):\n\u001b[1;32m    352\u001b[0m     disk_state_dict \u001b[39m=\u001b[39m extract_submodules_state_dict(model\u001b[39m.\u001b[39mstate_dict(), disk_modules)\n",
      "\u001b[0;31mValueError\u001b[0m: We need an `offload_dir` to dispatch this model according to this `device_map`, the following submodules need to be offloaded: base_model.model.model.layers.77, base_model.model.model.layers.78, base_model.model.model.layers.79, base_model.model.model.norm, base_model.model.lm_head."
     ]
    }
   ],
   "source": [
    "model = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "output_merged_dir = \"results/llama2/final_merged_checkpoint\"\n",
    "os.makedirs(output_merged_dir, exist_ok=True)\n",
    "model.save_pretrained(output_merged_dir, safe_serialization=True)\n",
    "\n",
    "# save tokenizer for easy inference\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.save_pretrained(output_merged_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/LLM-as-a-Service/llama-fine-tuning/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_pipeline = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task='text-generation',\n",
    "    temperature=0.01,\n",
    "    repetition_penalty=1.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=llama_pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "falcon_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
