{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/LLM-as-a-Service/falcon_env/lib/python3.8/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.6.12) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, pipeline,BitsAndBytesConfig, LlamaForCausalLM, LlamaTokenizer, GenerationConfig,LlamaConfig,LlamaModel,AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter\n",
    "from langchain.document_loaders import PDFPlumberLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import os\n",
    "import streamlit as st\n",
    "from llama_index import Document, LangchainEmbedding, SimpleDirectoryReader, ListIndex,VectorStoreIndex, StorageContext, GPTVectorStoreIndex, LLMPredictor, ServiceContext, load_index_from_storage\n",
    "#from llama_index.vector_stores import DeepLakeVectorStore\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import base64\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import torch.cuda as cuda\n",
    "\n",
    "from langchain.agents.agent_toolkits import (VectorStoreInfo, create_vectorstore_agent, VectorStoreToolkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'meta-llama/Llama-2-70b-chat-hf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bnb configuration for quantization of the model in 4bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/ubuntu/LLM-as-a-Service/falcon_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/ubuntu/LLM-as-a-Service/falcon_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/LLM-as-a-Service/falcon_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/home/ubuntu/LLM-as-a-Service/falcon_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/home/ubuntu/LLM-as-a-Service/falcon_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/home/ubuntu/LLM-as-a-Service/falcon_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8279d61d6b694d528bf8a5184d726819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/ubuntu/LLM-as-a-Service/falcon_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/ubuntu/LLM-as-a-Service/falcon_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/LLM-as-a-Service/falcon_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/home/ubuntu/LLM-as-a-Service/falcon_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/home/ubuntu/LLM-as-a-Service/falcon_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/home/ubuntu/LLM-as-a-Service/falcon_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9647573fde433997d7e94292d466c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 8192, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-79): 80 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=8192, out_features=28672, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=8192, out_features=28672, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=28672, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=8192, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = LlamaConfig()\n",
    "model = LlamaModel(configuration)\n",
    "configuration = model.config\n",
    "configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"max_length\": 4096,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"temperature\": 0.9,\n",
       "  \"top_p\": 0.6,\n",
       "  \"transformers_version\": \"4.32.0.dev0\"\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "generation_config = GenerationConfig.from_pretrained(model_id)\n",
    "generation_config   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = model.eval()\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the difference in architacture between the gpt model and the llama large language model?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min, sys: 1min 7s, total: 3min 8s\n",
      "Wall time: 3min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **encoding,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0,\n",
    "        generation_config=generation_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,  1724,   338,   278,  4328,   297,  3190,   277,   627,   545,\n",
       "          1546,   278,   330,   415,  1904,   322,   278, 11148,  3304,  2919,\n",
       "          4086,  1904, 29973,    13,    13, 22550, 29901,   450, 11258,   310,\n",
       "           278,   402,  7982,  1904,   322,   278,   365,  5661,  1529,  2919,\n",
       "          4086,  1904,   526,  2788,   297,  1784,  5837, 29892,   408,   896,\n",
       "           526,  1716,  2729,   373,  4327,   261, 29899,  6707,  6956,  1973,\n",
       "         29889,  2398, 29892,   727,   526,   777,  1820, 12651,  1546,   278,\n",
       "          1023,  4733, 29889,    13,    13, 29896, 29889,  8125, 21179, 29901,\n",
       "           450,  1556,  6924,  4328,   338,   278,  2159,   310,   278,  4733,\n",
       "         29889,   402,  7982,   338,   263,  1568,  7968,  1904,  1135,   365,\n",
       "          5661,  1529, 29892,   411, 14235, 29871, 29955, 29900,  7284,  4128,\n",
       "          9401,   304,   365,  5661,  1529, 29915, 29879, 29871, 29896, 29889,\n",
       "         29945, 24464,  4128, 29889,    13, 29906, 29889, 26101,  4669,   573,\n",
       "         29901,   402,  7982,   338, 16370,   411,   263,  4086,  1904,   292,\n",
       "         12091, 29892,   988,   278,  7306,   338,   304,  8500,   278,  2446,\n",
       "          1734,   297,   263,  5665,   310,  1426,  2183,   278,  3030,   310,\n",
       "           278,  3517,  3838, 29889,   365,  5661,  1529, 29892,   373,   278,\n",
       "           916,  1361, 29892,   338, 16370,   411,   263, 10296,   310,   263,\n",
       "          4086,  1904,   292, 12091,   322,   263,  2446, 10541, 18988, 12091,\n",
       "         29889,   910,  2794,   393,   365,  5661,  1529,   338, 16370,   304,\n",
       "          8500,   451,   871,   278,  2446,  1734,   297,   263,  5665,   541,\n",
       "           884,   278,  2446, 10541,   297,   263,  5665,   310,  1426, 29889,\n",
       "            13, 29941, 29889, 26101,  3630, 29901,   402,  7982,   338, 16370,\n",
       "           373,   263,  8783,   310,  1426,   515,   278,  8986, 29892,  1550,\n",
       "           365,  5661,  1529,   338, 16370,   373,   263,  8783,   310,  1426,\n",
       "           515,   278,  8986,   322,  8277, 29889,   910,  4328,   297,  6694,\n",
       "           848,  2794,   393,   365,  5661,  1529,   338, 19884,   304,   263,\n",
       "         25734,  3464,   310, 26442,   322, 11949, 29892,   607,   508]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the difference in architacture between the gpt model and the llama large language model?\n",
      "\n",
      "Answer: The architecture of the GPT model and the LLaMA large language model are similar in many ways, as they are both based on transformer-based architectures. However, there are some key differences between the two models.\n",
      "\n",
      "1. Model Size: The most obvious difference is the size of the models. GPT is a much smaller model than LLaMA, with approximately 70 million parameters compared to LLaMA's 1.5 billion parameters.\n",
      "2. Training Objective: GPT is trained with a language modeling objective, where the goal is to predict the next word in a sequence of text given the context of the previous words. LLaMA, on the other hand, is trained with a combination of a language modeling objective and a next sentence prediction objective. This means that LLaMA is trained to predict not only the next word in a sequence but also the next sentence in a sequence of text.\n",
      "3. Training Data: GPT is trained on a dataset of text from the internet, while LLaMA is trained on a dataset of text from the internet and books. This difference in training data means that LLaMA is exposed to a wider range of texts and styles, which can\n"
     ]
    }
   ],
   "source": [
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open source LLMs have several advantages over ChatGPT, including:\n",
      "\n",
      "Pros of Open Source LLMs:\n",
      "\n",
      "1. Customizability: Open source LLMs can be customized and fine-tuned to suit specific use cases, allowing for greater flexibility and adaptability.\n",
      "2. Cost-effectiveness: Open source LLMs are free and do not require expensive subscriptions or licensing fees, making them a cost-effective option for individuals and organizations.\n",
      "3. Community-driven development: Open source LLMs are often developed and maintained by a community of developers, researchers, and users, which can lead to faster bug fixes, feature updates, and improvements.\n",
      "4. Transparency: Open source LLMs provide a transparent view of the model's architecture, parameters, and training data, allowing for greater understanding and trust in the model's decision-making process.\n",
      "5. Privacy: Open source LLMs can be trained on private data, ensuring that sensitive information remains confidential and is not shared with third-party services.\n",
      "\n",
      "However, open source LLMs also have some disadvantages, such as:\n"
     ]
    }
   ],
   "source": [
    "answer_tokens = outputs[:, encoding.input_ids.shape[1] :]\n",
    "output_text = tokenizer.decode(answer_tokens[0], skip_special_tokens=True).strip()\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"what are the danger of using AI?\"\n",
    "template=f'''SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "USER: {prompt}\n",
    "ASSISTANT:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15    \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "USER: what are the danger of using AI?\n",
      "ASSISTANT:\n",
      "There are several potential dangers associated with the use of artificial intelligence (AI). Some of these include:\n",
      "\n",
      "1. Bias and discrimination: AI systems can perpetuate existing biases if they are trained on biased data or designed with a particular worldview. This can lead to unfair outcomes and discrimination against certain groups of people.\n",
      "2. Job displacement: The increasing use of AI could potentially displace human workers, especially in industries where tasks are repetitive or can be easily automated. This could lead to job losses and economic instability for some individuals and communities.\n",
      "3. Privacy concerns: AI systems often rely on collecting and processing large amounts of personal data, which raises concerns about privacy and data protection. This is particularly true when it comes to sensitive information such as health records or financial data.\n",
      "4. Security risks: AI systems can be vulnerable to cyber attacks and hacking, which could have serious consequences if they are used in critical infrastructure or for sensitive applications.\n",
      "5. Autonomous weapons: There is a risk that AI could be used to create autonomous weapons, which could make decisions about who to target and kill without human oversight. This raises significant ethical concerns.\n",
      "6. Unintended consequences: AI systems can have unintended consequences, especially if they are not carefully designed and tested. For example, an AI system designed to optimize traffic flow might inadvertently create new traffic jams or accidents.\n",
      "7. Lack of transparency: It can be difficult to understand how AI systems arrive at their decisions, which raises questions about accountability and trustworthiness. This lack of transparency can also make it harder to identify and address bias or errors in the decision-making process.\n",
      "8. Dependence on AI: Over-reliance on AI can lead to a loss of human skills and abilities, particularly in areas where AI is used to augment human capabilities.\n",
      "\n",
      "It's important to note that these dangers are not inevitable and can be mitigated through careful design, implementation, and regulation of AI systems. Additionally, AI has the potential to bring many benefits, such as improved efficiency, accuracy, and decision-making, and it's important to weigh the potential risks against the potential benefits.\n"
     ]
    }
   ],
   "source": [
    "print(llama_pipeline(template)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using HuggingFace Pipeline and LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = HuggingFacePipeline(pipeline=llama_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Who won the FIFA World Cup in the year 1994? \"\n",
    "\n",
    "template = '''SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "USER: {prompt}\n",
    "ASSISTANT:\n",
    "'''\n",
    "\n",
    "prompt_template = PromptTemplate(template=template, input_variables=[\"prompt\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "falcon_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
