{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fdaf82b-e14d-4fde-9576-6f23692a8cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: minio in /opt/conda/lib/python3.8/site-packages (6.0.2)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.8/site-packages (from minio) (1.26.15)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.8/site-packages (from minio) (2023.2)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.8/site-packages (from minio) (2021.5.30)\n",
      "Requirement already satisfied: configparser in /opt/conda/lib/python3.8/site-packages (from minio) (5.3.0)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.8/site-packages (from minio) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil->minio) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab9924a3-f71f-426a-90ad-65d865a733bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/3fe43db6-707f-4cc4-b745-878245dfcfa4\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/e08025f2-0340-40a2-b73c-b3123cab2849\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp import components\n",
    "\n",
    "\"\"\"\n",
    "This code demonstrates an example of two Kubeflow components sharing a path in a volume. In this example, the first component, \n",
    "`write_to_volume`, writes a simple message into a file named `file.txt` located in the `/mnt` path of a persistent volume created by `create-pvc`. \n",
    "The second component, `read_from_volume`, reads the content of the `file.txt` file from the same path and prints it.\n",
    "\n",
    "The `dsl.VolumeOp` class creates a persistent volume claim (PVC) named `my-pvc` using `modes=dsl.VOLUME_MODE_RWM` which allows for both read and write operations to the volume.\n",
    "The `write_to_volume` component uses `add_pvolumes` to specify the `/mnt` path on the volume created by `create-pvc`.\n",
    "Then `read_from_volume` component uses `add_pvolumes` to mount the same volume at `/mnt` and reads the content of the `file.txt` file written by the previous component.\n",
    "\n",
    "Finally, the pipeline is compiled using `kfp.compiler.Compiler().compile()` and submitted for execution to Kubeflow using the `Client().run_pipeline()` method. \n",
    "This code can serve as a starting point for users who want to create Kubeflow components that share a path in a volume.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Create component\n",
    "@components.create_component_from_func\n",
    "def write_to_volume():\n",
    "    with open(\"/mnt/file.txt\", \"w\") as file:\n",
    "        file.write(\"Hello world 5\")\n",
    "\n",
    "\n",
    "# Create component\n",
    "@components.create_component_from_func\n",
    "def read_from_volume():\n",
    "    with open(\"/mnt/file.txt\", \"r\") as file:\n",
    "        contents = file.read()\n",
    "        print(f\"File contents would be  : {contents}\")\n",
    "\n",
    "\n",
    "# Define pipeline\n",
    "@dsl.pipeline(name=\"volumeop-basic\", description=\"A Basic Example on VolumeOp Usage.\")\n",
    "def volumeop_basic(size: str = \"1Gi\"):\n",
    "    vop = dsl.VolumeOp(\n",
    "        name=\"create-pvc\", resource_name=\"my-pvc\", modes=dsl.VOLUME_MODE_RWM, size=size\n",
    "    )\n",
    "\n",
    "    write_op = write_to_volume().add_pvolumes({\"/mnt\": vop.volume})\n",
    "    read_op = read_from_volume().add_pvolumes({\"/mnt\": write_op.pvolume})\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Compile the pipeline\n",
    "    pipeline_func = volumeop_basic\n",
    "    pipeline_filename = pipeline_func.__name__ + \".yaml\"\n",
    "    kfp.compiler.Compiler().compile(pipeline_func, pipeline_filename)\n",
    "\n",
    "    # Connect to the Kubeflow Pipeline and submit the pipeline for execution\n",
    "    client = kfp.Client()\n",
    "    experiment_name = \"VolumeOp Basic Pipeline\"\n",
    "    experiment = client.create_experiment(experiment_name)\n",
    "    run_name = \"run\"\n",
    "    run = client.run_pipeline(experiment.id, run_name, pipeline_filename, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "142efa94-4d63-4387-87d7-57ce978140f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Upload data into minio Bucket\n",
    "\n",
    "# import os\n",
    "# from minio import Minio\n",
    "\n",
    "\n",
    "# def upload_tar_to_minio(minio_ip,bucket_name, access_key, secret_key, file_path, object_name):\n",
    "#     # Connect to MinIO server\n",
    "#     minio_client = Minio(\n",
    "#         minio_ip,\n",
    "#         access_key=access_key,\n",
    "#         secret_key=secret_key,\n",
    "#         secure=False  # Change to True if using HTTPS\n",
    "#     )\n",
    "\n",
    "#     # Check if the bucket already exists, create it if not\n",
    "#     if not minio_client.bucket_exists(bucket_name):\n",
    "#         minio_client.make_bucket(bucket_name)\n",
    "\n",
    "#     # Upload the .tar file to the bucket\n",
    "#     minio_client.fput_object(bucket_name, object_name, file_path)\n",
    "\n",
    "#     print(f\"File {object_name} uploaded successfully to bucket {bucket_name}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# bucket_name = \"voc2012-bucket\"\n",
    "# access_key = \"Eschercloud_CV_demo\"\n",
    "# secret_key = \"urF2FbUWv85dYtjB\"\n",
    "# file_path = \"/home/jovyan/VOCtrainval_11-May-2012.tar\"\n",
    "# object_name = \"VOCtrainval_11-May-2012.tar\"\n",
    "# minio_ip = \"185.47.227.207:9000\"\n",
    "\n",
    "# upload_tar_to_minio(minio_ip,bucket_name, access_key, secret_key, file_path, object_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d0b077a-1b59-4179-8482-6b2b26128c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.components import func_to_container_op\n",
    "from typing import NamedTuple\n",
    "\n",
    "def download_data_from_minio(ip_address: str, port: int, access_key: str, secret_key: str, bucket_name: str, object_name: str):\n",
    "\n",
    "    import sys\n",
    "    import tarfile\n",
    "    from minio import Minio\n",
    "    import os\n",
    "\n",
    "\n",
    "    client = Minio(\n",
    "        f\"{ip_address}:{port}\",\n",
    "        access_key=access_key,\n",
    "        secret_key=secret_key,\n",
    "        secure=False\n",
    "    )\n",
    "    found = client.bucket_exists(bucket_name)\n",
    "    if found:\n",
    "        print(\"Found the bucket\")\n",
    "        #Download data from MinIO bucket\n",
    "        client.fget_object(\n",
    "            bucket_name,\n",
    "            object_name,\n",
    "            object_name\n",
    "        )\n",
    "        print(f\"Downloaded {object_name} from {bucket_name} bucket\")\n",
    "        # create a TarFile object for the specified file\n",
    "        tar = tarfile.open(object_name)\n",
    "\n",
    "        # extract all files in the tar file to the current working directory\n",
    "        tar.extractall(\"/mnt/\")\n",
    "        directory_path = '/mnt/'\n",
    "\n",
    "        # Use os.listdir() to get a list of all files and directories in the directory\n",
    "        contents = os.listdir(directory_path)\n",
    "        for root, dirs, files in os.walk(directory_path):\n",
    "            for file in files:\n",
    "                print(os.path.join(root, file))\n",
    "\n",
    "        # Print the list of contents\n",
    "        print(contents)\n",
    "\n",
    "        # close the TarFile object\n",
    "        tar.close()\n",
    "    else:\n",
    "        print(f\"{bucket_name} bucket does not exist.\")\n",
    "        sys.exit(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad37f76-919e-477b-97d9-a720f0eb4230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bef891fb-9fee-414c-a06a-7ae282b9bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "\n",
    "import os\n",
    "\n",
    "def upload_zip_to_minio(zip_file_path, bucket_name, minio_ip, minio_access_key, minio_secret_key):\n",
    "   # Create a client with the MinIO server endpoint, access key, and secret key\n",
    "    client = Minio(minio_ip,\n",
    "                   access_key=minio_access_key,\n",
    "                   secret_key=minio_secret_key,\n",
    "                   secure=False)\n",
    "    \n",
    "    #Check if the bucket already exists\n",
    "    bucket_exists = client.bucket_exists(bucket_name)\n",
    "    if not bucket_exists:\n",
    "        client.make_bucket(bucket_name)\n",
    "    #Upload the local zip file to the bucket\n",
    "        \n",
    "    object_name = os.path.basename(zip_file_path)\n",
    "    client.fput_object(bucket_name, object_name, zip_file_path)\n",
    "    print(f\"File {zip_file_path} uploaded to MinIO bucket {bucket_name} with object name {object_name}\")\n",
    "\n",
    "zip_file_path = '/Users/amin/Downloads/VOCtrainval_11-May-2012.tar'\n",
    "bucket_name = 'voc2012-bucket'\n",
    "minio_ip = '185.47.227.233:9000'\n",
    "minio_access_key = 'Eschercloud_CV_demo'\n",
    "minio_secret_key = 'urF2FbUWv85dYtjB'\n",
    "#upload_zip_to_minio(zip_file_path, bucket_name, minio_ip, minio_access_key, minio_secret_key)\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.components import func_to_container_op\n",
    "from kfp.components import InputPath, OutputPath\n",
    "\n",
    "#Define the component for our training function\n",
    "def train_op(data_path: InputPath(str)):\n",
    "        \n",
    "    from PIL import Image\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset\n",
    "    from torchvision.transforms import ToTensor\n",
    "    import os\n",
    "    class VOC2012SegmentationDataset(Dataset):\n",
    "        def __init__(self, root_dir, split='train', transform=None, resize_shape=(375, 500)):\n",
    "            self.root_dir = root_dir\n",
    "            self.split = split\n",
    "            self.transform = transform\n",
    "            self.resize_shape = resize_shape\n",
    "            self.to_tensor = ToTensor()\n",
    "\n",
    "            # read list of image IDs\n",
    "            with open(os.path.join(root_dir, 'ImageSets', 'Segmentation', f'{split}.txt'), 'r') as f:\n",
    "                self.image_ids = [line.strip() for line in f.readlines()]\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.image_ids)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            # load image and label\n",
    "            image_path = os.path.join(self.root_dir, 'JPEGImages', f'{self.image_ids[idx]}.jpg')\n",
    "            label_path = os.path.join(self.root_dir, 'SegmentationClass', f'{self.image_ids[idx]}.png')\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            label = Image.open(label_path).convert('L')\n",
    "\n",
    "            # resize image and label\n",
    "            image = image.resize(self.resize_shape)\n",
    "            label = label.resize(self.resize_shape)\n",
    "\n",
    "            # apply transform if specified\n",
    "            if self.transform is not None:\n",
    "                image, label = self.transform(image, label)\n",
    "\n",
    "            # convert to PyTorch tensor\n",
    "            image = self.to_tensor(image)\n",
    "            label = self.to_tensor(label)\n",
    "\n",
    "            return image, label\n",
    "    \n",
    "    \"\"\" Parts of the U-Net model \"\"\"\n",
    "\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "\n",
    "    class DoubleConv(nn.Module):\n",
    "        \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "        def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "            super().__init__()\n",
    "            if not mid_channels:\n",
    "                mid_channels = out_channels\n",
    "            self.double_conv = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(mid_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.double_conv(x)\n",
    "\n",
    "\n",
    "    class Down(nn.Module):\n",
    "        \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "        def __init__(self, in_channels, out_channels):\n",
    "            super().__init__()\n",
    "            self.maxpool_conv = nn.Sequential(\n",
    "                nn.MaxPool2d(2),\n",
    "                DoubleConv(in_channels, out_channels)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "    class Up(nn.Module):\n",
    "        \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "        def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "            super().__init__()\n",
    "\n",
    "            # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "            if bilinear:\n",
    "                self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "                self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "            else:\n",
    "                self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "                self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "        def forward(self, x1, x2):\n",
    "            x1 = self.up(x1)\n",
    "            # input is CHW\n",
    "            diffY = x2.size()[2] - x1.size()[2]\n",
    "            diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "            x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                            diffY // 2, diffY - diffY // 2])\n",
    "            # if you have padding issues, see\n",
    "            # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "            # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "            x = torch.cat([x2, x1], dim=1)\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "    class OutConv(nn.Module):\n",
    "        def __init__(self, in_channels, out_channels):\n",
    "            super(OutConv, self).__init__()\n",
    "            self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "    \"\"\" Full assembly of the parts to form the complete network \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    class UNet(nn.Module):\n",
    "        def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "            super(UNet, self).__init__()\n",
    "            self.n_channels = n_channels\n",
    "            self.n_classes = n_classes\n",
    "            self.bilinear = bilinear\n",
    "\n",
    "            self.inc = (DoubleConv(n_channels, 64))\n",
    "            self.down1 = (Down(64, 128))\n",
    "            self.down2 = (Down(128, 256))\n",
    "            self.down3 = (Down(256, 512))\n",
    "            factor = 2 if bilinear else 1\n",
    "            self.down4 = (Down(512, 1024 // factor))\n",
    "            self.up1 = (Up(1024, 512 // factor, bilinear))\n",
    "            self.up2 = (Up(512, 256 // factor, bilinear))\n",
    "            self.up3 = (Up(256, 128 // factor, bilinear))\n",
    "            self.up4 = (Up(128, 64, bilinear))\n",
    "            self.outc = (OutConv(64, n_classes))\n",
    "\n",
    "        def forward(self, x):\n",
    "            x1 = self.inc(x)\n",
    "            x2 = self.down1(x1)\n",
    "            x3 = self.down2(x2)\n",
    "            x4 = self.down3(x3)\n",
    "            x5 = self.down4(x4)\n",
    "            x = self.up1(x5, x4)\n",
    "            x = self.up2(x, x3)\n",
    "            x = self.up3(x, x2)\n",
    "            x = self.up4(x, x1)\n",
    "            logits = self.outc(x)\n",
    "            return logits\n",
    "\n",
    "        def use_checkpointing(self):\n",
    "            self.inc = torch.utils.checkpoint(self.inc)\n",
    "            self.down1 = torch.utils.checkpoint(self.down1)\n",
    "            self.down2 = torch.utils.checkpoint(self.down2)\n",
    "            self.down3 = torch.utils.checkpoint(self.down3)\n",
    "            self.down4 = torch.utils.checkpoint(self.down4)\n",
    "            self.up1 = torch.utils.checkpoint(self.up1)\n",
    "            self.up2 = torch.utils.checkpoint(self.up2)\n",
    "            self.up3 = torch.utils.checkpoint(self.up3)\n",
    "            self.up4 = torch.utils.checkpoint(self.up4)\n",
    "            self.outc = torch.utils.checkpoint(self.outc)\n",
    "\n",
    "    def train(model, device, train_loader, optimizer, epoch):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = nn.CrossEntropyLoss()(output, target.squeeze(dim=1).long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 10 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader\n",
    "        # set up device (GPU if available)\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "        # set up data loaders\n",
    "    data_path = '/mnt/VOCdevkit/VOC2012/'\n",
    "    train_dataset = VOC2012SegmentationDataset(root_dir=data_path, split='train')\n",
    "    val_dataset = VOC2012SegmentationDataset(root_dir=data_path, split='val')\n",
    "    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    # set up model and optimizer\n",
    "    model = UNet(n_channels=3, n_classes=21).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # train and test loop\n",
    "    for epoch in range(1, 11):\n",
    "        train(model, device, train_loader, optimizer, epoch)\n",
    "\n",
    "    # save model\n",
    "    #torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21c5f4b7-07ec-41f4-ae30-26358730ecef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/c8c4df31-65f0-443e-b175-c1a418a9d4d8\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/b33024a5-d9fb-4ae2-855a-d436b6d5204c\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp.components import create_component_from_func\n",
    "from kfp import dsl\n",
    "from typing import List\n",
    "from kubernetes import client as k8s_client\n",
    "\n",
    "download_data_from_minio_op = create_component_from_func(download_data_from_minio, base_image='python:3.8', packages_to_install=['minio'])\n",
    "train_model_op = create_component_from_func(train_op,base_image='pytorch/pytorch:latest', packages_to_install=['torchvision', 'pillow'])\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='VOC2012 Preprocessing Pipeline test',\n",
    "    description='A pipeline that preprocesses VOC2012 dataset'\n",
    ")\n",
    "def voc2012_training_pipeline_test(\n",
    "    ip_address: str,\n",
    "    port: int,\n",
    "    access_key: str,\n",
    "    secret_key: str,\n",
    "    bucket_name: str,\n",
    "    object_name: str,\n",
    "    local_path: str,\n",
    "    root_dir: str\n",
    ") -> List[str]:\n",
    "    \n",
    "    vop = dsl.VolumeOp(\n",
    "        name=\"create-pvc\", resource_name=\"my-pvc\", modes=dsl.VOLUME_MODE_RWM, size=\"5Gi\"\n",
    "    )\n",
    "    \n",
    "    # Define the pipeline\n",
    "    download_task = download_data_from_minio_op(ip_address, port, access_key, secret_key, bucket_name, object_name).set_display_name('Download Data from Minio').add_pvolumes({\"/mnt\": vop.volume})\n",
    "    train_model_task = train_model_op(root_dir).after(download_task).set_display_name('Training model').add_pvolumes({\"/mnt\": download_task.pvolume})\n",
    "\n",
    "    return None\n",
    "\n",
    "# Compile the pipeline\n",
    "pipeline_func = voc2012_training_pipeline_test\n",
    "pipeline_filename = pipeline_func.__name__ + '.yaml'\n",
    "kfp.compiler.Compiler().compile(pipeline_func, pipeline_filename)\n",
    "\n",
    "# Connect to the Kubeflow Pipeline and submit the pipeline for execution\n",
    "client = kfp.Client()\n",
    "experiment_name = 'Training pipeline data Pipeline2'\n",
    "experiment = client.create_experiment(experiment_name)\n",
    "run_name = 'run2'\n",
    "\n",
    "arguments = {\n",
    "    'ip_address': \"185.47.227.207\",\n",
    "    'port': '9000',\n",
    "    'access_key': \"Eschercloud_CV_demo\",\n",
    "    'secret_key': \"urF2FbUWv85dYtjB\",\n",
    "    'bucket_name': \"voc2012-bucket\",\n",
    "    'object_name': \"VOCtrainval_11-May-2012.tar\",\n",
    "    'local_path': \"dataset/VOCtrainval_11-May-2012.tar\",\n",
    "    'root_dir': '/mnt/VOCdevkit/VOC2012/'\n",
    "}\n",
    "\n",
    "run = client.run_pipeline(experiment.id, run_name, pipeline_filename, arguments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97bf0f9a-332b-45ad-8ca5-88423b080172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_experiments():\n",
    "    \"\"\"\n",
    "    Deletes all experiments in the Kubeflow Pipeline client.\n",
    "    \"\"\"\n",
    "    import kfp\n",
    "\n",
    "    # Connect to the Kubeflow Pipeline client\n",
    "    client = kfp.Client()\n",
    "\n",
    "    # Get a list of all experiments\n",
    "    experiments = client.list_experiments()\n",
    "\n",
    "    # Delete all runs in each experiment\n",
    "    if experiments.experiments:\n",
    "        for experiment in experiments.experiments:\n",
    "            print(experiment.id)\n",
    "            client.delete_experiment(experiment.id)\n",
    "\n",
    "    return \"Experiments deleted successfully.\"\n",
    "\n",
    "\n",
    "def delete_runs():\n",
    "    \"\"\"\n",
    "    Deletes all runs in the Kubeflow Pipeline client.\n",
    "    \"\"\"\n",
    "    import kfp\n",
    "\n",
    "    # Connect to the Kubeflow Pipeline client\n",
    "    client = kfp.Client()\n",
    "\n",
    "    # Get a list of all runs\n",
    "    runs = client.list_runs()\n",
    "\n",
    "    # Delete all runs\n",
    "    if runs.runs:\n",
    "        for run in runs.runs:\n",
    "            print(run.id)\n",
    "            kfp.Client().runs.delete_run(run.id)\n",
    "\n",
    "    return \"Runs deleted successfully.\"\n",
    "\n",
    "\n",
    "def delete_pipelines():\n",
    "    \"\"\"\n",
    "    Deletes all pipelines in the Kubeflow Pipeline client.\n",
    "    \"\"\"\n",
    "    import kfp\n",
    "\n",
    "    # Connect to the Kubeflow Pipeline client\n",
    "    client = kfp.Client()\n",
    "\n",
    "    # Get a list of all pipelines\n",
    "    pipelines = client.list_pipelines()\n",
    "\n",
    "    # Delete all pipelines\n",
    "    if pipelines.pipelines:\n",
    "        for pipeline in pipelines.pipelines:\n",
    "            print(pipeline.id)\n",
    "            kfp.Client().delete_pipeline(pipeline.id)\n",
    "\n",
    "    return \"Pipelines deleted successfully.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c128dcd5-1411-4ff7-bc0b-7d8472b1fbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19e3072e-4da8-426b-96ce-d8ef79ef80b8\n",
      "a108f375-1da4-4263-81f4-21ae58afc57a\n",
      "e08025f2-0340-40a2-b73c-b3123cab2849\n",
      "3dbbe6a0-c769-4587-aa6d-ca4fa629db97\n",
      "c8a89728-5697-4ee4-8c26-3ee2a3d8eb5a\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Pipelines deleted successfully.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delete_runs()\n",
    "delete_pipelines()\n",
    "#delete_experiments()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
